<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="noahsnail.com/"/>
  <updated>2017-04-27T12:58:59.000Z</updated>
  <id>noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>matplotlib的基本用法(一)——figure的使用</title>
    <link href="noahsnail.com/2017/04/27/2017-4-27-matplotlib%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94figure%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>noahsnail.com/2017/04/27/2017-4-27-matplotlib的基本用法(一)——figure的使用/</id>
    <published>2017-04-27T12:50:51.000Z</published>
    <updated>2017-04-27T12:58:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于matplotlib的一些基本用法。</p>
<ul>
<li>Demo 1</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 绘制普通图像</div><div class="line">x = np.linspace(-1, 1, 50)</div><div class="line">y = 2 * x + 1</div><div class="line">plt.plot(x, y)</div><div class="line">plt.show()</div><div class="line"></div><div class="line"># 绘制普通图像</div><div class="line">y = x**2</div><div class="line">plt.plot(x, y)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<ul>
<li>结果</li>
</ul>
<p><img src="http://ocs628urt.bkt.clouddn.com/m1.png" alt="y=2x+1"><br><img src="http://ocs628urt.bkt.clouddn.com/m2.png" alt="y=x^2"></p>
<ul>
<li>Demo 2</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"># figure的使用</div><div class="line">x = np.linspace(-1, 1, 50)</div><div class="line">y1 = 2 * x + 1</div><div class="line"></div><div class="line"># figure 1</div><div class="line">plt.figure()</div><div class="line">plt.plot(x, y1)</div><div class="line"></div><div class="line"></div><div class="line"># figure 2</div><div class="line">y2 = x**2</div><div class="line">plt.figure()</div><div class="line">plt.plot(x, y2)</div><div class="line"></div><div class="line"></div><div class="line"># figure 3，指定figure的编号并指定figure的大小, 指定线的颜色, 宽度和类型</div><div class="line">y2 = x**2</div><div class="line">plt.figure(num = 5, figsize = (4, 4))</div><div class="line">plt.plot(x, y1)</div><div class="line">plt.plot(x, y2, color = &apos;red&apos;, linewidth = 1.0, linestyle = &apos;--&apos;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<ul>
<li>结果</li>
</ul>
<p><img src="http://ocs628urt.bkt.clouddn.com/m3.png" alt="y=2x+1"><br><img src="http://ocs628urt.bkt.clouddn.com/m4.png" alt="y=x^2"></p>
]]></content>
    
    <summary type="html">
    
      matplotlib的基本用法(一)——figure的使用
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>pandas的基本用法(一)——数据定义及使用</title>
    <link href="noahsnail.com/2017/04/27/2017-4-27-pandas%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
    <id>noahsnail.com/2017/04/27/2017-4-27-pandas的基本用法(一)——数据定义及使用/</id>
    <published>2017-04-27T12:21:00.000Z</published>
    <updated>2017-04-27T12:47:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于pandas的一些基本用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Test 1</div><div class="line"># 定义序列, pandas中的数据形式通常是float32或float64</div><div class="line">s = pd.Series([1, 3, 5, np.nan, 44,  1])</div><div class="line">print s</div><div class="line">print s[0]</div><div class="line">print s[3]</div><div class="line"></div><div class="line"># Test 1 result</div><div class="line">0     1.0</div><div class="line">1     3.0</div><div class="line">2     5.0</div><div class="line">3     NaN</div><div class="line">4    44.0</div><div class="line">5     1.0</div><div class="line">dtype: float64</div><div class="line">1.0</div><div class="line">nan</div><div class="line"></div><div class="line"># Test 2</div><div class="line"># 定义日期列表</div><div class="line">dates = pd.date_range(&apos;20170101&apos;, periods = 6)</div><div class="line">print dates</div><div class="line">print dates[5]</div><div class="line"></div><div class="line"># Test 2 result</div><div class="line">DatetimeIndex([&apos;2017-01-01&apos;, &apos;2017-01-02&apos;, &apos;2017-01-03&apos;, &apos;2017-01-04&apos;,</div><div class="line">               &apos;2017-01-05&apos;, &apos;2017-01-06&apos;],</div><div class="line">              dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;)</div><div class="line">2017-01-06 00:00:00</div><div class="line"></div><div class="line"># Test 3</div><div class="line"># DataFrame类似于numpy的array, 行索引为dates, 列索引为[a, b, c, d]</div><div class="line">df = pd.DataFrame(np.random.randn(6, 4), index = dates, columns = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])</div><div class="line">print df</div><div class="line"></div><div class="line"># 不指定索引的DataFrame</div><div class="line">df = pd.DataFrame(np.arange(12).reshape(3, 4))</div><div class="line">print df</div><div class="line"></div><div class="line"># DataFrame的定义</div><div class="line">df = pd.DataFrame(&#123;&apos;A&apos;: 1., &apos;B&apos;: &apos;Foo&apos;, &apos;C&apos;: np.array([3] * 4)&#125;)</div><div class="line">print df</div><div class="line"></div><div class="line"># Test 3 result</div><div class="line">                   a         b         c         d</div><div class="line">2017-01-01  1.104994  1.328379  0.410358 -1.661059</div><div class="line">2017-01-02 -0.642727 -0.152576  1.126191 -0.005317</div><div class="line">2017-01-03 -0.179257  0.160972 -0.824172 -0.175027</div><div class="line">2017-01-04  0.838328 -0.500909  0.714592  1.144800</div><div class="line">2017-01-05  0.803691 -3.979186 -1.037603 -0.747943</div><div class="line">2017-01-06  1.217289 -0.074413  0.504138 -0.077507</div><div class="line"></div><div class="line">   0  1   2   3</div><div class="line">0  0  1   2   3</div><div class="line">1  4  5   6   7</div><div class="line">2  8  9  10  11</div><div class="line"></div><div class="line">     A    B  C</div><div class="line">0  1.0  Foo  3</div><div class="line">1  1.0  Foo  3</div><div class="line">2  1.0  Foo  3</div><div class="line">3  1.0  Foo  3</div><div class="line"></div><div class="line"># Test 4</div><div class="line"># 查看DataFrame的数据类型</div><div class="line">df.dtypes</div><div class="line"></div><div class="line"># 查看DataFrame的索引</div><div class="line">df.index</div><div class="line"></div><div class="line"># 查看DataFrame的列索引</div><div class="line">df.columns</div><div class="line"></div><div class="line"># 查看DataFrame的值</div><div class="line">df.values</div><div class="line"></div><div class="line"># 查看DataFrame的描述</div><div class="line">df.describe()</div><div class="line"></div><div class="line"># DataFrame的转置</div><div class="line">df.T</div><div class="line"></div><div class="line"># DataFrame的index排序</div><div class="line">df.sort_index(axis = 1)</div><div class="line"></div><div class="line"># DataFrame的index排序, 逆序</div><div class="line">df.sort_index(axis = 1, ascending = False)</div><div class="line"></div><div class="line"># DataFrame按值排序</div><div class="line">df.sort_values(by = &apos;C&apos;)</div><div class="line"></div><div class="line"># Test 4 result</div><div class="line">A    float64</div><div class="line">B     object</div><div class="line">C      int64</div><div class="line">dtype: object</div><div class="line"></div><div class="line">RangeIndex(start=0, stop=4, step=1)</div><div class="line"></div><div class="line">Index([u&apos;A&apos;, u&apos;B&apos;, u&apos;C&apos;], dtype=&apos;object&apos;)</div><div class="line"></div><div class="line">array([[1.0, &apos;Foo&apos;, 3],</div><div class="line">       [1.0, &apos;Foo&apos;, 3],</div><div class="line">       [1.0, &apos;Foo&apos;, 3],</div><div class="line">       [1.0, &apos;Foo&apos;, 3]], dtype=object)</div><div class="line"></div><div class="line">       A      C</div><div class="line">count  4.0    4.0</div><div class="line">mean   1.0    3.0</div><div class="line">std    0.0    0.0</div><div class="line">min    1.0    3.0</div><div class="line">25%    1.0    3.0</div><div class="line">50%    1.0    3.0</div><div class="line">75%    1.0    3.0</div><div class="line">max    1.0    3.0</div><div class="line"></div><div class="line">       0      1      2      3</div><div class="line">A      1      1      1      1</div><div class="line">B      Foo    Foo    Foo    Foo</div><div class="line">C      3      3      3      3</div><div class="line"></div><div class="line"></div><div class="line">       A      B      C</div><div class="line">0      1.0    3      Foo</div><div class="line">1      1.0    3      Foo</div><div class="line">2      1.0    3      Foo</div><div class="line">3      1.0    3      Foo</div><div class="line"></div><div class="line">       C      B      A</div><div class="line">0      Foo    3      1.0</div><div class="line">1      Foo    3      1.0</div><div class="line">2      Foo    3      1.0</div><div class="line">3      Foo    3      1.0</div><div class="line"></div><div class="line">       A      B      C</div><div class="line">0      1.0    3      Foo</div><div class="line">1      1.0    3      Foo</div><div class="line">2      1.0    3      Foo</div><div class="line">3      1.0    3      Foo</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      pandas的基本用法(一)——数据定义及使用
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>numpy的基本用法(六)——numpy array的copy</title>
    <link href="noahsnail.com/2017/04/26/2017-4-26-numpy%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AD)%E2%80%94%E2%80%94numpy%20array%E7%9A%84copy/"/>
    <id>noahsnail.com/2017/04/26/2017-4-26-numpy的基本用法(六)——numpy array的copy/</id>
    <published>2017-04-26T14:15:45.000Z</published>
    <updated>2017-04-26T14:18:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于numpy的一些基本运算的用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Test 1</div><div class="line">a = np.arange(4)</div><div class="line">print a</div><div class="line"></div><div class="line"># 直接赋值, a,b,c,d是同一个array</div><div class="line">b = a</div><div class="line">c = a</div><div class="line">d = b</div><div class="line">a[0] = 10</div><div class="line">print b is a</div><div class="line">print c is a</div><div class="line">print d is a</div><div class="line"></div><div class="line"># Test 1 result</div><div class="line">[0 1 2 3]</div><div class="line">True</div><div class="line">True</div><div class="line">True</div><div class="line"></div><div class="line"># Test 2</div><div class="line"># 深拷贝</div><div class="line">b = a.copy()</div><div class="line">b[0] = 12</div><div class="line">print b is a</div><div class="line">print a</div><div class="line">print b</div><div class="line"></div><div class="line"># Test 2 result</div><div class="line">False</div><div class="line">[10  1  2  3]</div><div class="line">[12  1  2  3]</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      numpy的基本用法(六)——numpy array的copy
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>numpy的基本用法(五)——numpy array分割</title>
    <link href="noahsnail.com/2017/04/26/2017-4-26-numpy%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%94)%E2%80%94%E2%80%94numpy%20array%E5%88%86%E5%89%B2/"/>
    <id>noahsnail.com/2017/04/26/2017-4-26-numpy的基本用法(五)——numpy array分割/</id>
    <published>2017-04-26T13:57:26.000Z</published>
    <updated>2017-04-26T14:08:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于numpy的一些基本运算的用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Test 1</div><div class="line">A = np.arange(12).reshape(3, 4)</div><div class="line">print A</div><div class="line"></div><div class="line"># 纵向分割, 分成两部分, 按列分割</div><div class="line">print np.split(A, 2, axis = 1)</div><div class="line"># 横向分割, 分成三部分, 按行分割</div><div class="line">print np.split(A, 3, axis = 0)</div><div class="line"></div><div class="line"># Test 1 result</div><div class="line">[[ 0  1  2  3]</div><div class="line"> [ 4  5  6  7]</div><div class="line"> [ 8  9 10 11]]</div><div class="line">[array([[0, 1],</div><div class="line">       [4, 5],</div><div class="line">       [8, 9]]), array([[ 2,  3],</div><div class="line">       [ 6,  7],</div><div class="line">       [10, 11]])]</div><div class="line">[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]</div><div class="line"></div><div class="line"># Test 2</div><div class="line"># 不均等分割</div><div class="line">print np.array_split(A, 3, axis = 1)</div><div class="line"></div><div class="line"># Test 2 result</div><div class="line">[array([[0, 1],</div><div class="line">       [4, 5],</div><div class="line">       [8, 9]]), array([[ 2],</div><div class="line">       [ 6],</div><div class="line">       [10]]), array([[ 3],</div><div class="line">       [ 7],</div><div class="line">       [11]])]</div><div class="line">In [5]:</div><div class="line"></div><div class="line"># Test 3</div><div class="line"># 垂直方向分割</div><div class="line">print np.vsplit(A, 3)</div><div class="line"># 水平方向分割</div><div class="line">print np.hsplit(A, 2)</div><div class="line"></div><div class="line"># Test 3 result</div><div class="line">[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]</div><div class="line">[array([[0, 1],</div><div class="line">       [4, 5],</div><div class="line">       [8, 9]]), array([[ 2,  3],</div><div class="line">       [ 6,  7],</div><div class="line">       [10, 11]])]</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      numpy的基本用法(五)——numpy array分割
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>numpy的基本用法(四)——numpy array合并</title>
    <link href="noahsnail.com/2017/04/26/2017-4-26-numpy%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%9B%9B)%E2%80%94%E2%80%94numpy%20array%E5%90%88%E5%B9%B6/"/>
    <id>noahsnail.com/2017/04/26/2017-4-26-numpy的基本用法(四)——numpy array合并/</id>
    <published>2017-04-26T12:46:28.000Z</published>
    <updated>2017-04-26T12:50:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于numpy的一些基本运算的用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Test 1</div><div class="line">A = np.array([1, 1, 1])</div><div class="line">B = np.array([2, 2, 2])</div><div class="line"># 合并array, 竖直方向</div><div class="line">C = np.vstack((A, B))</div><div class="line">print A.shape</div><div class="line">print C.shape</div><div class="line">print C</div><div class="line"></div><div class="line"># 合并array, 水平方向</div><div class="line">D = np.hstack((A, B))</div><div class="line">print A.shape</div><div class="line">print D.shape</div><div class="line">print D</div><div class="line"></div><div class="line"># Test 1 result</div><div class="line">(3,)</div><div class="line">(2, 3)</div><div class="line">[[1 1 1]</div><div class="line"> [2 2 2]]</div><div class="line">(3,)</div><div class="line">(6,)</div><div class="line">[1 1 1 2 2 2]</div><div class="line"></div><div class="line"># Test 2</div><div class="line">A = np.array([1, 1, 1])</div><div class="line"># 添加维度</div><div class="line"># 列方向上添加维度</div><div class="line">B = A[:, np.newaxis]</div><div class="line">print A</div><div class="line">print B</div><div class="line">print A.shape</div><div class="line">print B.shape</div><div class="line"># 行方向上添加维度</div><div class="line">C = A[np.newaxis, :]</div><div class="line">print A</div><div class="line">print C</div><div class="line">print A.shape</div><div class="line">print C.shape</div><div class="line"></div><div class="line"># Test 2 result</div><div class="line">[1 1 1]</div><div class="line">[[1]</div><div class="line"> [1]</div><div class="line"> [1]]</div><div class="line">(3,)</div><div class="line">(3, 1)</div><div class="line">[1 1 1]</div><div class="line">[[1 1 1]]</div><div class="line">(3,)</div><div class="line">(1, 3)</div><div class="line"></div><div class="line"># Test 3</div><div class="line">A = np.array([1, 1, 1])</div><div class="line">B = np.array([2, 2, 2])</div><div class="line"># A, B列方向添加维度</div><div class="line">A = A[:, np.newaxis]</div><div class="line">B = B[:, np.newaxis]</div><div class="line"># 合并多个array并指定合并的维度, 列方向上合并</div><div class="line">C = np.concatenate((A, B, B, A), axis = 0)</div><div class="line"># 合并多个array并指定合并的维度, 行方向上合并</div><div class="line">D = np.concatenate((A, B, B, A), axis = 1)</div><div class="line">print A</div><div class="line">print B</div><div class="line">print C</div><div class="line">print D</div><div class="line"></div><div class="line"># Test 3 result</div><div class="line">[[1]</div><div class="line"> [1]</div><div class="line"> [1]]</div><div class="line">[[2]</div><div class="line"> [2]</div><div class="line"> [2]]</div><div class="line">[[1]</div><div class="line"> [1]</div><div class="line"> [1]</div><div class="line"> [2]</div><div class="line"> [2]</div><div class="line"> [2]</div><div class="line"> [2]</div><div class="line"> [2]</div><div class="line"> [2]</div><div class="line"> [1]</div><div class="line"> [1]</div><div class="line"> [1]]</div><div class="line">[[1 2 2 1]</div><div class="line"> [1 2 2 1]</div><div class="line"> [1 2 2 1]]</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      numpy的基本用法(四)——numpy array合并
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>numpy的基本用法(三)——numpy的索引</title>
    <link href="noahsnail.com/2017/04/25/2017-4-25-numpy%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%89)%E2%80%94%E2%80%94numpy%E7%9A%84%E7%B4%A2%E5%BC%95/"/>
    <id>noahsnail.com/2017/04/25/2017-4-25-numpy的基本用法(三)——numpy的索引/</id>
    <published>2017-04-25T12:39:38.000Z</published>
    <updated>2017-04-25T13:04:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于numpy的一些基本运算的用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Test 1</div><div class="line"># 一维矩阵</div><div class="line">a = np.arange(3, 15)</div><div class="line">print a</div><div class="line"># 输出矩阵的第三个元素</div><div class="line">print a[2]</div><div class="line"></div><div class="line"># Test 1 result</div><div class="line">[ 3  4  5  6  7  8  9 10 11 12 13 14]</div><div class="line">5</div><div class="line"></div><div class="line"># Test 2</div><div class="line"># 二维矩阵</div><div class="line">a = np.arange(3, 15).reshape(3, 4)</div><div class="line">print a</div><div class="line"># 输出矩阵的第二行</div><div class="line">print a[1]</div><div class="line"># 输出矩阵的第一个元素</div><div class="line">print a[0][0]</div><div class="line"># 输出矩阵某个位置上的元素</div><div class="line">print a[2][1]</div><div class="line">print a[2, 1]</div><div class="line"># 输出矩阵第三行的所有数字</div><div class="line"># :代表整行或整列</div><div class="line">print a[2, :]</div><div class="line"># 输出矩阵第二行的前三个数，左开右闭</div><div class="line">print a[1, 0:3]</div><div class="line"></div><div class="line"># Test 2 result</div><div class="line">[[ 3  4  5  6]</div><div class="line"> [ 7  8  9 10]</div><div class="line"> [11 12 13 14]]</div><div class="line">[ 7  8  9 10]</div><div class="line">3</div><div class="line">12</div><div class="line">12</div><div class="line">[11 12 13 14]</div><div class="line">[7 8 9]</div><div class="line"></div><div class="line"># Test 3</div><div class="line"># 迭代矩阵的行</div><div class="line">for row in a:</div><div class="line">    print row</div><div class="line"></div><div class="line"># 迭代矩阵的列</div><div class="line">for column in a.T:</div><div class="line">    print column</div><div class="line"></div><div class="line"># Test 3 result</div><div class="line">[3 4 5 6]</div><div class="line">[ 7  8  9 10]</div><div class="line">[11 12 13 14]</div><div class="line"></div><div class="line">[ 3  7 11]</div><div class="line">[ 4  8 12]</div><div class="line">[ 5  9 13]</div><div class="line">[ 6 10 14]</div><div class="line"></div><div class="line"># Test 4</div><div class="line"># 矩阵展开</div><div class="line">print a.flatten()</div><div class="line"># 迭代矩阵的元素</div><div class="line">for item in a.flat:</div><div class="line">    print item</div><div class="line"></div><div class="line"># Test 4 result</div><div class="line">[ 3  4  5  6  7  8  9 10 11 12 13 14]</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      numpy的基本用法(三)——numpy的索引
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>numpy的基本用法(二)——基本运算</title>
    <link href="noahsnail.com/2017/04/24/2017-4-24-numpy%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%8C)%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/"/>
    <id>noahsnail.com/2017/04/24/2017-4-24-numpy的基本用法(二)——基本运算/</id>
    <published>2017-04-24T14:14:40.000Z</published>
    <updated>2017-04-24T14:38:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于numpy的一些基本运算的用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Test 1</div><div class="line">a = np.arange(2, 14).reshape(3, 4)</div><div class="line">print a</div><div class="line"></div><div class="line"># Test 1 result</div><div class="line">[[ 2  3  4  5]</div><div class="line"> [ 6  7  8  9]</div><div class="line"> [10 11 12 13]]</div><div class="line"></div><div class="line"># Test 2</div><div class="line"># 计算矩阵最小值的索引</div><div class="line">print np.argmin(a)</div><div class="line"># 计算矩阵最大值的索引</div><div class="line">print np.argmax(a)</div><div class="line"># 计算矩阵的均值</div><div class="line">print np.mean(a)</div><div class="line">print a.mean()</div><div class="line">print np.average(a)</div><div class="line"># 计算矩阵的中位数</div><div class="line">print np.median(a)</div><div class="line"># 计算前n项之和</div><div class="line">print np.cumsum(a)</div><div class="line"># 计算相邻两位的差</div><div class="line">print np.diff(a)</div><div class="line"># 找出非零的数, 输出的是非零数的索引，分别为行的索引和列的索引</div><div class="line">print np.nonzero(a)</div><div class="line"></div><div class="line"># Test 2 result</div><div class="line">11</div><div class="line">0</div><div class="line">8.5</div><div class="line">8.5</div><div class="line">8.5</div><div class="line">8.5</div><div class="line">[ 14  27  39  50  60  69  77  84  90  95  99 102]</div><div class="line">[[-1 -1 -1]</div><div class="line"> [-1 -1 -1]</div><div class="line"> [-1 -1 -1]]</div><div class="line">(array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]))</div><div class="line"></div><div class="line"># Test 3</div><div class="line">a = np.arange(14, 2, -1).reshape(3, 4)</div><div class="line">print a</div><div class="line"># 矩阵排序，按行排序</div><div class="line">print np.sort(a)</div><div class="line"># 矩阵的转置</div><div class="line">print np.transpose(a)</div><div class="line">print a.transpose()</div><div class="line">print a.T</div><div class="line"></div><div class="line">print a.T.dot(a)</div><div class="line"></div><div class="line"># Test 3 result</div><div class="line">[[14 13 12 11]</div><div class="line"> [10  9  8  7]</div><div class="line"> [ 6  5  4  3]]</div><div class="line">[[11 12 13 14]</div><div class="line"> [ 7  8  9 10]</div><div class="line"> [ 3  4  5  6]]</div><div class="line">[[14 10  6]</div><div class="line"> [13  9  5]</div><div class="line"> [12  8  4]</div><div class="line"> [11  7  3]]</div><div class="line">[[14 10  6]</div><div class="line"> [13  9  5]</div><div class="line"> [12  8  4]</div><div class="line"> [11  7  3]]</div><div class="line">[[14 10  6]</div><div class="line"> [13  9  5]</div><div class="line"> [12  8  4]</div><div class="line"> [11  7  3]]</div><div class="line">[[332 302 272 242]</div><div class="line"> [302 275 248 221]</div><div class="line"> [272 248 224 200]</div><div class="line"> [242 221 200 179]]</div><div class="line"></div><div class="line"># Test 4</div><div class="line">print a</div><div class="line"># 矩阵的处理，所有小于5的数等于5，所有大于10的数等于10</div><div class="line">print np.clip(a, 5, 10)</div><div class="line"></div><div class="line"># 计算矩阵指定维度的均值, 0是列, 1是行</div><div class="line">print np.mean(a, axis = 0)</div><div class="line"></div><div class="line"># Test 4 result</div><div class="line">[[14 13 12 11]</div><div class="line"> [10  9  8  7]</div><div class="line"> [ 6  5  4  3]]</div><div class="line">[[10 10 10 10]</div><div class="line"> [10  9  8  7]</div><div class="line"> [ 6  5  5  5]]</div><div class="line">[ 10.   9.   8.   7.]</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      numpy的基本用法(二)——基本运算
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>numpy的基本用法(一)——基本运算</title>
    <link href="noahsnail.com/2017/04/23/2017-4-23-numpy%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/"/>
    <id>noahsnail.com/2017/04/23/2017-4-23-numpy的基本用法(一)——基本运算/</id>
    <published>2017-04-23T12:45:49.000Z</published>
    <updated>2017-04-23T13:07:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于numpy的一些基本运算的用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Test 1</div><div class="line"># 定义矩阵</div><div class="line">arr = np.array([[1, 2, 3],</div><div class="line">                [4, 5, 6]])</div><div class="line">print arr</div><div class="line"></div><div class="line"># Test 1 Result</div><div class="line">[[1 2 3]</div><div class="line"> [4 5 6]]</div><div class="line"></div><div class="line"># Test 2</div><div class="line"># 矩阵的维度</div><div class="line">print &apos;number of dim: &apos;, arr.ndim</div><div class="line"># 矩阵的shape,即每一维度上的元素个数</div><div class="line">print &apos;shape: &apos;, arr.shape</div><div class="line"># 矩阵的元素总数</div><div class="line">print &apos;size: &apos;, arr.size</div><div class="line"># 矩阵的元素类型</div><div class="line">print &apos;dtype: &apos;, arr.dtype</div><div class="line"></div><div class="line"># Test 2 Result</div><div class="line">number of dim:  2</div><div class="line">shape:  (2, 3)</div><div class="line">size:  6</div><div class="line">dtype:  int64</div><div class="line"></div><div class="line"># Test 3</div><div class="line"># 定义矩阵及矩阵的元素类型——int32, int64, float32, float64</div><div class="line">a = np.array([1, 2, 3], dtype = np.int32)</div><div class="line">print a</div><div class="line">print a.ndim</div><div class="line">print a.shape</div><div class="line">print a.size</div><div class="line">print a.dtype</div><div class="line"></div><div class="line"></div><div class="line"># Test 3 Result</div><div class="line">[1 2 3]</div><div class="line">1</div><div class="line">(3,)</div><div class="line">3</div><div class="line">int32</div><div class="line"></div><div class="line"># Test 4</div><div class="line"># 定义零矩阵</div><div class="line">z = np.zeros((3, 4), dtype = np.int16)</div><div class="line">print z</div><div class="line">print z.dtype</div><div class="line"></div><div class="line"># 定义空矩阵</div><div class="line">n = np.empty((3, 4))</div><div class="line">print n</div><div class="line"></div><div class="line"># Test 4 Result</div><div class="line">[[0 0 0 0]</div><div class="line"> [0 0 0 0]</div><div class="line"> [0 0 0 0]]</div><div class="line">int16</div><div class="line"></div><div class="line">[[ 0.  0.  0.  0.]</div><div class="line"> [ 0.  0.  0.  0.]</div><div class="line"> [ 0.  0.  0.  0.]]</div><div class="line"></div><div class="line"># Test 5</div><div class="line"># 定义向量, 10-20之间, 元素间隔为2, 左闭右开</div><div class="line">a = np.arange(10, 20, 2)</div><div class="line">print a</div><div class="line"></div><div class="line"># 定义向量并转为矩阵</div><div class="line">b = np.arange(12).reshape((3, 4))</div><div class="line">print b</div><div class="line"></div><div class="line"># 定义向量, 类型是线性间隔</div><div class="line">a = np.linspace(1, 10, 6).reshape((2, 3))</div><div class="line">print a</div><div class="line"></div><div class="line"># Test 5 Result</div><div class="line">[10 12 14 16 18]</div><div class="line"></div><div class="line">[[ 0  1  2  3]</div><div class="line"> [ 4  5  6  7]</div><div class="line"> [ 8  9 10 11]]</div><div class="line"></div><div class="line">[[  1.    2.8   4.6]</div><div class="line"> [  6.4   8.2  10. ]]</div><div class="line"></div><div class="line"># Test 6</div><div class="line"># 矩阵的加、减、点乘、平方</div><div class="line">a = np.array([10, 20, 30, 40])</div><div class="line">b = np.arange(4)</div><div class="line">c = a - b</div><div class="line">d = a + b</div><div class="line">print a, b</div><div class="line">print c, d</div><div class="line">e = a * b</div><div class="line">print e</div><div class="line">f = e ** 2</div><div class="line">print f</div><div class="line"></div><div class="line"># Test 6 Result</div><div class="line">[10 20 30 40] [0 1 2 3]</div><div class="line">[10 19 28 37] [10 21 32 43]</div><div class="line">[  0  20  60 120]</div><div class="line">[    0   400  3600 14400]</div><div class="line"></div><div class="line"># Test 7</div><div class="line"># 矩阵的三角运算——sin, cos, tan</div><div class="line">sin = 10 * np.sin(a)</div><div class="line">print sin</div><div class="line"></div><div class="line"># 矩阵的判断</div><div class="line">print b &lt; 3</div><div class="line">print b == 3</div><div class="line"></div><div class="line"># Test 7 Result</div><div class="line">[-5.44021111  9.12945251 -9.88031624  7.4511316 ]</div><div class="line"></div><div class="line">[ True  True  True False]</div><div class="line">[False False False  True]</div><div class="line"></div><div class="line"># Test 8</div><div class="line"># 矩阵的点乘及乘法</div><div class="line">a = [ [1, 1], [0, 1]]</div><div class="line">b = np.arange(4).reshape((2, 2))</div><div class="line">c = a * b</div><div class="line">d = np.dot(a, b)</div><div class="line">print c</div><div class="line">print d</div><div class="line"></div><div class="line"># Test 8 Result</div><div class="line">[[0 1]</div><div class="line"> [0 3]]</div><div class="line">[[2 4]</div><div class="line"> [2 3]]</div><div class="line"></div><div class="line"># Test 9</div><div class="line"># np.random返回随机的浮点数，在半开区间 [0.0, 1.0)</div><div class="line"># 定义随机矩阵</div><div class="line">a = np.random.random((2, 4))</div><div class="line">print a</div><div class="line"></div><div class="line"># Test 9 Result</div><div class="line">[[ 0.93213483  0.58102186  0.98259187  0.27387014]</div><div class="line"> [ 0.43796835  0.98195976  0.29343791  0.94752226]]</div><div class="line"></div><div class="line"># Test 10</div><div class="line"># 矩阵的求和, 最小值, 最大值</div><div class="line">print np.sum(a)</div><div class="line">print np.min(a)</div><div class="line">print np.max(a)</div><div class="line"></div><div class="line"># 矩阵某一维度的求和, 最小值, 最大值, 0是列, 1是行</div><div class="line">print np.sum(a, axis = 1)</div><div class="line">print np.max(a, axis = 1)</div><div class="line">print np.min(a, axis = 0)</div><div class="line"></div><div class="line"># Test 10 Result</div><div class="line">5.43050697485</div><div class="line">0.273870140282</div><div class="line">0.982591870104</div><div class="line"></div><div class="line">[ 2.7696187   2.66088828]</div><div class="line">[ 0.98259187  0.98195976]</div><div class="line">[ 0.43796835  0.58102186  0.29343791  0.27387014]</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      numpy的基本用法(一)——基本运算
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(十)——保存神经网络参数和加载神经网络参数</title>
    <link href="noahsnail.com/2017/04/20/2017-4-20-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%8D%81)%E2%80%94%E2%80%94%E4%BF%9D%E5%AD%98%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%92%8C%E5%8A%A0%E8%BD%BD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0/"/>
    <id>noahsnail.com/2017/04/20/2017-4-20-tensorflow的基本用法(十)——保存神经网络参数和加载神经网络参数/</id>
    <published>2017-04-20T11:45:30.000Z</published>
    <updated>2017-04-20T11:59:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是使用tensorfl保存神经网络参数和加载神经网络参数，不包括神经网络框架。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 保存神经网络参数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_para</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># 定义权重参数</span></div><div class="line">	W = tf.Variable([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype = tf.float32, name = <span class="string">'weights'</span>)</div><div class="line">	<span class="comment"># 定义偏置参数</span></div><div class="line">	b = tf.Variable([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], dtype = tf.float32, name = <span class="string">'biases'</span>)</div><div class="line">	<span class="comment"># 参数初始化</span></div><div class="line">	init = tf.global_variables_initializer()</div><div class="line">	<span class="comment"># 定义保存参数的saver</span></div><div class="line">	saver = tf.train.Saver()</div><div class="line"></div><div class="line">	<span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">		sess.run(init)</div><div class="line">		<span class="comment"># 保存session中的数据</span></div><div class="line">		save_path = saver.save(sess, <span class="string">'my_net/save_net.ckpt'</span>)</div><div class="line">		<span class="comment"># 输出保存路径</span></div><div class="line">		<span class="keyword">print</span> <span class="string">'Save to path: '</span>, save_path</div><div class="line"></div><div class="line"><span class="comment"># 恢复神经网络参数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_para</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># 定义权重参数</span></div><div class="line">	W = tf.Variable(np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>)), dtype = tf.float32, name = <span class="string">'weights'</span>)</div><div class="line">	<span class="comment"># 定义偏置参数</span></div><div class="line">	b = tf.Variable(np.arange(<span class="number">3</span>).reshape((<span class="number">1</span>, <span class="number">3</span>)), dtype = tf.float32, name = <span class="string">'biases'</span>)</div><div class="line">	<span class="comment"># 定义提取参数的saver</span></div><div class="line">	saver = tf.train.Saver()</div><div class="line"></div><div class="line">	<span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">		<span class="comment"># 加载文件中的参数数据，会根据name加载数据并保存到变量W和b中</span></div><div class="line">		save_path = saver.restore(sess, <span class="string">'my_net/save_net.ckpt'</span>)</div><div class="line">		<span class="comment"># 输出保存路径</span></div><div class="line">		<span class="keyword">print</span> <span class="string">'Weights: '</span>, sess.run(W)</div><div class="line">		<span class="keyword">print</span> <span class="string">'biases:  '</span>, sess.run(b)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># save_para()</span></div><div class="line">restore_para()</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># save</div><div class="line">Save to path:  my_net/save_net.ckpt</div><div class="line"></div><div class="line"></div><div class="line"># restore</div><div class="line">Weights:  [[ 1.  2.  3.]</div><div class="line"> [ 4.  5.  6.]]</div><div class="line">biases:   [[ 1.  2.  3.]]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(十)——保存神经网络参数和加载神经网络参数
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(九)——定义卷积神经网络训练MNIST</title>
    <link href="noahsnail.com/2017/04/19/2017-4-19-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B9%9D)%E2%80%94%E2%80%94%E5%AE%9A%E4%B9%89%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83MNIST/"/>
    <id>noahsnail.com/2017/04/19/2017-4-19-tensorflow的基本用法(九)——定义卷积神经网络训练MNIST/</id>
    <published>2017-04-19T14:40:40.000Z</published>
    <updated>2017-04-19T14:43:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是使用tensorflow定义卷积神经网络来训练MNIST数据集。定义的神经网络结构为两个卷积层+两个连接层，每个卷积层包括卷积层、ReLU层和Pooling层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"></div><div class="line"><span class="comment"># 定义神经网络模型的评估部分</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(test_xs, test_ys)</span>:</span></div><div class="line">	<span class="comment"># 使用全局变量prediction</span></div><div class="line">    <span class="keyword">global</span> prediction</div><div class="line">    <span class="comment"># 获得预测值y_pre</span></div><div class="line">    y_pre = sess.run(prediction, feed_dict = &#123; xs: test_xs, keep_prob: <span class="number">1</span>&#125;)</div><div class="line">    <span class="comment"># 判断预测值y和真实值y_中最大数的索引是否一致，y_pre的值为1-10概率</span></div><div class="line">    correct_prediction = tf.equal(tf.argmax(y_pre, <span class="number">1</span>), tf.argmax(test_ys, <span class="number">1</span>))</div><div class="line">    <span class="comment"># 定义准确率的计算</span></div><div class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">    <span class="comment"># 计算准确率</span></div><div class="line">    result = sess.run(accuracy)</div><div class="line">    <span class="keyword">return</span> result</div><div class="line"></div><div class="line"><span class="comment"># 下载mnist数据</span></div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 权重参数初始化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.truncated_normal(shape, stddev = <span class="number">0.1</span>)</div><div class="line">	<span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="comment"># 偏置参数初始化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.constant(<span class="number">0.1</span>, shape = shape)</div><div class="line">	<span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="comment"># 定义卷积层</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">	<span class="comment"># stride的四个参数：[batch, height, width, channels], [batch_size, image_rows, image_cols, number_of_colors]</span></div><div class="line">	<span class="comment"># height, width就是图像的高度和宽度，batch和channels在卷积层中通常设为1</span></div><div class="line">	<span class="keyword">return</span> tf.nn.conv2d(x, W, strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 输入输出数据的placeholder</span></div><div class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line"><span class="comment"># dropout的比例</span></div><div class="line">keep_prob = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line"><span class="comment"># 对数据进行重新排列，形成图像</span></div><div class="line">x_image = tf.reshape(xs, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="keyword">print</span> x_image.shape</div><div class="line"></div><div class="line"><span class="comment"># 卷积层一</span></div><div class="line"><span class="comment"># patch为5*5，in_size为1，即图像的厚度，如果是彩色，则为3，32是out_size，输出的大小</span></div><div class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</div><div class="line">b_conv1 = bias_variable([<span class="number">32</span>])</div><div class="line"><span class="comment"># ReLU操作，输出大小为28*28*32</span></div><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line"><span class="comment"># Pooling操作，输出大小为14*14*32</span></div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div><div class="line"></div><div class="line"><span class="comment"># 卷积层二</span></div><div class="line"><span class="comment"># patch为5*5，in_size为32，即图像的厚度，64是out_size，输出的大小</span></div><div class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line">b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line"><span class="comment"># ReLU操作，输出大小为14*14*64</span></div><div class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line"><span class="comment"># Pooling操作，输出大小为7*7*64</span></div><div class="line">h_pool2 = max_pool_2x2(h_conv2)</div><div class="line"></div><div class="line"><span class="comment"># 全连接层一</span></div><div class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</div><div class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</div><div class="line"><span class="comment"># 输入数据变换</span></div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</div><div class="line"><span class="comment"># 进行全连接操作</span></div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div><div class="line"><span class="comment"># 防止过拟合，dropout</span></div><div class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 全连接层二</span></div><div class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</div><div class="line">b_fc2 = bias_variable([<span class="number">10</span>])</div><div class="line"><span class="comment"># 预测</span></div><div class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</div><div class="line"></div><div class="line"><span class="comment"># 计算loss</span></div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[<span class="number">1</span>])) </div><div class="line"><span class="comment"># 神经网络训练</span></div><div class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(cross_entropy)</div><div class="line"></div><div class="line"><span class="comment"># 定义Session</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># 根据tensorflow版本选择初始化函数</span></div><div class="line"><span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</div><div class="line">    init = tf.initialize_all_variables()</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    init = tf.global_variables_initializer()</div><div class="line"><span class="comment"># 执行初始化</span></div><div class="line">sess.run(init)</div><div class="line"></div><div class="line"><span class="comment"># 进行训练迭代</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    <span class="comment"># 取出mnist数据集中的100个数据</span></div><div class="line">	batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">	<span class="comment"># 执行训练过程并传入真实数据</span></div><div class="line">	sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys, keep_prob: <span class="number">0.5</span>&#125;)</div><div class="line">	<span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">		<span class="keyword">print</span> compute_accuracy(mnist.test.images, mnist.test.labels)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">$ python practice4.py</div><div class="line">Extracting MNIST_data/train-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/train-labels-idx1-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</div><div class="line">0.0823</div><div class="line">0.875</div><div class="line">0.9243</div><div class="line">0.9427</div><div class="line">0.9502</div><div class="line">0.9573</div><div class="line">0.9595</div><div class="line">0.9623</div><div class="line">0.963</div><div class="line">0.9687</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(九)——定义卷积神经网络训练MNIST
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(八)——dropout的作用</title>
    <link href="noahsnail.com/2017/04/17/2017-4-17-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AB)%E2%80%94%E2%80%94dropout%E7%9A%84%E4%BD%9C%E7%94%A8/"/>
    <id>noahsnail.com/2017/04/17/2017-4-17-tensorflow的基本用法(八)——dropout的作用/</id>
    <published>2017-04-17T13:45:48.000Z</published>
    <updated>2017-04-17T13:51:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是介绍tensorflow中dropout的作用，dropout主要是用来防止过拟合，即提供模型的泛化能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</div><div class="line"></div><div class="line"><span class="comment"># 加载数据 </span></div><div class="line">digits = load_digits()</div><div class="line"><span class="comment"># 输入数据</span></div><div class="line">X = digits.data</div><div class="line"><span class="comment"># 输出数据</span></div><div class="line">y = digits.target</div><div class="line"><span class="comment"># 标签变换</span></div><div class="line">y = LabelBinarizer().fit_transform(y)</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.3</span>)</div><div class="line"></div><div class="line"><span class="comment"># 创建一个神经网络层</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(input, in_size, out_size, layer_name, activation_function = None)</span>:</span></div><div class="line">	<span class="string">"""</span></div><div class="line">	:param input:</div><div class="line">		神经网络层的输入</div><div class="line">	:param in_zize:</div><div class="line">		输入数据的大小</div><div class="line">	:param out_size:</div><div class="line">		输出数据的大小</div><div class="line">	:param layer_name</div><div class="line">		神经网络层的名字</div><div class="line">	:param activation_function:</div><div class="line">		神经网络激活函数，默认没有</div><div class="line">	"""</div><div class="line">	<span class="comment"># 定义神经网络的初始化权重</span></div><div class="line">	Weights = tf.Variable(tf.random_normal([in_size, out_size]))</div><div class="line">	<span class="comment"># 定义神经网络的偏置</span></div><div class="line">	biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</div><div class="line">	<span class="comment"># 计算w*x+b</span></div><div class="line">	W_mul_x_plus_b = tf.matmul(input, Weights) + biases</div><div class="line">	<span class="comment"># 进行dropout，可以注释和不注释来对比dropout的效果</span></div><div class="line"><span class="comment">#	W_mul_x_plus_b = tf.nn.dropout(W_mul_x_plus_b, keep_prob)</span></div><div class="line">	<span class="comment"># 根据是否有激活函数进行处理</span></div><div class="line">	<span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">		output = W_mul_x_plus_b</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		output = activation_function(W_mul_x_plus_b)</div><div class="line">	<span class="comment"># 查看权重变化</span></div><div class="line">	tf.summary.histogram(layer_name + <span class="string">'/output'</span>, output)</div><div class="line">	<span class="keyword">return</span> output</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义dropout的placeholder</span></div><div class="line">keep_prob = tf.placeholder(tf.float32)</div><div class="line"><span class="comment"># 输入数据64个特征</span></div><div class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">64</span>])  <span class="comment"># 8x8</span></div><div class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line"></div><div class="line"><span class="comment"># 添加隐藏层和输出层</span></div><div class="line">l1 = add_layer(xs, <span class="number">64</span>, <span class="number">50</span>, <span class="string">'l1'</span>, activation_function=tf.nn.tanh)</div><div class="line">prediction = add_layer(l1, <span class="number">50</span>, <span class="number">10</span>, <span class="string">'l2'</span>, activation_function=tf.nn.softmax)</div><div class="line"></div><div class="line"><span class="comment"># 计算loss</span></div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[<span class="number">1</span>]))</div><div class="line"><span class="comment"># 存储loss</span></div><div class="line">tf.summary.scalar(<span class="string">'loss'</span>, cross_entropy)</div><div class="line"><span class="comment"># 神经网络训练</span></div><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</div><div class="line"><span class="comment"># 定义Session</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># 收集所有的数据</span></div><div class="line">merged = tf.summary.merge_all()</div><div class="line"><span class="comment"># 将数据写入到tensorboard中</span></div><div class="line">train_writer = tf.summary.FileWriter(<span class="string">"logs/train"</span>, sess.graph)</div><div class="line">test_writer = tf.summary.FileWriter(<span class="string">"logs/test"</span>, sess.graph)</div><div class="line"></div><div class="line"><span class="comment"># 根据tensorflow版本选择初始化函数</span></div><div class="line"><span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</div><div class="line">    init = tf.initialize_all_variables()</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    init = tf.global_variables_initializer()</div><div class="line"><span class="comment"># 执行初始化</span></div><div class="line">sess.run(init)</div><div class="line"><span class="comment"># 进行训练迭代</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># 执行训练，dropout为1-0.5=0.5</span></div><div class="line">    sess.run(train_step, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: <span class="number">0.5</span>&#125;)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</div><div class="line">        <span class="comment"># 记录损失</span></div><div class="line">        train_result = sess.run(merged, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: <span class="number">1</span>&#125;)</div><div class="line">        test_result = sess.run(merged, feed_dict=&#123;xs: X_test, ys: y_test, keep_prob: <span class="number">1</span>&#125;)</div><div class="line">        train_writer.add_summary(train_result, i)</div><div class="line">        test_writer.add_summary(test_result, i)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<ul>
<li>没有dropout</li>
</ul>
<p><img src="http://ocs628urt.bkt.clouddn.com/no_dropout.png" alt="no_dropout"></p>
<p>测试误差与训练误差的损失差的较大，说明模型更拟合训练数据。</p>
<ul>
<li>有dropout</li>
</ul>
<p><img src="http://ocs628urt.bkt.clouddn.com/dropout.png" alt="dropout"></p>
<p>测试误差与训练误差相差不大，说明模型泛化能力较好。</p>
]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(八)——dropout的作用
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(七)——使用MNIST训练神经网络</title>
    <link href="noahsnail.com/2017/04/15/2017-4-15-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%83)%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8MNIST%E8%AE%AD%E7%BB%83/"/>
    <id>noahsnail.com/2017/04/15/2017-4-15-tensorflow的基本用法(七)——使用MNIST训练/</id>
    <published>2017-04-15T15:32:42.000Z</published>
    <updated>2017-04-15T16:23:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是使用tensorflow和mnist数据集来训练神经网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"></div><div class="line"><span class="comment"># 下载mnist数据</span></div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义神经网络模型的评估部分</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(W, b)</span>:</span></div><div class="line">	<span class="comment"># 定义测试数据的placeholder</span></div><div class="line">	x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">	<span class="comment"># 定义测试数据的真实标签的placeholder</span></div><div class="line">	y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line">	<span class="comment"># 定义预测值</span></div><div class="line">	y = tf.nn.softmax(tf.matmul(x, W) + b)</div><div class="line">	<span class="comment"># 判断预测值y和真实值y_中最大数的索引是否一致，y的值为1-10概率</span></div><div class="line">	correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div><div class="line">	<span class="comment"># 计算准确率</span></div><div class="line">	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">	<span class="comment"># 输入测试数据，执行准确率的计算并返回</span></div><div class="line">	<span class="keyword">return</span> sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</div><div class="line"></div><div class="line"><span class="comment"># 定义神经网络模型的训练部分</span></div><div class="line"><span class="comment"># 下面定义的神经网络只有一层W*x+b</span></div><div class="line"><span class="comment"># 定义输入数据placeholder，不定义输入样本的数目——None，但定义每个样本的大小为784</span></div><div class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line"><span class="comment"># 定义神经网络层的权重参数</span></div><div class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</div><div class="line"><span class="comment"># 定义神经网络层的偏置参数</span></div><div class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</div><div class="line"><span class="comment"># 定义一层神经网络运算，激活函数为softmax</span></div><div class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</div><div class="line"><span class="comment"># 定义训练数据真实标签的placeholder</span></div><div class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line"><span class="comment"># 定义损失函数，损失函数为交叉熵，reduction_indices表示沿着tensor的哪个纬度来求和</span></div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[<span class="number">1</span>]))</div><div class="line"><span class="comment"># 定义神经网络的训练步骤，使用的是梯度下降法，学习率为0.5</span></div><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</div><div class="line"><span class="comment"># 初始化所有变量</span></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"><span class="comment"># 定义Session</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># 执行变量的初始化</span></div><div class="line">sess.run(init)</div><div class="line"><span class="comment"># 迭代进行训练</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">	<span class="comment"># 取出mnist数据集中的100个数据</span></div><div class="line">	batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">	<span class="comment"># 执行训练过程并传入真实数据x, y_</span></div><div class="line">	sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</div><div class="line">	<span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">		<span class="keyword">print</span> compute_accuracy(W, b)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Extracting MNIST_data/train-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/train-labels-idx1-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</div><div class="line">0.4075</div><div class="line">0.8948</div><div class="line">0.9031</div><div class="line">0.9074</div><div class="line">0.9037</div><div class="line">0.9125</div><div class="line">0.9158</div><div class="line">0.912</div><div class="line">0.9181</div><div class="line">0.9169</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(七)——使用MNIST训练神经网络
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(六)——神经网络可视化</title>
    <link href="noahsnail.com/2017/04/14/2017-4-14-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AD)%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>noahsnail.com/2017/04/14/2017-4-14-tensorflow的基本用法(六)——神经网络可视化/</id>
    <published>2017-04-14T14:40:35.000Z</published>
    <updated>2017-04-14T14:54:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是对tensorflow的神经网络训练过程以及神经网络结构进行可视化工作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 创建一个神经网络层</div><div class="line">def add_layer(input, in_size, out_size, activation_function = None):</div><div class="line">	&quot;&quot;&quot;</div><div class="line">	:param input:</div><div class="line">		神经网络层的输入</div><div class="line">	:param in_zize:</div><div class="line">		输入数据的大小</div><div class="line">	:param out_size:</div><div class="line">		输出数据的大小</div><div class="line">	:param activation_function:</div><div class="line">		神经网络激活函数，默认没有</div><div class="line">	&quot;&quot;&quot;</div><div class="line">	with tf.name_scope(&apos;layer&apos;):</div><div class="line">		with tf.name_scope(&apos;weights&apos;):</div><div class="line">			# 定义神经网络的初始化权重</div><div class="line">			Weights = tf.Variable(tf.random_normal([in_size, out_size]))</div><div class="line">		with tf.name_scope(&apos;biases&apos;):</div><div class="line">			# 定义神经网络的偏置</div><div class="line">			biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)</div><div class="line">		with tf.name_scope(&apos;W_mul_x_plus_b&apos;):</div><div class="line">			# 计算w*x+b</div><div class="line">			W_mul_x_plus_b = tf.matmul(input, Weights) + biases</div><div class="line">		# 根据是否有激活函数进行处理</div><div class="line">		if activation_function is None:</div><div class="line">			output = W_mul_x_plus_b</div><div class="line">		else:</div><div class="line">			output = activation_function(W_mul_x_plus_b)</div><div class="line"></div><div class="line">		return output</div><div class="line"></div><div class="line"># 创建一个具有输入层、隐藏层、输出层的三层神经网络，神经元个数分别为1,10,1</div><div class="line"># 创建只有一个特征的输入数据，数据数目为300，输入层</div><div class="line">x_data = np.linspace(-1, 1, 300)[:, np.newaxis]</div><div class="line"># 创建数据中的噪声</div><div class="line">noise = np.random.normal(0, 0.05, x_data.shape)</div><div class="line"># 创建输入数据对应的输出</div><div class="line">y_data = np.square(x_data) + 1 + noise</div><div class="line"></div><div class="line">with tf.name_scope(&apos;input&apos;):</div><div class="line">	# 定义输入数据，None是样本数目，表示多少输入数据都行，1是输入数据的特征数目</div><div class="line">	xs = tf.placeholder(tf.float32, [None, 1], name = &apos;x_input&apos;)</div><div class="line">	# 定义输出数据，与xs同理</div><div class="line">	ys = tf.placeholder(tf.float32, [None, 1], name = &apos;y_input&apos;)</div><div class="line"></div><div class="line"># 定义一个隐藏层</div><div class="line">hidden_layer = add_layer(xs, 1, 10, activation_function = tf.nn.relu)</div><div class="line"># 定义输出层</div><div class="line">prediction = add_layer(hidden_layer, 10, 1, activation_function = None)</div><div class="line"></div><div class="line"># 求解神经网络参数</div><div class="line"></div><div class="line"># 定义损失函数</div><div class="line">with tf.name_scope(&apos;loss&apos;):</div><div class="line">	loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices = [1]))</div><div class="line"># 定义训练过程</div><div class="line">with tf.name_scope(&apos;train&apos;):</div><div class="line">	train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</div><div class="line"># 变量初始化</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"># 定义Session</div><div class="line">sess = tf.Session()</div><div class="line"># 将网络结构图写到文件中</div><div class="line">writer = tf.summary.FileWriter(&apos;logs/&apos;, sess.graph)</div><div class="line"># 执行初始化工作</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line"># 绘制求解的曲线</div><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(1, 1, 1)</div><div class="line">ax.scatter(x_data, y_data)</div><div class="line">plt.ion()</div><div class="line">plt.show()</div><div class="line"></div><div class="line"># 进行训练</div><div class="line">for i in range(1000):</div><div class="line">	# 执行训练，并传入数据</div><div class="line">	sess.run(train_step, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line">	if i % 100 == 0:</div><div class="line">		try:</div><div class="line">			ax.lines.remove(lines[0])</div><div class="line">		except Exception:</div><div class="line">			pass</div><div class="line"></div><div class="line">		# print sess.run(loss, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line">		# 计算预测值</div><div class="line">		prediction_value = sess.run(prediction, feed_dict = &#123;xs: x_data&#125;)</div><div class="line">		绘制预测值</div><div class="line">		lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw = 5)</div><div class="line">		plt.pause(0.1)</div><div class="line"># 关闭Session</div><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/visualization.png" alt="优化"></p>
<p>在网络结果代码中添加<code>tf.name_scope(&#39;name&#39;)</code>并将网络结构图写入文件后，可用tensorboard命令查看神经网络的结果图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=log/</div></pre></td></tr></table></figure>
<p>结果如图：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/neural_network.png" alt="神经网络结构图"></p>
]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(六)——神经网络可视化
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(五)——创建神经网络并训练</title>
    <link href="noahsnail.com/2017/04/12/2017-4-12-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%94)%E2%80%94%E2%80%94%E5%88%9B%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82/"/>
    <id>noahsnail.com/2017/04/12/2017-4-12-tensorflow的基本用法(五)——创建神经网络层/</id>
    <published>2017-04-12T14:09:21.000Z</published>
    <updated>2017-04-12T15:12:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是介绍利用tensorflow创建一个简单的神经网络并进行训练。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 创建一个神经网络层</div><div class="line">def add_layer(input, in_size, out_size, activation_function = None):</div><div class="line">	&quot;&quot;&quot;</div><div class="line">	:param input:</div><div class="line">		神经网络层的输入</div><div class="line">	:param in_zize:</div><div class="line">		输入数据的大小</div><div class="line">	:param out_size:</div><div class="line">		输出数据的大小</div><div class="line">	:param activation_function:</div><div class="line">		神经网络激活函数，默认没有</div><div class="line">	&quot;&quot;&quot;</div><div class="line"></div><div class="line">	# 定义神经网络的初始化权重</div><div class="line">	Weights = tf.Variable(tf.random_normal([in_size, out_size]))</div><div class="line">	# 定义神经网络的偏置</div><div class="line">	biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)</div><div class="line">	# 计算w*x+b</div><div class="line">	W_mul_x_plus_b = tf.matmul(input, Weights) + biases</div><div class="line">	# 根据是否有激活函数进行处理</div><div class="line">	if activation_function is None:</div><div class="line">		output = W_mul_x_plus_b</div><div class="line">	else:</div><div class="line">		output = activation_function(W_mul_x_plus_b)</div><div class="line"></div><div class="line">	return output</div><div class="line"></div><div class="line"># 创建一个具有输入层、隐藏层、输出层的三层神经网络，神经元个数分别为1,10,1</div><div class="line"># 创建只有一个特征的输入数据，数据数目为300，输入层</div><div class="line">x_data = np.linspace(-1, 1, 300)[:, np.newaxis]</div><div class="line"># 创建数据中的噪声</div><div class="line">noise = np.random.normal(0, 0.05, x_data.shape)</div><div class="line"># 创建输入数据对应的输出</div><div class="line">y_data = np.square(x_data) + 1 + noise</div><div class="line"></div><div class="line"># 定义输入数据，None是样本数目，表示多少输入数据都行，1是输入数据的特征数目</div><div class="line">xs = tf.placeholder(tf.float32, [None, 1])</div><div class="line"># 定义输出数据，与xs同理</div><div class="line">ys = tf.placeholder(tf.float32, [None, 1])</div><div class="line"></div><div class="line"># 定义一个隐藏层</div><div class="line">hidden_layer = add_layer(xs, 1, 10, activation_function = tf.nn.relu)</div><div class="line"># 定义输出层</div><div class="line">prediction = add_layer(hidden_layer, 10, 1, activation_function = None)</div><div class="line"></div><div class="line"># 求解神经网络参数</div><div class="line"># 定义损失函数</div><div class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices = [1]))</div><div class="line"># 定义训练过程</div><div class="line">train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</div><div class="line"># 变量初始化</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"># 定义Session</div><div class="line">sess = tf.Session()</div><div class="line"># 执行初始化工作</div><div class="line">sess.run(init)</div><div class="line"># 进行训练</div><div class="line">for i in range(1000):</div><div class="line">	# 执行训练，并传入数据</div><div class="line">	sess.run(train_step, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line">	if i % 100 == 0:</div><div class="line">		print sess.run(loss, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line"># 关闭Session</div><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">1.06731</div><div class="line">0.0111914</div><div class="line">0.00651229</div><div class="line">0.00530187</div><div class="line">0.00472237</div><div class="line">0.00429948</div><div class="line">0.00399815</div><div class="line">0.00377548</div><div class="line">0.00359714</div><div class="line">0.00345819</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(五)——创建神经网络并训练
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(四)——placeholder</title>
    <link href="noahsnail.com/2017/04/11/2017-4-11-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%9B%9B)/"/>
    <id>noahsnail.com/2017/04/11/2017-4-11-tensorflow的基本用法(四)/</id>
    <published>2017-04-11T14:26:39.000Z</published>
    <updated>2017-04-12T15:08:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是介绍tensorflow中的placeholder及用法。placeholder，中文意思是占位符，在tensorflow中类似于函数参数，运行时必须传入值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 定义placeholder</div><div class="line">input1 = tf.placeholder(tf.float32)</div><div class="line">input2 = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line"># 定义乘法运算</div><div class="line">output = tf.multiply(input1, input2)</div><div class="line"></div><div class="line"># 通过session执行乘法运行</div><div class="line">with tf.Session() as sess:</div><div class="line">	# 执行时要传入placeholder的值</div><div class="line">	print sess.run(output, feed_dict = &#123;input1:[7.], input2: [2.]&#125;)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[ 14.]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(四)——placeholder
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(三)——Variable</title>
    <link href="noahsnail.com/2017/04/10/2017-4-10-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%89)/"/>
    <id>noahsnail.com/2017/04/10/2017-4-10-tensorflow的基本用法(三)/</id>
    <published>2017-04-10T10:58:57.000Z</published>
    <updated>2017-04-11T14:34:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是介绍tensorflow中的变量Variable及用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 定义变量</div><div class="line">state = tf.Variable(0, name = &apos;counter&apos;)</div><div class="line">print(state.name)</div><div class="line"></div><div class="line"># 定义常量</div><div class="line">one = tf.constant(1)</div><div class="line"># 定义新的变量，值等于state + one</div><div class="line">new_value = tf.add(state, one)</div><div class="line"># 定义update操作，将new_value赋值给state</div><div class="line">update = tf.assign(state, new_value)</div><div class="line"></div><div class="line"># 定义的变量激活，如果定义了Variable，必须有</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"> </div><div class="line">with tf.Session() as sess:</div><div class="line">	sess.run(init)</div><div class="line">	for _ in range(3):</div><div class="line">		# 执行update操作，update操作包括add,assign两部分</div><div class="line">		sess.run(update)</div><div class="line">		# 输出state结果，必须放到sess.run()中</div><div class="line">		print sess.run(state)</div></pre></td></tr></table></figure>
<p>执行结果如下图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">counter:0</div><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(三)——Variable
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法(二)——感知机</title>
    <link href="noahsnail.com/2017/04/10/2017-4-10-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95(%E4%BA%8C)%E2%80%94%E2%80%94%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>noahsnail.com/2017/04/10/2017-4-10-统计学习方法(二)——感知机/</id>
    <published>2017-04-10T09:27:03.000Z</published>
    <updated>2017-04-18T11:09:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="第2章-感知机"><a href="#第2章-感知机" class="headerlink" title="第2章 感知机"></a>第2章 感知机</h1><p>感知机（perceptron）是二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1值。感知机将实例分为正负两类的分离超平面，属于判别模型。感知机旨在求出将训练数据进行线性可分的分离超平面，根据损失函数用梯度下降法对损失函数进行极小化，求的感知机模型。</p>
<h2 id="2-1-感知机模型"><a href="#2-1-感知机模型" class="headerlink" title="2.1 感知机模型"></a>2.1 感知机模型</h2><p><strong>感知机定义</strong></p>
<p>假设输入空间是$x \in R^n$，输出空间是$y=\lbrace +1,-1 \rbrace$，输入$x \in X$是实例的特征向量，是输入空间的点，输出$y \in Y$表示实例的类别，输入输出之间的映射函数为：</p>
<p>$$f(x)=sign(w * x + b)$$</p>
<p>f(x)称为感知机。其中，w和b为感知机的模型参数，$w \in R^n$叫做权值(weight)或权值向量(weight vector)，$b \in R$叫做偏置（bias）。$w*x$表示w与x的内积。sign是符号函数，即</p>
<p>$$sign(x)=<br>\begin{cases}<br>+1，x \geq 0 \\<br>-1，x \lt 0<br>\end{cases}$$</p>
<p>感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合$\lbrace f|f(x) = w * x + b\rbrace$。感知机的几何解释为：线性方程</p>
<p>$$w * x + b = 0$$</p>
<p>对应于特征空间$R^n$中的一个超平面S，其中w是超平面的法向量，b是超平面的截距，这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正负两类，因此超平面S称为分离超平面（separating hyperplane），如下图所示：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/perceptron.png" alt="感知机模型"></p>
<h2 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h2><h3 id="2-2-1-数据集的线性可分性"><a href="#2-2-1-数据集的线性可分性" class="headerlink" title="2.2.1 数据集的线性可分性"></a>2.2.1 数据集的线性可分性</h3><p><strong>线性可分性</strong></p>
<p>给定一个数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i \in X = R^n$，$y \in Y=\lbrace +1,-1 \rbrace$，$i=1,2,…,N$，如果存在超平面S</p>
<p>$$w * x + b = 0$$</p>
<p>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例i，有$w<em>_i+b&gt;0$，对所有$y_i=-1$的实例i，有$w</em>_i+b&lt;0$，则称数据集T为线性可分数据集（linear separable data set）；否则，称数据集T线性不可分。</p>
<h3 id="2-2-2-感知机学习策略"><a href="#2-2-2-感知机学习策略" class="headerlink" title="2.2.2 感知机学习策略"></a>2.2.2 感知机学习策略</h3><p>假定训练数据集是线性可分的，感知机的学习目标是求得一个能将训练集正负实例点完全正确分开的分离超平面。为了找出超平面，即确定模型参数w，b，需要确定学习策略，即定义损失函数并将损失函数最小化。损失函数的一个自然选择是分类点的错误数。但这样的损失函数不是连续可导函数，不易优化。因此感知机采用误分类点到超平面S的总距离作为损失函数。输入空间中任意一点$x_0$到超平面S的距离为：</p>
<p>$$\frac {1} {||w||} |w * x_0 + b|$$</p>
<p>$||w||$为w的$L_2$范数。对于误分类的数据$(x_i,y_i)$，$-y_i(w * x + b) \lt 0$始终成立。因此，误分类点$x_i$到超平面S的距离为：</p>
<p>$$-\frac {1} {||w||} y_i(w * x_i + b)$$</p>
<p>假设误分类点集合为M，M到超平面S的总距离为</p>
<p>$$-\frac {1} {||w||} \sum_{x_i \in M} y_i(w * x_i + b)$$</p>
<p>去掉$||w||$，就得到了感知机的损失函数。感知机${sign}(w * x + b)$的损失函数为</p>
<p>$$L(w,b)=-\sum_{x_i \in M} y_i(w * x_i + b)$$</p>
<p>损失函数L(w,b)是非负的，如果没有误分类点，损失函数为0。误分类点离超平面越近，损失函数值越小。</p>
<h2 id="2-3-感知机学习算法"><a href="#2-3-感知机学习算法" class="headerlink" title="2.3 感知机学习算法"></a>2.3 感知机学习算法</h2><p>感知机学习问题转化为求解损失函数L(w,b)的问题，最优化的方法是随机梯度下降法。</p>
<h3 id="2-3-1-感知机学习算法的原始形式"><a href="#2-3-1-感知机学习算法的原始形式" class="headerlink" title="2.3.1 感知机学习算法的原始形式"></a>2.3.1 感知机学习算法的原始形式</h3><p>感知机是对以下最优化问题的算法。给定一个训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i \in X = R^n$，$y \in Y=\lbrace +1,-1 \rbrace$，$i=1,2,…,N$，求参数w，b，使其为以下损失函数极小化问题的解</p>
<p>$${min}_{w,b} L(w,b) = -\sum_{x_i \in M} y_i(w * x_i + b)$$</p>
<p>其中M为误分类点的集合。感知机学习算法是通过误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent）。首先，选取任意一个超平面$w_0，b_0$，然后用梯度下降法不断的极小化目标函数。极小化过程不是一次使M中的所有误分类点的梯度下降，而是一次随机选择一个误分类点使其梯度下降。</p>
<p>假设误分类点集合M是固定的，那么损失函数的L(w,b)的梯度由<br>$$\nabla_wL(w,b)=-\sum_{x_i \in M} y_ix_i$$</p>
<p>$$\nabla_bL(w,b)=-\sum_{x_i \in M} y_i$$</p>
<p>给出。随机选取一个误分类点(x_i,y_i)，对w，b进行更新：</p>
<p>$$w \longleftarrow w + \eta y_ix_i$$<br>$$b \longleftarrow b + \eta y_i$$</p>
<p>式子中$\eta(0 &lt; \eta \le 1)$是步长，在统计学习中又称为学习率（learning rate）。通过迭代可以期待损失函数L(w,b)不断减小，直到为0。</p>
<p>感知机学习算法的流程如下：</p>
<p>输入：训练数据集$T={(x_1,y_1)，(x_2,y_2)，…，(x_n,y_n)}$，其中$x_i \in X = R^n$，$y \in Y=\lbrace +1,-1 \rbrace$，$i=1,2,…,N$，学习率$\eta(0&lt;\eta \le 1)$；输出w，b；感知机学习模型$f(x) = {sign}(w * x + b)$。</p>
<p>(1) 选取初值$w_0，b_0$<br>(2) 在训练集中选取数据$(x_i,y_i)$<br>(3) 如果$y_i(w * x_i + b) \le 0$</p>
<p>$$w \longleftarrow w + \eta y_ix_i$$</p>
<p>$$b \longleftarrow b + \eta y_i$$<br>(4) 转至(2)，知道训练集中没有误分类点。</p>
<p><strong>解释：</strong>当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w，b的值，使分离超平面向该误分类点的一侧移动，以减少改误分类点与超平面间的距离，直至超平面越过该分类点使其被正确分类。</p>
<h3 id="2-3-2-算法的收敛性"><a href="#2-3-2-算法的收敛性" class="headerlink" title="2.3.2 算法的收敛性"></a>2.3.2 算法的收敛性</h3><p>对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。</p>
<p>证明略。</p>
<h3 id="2-3-3-感知机学习算法的对偶形式"><a href="#2-3-3-感知机学习算法的对偶形式" class="headerlink" title="2.3.3 感知机学习算法的对偶形式"></a>2.3.3 感知机学习算法的对偶形式</h3>]]></content>
    
    <summary type="html">
    
      统计学习方法(二)——感知机
    
    </summary>
    
      <category term="机器学习" scheme="noahsnail.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="统计学习方法" scheme="noahsnail.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Python版OpenCV</title>
    <link href="noahsnail.com/2017/04/10/2017-4-10-Mac%E5%AE%89%E8%A3%85Python%E7%89%88OpenCV/"/>
    <id>noahsnail.com/2017/04/10/2017-4-10-Mac安装Python版OpenCV/</id>
    <published>2017-04-10T08:10:24.000Z</published>
    <updated>2017-04-10T08:53:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-brew安装opencv"><a href="#1-brew安装opencv" class="headerlink" title="1. brew安装opencv"></a>1. brew安装opencv</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">brew install homebrew/science/opencv</div><div class="line"></div><div class="line">brew install homebrew/science/opencv3</div></pre></td></tr></table></figure>
<h2 id="2-把opencv添加到python中"><a href="#2-把opencv添加到python中" class="headerlink" title="2. 把opencv添加到python中"></a>2. 把opencv添加到python中</h2><p>执行下面的命令，或直接将<code>/usr/local/Cellar/opencv3/3.1.0_4/lib/python2.7/site-packages/</code>下的cv2.so拷贝到<code>/Users/ltc/anaconda/lib/python2.7/site-packages/</code>目录中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo /usr/local/Cellar/opencv3/3.1.0_4/lib/python2.7/site-packages/ &gt;&gt; /Users/ltc/anaconda/lib/python2.7/site-packages/opencv3.pth</div></pre></td></tr></table></figure>
<h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3. 测试"></a>3. 测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">$ import cv2</div></pre></td></tr></table></figure>
<p>不报错，安装成功。</p>
]]></content>
    
    <summary type="html">
    
      Mac安装Python版OpenCV
    
    </summary>
    
      <category term="OpenCV" scheme="noahsnail.com/categories/OpenCV/"/>
    
    
      <category term="OpenCV" scheme="noahsnail.com/tags/OpenCV/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode402——Remove K Digits</title>
    <link href="noahsnail.com/2017/04/09/2017-4-9-Leetcode402%E2%80%94%E2%80%94%20Remove%20K%20Digits/"/>
    <id>noahsnail.com/2017/04/09/2017-4-9-Leetcode402—— Remove K Digits/</id>
    <published>2017-04-09T06:09:34.000Z</published>
    <updated>2017-04-09T10:16:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>Given a non-negative integer num represented as a string, remove k digits from the number so that the new number is the smallest possible.</p>
<p>Note:<br>The length of num is less than 10002 and will be ≥ k.<br>The given num does not contain any leading zero.<br>Example 1:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = &quot;1432219&quot;, k = 3</div><div class="line">Output: &quot;1219&quot;</div><div class="line">Explanation: Remove the three digits 4, 3, and 2 to form the new number 1219 which is the smallest.</div></pre></td></tr></table></figure>
<p>Example 2:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = &quot;10200&quot;, k = 1</div><div class="line">Output: &quot;200&quot;</div><div class="line">Explanation: Remove the leading 1 and the number is 200. Note that the output must not contain leading zeroes.</div></pre></td></tr></table></figure>
<p>Example 3:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = &quot;10&quot;, k = 2</div><div class="line">Output: &quot;0&quot;</div><div class="line">Explanation: Remove all the digits from the number and it is left with nothing which is 0.</div></pre></td></tr></table></figure>
<h2 id="2-求解"><a href="#2-求解" class="headerlink" title="2. 求解"></a>2. 求解</h2><p>每移除一个字符，找出移除一个字符串后得到的字符串中最小的那个，作为下一次移除字符的输入，这样每次移除字符后得到子串都是最小子串。这里必须要明确每次移除字符的最优解必定是下一次移除字符最优解的输入，即f(n)的最优解必定是求解f(n-1)的最优解的一部分。问题的最优解解中包含了子问题的最优解。</p>
<p><strong>方法一：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">    public String removeKdigits(String num, int k) &#123;</div><div class="line">        String min = remove(num, k);</div><div class="line">        if(min.equals(&quot;&quot;)) &#123;</div><div class="line">            return &quot;0&quot;;</div><div class="line">        &#125;</div><div class="line">        while(min.charAt(0) == &apos;0&apos;) &#123;</div><div class="line">            if(min.length() == 1) &#123;</div><div class="line">                break;</div><div class="line">            &#125;</div><div class="line">            min = min.substring(1, min.length());</div><div class="line">        &#125;</div><div class="line">        return min;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    public String remove(String num, int k) &#123;</div><div class="line">        if(k == 0) &#123;</div><div class="line">            return num;</div><div class="line">        &#125;</div><div class="line">        int n = num.length();</div><div class="line">        String minString = num.substring(0, num.length() - 1);</div><div class="line">        for(int i = 0; i &lt; n; i++) &#123;</div><div class="line">            String temp = num.substring(0, i) + num.substring(i + 1, n);</div><div class="line">            if(temp.compareTo(minString) &lt; 0) &#123;</div><div class="line">                minString = temp;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        return remove(minString, k - 1);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Leetcode超时。</p>
<p><strong>方法二</strong></p>
<p>要想数字变小，应该从前往后删除字符，因为前面的字符是数字的高位，在删除一个字符的情况下，删除数字的位置会被它的后一位替代，因此应该删除当前数字大于后一位数字的字符。如果前面没有找到符合条件的数字，则删除最后一位数字。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">       public String removeKdigits(String num, int k) &#123;</div><div class="line">    	//用StringBuilder是因为StringBuilder有删除指定字符的功能</div><div class="line">        StringBuilder min = new StringBuilder(num);</div><div class="line">        for(int m = 0; m &lt; k; m++) &#123;</div><div class="line">            int index = 0;</div><div class="line">            int n = min.length();</div><div class="line">            for(int i = 0; i &lt; n; i++) &#123;</div><div class="line"></div><div class="line">                if((i == n -1) || min.charAt(i) &gt; min.charAt(i + 1)) &#123;</div><div class="line">                    index = i;</div><div class="line">                    break;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            min = min.deleteCharAt(index);</div><div class="line">        &#125;</div><div class="line">        //判断字符串为空的情况</div><div class="line">        if(min.length() == 0) &#123;</div><div class="line">            return &quot;0&quot;;</div><div class="line">        &#125;</div><div class="line">        //去掉字符串前面的“0”</div><div class="line">        while(min.charAt(0) == &apos;0&apos;) &#123;</div><div class="line">            if(min.length() == 1) &#123;</div><div class="line">                break;</div><div class="line">            &#125;</div><div class="line">            min = min.deleteCharAt(0);</div><div class="line">        &#125;</div><div class="line">        return min.toString();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Leetcode402—— Remove K Digits
    
    </summary>
    
      <category term="基础算法" scheme="noahsnail.com/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="noahsnail.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(二)——Session</title>
    <link href="noahsnail.com/2017/04/08/2017-4-8-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%8C)/"/>
    <id>noahsnail.com/2017/04/08/2017-4-8-tensorflow的基本用法(二)/</id>
    <published>2017-04-08T15:37:15.000Z</published>
    <updated>2017-04-11T14:35:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是使用tensorflow进行矩阵的乘法运算。代码中介绍了两种不同的使用session的方式。Demo源码及解释如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 定义两个矩阵</div><div class="line">matrix1 = tf.constant([[3, 3]])</div><div class="line">matrix2 = tf.constant([[2], [2]])</div><div class="line"></div><div class="line"># 定义矩阵乘法</div><div class="line">product = tf.matmul(matrix1, matrix2)</div><div class="line"></div><div class="line"># 运行矩阵乘法，session用法一</div><div class="line">sess = tf.Session()</div><div class="line">result = sess.run(product)</div><div class="line">print &apos;Session用法一&apos;</div><div class="line">print result</div><div class="line">sess.close()</div><div class="line"></div><div class="line">## session用法二，不用考虑close，会自动关闭</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">	result = sess.run(product)</div><div class="line">	print &apos;Session用法二&apos;</div><div class="line">	print result</div></pre></td></tr></table></figure>
<p>执行结果如下图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Session用法一</div><div class="line">[[12]]</div><div class="line">Session用法二</div><div class="line">[[12]]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(二)——Session
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
</feed>
