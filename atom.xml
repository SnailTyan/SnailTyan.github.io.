<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://noahsnail.com/"/>
  <updated>2020-06-18T02:01:18.000Z</updated>
  <id>http://noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python中list的append, extend, +=, +区别</title>
    <link href="http://noahsnail.com/2020/06/17/2020-06-17-python%E4%B8%ADlist%E7%9A%84append,%20extend%E5%8C%BA%E5%88%AB/"/>
    <id>http://noahsnail.com/2020/06/17/2020-06-17-python中list的append, extend区别/</id>
    <published>2020-06-17T01:48:53.000Z</published>
    <updated>2020-06-18T02:01:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="0-测试环境"><a href="#0-测试环境" class="headerlink" title="0. 测试环境"></a>0. 测试环境</h2><p>Python 3.6.9，<code>dis</code>库是Python自带的一个库，可以用来分析字节码，而字节码是CPython解释器的实现细节。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在Python中，扩展<code>list</code>的方法有多种，<code>append</code>，<code>extend</code>，<code>+=</code>，<code>+</code>都是列表扩展的方式，但它们的使用又有些许不同，需要根据具体情况来选择，本文主要分析它们的差异。</p>
<h2 id="2-对比与分析"><a href="#2-对比与分析" class="headerlink" title="2. 对比与分析"></a>2. 对比与分析</h2><h3 id="2-1-list的函数方法"><a href="#2-1-list的函数方法" class="headerlink" title="2.1 list的函数方法"></a>2.1 <code>list</code>的函数方法</h3><ul>
<li>list.append(x)</li>
</ul>
<p><code>append</code>方法会将<code>x</code>作为<code>list</code>的一项添加到末尾。等价于<code>a[len(a):] = [x]</code>。</p>
<ul>
<li>list.extend(iterable)</li>
</ul>
<p><code>extend</code>方法会将后面的可迭代对象的所有项添加到列表中。</p>
<h3 id="2-2-代码测试"><a href="#2-2-代码测试" class="headerlink" title="2.2 代码测试"></a>2.2 代码测试</h3><ul>
<li>Test Case 1</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Code</span></div><div class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line">b = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">a += b</div><div class="line">print(a)</div><div class="line"></div><div class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line">b = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">a.append(b)</div><div class="line">print(a)</div><div class="line"></div><div class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line">b = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">a.extend(b)</div><div class="line">print(a)</div><div class="line"></div><div class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line">b = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">c = a + b</div><div class="line">print(a)</div><div class="line">print(c)</div><div class="line"></div><div class="line"></div><div class="line">a = [<span class="string">'ab'</span>, <span class="string">'cd'</span>]</div><div class="line">b = <span class="string">'ef'</span></div><div class="line">a += b</div><div class="line">print(a)</div><div class="line"></div><div class="line">a = [<span class="string">'ab'</span>, <span class="string">'cd'</span>]</div><div class="line">b = <span class="string">'ef'</span></div><div class="line">a.append(b)</div><div class="line">print(a)</div><div class="line"></div><div class="line">a = [<span class="string">'ab'</span>, <span class="string">'cd'</span>]</div><div class="line">b = <span class="string">'ef'</span></div><div class="line">a.extend(b)</div><div class="line">print(a)</div><div class="line"></div><div class="line">a = [<span class="string">'ab'</span>, <span class="string">'cd'</span>]</div><div class="line">b = <span class="string">'ef'</span></div><div class="line">c = a + b</div><div class="line">print(a)</div><div class="line">print(c)</div><div class="line"></div><div class="line"><span class="comment"># Output</span></div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">[<span class="string">'ab'</span>, <span class="string">'cd'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>]</div><div class="line">[<span class="string">'ab'</span>, <span class="string">'cd'</span>, <span class="string">'ef'</span>]</div><div class="line">[<span class="string">'ab'</span>, <span class="string">'cd'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>]</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"list_test.py"</span>, line <span class="number">40</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    c = a + b</div><div class="line">TypeError: can only concatenate list (<span class="keyword">not</span> <span class="string">"str"</span>) to list</div></pre></td></tr></table></figure>
<p>从输出结果来看，<code>extend</code>与<code>+=</code>是等价的，会扩展原有的列表，<code>+</code>只能用来连接列表，且不改变原有的列表，会返回一个新列表，<code>append</code>会往原有列表中添加一个新的元素。</p>
<ul>
<li>Test Case 2</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Code</span></div><div class="line"><span class="keyword">import</span> dis</div><div class="line"></div><div class="line">a = [<span class="string">'ab'</span>, <span class="string">'cd'</span>]</div><div class="line">b = <span class="string">'ef'</span></div><div class="line">print(<span class="string">'Test +'</span>)</div><div class="line">dis.dis(<span class="keyword">lambda</span> : a + b)</div><div class="line">a = [<span class="string">'ab'</span>, <span class="string">'cd'</span>]</div><div class="line">b = <span class="string">'ef'</span></div><div class="line">print(<span class="string">'Test extend'</span>)</div><div class="line">dis.dis(<span class="keyword">lambda</span> : a.extend(b))</div><div class="line">a = [<span class="string">'ab'</span>, <span class="string">'cd'</span>]</div><div class="line">b = <span class="string">'ef'</span></div><div class="line">print(<span class="string">'Test append'</span>)</div><div class="line">dis.dis(<span class="keyword">lambda</span> : a.append(b))</div><div class="line"></div><div class="line"></div><div class="line">a = [<span class="string">'ab'</span>, <span class="string">'cd'</span>]</div><div class="line">b = <span class="string">'ef'</span></div><div class="line">print(<span class="string">'Test +='</span>)</div><div class="line"><span class="comment">#dis.dis(lambda : a += b)</span></div><div class="line"></div><div class="line">print(<span class="string">'Test extend'</span>)</div><div class="line">dis.dis(compile(<span class="string">"s = []; s.extend('abc')"</span>, <span class="string">''</span>, <span class="string">'exec'</span>))</div><div class="line">print(<span class="string">'Test +='</span>)</div><div class="line">dis.dis(compile(<span class="string">"s = []; s += 'abc'"</span>, <span class="string">''</span>, <span class="string">'exec'</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Ouput</span></div><div class="line">Test +</div><div class="line">  <span class="number">6</span>           <span class="number">0</span> LOAD_GLOBAL              <span class="number">0</span> (a)</div><div class="line">              <span class="number">2</span> LOAD_GLOBAL              <span class="number">1</span> (b)</div><div class="line">              <span class="number">4</span> BINARY_ADD</div><div class="line">              <span class="number">6</span> RETURN_VALUE</div><div class="line">Test extend</div><div class="line"> <span class="number">10</span>           <span class="number">0</span> LOAD_GLOBAL              <span class="number">0</span> (a)</div><div class="line">              <span class="number">2</span> LOAD_ATTR                <span class="number">1</span> (extend)</div><div class="line">              <span class="number">4</span> LOAD_GLOBAL              <span class="number">2</span> (b)</div><div class="line">              <span class="number">6</span> CALL_FUNCTION            <span class="number">1</span></div><div class="line">              <span class="number">8</span> RETURN_VALUE</div><div class="line">Test append</div><div class="line"> <span class="number">14</span>           <span class="number">0</span> LOAD_GLOBAL              <span class="number">0</span> (a)</div><div class="line">              <span class="number">2</span> LOAD_ATTR                <span class="number">1</span> (append)</div><div class="line">              <span class="number">4</span> LOAD_GLOBAL              <span class="number">2</span> (b)</div><div class="line">              <span class="number">6</span> CALL_FUNCTION            <span class="number">1</span></div><div class="line">              <span class="number">8</span> RETURN_VALUE</div><div class="line">Test +=</div><div class="line">Test extend</div><div class="line">  <span class="number">1</span>           <span class="number">0</span> BUILD_LIST               <span class="number">0</span></div><div class="line">              <span class="number">2</span> STORE_NAME               <span class="number">0</span> (s)</div><div class="line">              <span class="number">4</span> LOAD_NAME                <span class="number">0</span> (s)</div><div class="line">              <span class="number">6</span> LOAD_ATTR                <span class="number">1</span> (extend)</div><div class="line">              <span class="number">8</span> LOAD_CONST               <span class="number">0</span> (<span class="string">'abc'</span>)</div><div class="line">             <span class="number">10</span> CALL_FUNCTION            <span class="number">1</span></div><div class="line">             <span class="number">12</span> POP_TOP</div><div class="line">             <span class="number">14</span> LOAD_CONST               <span class="number">1</span> (<span class="keyword">None</span>)</div><div class="line">             <span class="number">16</span> RETURN_VALUE</div><div class="line">Test +=</div><div class="line">  <span class="number">1</span>           <span class="number">0</span> BUILD_LIST               <span class="number">0</span></div><div class="line">              <span class="number">2</span> STORE_NAME               <span class="number">0</span> (s)</div><div class="line">              <span class="number">4</span> LOAD_NAME                <span class="number">0</span> (s)</div><div class="line">              <span class="number">6</span> LOAD_CONST               <span class="number">0</span> (<span class="string">'abc'</span>)</div><div class="line">              <span class="number">8</span> INPLACE_ADD</div><div class="line">             <span class="number">10</span> STORE_NAME               <span class="number">0</span> (s)</div><div class="line">             <span class="number">12</span> LOAD_CONST               <span class="number">1</span> (<span class="keyword">None</span>)</div><div class="line">             <span class="number">14</span> RETURN_VALUE</div><div class="line"></div><div class="line"><span class="comment"># Errors</span></div><div class="line">  File <span class="string">"dis_test.py"</span>, line <span class="number">20</span></div><div class="line">    dis.dis(<span class="keyword">lambda</span> : a += b)</div><div class="line">                        ^</div><div class="line">SyntaxError: invalid syntax</div></pre></td></tr></table></figure>
<p>从输出结果来看，<code>+</code>，<code>+=</code>操作不会进行函数调用，而<code>extend</code>、<code>append</code>执行过程中会进行函数调用，当不注释<code>dis.dis(lambda : a += b)</code>时，执行会报错，虽然<code>extend</code>效果与<code>+=</code>是等价的，但<code>+=</code>在函数中不能使用非局部变量，而<code>extend</code>方法可以。</p>
<ul>
<li>Test case 3</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Code</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.a</div><div class="line"></div><div class="line">b = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">temp = Test()</div><div class="line">print(<span class="string">'Before extend'</span>)</div><div class="line">print(temp.a)</div><div class="line">temp.get().extend(b)</div><div class="line">print(<span class="string">'After extend'</span>)</div><div class="line">print(temp.a)</div><div class="line"></div><div class="line">print(<span class="string">'+= ok'</span>)</div><div class="line">print(<span class="string">'Before +='</span>)</div><div class="line">print(temp.a)</div><div class="line">temp.a += b</div><div class="line">print(<span class="string">'After +='</span>)</div><div class="line">print(temp.a)</div><div class="line"></div><div class="line">print(<span class="string">'+= error'</span>)</div><div class="line"><span class="comment">#temp.get() += b</span></div><div class="line"></div><div class="line"><span class="comment"># Ouput</span></div><div class="line">Before extend</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line">After extend</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">+= ok</div><div class="line">Before +=</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">After +=</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div><div class="line">+= error</div><div class="line"></div><div class="line"><span class="comment"># Error</span></div><div class="line">    temp.get() += b</div><div class="line">    ^</div><div class="line">SyntaxError: can<span class="string">'t assign to function call</span></div></pre></td></tr></table></figure>
<p>上面这个例子是对<code>+=</code>与<code>extend</code>使用范围的对比。</p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><ul>
<li><code>extend</code>效果与<code>+=</code>是等价的，主要差异在于字节码执行的方式不同，<code>extend</code>方法涉及了函数调用，开销更大一些。<code>extend</code>比<code>+=</code>应用范围更广，某些情况下只能使用<code>extend</code>。</li>
<li><code>+=</code>会将后面的数据添加到原有的列表中，而<code>+</code>会返回一个新的列表，不改变原有列表。<code>+</code>只能连接列表。</li>
<li><code>append</code>方式会将参数作为列表的一项添加到原有的列表中。</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p><a href="https://stackoverflow.com/questions/725782/in-python-what-is-the-difference-between-append-and/725882" target="_blank" rel="external">https://stackoverflow.com/questions/725782/in-python-what-is-the-difference-between-append-and/725882</a></p>
</li>
<li><p><a href="https://stackoverflow.com/questions/252703/what-is-the-difference-between-pythons-list-methods-append-and-extend" target="_blank" rel="external">https://stackoverflow.com/questions/252703/what-is-the-difference-between-pythons-list-methods-append-and-extend</a></p>
</li>
<li><p><a href="https://docs.python.org/3.6/tutorial/datastructures.html" target="_blank" rel="external">https://docs.python.org/3.6/tutorial/datastructures.html</a></p>
</li>
<li><p><a href="https://stackoverflow.com/questions/3653298/concatenating-two-lists-difference-between-and-extend" target="_blank" rel="external">https://stackoverflow.com/questions/3653298/concatenating-two-lists-difference-between-and-extend</a></p>
</li>
<li><p><a href="https://stackoverflow.com/questions/39689099/can-someone-explain-this-expression-alena-x-equivalent-to-list-append" target="_blank" rel="external">https://stackoverflow.com/questions/39689099/can-someone-explain-this-expression-alena-x-equivalent-to-list-append</a></p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python中list的append, extend, +=, +区别
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>玩转pytorch中的torchvision.transforms</title>
    <link href="http://noahsnail.com/2020/06/12/2020-06-12-%E7%8E%A9%E8%BD%ACpytorch%E4%B8%AD%E7%9A%84torchvision.transforms/"/>
    <id>http://noahsnail.com/2020/06/12/2020-06-12-玩转pytorch中的torchvision.transforms/</id>
    <published>2020-06-12T01:54:25.000Z</published>
    <updated>2020-06-15T08:55:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="0-运行环境"><a href="#0-运行环境" class="headerlink" title="0. 运行环境"></a>0. 运行环境</h2><p>python 3.6.8, pytorch 1.5.0</p>
<h2 id="1-torchvision-transforms"><a href="#1-torchvision-transforms" class="headerlink" title="1. torchvision.transforms"></a>1. torchvision.transforms</h2><p>在深度学习中，计算机视觉(CV)是其中的一大方向，而在CV任务中，图像变换(Image Transform)通常是必不可少的一环，其可以用来对图像进行预处理，数据增强等。本文主要整理PyTorch中<code>torchvision.transforms</code>提供的一些功能(代码加示例)。具体定义及参数可参考<a href="https://pytorch.org/docs/stable/torchvision/transforms.html" target="_blank" rel="external">PyTorch文档</a>。</p>
<h3 id="1-1-torchvision-transforms-Compose"><a href="#1-1-torchvision-transforms-Compose" class="headerlink" title="1.1 torchvision.transforms.Compose"></a>1.1 torchvision.transforms.Compose</h3><p><code>Compose</code>的主要作用是将多个变换组合在一起，具体用法可参考2.5。下面的示例结果左边为原图，右边为保存的结果。</p>
<h2 id="2-Transforms-on-PIL-Image"><a href="#2-Transforms-on-PIL-Image" class="headerlink" title="2. Transforms on PIL Image"></a>2. Transforms on PIL Image</h2><p>这部分主要是对Python最常用的图像处理库Pillow中Image的处理。基本环境及图像如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</div><div class="line"></div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"></div><div class="line">img = Image.open(<span class="string">'tina.jpg'</span>)</div><div class="line"></div><div class="line">...</div><div class="line"></div><div class="line"><span class="comment"># Save image</span></div><div class="line">img.save(<span class="string">'image.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/tina.jpg" alt="Demo"></p>
<h3 id="2-1-torchvision-transforms-CenterCrop-size"><a href="#2-1-torchvision-transforms-CenterCrop-size" class="headerlink" title="2.1 torchvision.transforms.CenterCrop(size)"></a>2.1 torchvision.transforms.CenterCrop(size)</h3><p><code>CenterCrop</code>的作用是从图像的中心位置裁剪指定大小的图像。例如一些神经网络的输入图像大小为<code>224*224</code>，而训练图像的大小为<code>256*256</code>，此时就需要对训练图像进行裁剪。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">224</span>, <span class="number">224</span>)</div><div class="line">transform = transforms.CenterCrop(size)</div><div class="line">center_crop = transform(img)</div><div class="line">center_crop.save(<span class="string">'center_crop.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/center_crop.jpg" alt="CenterCrop"></p>
<h3 id="2-2-torchvision-transforms-ColorJitter-brightness-0-contrast-0-saturation-0-hue-0"><a href="#2-2-torchvision-transforms-ColorJitter-brightness-0-contrast-0-saturation-0-hue-0" class="headerlink" title="2.2 torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)"></a>2.2 torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)</h3><p><code>ColorJitter</code>的作用是随机修改图片的亮度、对比度和饱和度，常用来进行数据增强，尤其是训练图像类别不均衡或图像数量较少时。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">brightness = (<span class="number">1</span>, <span class="number">10</span>)</div><div class="line">contrast = (<span class="number">1</span>, <span class="number">10</span>)</div><div class="line">saturation = (<span class="number">1</span>, <span class="number">10</span>)</div><div class="line">hue = (<span class="number">0.2</span>, <span class="number">0.4</span>)</div><div class="line">transform = transforms.ColorJitter(brightness, contrast, saturation, hue)</div><div class="line">color_jitter = transform(img)</div><div class="line">color_jitter.save(<span class="string">'color_jitter.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/color_jitter.jpg" alt="ColorJitter"></p>
<h3 id="2-3-torchvision-transforms-FiveCrop-size"><a href="#2-3-torchvision-transforms-FiveCrop-size" class="headerlink" title="2.3 torchvision.transforms.FiveCrop(size)"></a>2.3 torchvision.transforms.FiveCrop(size)</h3><p><code>FiveCrop</code>的作用是分别从图像的四个角以及中心进行五次裁剪，图像分类评估时分为<code>Singl Crop Evaluation/Test</code>和<code>Multi Crop Evaluation/Test</code>，<code>FiveCrop</code>可以用在<code>Multi Crop Evaluation/Test</code>中。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">224</span>, <span class="number">224</span>)</div><div class="line">transform = transforms.FiveCrop(size)</div><div class="line">five_crop = transform(img)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/five_crop.jpg" alt="FiveCrop"></p>
<h3 id="2-4-torchvision-transforms-Grayscale-num-output-channels-1"><a href="#2-4-torchvision-transforms-Grayscale-num-output-channels-1" class="headerlink" title="2.4 torchvision.transforms.Grayscale(num_output_channels=1)"></a>2.4 torchvision.transforms.Grayscale(num_output_channels=1)</h3><p><code>Grayscale</code>的作用是将图像转换为灰度图像，默认通道数为1，通道数为3时，RGB三个通道的值相等。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">transform = transforms.Grayscale()</div><div class="line">grayscale = transform(img)</div><div class="line">grayscale.save(<span class="string">'grayscale.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/grayscale.jpg" alt="Grayscale"></p>
<h3 id="2-5-torchvision-transforms-Pad-padding-fill-0-padding-mode-’constant’"><a href="#2-5-torchvision-transforms-Pad-padding-fill-0-padding-mode-’constant’" class="headerlink" title="2.5 torchvision.transforms.Pad(padding, fill=0, padding_mode=’constant’)"></a>2.5 torchvision.transforms.Pad(padding, fill=0, padding_mode=’constant’)</h3><p><code>Pad</code>的作用是对图像进行填充，可以设置要填充的值及填充的大小，默认是图像四边都填充。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">224</span>, <span class="number">224</span>)</div><div class="line">padding = <span class="number">16</span></div><div class="line">fill = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">transform = transforms.Compose([</div><div class="line">        transforms.CenterCrop(size),</div><div class="line">        transforms.Pad(padding, fill)</div><div class="line">])</div><div class="line">pad = transform(img)</div><div class="line">pad.save(<span class="string">'pad.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/pad.jpg" alt="Pad"></p>
<h3 id="2-6-torchvision-transforms-RandomAffine-degrees-translate-None-scale-None-shear-None-resample-False-fillcolor-0"><a href="#2-6-torchvision-transforms-RandomAffine-degrees-translate-None-scale-None-shear-None-resample-False-fillcolor-0" class="headerlink" title="2.6 torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)"></a>2.6 torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)</h3><p><code>RandomAffine</code>的作用是保持图像中心不变的情况下对图像进行随机的仿射变换。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">degrees = (<span class="number">15</span>, <span class="number">30</span>)</div><div class="line">translate=(<span class="number">0</span>, <span class="number">0.2</span>)</div><div class="line">scale=(<span class="number">0.8</span>, <span class="number">1</span>)</div><div class="line">fillcolor = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">transform = transforms.RandomAffine(degrees=degrees, translate=translate, scale=scale, fillcolor=fillcolor)</div><div class="line">random_affine = transform(img)</div><div class="line">random_affine.save(<span class="string">'random_affine.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_affine.jpg" alt="RandomAffine"></p>
<h3 id="2-7-torchvision-transforms-RandomApply-transforms-p-0-5"><a href="#2-7-torchvision-transforms-RandomApply-transforms-p-0-5" class="headerlink" title="2.7 torchvision.transforms.RandomApply(transforms, p=0.5)"></a>2.7 torchvision.transforms.RandomApply(transforms, p=0.5)</h3><p><code>RandomApply</code>的作用是以一定的概率执行提供的<code>transforms</code>操作，即可能执行，也可能不执行。<code>transforms</code>可以是一个，也可以是一系列。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">224</span>, <span class="number">224</span>)</div><div class="line">padding = <span class="number">16</span></div><div class="line">fill = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">transform = transforms.RandomApply([transforms.CenterCrop(size), transforms.Pad(padding, fill)])</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">    random_apply = transform(img)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_apply.jpg" alt="RandomApply"></p>
<h3 id="2-8-torchvision-transforms-RandomChoice-transforms"><a href="#2-8-torchvision-transforms-RandomChoice-transforms" class="headerlink" title="2.8 torchvision.transforms.RandomChoice(transforms)"></a>2.8 torchvision.transforms.RandomChoice(transforms)</h3><p><code>RandomChoice</code>的作用是从提供的<code>transforms</code>操作中随机选择一个执行。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">224</span>, <span class="number">224</span>)</div><div class="line">padding = <span class="number">16</span></div><div class="line">fill = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">degrees = (<span class="number">15</span>, <span class="number">30</span>)</div><div class="line">transform = transforms.RandomChoice([transforms.RandomAffine(degrees), transforms.CenterCrop(size), transforms.Pad(padding, fill)])</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">    random_choice = transform(img)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_choice.jpg" alt="RandomChoice"></p>
<h3 id="2-9-torchvision-transforms-RandomCrop-size-padding-None-pad-if-needed-False-fill-0-padding-mode-’constant’"><a href="#2-9-torchvision-transforms-RandomCrop-size-padding-None-pad-if-needed-False-fill-0-padding-mode-’constant’" class="headerlink" title="2.9 torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=’constant’)"></a>2.9 torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=’constant’)</h3><p><code>RandomCrop</code>的作用是在一个随机位置上对图像进行裁剪。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">224</span>, <span class="number">224</span>)</div><div class="line">transform = transforms.RandomCrop(size)</div><div class="line">random_crop = transform(img)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_crop.jpg" alt="RandomCrop"></p>
<h3 id="2-10-torchvision-transforms-RandomGrayscale-p-0-1"><a href="#2-10-torchvision-transforms-RandomGrayscale-p-0-1" class="headerlink" title="2.10 torchvision.transforms.RandomGrayscale(p=0.1)"></a>2.10 torchvision.transforms.RandomGrayscale(p=0.1)</h3><p><code>RandomGrayscale</code>的作用是以一定的概率将图像变为灰度图像。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">0.5</span></div><div class="line">transform = transforms.RandomGrayscale(p)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">    random_grayscale = transform(img)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_grayscale.jpg" alt="RandomGrayscale"></p>
<h3 id="2-11-torchvision-transforms-RandomHorizontalFlip-p-0-5"><a href="#2-11-torchvision-transforms-RandomHorizontalFlip-p-0-5" class="headerlink" title="2.11 torchvision.transforms.RandomHorizontalFlip(p=0.5)"></a>2.11 torchvision.transforms.RandomHorizontalFlip(p=0.5)</h3><p><code>RandomHorizontalFlip</code>的作用是以一定的概率对图像进行水平翻转。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">0.5</span></div><div class="line">transform = transforms.RandomHorizontalFlip(p)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">    random_horizontal_filp = transform(img)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_horizontal_filp.jpg" alt="RandomHorizontalFlip"></p>
<h3 id="2-12-torchvision-transforms-RandomOrder-transforms"><a href="#2-12-torchvision-transforms-RandomOrder-transforms" class="headerlink" title="2.12 torchvision.transforms.RandomOrder(transforms)"></a>2.12 torchvision.transforms.RandomOrder(transforms)</h3><p><code>RandomOrder</code>的作用是以随机顺序执行提供的<code>transforms</code>操作。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">224</span>, <span class="number">224</span>)</div><div class="line">padding = <span class="number">16</span></div><div class="line">fill = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">degrees = (<span class="number">15</span>, <span class="number">30</span>)</div><div class="line">transform = transforms.RandomOrder([transforms.RandomAffine(degrees), transforms.CenterCrop(size), transforms.Pad(padding, fill)])</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">    random_order = transform(img)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_order.jpg" alt="RandomOrder"></p>
<h3 id="2-13-torchvision-transforms-RandomPerspective-distortion-scale-0-5-p-0-5-interpolation-3-fill-0"><a href="#2-13-torchvision-transforms-RandomPerspective-distortion-scale-0-5-p-0-5-interpolation-3-fill-0" class="headerlink" title="2.13 torchvision.transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0)"></a>2.13 torchvision.transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0)</h3><p><code>RandomPerspective</code>的作用是以一定的概率对图像进行随机的透视变换。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">distortion_scale = <span class="number">0.5</span></div><div class="line">p = <span class="number">1</span></div><div class="line">fill = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">transform = transforms.RandomPerspective(distortion_scale=distortion_scale, p=p, fill=fill)</div><div class="line">random_perspective = transform(img)</div><div class="line">random_perspective.save(<span class="string">'random_perspective.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_perspective.jpg" alt="RandomPerspective"></p>
<h3 id="2-14-torchvision-transforms-RandomResizedCrop-size-scale-0-08-1-0-ratio-0-75-1-3333333333333333-interpolation-2"><a href="#2-14-torchvision-transforms-RandomResizedCrop-size-scale-0-08-1-0-ratio-0-75-1-3333333333333333-interpolation-2" class="headerlink" title="2.14 torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)"></a>2.14 torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)</h3><p><code>RandomResizedCrop</code>的作用是以随机大小和随机长宽比裁剪图像并缩放到指定的大小。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">256</span>, <span class="number">256</span>)</div><div class="line">scale=(<span class="number">0.8</span>, <span class="number">1.0</span>)</div><div class="line">ratio=(<span class="number">0.75</span>, <span class="number">1.0</span>)</div><div class="line">transform = transforms.RandomResizedCrop(size=size, scale=scale, ratio=ratio)</div><div class="line">random_resized_crop = transform(img)</div><div class="line">random_resized_crop.save(<span class="string">'random_resized_crop.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_resized_crop.jpg" alt="RandomResizedCrop"></p>
<h3 id="2-15-torchvision-transforms-RandomRotation-degrees-resample-False-expand-False-center-None-fill-None"><a href="#2-15-torchvision-transforms-RandomRotation-degrees-resample-False-expand-False-center-None-fill-None" class="headerlink" title="2.15 torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None, fill=None)"></a>2.15 torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None, fill=None)</h3><p><code>RandomRotation</code>的作用是对图像进行随机旋转。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">degrees = (<span class="number">15</span>, <span class="number">30</span>)</div><div class="line">fill = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">transform = transforms.RandomRotation(degrees=degrees, fill=fill)</div><div class="line">random_rotation = transform(img)</div><div class="line">random_rotation.save(<span class="string">'random_rotation.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_rotation.jpg" alt="RandomRotation"></p>
<h3 id="2-16-torchvision-transforms-RandomSizedCrop-args-kwargs"><a href="#2-16-torchvision-transforms-RandomSizedCrop-args-kwargs" class="headerlink" title="2.16 torchvision.transforms.RandomSizedCrop(args, *kwargs)"></a>2.16 torchvision.transforms.RandomSizedCrop(<em>args, *</em>kwargs)</h3><p>已废弃，参见<code>RandomResizedCrop</code>。</p>
<h3 id="2-17-torchvision-transforms-RandomVerticalFlip-p-0-5"><a href="#2-17-torchvision-transforms-RandomVerticalFlip-p-0-5" class="headerlink" title="2.17 torchvision.transforms.RandomVerticalFlip(p=0.5)"></a>2.17 torchvision.transforms.RandomVerticalFlip(p=0.5)</h3><p><code>RandomVerticalFlip</code>的作用是以一定的概率对图像进行垂直翻转。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">1</span></div><div class="line">transform = transforms.RandomVerticalFlip(p)</div><div class="line">random_vertical_filp = transform(img)</div><div class="line">random_vertical_filp.save(<span class="string">'random_vertical_filp.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_vertical_filp.jpg" alt="RandomVerticalFlip"></p>
<h3 id="2-18-torchvision-transforms-Resize-size-interpolation-2"><a href="#2-18-torchvision-transforms-Resize-size-interpolation-2" class="headerlink" title="2.18 torchvision.transforms.Resize(size, interpolation=2)"></a>2.18 torchvision.transforms.Resize(size, interpolation=2)</h3><p><code>Resize</code>的作用是对图像进行缩放。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">size = (<span class="number">224</span>, <span class="number">224</span>)</div><div class="line">transform = transforms.Resize(size)</div><div class="line">resize_img = transform(img)</div><div class="line">resize_img.save(<span class="string">'resize_img.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/resize_img.jpg" alt="Resize"></p>
<h3 id="2-19-torchvision-transforms-Scale-args-kwargs"><a href="#2-19-torchvision-transforms-Scale-args-kwargs" class="headerlink" title="2.19 torchvision.transforms.Scale(args, *kwargs)"></a>2.19 torchvision.transforms.Scale(<em>args, *</em>kwargs)</h3><p>已废弃，参加<code>Resize</code>。</p>
<h3 id="2-20-torchvision-transforms-TenCrop-size-vertical-flip-False"><a href="#2-20-torchvision-transforms-TenCrop-size-vertical-flip-False" class="headerlink" title="2.20 torchvision.transforms.TenCrop(size, vertical_flip=False)"></a>2.20 torchvision.transforms.TenCrop(size, vertical_flip=False)</h3><p><code>TenCrop</code>与2.3类似，除了对原图裁剪5个图像之外，还对其翻转图像裁剪了5个图像。</p>
<h2 id="3-Transforms-on-torch-Tensor"><a href="#3-Transforms-on-torch-Tensor" class="headerlink" title="3. Transforms on torch.*Tensor"></a>3. Transforms on torch.*Tensor</h2><h3 id="3-1-torchvision-transforms-LinearTransformation-transformation-matrix-mean-vector"><a href="#3-1-torchvision-transforms-LinearTransformation-transformation-matrix-mean-vector" class="headerlink" title="3.1 torchvision.transforms.LinearTransformation(transformation_matrix, mean_vector)"></a>3.1 torchvision.transforms.LinearTransformation(transformation_matrix, mean_vector)</h3><p><code>LinearTransformation</code>的作用是使用变换矩阵和离线计算的均值向量对图像张量进行变换，可以用在白化变换中，白化变换用来去除输入数据的冗余信息。常用在数据预处理中。</p>
<h3 id="3-2-torchvision-transforms-Normalize-mean-std-inplace-False"><a href="#3-2-torchvision-transforms-Normalize-mean-std-inplace-False" class="headerlink" title="3.2 torchvision.transforms.Normalize(mean, std, inplace=False)"></a>3.2 torchvision.transforms.Normalize(mean, std, inplace=False)</h3><p><code>Normalize</code>的作用是用均值和标准差对<code>Tensor</code>进行归一化处理。常用在对输入图像的预处理中，例如Imagenet竞赛的许多分类网络都对输入图像进行了归一化操作。</p>
<h3 id="3-3-torchvision-transforms-RandomErasing-p-0-5-scale-0-02-0-33-ratio-0-3-3-3-value-0-inplace-False"><a href="#3-3-torchvision-transforms-RandomErasing-p-0-5-scale-0-02-0-33-ratio-0-3-3-3-value-0-inplace-False" class="headerlink" title="3.3 torchvision.transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)"></a>3.3 torchvision.transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)</h3><p><code>RandomErasing</code>的作用是随机选择图像中的一块区域，擦除其像素，主要用来进行数据增强。示例代码及结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">1.0</span></div><div class="line">scale = (<span class="number">0.2</span>, <span class="number">0.3</span>)</div><div class="line">ratio = (<span class="number">0.5</span>, <span class="number">1.0</span>)</div><div class="line">value = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line"></div><div class="line">transform = transforms.Compose([</div><div class="line">                transforms.ToTensor(),</div><div class="line">                transforms.RandomErasing(p=p, scale=scale, ratio=ratio, value=value),</div><div class="line">                transforms.ToPILImage()</div><div class="line">            ])</div><div class="line">random_erasing = transform(img)</div><div class="line">random_erasing.save(<span class="string">'random_erasing.jpg'</span>)</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/random_erasing.jpg" alt="RandomErasing"></p>
<h2 id="4-Conversion-Transforms"><a href="#4-Conversion-Transforms" class="headerlink" title="4 Conversion Transforms"></a>4 Conversion Transforms</h2><h3 id="4-1-torchvision-transforms-ToPILImage-mode-None"><a href="#4-1-torchvision-transforms-ToPILImage-mode-None" class="headerlink" title="4.1 torchvision.transforms.ToPILImage(mode=None)"></a>4.1 torchvision.transforms.ToPILImage(mode=None)</h3><p><code>ToPILImage</code>的作用是将pytorch的<code>Tensor</code>或<code>numpy.ndarray</code>转为PIL的Image。示例代码及结果如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">img = Image.open('tina.jpg')</div><div class="line">transform = transforms.ToTensor()</div><div class="line">img = transform(img)</div><div class="line">print(img.size())</div><div class="line">img_r = img[0, :, :]</div><div class="line">img_g = img[1, :, :]</div><div class="line">img_b = img[2, :, :]</div><div class="line">print(type(img_r))</div><div class="line">print(img_r.size())</div><div class="line">transform = transforms.ToPILImage()</div><div class="line">img_r = transform(img_r)</div><div class="line">img_g = transform(img_g)</div><div class="line">img_b = transform(img_b)</div><div class="line">print(type(img_r))</div><div class="line">img_r.save('img_r.jpg')</div><div class="line">img_g.save('img_g.jpg')</div><div class="line">img_b.save('img_b.jpg')</div><div class="line"></div><div class="line"># output</div><div class="line">torch.Size([3, 256, 256])</div><div class="line">&lt;class 'torch.Tensor'&gt;</div><div class="line">torch.Size([256, 256])</div><div class="line">&lt;class 'PIL.Image.Image'&gt;</div></pre></td></tr></table></figure>
<p><img src="http://noahsnail.com/images/transform/pil_img.jpg" alt="ToPILImage"></p>
<h3 id="4-2-torchvision-transforms-ToTensor"><a href="#4-2-torchvision-transforms-ToTensor" class="headerlink" title="4.2 torchvision.transforms.ToTensor"></a>4.2 torchvision.transforms.ToTensor</h3><p><code>ToTensor</code>的作用是将<code>PIL Image</code>或<code>numpy.ndarray</code>转为pytorch的<code>Tensor</code>，并会将像素值由<code>[0, 255]</code>变为<code>[0, 1]</code>之间。通常是在神经网络训练中读取输入图像之后使用。示例代码如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">img = Image.open('tina.jpg')</div><div class="line">print(type(img))</div><div class="line">print(img.size)</div><div class="line">transform = transforms.ToTensor()</div><div class="line">img = transform(img)</div><div class="line">print(type(img))</div><div class="line">print(img.size())</div><div class="line"></div><div class="line"># output</div><div class="line">&lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt;</div><div class="line">(256, 256)</div><div class="line">&lt;class 'torch.Tensor'&gt;</div><div class="line">torch.Size([3, 256, 256])</div></pre></td></tr></table></figure>
<h2 id="5-Code"><a href="#5-Code" class="headerlink" title="5. Code"></a>5. Code</h2><p>代码参见<a href="https://github.com/SnailTyan/deep-learning-tools/blob/master/transforms.py" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-tools/blob/master/transforms.py</a>。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html" target="_blank" rel="external">https://pytorch.org/docs/stable/torchvision/transforms.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      玩转pytorch中的torchvision.transforms
    
    </summary>
    
      <category term="PyTorch" scheme="http://noahsnail.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://noahsnail.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Python中的list(), dict(), [], {}</title>
    <link href="http://noahsnail.com/2020/06/03/2020-06-03-Python%E4%B8%AD%E7%9A%84list(),%20dict(),%20%5B%5D,%20%7B%7D/"/>
    <id>http://noahsnail.com/2020/06/03/2020-06-03-Python中的list(), dict(), [], {}/</id>
    <published>2020-06-03T06:50:15.000Z</published>
    <updated>2020-06-03T09:17:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="0-测试环境"><a href="#0-测试环境" class="headerlink" title="0. 测试环境"></a>0. 测试环境</h2><p>Python 3.6.9</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在Python中，<code>list</code>，<code>dict</code>作为Python的基础数据结构，经常会用到，其定义形式通常有下面两种：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">a = []</div><div class="line">b = list()</div><div class="line"></div><div class="line">c = &#123;&#125;</div><div class="line">d = dict()</div></pre></td></tr></table></figure>
<p>二者有什么区别呢？</p>
<h2 id="2-list-vs-，dict-vs"><a href="#2-list-vs-，dict-vs" class="headerlink" title="2. list() vs []，dict() vs {}"></a>2. <code>list()</code> vs <code>[]</code>，<code>dict()</code> vs <code>{}</code></h2><ul>
<li>运行时间</li>
</ul>
<p>首先比较一下二者的运行时间，<code>timeit</code>模块主要用来测量Python小段代码的执行时间，默认执行100万次。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> timeit <span class="keyword">import</span> timeit</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>timeit(<span class="string">'[]'</span>)</div><div class="line"><span class="number">0.05389202758669853</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>timeit(<span class="string">'list()'</span>)</div><div class="line"><span class="number">0.1250211838632822</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>timeit(<span class="string">'&#123;&#125;'</span>)</div><div class="line"><span class="number">0.06583642773330212</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>timeit(<span class="string">'dict()'</span>)</div><div class="line"><span class="number">0.1366278938949108</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>type(&#123;&#125;)</div><div class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">dict</span>'&gt;</span></div></pre></td></tr></table></figure>
<p>从时间上来看，明显<code>[]</code>与<code>{}</code>的定义形式更快。</p>
<ul>
<li>数据类型转换</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b = [a]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>c = list(a)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b</div><div class="line">[(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>c</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="string">'abc'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = [s]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = list(s)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x</div><div class="line">[<span class="string">'abc'</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y</div><div class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</div></pre></td></tr></table></figure>
<p>从上面的代码可以看出，<code>list()</code>除了可以定义之外，还可以对将其它数据类型转换为<code>list</code>，而<code>[]</code>则没有数据类型转换的功能。</p>
<h2 id="3-为什么-比list-更快"><a href="#3-为什么-比list-更快" class="headerlink" title="3. 为什么[]比list()更快"></a>3. 为什么<code>[]</code>比<code>list()</code>更快</h2><p>dis库是Python自带的一个库，可以用来分析字节码，而字节码是CPython解释器的实现细节。<code>[]</code>，<code>list()</code>的字节码对比如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> dis</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>dis.dis(<span class="keyword">lambda</span> : [])</div><div class="line">  <span class="number">1</span>           <span class="number">0</span> BUILD_LIST               <span class="number">0</span></div><div class="line">              <span class="number">2</span> RETURN_VALUE</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>dis.dis(<span class="keyword">lambda</span> : list())</div><div class="line">  <span class="number">1</span>           <span class="number">0</span> LOAD_GLOBAL              <span class="number">0</span> (list)</div><div class="line">              <span class="number">2</span> CALL_FUNCTION            <span class="number">0</span></div><div class="line">              <span class="number">4</span> RETURN_VALUE</div></pre></td></tr></table></figure>
<p>从上面的代码可以看出，<code>list()</code>有符号查找和函数调用的开销，因此其速度更慢。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><p><code>[]</code>，<code>{}</code>定义数据类型速度更快，<code>list()</code>，<code>dict()</code>除了能定义数据类型之外，还可以对数据进行类型转换。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>1.<a href="https://stackoverflow.com/questions/5790860/and-vs-list-and-dict-which-is-better" target="_blank" rel="external">https://stackoverflow.com/questions/5790860/and-vs-list-and-dict-which-is-better</a></p>
<p>2.<a href="https://www.quora.com/In-Python-any-difference-between-using-and-list-or-between-and-dict" target="_blank" rel="external">https://www.quora.com/In-Python-any-difference-between-using-and-list-or-between-and-dict</a></p>
<p>3.<a href="https://docs.python.org/zh-cn/3/library/timeit.html" target="_blank" rel="external">https://docs.python.org/zh-cn/3/library/timeit.html</a></p>
<p>4.<a href="https://docs.python.org/zh-cn/3/library/dis.html" target="_blank" rel="external">https://docs.python.org/zh-cn/3/library/dis.html</a></p>
<p>5.<a href="https://stackoverflow.com/questions/30216000/why-is-faster-than-list" target="_blank" rel="external">https://stackoverflow.com/questions/30216000/why-is-faster-than-list</a></p>
]]></content>
    
    <summary type="html">
    
      Python中的list(), dict(), [], {}
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中requires_grad_(), detach(), torch.no_grad()的区别</title>
    <link href="http://noahsnail.com/2020/05/29/2020-05-29-Pytorch%E4%B8%ADrequires_grad_(),%20detach(),%20torch.no_grad()%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://noahsnail.com/2020/05/29/2020-05-29-Pytorch中requires_grad_(), detach(), torch.no_grad()的区别/</id>
    <published>2020-05-29T02:44:02.000Z</published>
    <updated>2020-05-29T08:58:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="0-测试环境"><a href="#0-测试环境" class="headerlink" title="0. 测试环境"></a>0. 测试环境</h2><p>Python 3.6.9, Pytorch 1.5.0</p>
<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><p><code>Tensor</code>是一个多维矩阵，其中包含所有的元素为同一数据类型。默认数据类型为<code>torch.float32</code>。</p>
<ul>
<li>示例一</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = torch.tensor([1.0])</div><div class="line">&gt;&gt;&gt; a.data</div><div class="line">tensor([1.])</div><div class="line">&gt;&gt;&gt; a.grad</div><div class="line">&gt;&gt;&gt; a.requires_grad</div><div class="line">False</div><div class="line">&gt;&gt;&gt; a.dtype</div><div class="line">torch.float32</div><div class="line">&gt;&gt;&gt; a.item()</div><div class="line">1.0</div><div class="line">&gt;&gt;&gt; type(a.item())</div><div class="line">&lt;class &apos;float&apos;&gt;</div></pre></td></tr></table></figure>
<p><code>Tensor</code>中只有一个数字时，使用<code>torch.Tensor.item()</code>可以得到一个Python数字。<code>requires_grad</code>为<code>True</code>时，表示需要计算<code>Tensor</code>的梯度。<code>requires_grad=False</code>可以用来冻结部分网络，只更新另一部分网络的参数。</p>
<ul>
<li>示例二</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = torch.tensor([1.0, 2.0])</div><div class="line">&gt;&gt;&gt; b = a.data</div><div class="line">&gt;&gt;&gt; id(b)</div><div class="line">139808984381768</div><div class="line">&gt;&gt;&gt; id(a)</div><div class="line">139811772112328</div><div class="line">&gt;&gt;&gt; b.grad</div><div class="line">&gt;&gt;&gt; a.grad</div><div class="line">&gt;&gt;&gt; b[0] = 5.0</div><div class="line">&gt;&gt;&gt; b</div><div class="line">tensor([5., 2.])</div><div class="line">&gt;&gt;&gt; a</div><div class="line">tensor([5., 2.])</div></pre></td></tr></table></figure>
<p><code>a.data</code>返回的是一个新的<code>Tensor</code>对象<code>b</code>，<code>a, b</code>的<code>id</code>不同，说明二者不是同一个<code>Tensor</code>，但<code>b</code>与<code>a</code>共享数据的存储空间，即二者的数据部分指向同一块内存，因此修改<code>b</code>的元素时，<code>a</code>的元素也对应修改。</p>
<h2 id="2-requiresgrad-与detach"><a href="#2-requiresgrad-与detach" class="headerlink" title="2. requiresgrad()与detach()"></a>2. requires<em>grad</em>()与detach()</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = torch.tensor([1.0, 2.0])</div><div class="line">&gt;&gt;&gt; a.data</div><div class="line">tensor([1., 2.])</div><div class="line">&gt;&gt;&gt; a.grad</div><div class="line">&gt;&gt;&gt; a.requires_grad</div><div class="line">False</div><div class="line">&gt;&gt;&gt; a.requires_grad_()</div><div class="line">tensor([1., 2.], requires_grad=True)</div><div class="line">&gt;&gt;&gt; c = a.pow(2).sum()</div><div class="line">&gt;&gt;&gt; c.backward()</div><div class="line">&gt;&gt;&gt; a.grad</div><div class="line">tensor([2., 4.])</div><div class="line">&gt;&gt;&gt; b = a.detach()</div><div class="line">&gt;&gt;&gt; b.grad</div><div class="line">&gt;&gt;&gt; b.requires_grad</div><div class="line">False</div><div class="line">&gt;&gt;&gt; b</div><div class="line">tensor([1., 2.])</div><div class="line">&gt;&gt;&gt; b[0] = 6</div><div class="line">&gt;&gt;&gt; b</div><div class="line">tensor([6., 2.])</div><div class="line">&gt;&gt;&gt; a</div><div class="line">tensor([6., 2.], requires_grad=True)</div></pre></td></tr></table></figure>
<ul>
<li><code>requires_grad_()</code></li>
</ul>
<p><code>requires_grad_()</code>函数会改变<code>Tensor</code>的<code>requires_grad</code>属性并返回<code>Tensor</code>，修改<code>requires_grad</code>的操作是原位操作(in place)。其默认参数为<code>requires_grad=True</code>。<code>requires_grad=True</code>时，自动求导会记录对<code>Tensor</code>的操作，<code>requires_grad_()</code>的主要用途是告诉自动求导开始记录对<code>Tensor</code>的操作。</p>
<ul>
<li><code>detach()</code></li>
</ul>
<p><code>detach()</code>函数会返回一个新的<code>Tensor</code>对象<code>b</code>，并且新<code>Tensor</code>是与当前的计算图分离的，其<code>requires_grad</code>属性为<code>False</code>，反向传播时不会计算其梯度。<code>b</code>与<code>a</code>共享数据的存储空间，二者指向同一块内存。</p>
<p><strong>注</strong>：共享内存空间只是共享的数据部分，<code>a.grad</code>与<code>b.grad</code>是不同的。</p>
<h2 id="3-torch-no-grad"><a href="#3-torch-no-grad" class="headerlink" title="3. torch.no_grad()"></a>3. torch.no_grad()</h2><p><code>torch.no_grad()</code>是一个上下文管理器，用来禁止梯度的计算，通常用来网络推断中，它可以减少计算内存的使用量。</p>
<figure class="highlight python"><figcaption><span>3</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>], requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.no_grad():</div><div class="line"><span class="meta">... </span>    b = n.pow(<span class="number">2</span>).sum()</div><div class="line">...</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b</div><div class="line">tensor(<span class="number">5.</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>c = a.pow(<span class="number">2</span>).sum()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>c.requires_grad</div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure>
<p>上面的例子中，当<code>a</code>的<code>requires_grad=True</code>时，不使用<code>torch.no_grad()</code>，<code>c.requires_grad</code>为<code>True</code>，使用<code>torch.no_grad()</code>时，<code>b.requires_grad</code>为<code>False</code>，当不需要进行反向传播时（推断）或不需要计算梯度（网络输入）时，<code>requires_grad=True</code>会占用更多的计算资源及存储资源。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><p><code>requires_grad_()</code>会修改<code>Tensor</code>的<code>requires_grad</code>属性。</p>
<p><code>detach()</code>会返回一个与计算图分离的新<code>Tensor</code>，新<code>Tensor</code>不会在反向传播中计算梯度，会在特定场合使用。</p>
<p><code>torch.no_grad()</code>更节省计算资源和存储资源，其作用域范围内的操作不会构建计算图，常用在网络推断中。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://pytorch.org/docs/stable/tensors.html" target="_blank" rel="external">https://pytorch.org/docs/stable/tensors.html</a></li>
<li><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.requires_grad_" target="_blank" rel="external">https://pytorch.org/docs/stable/tensors.html#torch.Tensor.requires<em>grad</em></a></li>
<li><a href="https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach" target="_blank" rel="external">https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach</a></li>
<li><a href="https://pytorch.org/docs/master/generated/torch.no_grad.html" target="_blank" rel="external">https://pytorch.org/docs/master/generated/torch.no_grad.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Pytorch中requires_grad_(), detach(), torch.no_grad()的区别
    
    </summary>
    
      <category term="PyTorch" scheme="http://noahsnail.com/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://noahsnail.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Python的&quot;is None&quot; vs &quot;==None&quot;</title>
    <link href="http://noahsnail.com/2020/05/25/2020-05-25-Python%E7%9A%84is%20None%20vs%20==None/"/>
    <id>http://noahsnail.com/2020/05/25/2020-05-25-Python的is None vs ==None/</id>
    <published>2020-05-25T08:21:15.000Z</published>
    <updated>2020-05-25T09:13:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-is-vs"><a href="#1-is-vs" class="headerlink" title="1. is vs =="></a>1. <code>is</code> vs <code>==</code></h2><p>想要弄清楚<code>is None</code>和<code>==None</code>的区别，首先要清楚<code>==</code>和<code>is</code>的区别。<code>==</code>和<code>is</code>的区别如下：</p>
<ul>
<li><p><code>is</code><br>“is”运算符主要是用来比较两个操作对象的引用是否是同一个，指向的是否是同一块内存，比较的是对象的id。</p>
</li>
<li><p><code>==</code><br><code>==</code>运算符主要是用来比较两个操作对象之间是否相等，比较的是值(Value)相等，默认会调用对象的<code>__eq__()</code>方法。</p>
</li>
</ul>
<p>测试环境为Python 3.6.9，测试如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = [1]</div><div class="line">&gt;&gt;&gt; b = [1]</div><div class="line">&gt;&gt;&gt; c = a</div><div class="line">&gt;&gt;&gt; id(a)</div><div class="line">140466547692424</div><div class="line">&gt;&gt;&gt; id(b)</div><div class="line">140466547695112</div><div class="line">&gt;&gt;&gt; id(c)</div><div class="line">140466547692424</div><div class="line">&gt;&gt;&gt; a == b</div><div class="line">True</div><div class="line">&gt;&gt;&gt; a is b</div><div class="line">False</div><div class="line">&gt;&gt;&gt; a == c</div><div class="line">True</div><div class="line">&gt;&gt;&gt; a is c</div><div class="line">True</div></pre></td></tr></table></figure>
<p>注：id()函数返回对象的唯一标识符，用于获取对象的内存地址。</p>
<h2 id="2-is-None-vs-None"><a href="#2-is-None-vs-None" class="headerlink" title="2. is None vs == None"></a>2. <code>is None</code> vs <code>== None</code></h2><p>清楚了<code>==</code>与<code>is</code>的区别，就知道”==None”是<code>True</code>还是<code>False</code>是由对象的<code>__eq__()</code>方法决定的。测试代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">class Foo:</div><div class="line">    def __eq__(self, other):</div><div class="line">        return True</div><div class="line"></div><div class="line">&gt;&gt;&gt; a = Foo()</div><div class="line">&gt;&gt;&gt; b = None</div><div class="line">&gt;&gt;&gt; a == b</div><div class="line">True</div><div class="line">&gt;&gt;&gt; a is None</div><div class="line">False</div><div class="line">&gt;&gt;&gt; a == b</div><div class="line">True</div><div class="line">&gt;&gt;&gt; a is b</div><div class="line">False</div><div class="line">&gt;&gt;&gt; a == None</div><div class="line">True</div><div class="line">&gt;&gt;&gt; a is None</div><div class="line">False</div><div class="line">&gt;&gt;&gt; b == None</div><div class="line">True</div><div class="line">&gt;&gt;&gt; b is None</div><div class="line">True</div><div class="line">&gt;&gt;&gt; id(a)</div><div class="line">140466547708592</div><div class="line">&gt;&gt;&gt; id(b)</div><div class="line">10306432</div><div class="line">&gt;&gt;&gt; id(None)</div><div class="line">10306432</div></pre></td></tr></table></figure>
<p>在上面的代码中，<code>a</code>与任何对象的<code>==</code>比较都为<code>True</code>。</p>
<p>注：理解<code>is None</code>和<code>== None</code>可以这样写代码测试，但根据PEP 8规范，比较单例时，例如<code>None</code>，应该使用<code>is</code>或<code>is not</code>，不能使用<code>==</code>。</p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>当进行值比较时，使用<code>==</code>，比较内存地址是否相同时使用<code>is</code>。比较<code>None</code>时使用<code>is</code>或<code>is not</code>。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://www.geeksforgeeks.org/difference-operator-python/" target="_blank" rel="external">https://www.geeksforgeeks.org/difference-operator-python/</a></li>
<li><a href="http://jaredgrubb.blogspot.com/2009/04/python-is-none-vs-none.html" target="_blank" rel="external">http://jaredgrubb.blogspot.com/2009/04/python-is-none-vs-none.html</a></li>
<li><a href="https://stackoverflow.com/questions/3257919/what-is-the-difference-between-is-none-and-none" target="_blank" rel="external">https://stackoverflow.com/questions/3257919/what-is-the-difference-between-is-none-and-none</a></li>
<li><a href="https://juejin.im/entry/5a3b62446fb9a0451f311b5c" target="_blank" rel="external">https://juejin.im/entry/5a3b62446fb9a0451f311b5c</a></li>
<li><a href="https://www.cjavapy.com/article/198/" target="_blank" rel="external">https://www.cjavapy.com/article/198/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python的&quot;is None&quot; vs &quot;==None&quot;
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>ESRGAN - Enhanced Super-Resolution Generative Adversarial Networks论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2020/04/13/2020-04-13-ESRGAN%20-%20Enhanced%20Super-Resolution%20Generative%20Adversarial%20Networks%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2020/04/13/2020-04-13-ESRGAN - Enhanced Super-Resolution Generative Adversarial Networks论文翻译——中英文对照/</id>
    <published>2020-04-13T08:49:15.000Z</published>
    <updated>2020-05-22T03:33:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="ESRGAN-Enhanced-Super-Resolution-Generative-Adversarial-Networks"><a href="#ESRGAN-Enhanced-Super-Resolution-Generative-Adversarial-Networks" class="headerlink" title="ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks"></a>ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>The Super-Resolution Generative Adversarial Network (SR-GAN) [1] is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN [2] to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge [3]. The code is available at <a href="https://github.com/xinntao/ESRGAN" target="_blank" rel="external">https://github.com/xinntao/ESRGAN</a>.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>超分辨率生成对抗网络(SR-GAN)[1]是一项开创性的工作，其能够在单图像超分辨率期间生成逼真的纹理。然而，虚幻的细节常常伴随讨厌的伪像。为了进一步增强视觉质量，我们充分研究了SRGAN的三个关键组成部分——网络架构、对抗损失和感知损失，并对每一个都进行了改进以取得增强的SRGAN(ESRGAN)。特别的是，我们引入了没有批归一化的Residual-in-Residual Dense Block(RRDB)作为基本的网络构架单元。此外，我们借鉴了相对GAN[2]中的思想，让判别器预测相对真实性而不是绝对值。最后，我们通过使用激活前的特征改进感知损失，这可以对亮度一致性和纹理复原提供更强的监督。得益于这些改进，相比于SRGAN，提出的ESRGAN一致地取得了更好的视觉质量、更多真实自然的纹理，并在PIRM2018-SR Challenge[3]中获得了第一名。源码地址：<a href="https://github.com/xinntao/ESRGAN" target="_blank" rel="external">https://github.com/xinntao/ESRGAN</a>。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Single image super-resolution (SISR), as a fundamental low-level vision problem, has attracted increasing attention in the research community and AI companies. SISR aims at recovering a high-resolution (HR) image from a single low-resolution (LR) one. Since the pioneer work of SRCNN proposed by Dong et al. [4], deep convolution neural network (CNN) approaches have brought prosperous development. Various network architecture designs and training strategies have continuously improved the SR performance, especially the Peak Signal-toNoise Ratio (PSNR) value [5,6,7,1,8,9,10,11,12]. However, these PSNR-oriented approaches tend to output over-smoothed results without sufficient high-frequency details, since the PSNR metric fundamentally disagrees with the subjective evaluation of human observers [1].</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>作为一个基本的低级视觉问题，单图像超分辨率(SISR)在研究领域和AI公司中引起了越来越多的关注。SISR目标是从一张低分辨率(LR)图像复原出一张高分辨率(HR)图像。从Dong等[4]提出SRCNN的开创性工作以来，深度卷积神经网络(CNN)方法带来了繁荣的发展。各种网络架构设计和训练策略持续地改善SR性能，尤其是峰值信噪比(PSNR)的值[5,6,7,1,8,9,10,11,12]。然而，这些面向PSNR的方法趋向于输出过于平滑的结果，缺少足够的高频细节，因为PSNR度量从根本上与人类观察者的主观评价[1]不符。</p>
<p>Several perceptual-driven methods have been proposed to improve the visual quality of SR results. For instance, perceptual loss [13,14] is proposed to optimize super-resolution model in a feature space instead of pixel space. Generative adversarial network [15] is introduced to SR by [1,16] to encourage the network to favor solutions that look more like natural images. The semantic image prior is further incorporated to improve recovered texture details [17]. One of the milestones in the way pursuing visually pleasing results is SRGAN [1]. The basic model is built with residual blocks [18] and optimized using perceptual loss in a GAN framework. With all these techniques, SRGAN significantly improves the overall visual quality of reconstruction over PSNR-oriented methods.</p>
<p>已经提出了一些感知驱动的方法来改进SR结果的视觉质量。例如，提出感知损失[13,14]来优化在特征空间而不是像素空间中的超分辨率模型。[1,16]引入生成对抗网络[15]到SR中以鼓励网络支持看起来更像自然图像的解。语义图像先验被进一步合并以改善恢复的纹理细节[17]。追寻视觉愉悦效果的方法中的里程碑之一是SRGAN[1]。基本模型是用残差块构建的[18]，并在GAN框架中使用感知损失来进行优化。通过所有这些技术，与面向PSNR的方法相比，SRGAN显著改善了重建的整体视觉质量。</p>
<p>However, there still exists a clear gap between SRGAN results and the ground-truth (GT) images, as shown in Fig. 1. In this study, we revisit the key components of SRGAN and improve the model in three aspects. First, we improve the network structure by introducing the Residual-in-Residual Dense Block (RDDB), which is of higher capacity and easier to train. We also remove Batch Normalization (BN) [19] layers as in [20] and use residual scaling [21,20] and smaller initialization to facilitate training a very deep network. Second, we improve the discriminator using Relativistic average GAN (RaGAN) [2], which learns to judge “whether one image is more realistic than the other” rather than “whether one image is real or fake”. Our experiments show that this improvement helps the generator recover more realistic texture details. Third, we propose an improved perceptual loss by using the VGG features <em>before activation</em> instead of after activation as in SRGAN. We empirically find that the adjusted perceptual loss provides sharper edges and more visually pleasing results, as will be shown in Sec. 4.4. Extensive experiments show that the enhanced SRGAN, termed ESRGAN, consistently outperforms state-of-the-art methods in both sharpness and details (see Fig. 1 and Fig. 7).</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_1.jpeg" alt="Figure 1"></p>
<p>Fig.1: The super-resolution results of ×4 for SRGAN, the proposed ESRGAN and the ground-truth. ESRGAN outperforms SRGAN in sharpness and details.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_7.jpeg" alt="Figure 7"></p>
<p>Fig.7: Qualitative results of ESRGAN. ESRGAN produces more natural textures, e.g., animal fur, building structure and grass texture, and also less unpleasant artifacts, e.g., artifacts in the face by SRGAN.</p>
<p>然而，如图1所示，SRGAN结果与真实(GT)图像之间仍然存在明显的差距。在本研究中，我们重新审视SRGAN的关键组件，并在三个方面改进模型。首先，我们通过引入Residual-in-Residual Dense Block(RDDB)改进网络架构，该结构具有较高的能力且更容易训练。我们像[20]中一样也移除了批归一化(BN)[19]层，使用残差缩放[21,20]和更小的初始化来促进训练一个非常深的网络。其次，我们使用相对平均GAN(RaGAN)[2]来改进判别器，RaGAN学习判断“一张图像是否比另一张更真实”而不是“一张图像时真的还是假的”。我们的实验表明这个改进有助于生成器恢复更多的真实纹理细节。第三，我们提出了一种改进的感知损失，使用激活之前的VGG特征来代替SRGAN中激活之后的VGG特征。从经验上我们发现调整之后的感知损失提供了更清晰的边缘和视觉上更令人满意的结果，如4.4节所示。大量的实验表明增强SRGAN(称为ESRGAN)在清晰度和细节方面都始终优于最新的方法（见图1和图7）。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_1.jpeg" alt="Figure 1"></p>
<p>图1：SRGAN、提出的ESRGAN和实际的4倍超分辨率结果。ESRGAN在清晰度和细节方面优于SRGAN。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_7.jpeg" alt="Figure 7"></p>
<p>图7：ESRGAN的定性结果。ESRGAN生成了更自然的纹理，例如，动物皮毛，建筑物结构和草坪纹理，以及更少的令人不快的伪影，例如SRGAN中脸上的伪影。</p>
<p>We take a variant of ESRGAN to participate in the PIRM-SR Challenge [3]. This challenge is the first SR competition that evaluates the performance in a perceptual-quality aware manner based on [22], where the authors claim that distortion and perceptual quality are at odds with each other. The perceptual quality is judged by the non-reference measures of Ma’s score [23] and NIQE [24], i.e., perceptual index  $=\frac {1} {2}((10−Ma)+NIQE)$. A lower perceptual index represents a better perceptual quality.</p>
<p>我们采用ESRGAN的一个变种来参加PIRM-SR挑战赛[3]。这个挑战是第一个在[22]的基础上以察觉感知质量的方式评估性能的SR竞赛，[22]中作者声称失真和感知质量相互矛盾。感知质量是通过Ma分数[23]和NIQE[24]的非参考度量来判断的，即感知指数$=\frac {1} {2}((10−Ma)+NIQE)$。更低的感知指数表示更好的感知质量。</p>
<p>As shown in Fig. 2, the perception-distortion plane is divided into three regions defined by thresholds on the Root-Mean-Square Error (RMSE), and the algorithm that achieves the lowest perceptual index in each region becomes the regional champion. We mainly focus on region 3 as we aim to bring the perceptual quality to a new high. Thanks to the aforementioned improvements and some other adjustments as discussed in Sec. 4.6, our proposed ESRGAN won the first place in the PIRM-SR Challenge (region 3) with the best perceptual index.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_2.jpeg" alt="Figure 2"></p>
<p>Fig.2: Perception-distortion plane on PIRM self validation dataset. We show the baselines of EDSR [20], RCAN [12] and EnhanceNet [16], and the submitted ESRGAN model. The blue dots are produced by image interpolation.</p>
<p>如图2所示，通过均方根误差(RMSE)的阈值，将感知失真平面分成三个区域，每个区域中取得最低感知指数的算法为区域冠军。我们主要关注区域3，因为我们旨在将感知质量提升到新的高度。由于上述的改进和4.6节中讨论的一些其它调整，我们提出的ESRGAN在PIRM-SR挑战赛（区域3）中以最好的感知指数赢得了第一名。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_2.jpeg" alt="Figure 2"></p>
<p>图2：PIRM自验证集上的感知失真平面。我们展示了EDSR[20]，RCAN[12]，EnhanceNet[16]以及提交的ESRGAN模型的基准线。蓝色的点通过图像插值生成。</p>
<p>In order to balance the visual quality and RMSE/PSNR, we further propose the network interpolation strategy, which could continuously adjust the reconstruction style and smoothness. Another alternative is image interpolation, which directly interpolates images pixel by pixel. We employ this strategy to participate in region 1 and region 2. The network interpolation and image interpolation strategies and their differences are discussed in Sec. 3.4.</p>
<p>为了平衡视觉质量和RMSE/PSNR，我们进一步提出了网络插值策略，其可以持续地调整重建风格和平滑度。另一种替代方案是图像插值，其直接逐像素地插值图像。我们采用这个策略来参加区域1和区域2。网络插值和图像插值策略以及它们的差异在3.4节中讨论。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><p>We focus on deep neural network approaches to solve the SR problem. As a pioneer work, Dong et al. [4,25] propose SRCNN to learn the mapping from LR to HR images in an end-to-end manner, achieving superior performance against previous works. Later on, the field has witnessed a variety of network architectures, such as a deeper network with residual learning [5], Laplacian pyramid structure [6], residual blocks [1], recursive learning [7,8], densely connected network [9], deep back projection [10] and residual dense network [11]. Specifically, Lim et al. [20] propose EDSR model by removing unnecessary BN layers in the residual block and expanding the model size, which achieves significant improvement. Zhang et al. [11] propose to use effective residual dense block in SR, and they further explore a deeper network with channel attention [12], achieving the state-of-the-art PSNR performance. Besides supervised learning, other methods like reinforcement learning [26] and unsupervised learning [27] are also introduced to solve general image restoration problems.</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>我们专注于解决SR问题的深度神经网络方法。作为开创性工作，Dong等[4,25]提出了SRCNN以端到端的方式来学习从LR到SR图像的映射，取得了优于之前工作的性能。后来，这个领域见证了各种网络架构，例如具有残差学习的神经网络[5]，拉普拉斯金字塔结构[6]，残差块[1]，递归学习[7,8]，密集连接网络[9]，深度反向投影[10]和残差密集网络[11]。具体来说，Lim等[20]通过移除残差块中不必要的BN层以及扩展模型尺寸提出了EDSR模型，取得了显著的改善。Zhang等[11]在SR中提出了使用有效的残差密集块，并且他们进一步开发了一个使用通道注意力[12]的更深网络，取得了最佳的PSNR性能。除了监督学习之外，也引入了其它的方法像强化学习[26]以及无监督学习[27]来解决一般的图像复原问题。</p>
<p>Several methods have been proposed to stabilize training a very deep model. For instance, residual path is developed to stabilize the training and improve the performance [18,5,12]. Residual scaling is first employed by Szegedy et al. [21] and also used in EDSR. For general deep networks, He et al. [28] propose a robust initialization method for VGG-style networks without BN. To facilitate training a deeper network, we develop a compact and effective residual-in-residual dense block, which also helps to improve the perceptual quality.</p>
<p>已经提出了一些方法来稳定训练非常深的模型。例如，开发残差路径来稳定训练并改善性能[18,5,12]。Szegedy等[21]首次采用残差缩放，也在EDSR中使用。对于一般的深度网络，He等[28]为没有BN的VGG风格的网络提出了一个鲁棒的初始化方法。为了便于训练更深的网络，我们也开发了一个简洁有效的残差套残差密集块，这有助于改善感知质量。</p>
<p>Perceptual-driven approaches have also been proposed to improve the visual quality of SR results. Based on the idea of being closer to perceptual similarity [29,14] perceptual loss [13] is proposed to enhance the visual quality by minimizing the error in a feature space instead of pixel space. Contextual loss [30] is developed to generate images with natural image statistics by using an objective that focuses on the feature distribution rather than merely comparing the appearance. Ledig et al. [1] propose SRGAN model that uses perceptual loss and adversarial loss to favor outputs residing on the manifold of natural images. Sajjadi et al. [16] develop a similar approach and further explored the local texture matching loss. Based on these works, Wang et al. [17] propose spatial feature transform to effectively incorporate semantic prior in an image and improve the recovered textures.</p>
<p>感知驱动的方法已经被提出用来改善SR结果的视觉质量。基于更接近于感知相似度[29,14]的想法提出感知损失[13]，通过最小化特征空间而不是像素空间的误差来增强视觉质量。通过使用专注于特征分布而不是只比较外观的目标函数，开发上下文损失[30]来生成具有自然图像统计的图像。Ledig等[1]提出SRGAN模型，使用感知损失和对抗损失来支持位于自然图像流形的输出。Sajjadi等[16]开发了类似的方法并进一步探索了局部纹理匹配损失。基于这些工作，Wang等[17]提出空间特征变换来有效地将语义先验合并到图像中并改进恢复的纹理。</p>
<p>Throughout the literature, photo-realism is usually attained by adversarial training with GAN [15]. Recently there are a bunch of works that focus on developing more effective GAN frameworks. WGAN [31] proposes to minimize a reasonable and efficient approximation of Wasserstein distance and regularizes discriminator by weight clipping. Other improved regularization for discriminator includes gradient clipping [32] and spectral normalization [33]. Relativistic discriminator [2] is developed not only to increase the probability that generated data are real, but also to simultaneously decrease the probability that real data are real. In this work, we enhance SRGAN by employing a more effective relativistic average GAN.</p>
<p>在整个文献中，通常通过与GAN[15]的对抗训练来获得写实主义照片。最近有很多工作致力于开发更有效的GAN框架。WGAN[31]提出最小化Wasserstein距离的合理和有效近似，并通过权重修剪来正则化判别器。其它对判别器的正则化包括梯度修剪[32]和谱归一化[33]。开发的相对判别器[2]不仅提高了生成数据真实性的概率，而且同时降低了真实数据真实性的概率。在这项工作中，我们通过采用更有效的相对平均GAN来增强SRGAN。</p>
<p>SR algorithms are typically evaluated by several widely used distortion measures, e.g., PSNR and SSIM. However, these metrics fundamentally disagree with the subjective evaluation of human observers [1]. Non-reference measures are used for perceptual quality evaluation, including Ma’s score [23] and NIQE [24], both of which are used to calculate the perceptual index in the PIRM-SR Challenge [3]. In a recent study, Blau et al. [22] find that the distortion and perceptual quality are at odds with each other.</p>
<p>SR通常通过几种广泛使用的失真测量方式来进行评估，例如PSNR和SSIM。然而，这些度量从根本上与人类观察者的主观评估不一致[1]。非参考度量通常用于感知质量评估，包括Ma的分数[23]和NIQE[24]，两者都用于PIRM-SR挑战赛中[3]计算感知指数。在最近的一项研究中，Blau等[22]发现失真和感知质量相互矛盾。</p>
<h2 id="3-Proposed-Methods"><a href="#3-Proposed-Methods" class="headerlink" title="3 Proposed Methods"></a>3 Proposed Methods</h2><p>Our main aim is to improve the overall perceptual quality for SR. In this section, we first describe our proposed network architecture and then discuss the improvements from the discriminator and perceptual loss. At last, we describe the network interpolation strategy for balancing perceptual quality and PSNR.</p>
<h2 id="3-提出的方法"><a href="#3-提出的方法" class="headerlink" title="3 提出的方法"></a>3 提出的方法</h2><p>我们的主要目标是提高SR的整体感知质量。在本节中，我们首先描述我们提出的网络架构，然后讨论判别器和感知损失的改进。最后，我们描述用于平衡感知质量和PSNR的网络插值策略。</p>
<h3 id="3-1-Network-Architecture"><a href="#3-1-Network-Architecture" class="headerlink" title="3.1 Network Architecture"></a>3.1 Network Architecture</h3><p>In order to further improve the recovered image quality of SRGAN, we mainly make two modifications to the structure of generator G: 1) remove all BN layers; 2) replace the original basic block with the proposed Residual-in-Residual Dense Block (RRDB), which combines multi-level residual network and dense connections as depicted in Fig. 4.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_4.jpeg" alt="Figure 4"></p>
<p>Fig.4: Left: We remove the BN layers in residual block in SRGAN. Right: RRDB block is used in our deeper model and $\beta$ is the residual scaling parameter.</p>
<h3 id="3-1-网络架构"><a href="#3-1-网络架构" class="headerlink" title="3.1 网络架构"></a>3.1 网络架构</h3><p>为了进一步改进SRGAN复原的图像质量，我们主要对生成器G的架构进行了两个修改：1）移除所有的BN层；2）用提出的残差套残差密集块(RRDB)替换原始的基本块，它结合了多层残差网络和密集连接，如图4所示。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_4.jpeg" alt="Figure 4"></p>
<p>图4：左：我们移除了SRGAN残差块中的BN层。右：RRDB块用在我们的更深模型中，$\beta$是残差尺度参数。</p>
<p>Removing BN layers has proven to increase performance and reduce computational complexity in different PSNR-oriented tasks including SR [20] and deblurring [35]. BN layers normalize the features using mean and variance in a batch during training and use estimated mean and variance of the whole training dataset during testing. When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. We empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework. These artifacts occasionally appear among iterations and different settings, violating the needs for a stable performance over training. We therefore remove BN layers for stable training and consistent performance. Furthermore, removing BN layers helps to improve generalization ability and to reduce computational complexity and memory usage.</p>
<p>在不同的面向PSNR的任务（包括SR[20]和去模糊[35]）中，已经证实了移除BN层可以提高性能并降低计算复杂度。BN层在训练中使用一批数据的均值和方差对特征进行归一化，并在测试中使用整个训练集估计的均值和方差。当训练集和测试集的统计差别很大时，BN层趋向于引入令人不快的伪影并限制泛化能力。我们凭经验观察到，当网络较深且在GAN架构下训练时，BN层更可能带来伪影。这些伪影有时会在迭代中间和不同的设置下出现，违背了训练过程中对于稳定性能的需求。因此，我们为了稳定的训练和一致的性能移除了BN层。此外，移除BN层有助于提高泛化能力并降低计算复杂度及内存使用。</p>
<p>We keep the high-level architecture design of SRGAN (see Fig. 3), and use a novel basic block namely RRDB as depicted in Fig. 4. Based on the observation that more layers and connections could always boost performance [20,11,12], the proposed RRDB employs a deeper and more complex structure than the original residual block in SRGAN. Specifically, as shown in Fig. 4, the proposed RRDB has a residual-in-residual structure, where residual learning is used in different levels. A similar network structure is proposed in [36] that also applies a multilevel residual network. However, our RRDB differs from [36] in that we use dense block [34] in the main path as [11], where the network capacity becomes higher benefiting from the dense connections.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_3.jpeg" alt="Figure 3"></p>
<p>Fig. 3: We employ the basic architecture of SRResNet [1], where most computation is done in the LR feature space. We could select or design “basic blocks” (e.g., residual block [18], dense block [34], RRDB) for better performance.</p>
<p>我们保留了SRGAN的高级架构设计（见图3），并使用了一个新颖的名为RRDB的基本块，如图4所示。基于观测，更多的层和连接总是可以提升性能[20,11,12]，与SRGAN中的原始残差块相比，提出的RRDB采用了更深更复杂的架构。具体地说，如图4所示，提出了的RRDB有残差套残差的结构，其中残差学习用在不同的级别中。[36]中提出的类似结构也适用于多级残差网络。然而，我们的RRDB与[36]的不同在于我们在主路径中使用了如[11]的密集块[34]，受益于密集连接其网络容量变得更高。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_3.jpeg" alt="Figure 3"></p>
<p>图3：我们采用SRResNet[1]的基本架构，大多数计算都在LR特征空间进行。我们可以为了更佳的性能选择或设计“基础块”（例如，残差块[18]，密集块[34]，RRDB）。</p>
<p>In addition to the improved architecture, we also exploit several techniques to facilitate training a very deep network: 1) residual scaling [21,20], i.e., scaling down the residuals by multiplying a constant between 0 and 1 before adding them to the main path to prevent instability; 2) smaller initialization, as we empirically find residual architecture is easier to train when the initial parameter variance becomes smaller. More discussion can be found in the <em>supplementary material</em>.</p>
<p>除了改进架构之外，我们也利用几种技术来促进训练非常深的网络：1）残差缩放[21,20]，例如在将残差加到主路径上之前，通过将其乘以一个0-1之间的常量来缩小残差以防止不稳定性；2）更小的初始化，因为我们凭经验发现当初始参数方差变得更小时，残差结构更容易训练。更多讨论可在<em>补充材料</em>中找到。</p>
<p>The training details and the effectiveness of the proposed network will be presented in Sec. 4.</p>
<p>训练细节和提出网络的有效性将在第4节中介绍。</p>
<h3 id="3-2-Relativistic-Discriminator"><a href="#3-2-Relativistic-Discriminator" class="headerlink" title="3.2 Relativistic Discriminator"></a>3.2 Relativistic Discriminator</h3><p>Besides the improved structure of generator, we also enhance the discriminator based on the Relativistic GAN [2]. Different from the standard discriminator $D$<br>in SRGAN, which estimates the probability that one input image $x$ is real and natural, a relativistic discriminator tries to predict the probability that a real<br>image $x_r$ is relatively more realistic than a fake one $x_f$ , as shown in Fig. 5.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_5.jpeg" alt="Figure 5"></p>
<p>Fig. 5: Difference between standard discriminator and relativistic discriminator.</p>
<h3 id="3-2-相对判别器"><a href="#3-2-相对判别器" class="headerlink" title="3.2 相对判别器"></a>3.2 相对判别器</h3><p>除了改进生成器架构之外，我们还在相对GAN[2]的基础上增强了判断器。不同于SRGAN中的标注判别器$D$，$D$估算输入图像$x$是真实自然的概率，相对判别器尝试预测真实图像$x_r$比假图像$x_f$相对更真实的概率，如图5所示。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_5.jpeg" alt="Figure 5"></p>
<p>图5：标准判别器和相对判别器的差异。</p>
<p>Specifically, we replace the standard discriminator with the Relativistic average Discriminator RaD [2], denoted as $D_{Ra}$. The standard discriminator in SRGAN can be expressed as $D(x) = \sigma(C(x))$, where $\sigma$ is the sigmoid function and $C(x)$ is the non-transformed discriminator output. Then the RaD is formulated as $D_{Ra}(x_r, x_f) = \sigma(C(x_r) − \mathbb{E}_{x_f}[C(x_f)])$, where $\mathbb{E}_{x_f}[\bullet]$ represents the operation of taking average for all fake data in the mini-batch. The discriminator loss is then defined as: $$L^{Ra}_{D} =−\mathbb{E}_{x_r}[log(D_{Ra}(x_r, x_f))]−\mathbb{E}_{x_f}[1 - log(D_{Ra}(x_f, x_r))]. \tag{1}$$</p>
<p>The adversarial loss for generator is in a symmetrical form: $$L^{Ra}_{G} =−\mathbb{E}_{x_r}[1-log(D_{Ra}(x_r, x_f))]−\mathbb{E}_{x_f}[log(D_{Ra}(x_f, x_r))], \tag{2}$$</p>
<p>where $x_f = G(x_i)$ and $x_i$ stands for the input LR image. It is observed that the adversarial loss for generator contains both $x_r$ and $x_f$. Therefore, our generator benefits from the gradients from both generated data and real data in adversarial training, while in SRGAN only generated part takes effect. In Sec. 4.4, we will show that this modification of discriminator helps to learn sharper edges and more detailed textures.</p>
<p>具体来说，我们用相对平均判别器RaD[2]代替标准判别器，记为$D_{Ra}$。SRGAN中的标准判别器可表示为$D(x) = \sigma(C(x))$，其中$\sigma$是sigmoid函数，$C(x)$是非变换判别器输出。然后RaD用公式表示为$D_{Ra}(x_r, x_f) = \sigma(C(x_r) − \mathbb{E}_{x_f}[C(x_f)])$，其中$\mathbb{E}_{x_f}[\bullet]$表示对小批次中所有假数据取平均值的操作。然后判别器损失定义为：$$L^{Ra}_{D} =−\mathbb{E}_{x_r}[log(D_{Ra}(x_r, x_f))]−\mathbb{E}_{x_f}[1 - log(D_{Ra}(x_f, x_r))]. \tag{1}$$</p>
<p>生成器的对抗损失呈对称形式：$$L^{Ra}_{G} =−\mathbb{E}_{x_r}[1-log(D_{Ra}(x_r, x_f))]−\mathbb{E}_{x_f}[log(D_{Ra}(x_f, x_r))], \tag{2}$$</p>
<p>其中$x_f=G(x_i)$和$x_i$代表输入LR图像。可以看出，生成器的对抗损失包含$x_r$和$x_f$。因此，在对抗训练中，我们的生成器受益于生成数据和真实数据的梯度，而在SRGAN中仅生成部分起作用。在4.4节中，我们将展示判别器的这种修改有助于学习更清晰的边缘和更细致的纹理。</p>
<h3 id="3-3-Perceptual-Loss"><a href="#3-3-Perceptual-Loss" class="headerlink" title="3.3 Perceptual Loss"></a>3.3 Perceptual Loss</h3><p>We also develop a more effective perceptual loss $L_{percep}$ by constraining on features before activation rather than after activation as practiced in SRGAN.</p>
<h3 id="3-3-感知损失"><a href="#3-3-感知损失" class="headerlink" title="3.3 感知损失"></a>3.3 感知损失</h3><p>通过约束激活之前的特征而不是SRGAN中实践的激活之后的特征，我们还开发了一种更有效的感知损失$L_{percep}$。</p>
<p>Based on the idea of being closer to perceptual similarity [29,14], Johnson et al. [13] propose perceptual loss and it is extended in SRGAN [1]. Perceptual loss is previously defined on the activation layers of a pre-trained deep network, where the distance between two activated features is minimized. Contrary to the convention, we propose to use features before the activation layers, which will overcome two drawbacks of the original design. First, the activated features are very sparse, especially after a very deep network, as depicted in Fig. 6. For example, the average percentage of activated neurons for image ‘baboon’ after VGG19-54 layer is merely $11.17\%$. The sparse activation provides weak supervision and thus leads to inferior performance. Second, using features after activation also causes inconsistent reconstructed brightness compared with the ground-truth image, which we will show in Sec. 4.4.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_6.jpeg" alt="Figure 6"></p>
<p>Fig.6: Representative feature maps before and after activation for image ‘baboon’. With the network going deeper, most of the features after activation become inactive while features before activation contains more information.</p>
<p>基于更接近感知相似[29,14]的想法，Johnson等[13]提出了感知损失并在SRGAN[1]中得到了扩展。之前的感知损失定义在预训练深度网络的激活层上，最小化两个激活特征之间的距离。与常规用法相反，我们提出使用激活层之前的特征，这将克服原始设计的两个缺点。首先，激活特征非常稀疏，尤其是在非常深的网络之后，如图6所示。例如，图像“狒狒”在VGG19-54层之后激活神经元的平均百分比只有$11.17\%$。稀疏的激活提供了弱监督，因此导致性能较差。其次，与真实图像相比，使用激活之后的特征也会导致重建亮度不一致，这将在4.4节中展示。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_6.jpeg" alt="Figure 6"></p>
<p>图6：图像“狒狒”激活之前和激活之后代表性的特征映射。随着网络加深，大多数激活之后的特征变得不活跃而激活之前的特征包含更多的信息。</p>
<p>Therefore, the total loss for the generator is: $$L_G = L_{percep} + \lambda L^{Ra}_G + \eta L_1 \tag{3}$$ where $L_1 = \mathbb{E}_{x_i} || G(x_i) − y||_1$ is the content loss that evaluate the 1-norm distance between recovered image $G(x_i)$ and the ground-truth $y$, and $\lambda, \eta$ are the coefficients to balance different loss terms.</p>
<p>因此，生成器的全部损失为：$$L_G = L_{percep} + \lambda L^{Ra}_G + \eta L_1 \tag{3}$$，其中$L_1 = \mathbb{E}_{x_i} || G(x_i) − y||_1$是内容损失，用来评估恢复图像$G(x_i)$和真实图像$y$之间的1范数距离，$\lambda, \eta$是平衡不同损失项的系数。</p>
<p>We also explore a variant of perceptual loss in the PIRM-SR Challenge. In contrast to the commonly used perceptual loss that adopts a VGG network trained for image classification, we develop a more suitable perceptual loss for SR–MINC loss. It is based on a fine-tuned VGG network for material recognition [38], which focuses on textures rather than object. Although the gain of perceptual index brought by MINC loss is marginal, we still believe that exploring perceptual loss that focuses on texture is critical for SR.</p>
<p>我们在PIRM-SR挑战赛中也探索了感知损失的变种。与采用图像分类训练的VGG网络的常用感知损失相比，我们为SR–MINC损失开发了一种更合适的感知损失。它是基于材料识别[38]的微调VGG网络，该网络注重于纹理而不是目标。尽管MINC损失带来的感知指数收益是微不足道的，但我们仍然认为，采用注重纹理的感知损失对于SR至关重要。</p>
<h3 id="3-4-Network-Interpolation"><a href="#3-4-Network-Interpolation" class="headerlink" title="3.4 Network Interpolation"></a>3.4 Network Interpolation</h3><p>To remove unpleasant noise in GAN-based methods while maintain a good perceptual quality, we propose a flexible and effective strategy – network interpolation. Specifically, we first train a PSNR-oriented network $G_{PSNR}$ and then obtain a GAN-based network $G_{GAN}$ by fine-tuning. We interpolate all the corresponding parameters of these two networks to derive an interpolated model $G_{INTERP}$, whose parameters are: $$\theta^{INTERP}_{G} = (1 − \alpha) \theta^{PSNR}_{G} + \alpha \theta^{GAN}_{G} \tag{4}$$ where $G_{INTERP}$, $G_{PSNR}$ and $G_{GAN}$ are the parameters of $\theta^{INTERP}_{G}$, $\theta^{PSNR}_{G}$ and $\theta^{GAN}_{G}$, respectively, and $\alpha \in [0, 1]$ is the interpolation parameter.</p>
<h3 id="3-4-网络插值"><a href="#3-4-网络插值" class="headerlink" title="3.4 网络插值"></a>3.4 网络插值</h3><p>为了去除基于GAN方法中讨厌的噪声同时保持好的感知质量，我们提出了一种弹性有效的策略——网络插值。具体来说，我们首先训练一个面向PSNR的网络$G_{PSNR}$，然后通过微调获得一个基于GAN的网络$G_{GAN}$。我们插值这两个网络的所有对应参数来取得插值模型$G_{INTERP}$，其参数为：$$\theta^{INTERP}_{G} = (1 − \alpha) \theta^{PSNR}_{G} + \alpha \theta^{GAN}_{G} \tag{4}$$ 其中$G_{INTERP}$, $G_{PSNR}$和$G_{GAN}$分别是$\theta^{INTERP}_{G}$, $\theta^{PSNR}_{G}$和$\theta^{GAN}_{G}$的参数，$\alpha \in [0, 1]$为插值参数。</p>
<p>The proposed network interpolation enjoys two merits. First, the interpolated model is able to produce meaningful results for any feasible $\alpha$ without introducing artifacts. Second, we can continuously balance perceptual quality and fidelity without re-training the model.</p>
<p>提出的网络插值有两个优点。首先，插值模型对于任何合理的$\alpha$都能产生有意义的结果而不会产生伪影。其次，我们可以持续平衡感知质量和保真度都不必重新训练模型。</p>
<p>We also explore alternative methods to balance the effects of PSNR-oriented and GAN-based methods. For instance, one can directly interpolate their output images (pixel by pixel) rather than the network parameters. However, such an approach fails to achieve a good trade-off between noise and blur, i.e., the interpolated image is either too blurry or noisy with artifacts (see Sec. 4.5). Another method is to tune the weights of content loss and adversarial loss, i.e., the parameter $\lambda$ and $\eta$ in Eq. (3). But this approach requires tuning loss weights and fine-tuning the network, and thus it is too costly to achieve continuous control of the image style.</p>
<p>我们也探索了替代方法来平衡面向PSNR方法和基于GAN方法的影响。例如，可以直接插值它们的输出图像（逐像素）而不是网络参数。然而，这种方法不会在噪声和模糊之间取得良好的权衡，即插值图像或太模糊或带有伪影的噪声太大（见4.5节）。另一种方法是调整内容损失和对抗损失的权重，即方程3中的参数$\lambda$和$\eta$。但这种方法要求调整损失权重并微调网络，因此实现图像风格的连续控制代价很高。</p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h2><h3 id="4-1-Training-Details"><a href="#4-1-Training-Details" class="headerlink" title="4.1 Training Details"></a>4.1 Training Details</h3><p>Following SRGAN [1], all experiments are performed with a scaling factor of ×4 between LR and HR images. We obtain LR images by down-sampling HR images using the MATLAB bicubic kernel function. The mini-batch size is set to 16. The spatial size of cropped HR patch is 128 × 128. We observe that training a deeper network benefits from a larger patch size, since an enlarged receptive field helps to capture more semantic information. However, it costs more training time and consumes more computing resources. This phenomenon is also observed in PSNR-oriented methods (see supplementary material).</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><h3 id="4-1-训练细节"><a href="#4-1-训练细节" class="headerlink" title="4.1 训练细节"></a>4.1 训练细节</h3><p>按照SRGAN[1]，所有实验在LR和HR图像间均以4倍的尺度系数进行。我们通过使用MATLAB双三次核函数对HR图像进行下采样来获得LR图像。最小批次大小设置为16。裁剪的HR图像块的空间大小为128×128。我们观察到，训练更深的网络可以从更大的批次大小中获益，因为扩大的感受野有助于捕获更多的语义信息。但是，这会花费更多的训练时间并消耗更多的计算资源。这种现象也可以在面向PSNR的方法中观察到（见补充材料）。</p>
<p>The training process is divided into two stages. First, we train a PSNR-oriented model with the L1 loss. The learning rate is initialized as $2 × 10^{−4}$ and decayed by a factor of 2 every $2 × 10^5$ of mini-batch updates. We then employ the trained PSNR-oriented model as an initialization for the generator. The generator is trained using the loss function in Eq. (3) with $\lambda = 5×10^{−3}$ and $\eta = 1×10^{−2}$. The learning rate is set to $1×10^{−4}$ and halved at [50k, 100k, 200k, 300k] iterations. Pre-training with pixel-wise loss helps GAN-based methods to obtain more visually pleasing results. The reasons are that 1) it can avoid undesired local optima for the generator; 2) after pre-training, the discriminator receives relatively good super-resolved images instead of extreme fake ones (black or noisy images) at the very beginning, which helps it to focus more on texture discrimination.</p>
<p>训练过程分为两个阶段。首先，我们训练一个具有L1损失的面向PSNR的模型。学习率初始化为$2 × 10^{−4}$，每$2 × 10^5$个小批次更新的衰减因子为2。然后，我们采用训练的面向PSNR的模型作为生成器的初始化。生成器训练使用等式3中的损失函数，$\lambda = 5×10^{−3}$，$\eta = 1×10^{−2}$。学习率设置为$1×10^{−4}$，在[50k, 100k, 200k, 300k]次迭代之后减半。使用逐像素损失进行预训练有助于基于GAN的方法获得视觉上更好的结果。原因是：1）它可以避免生成器不希望的局部最优；2）在预训练之后，最初判别器可以收到相对好的超分辨率图像而不是极端假的图像（黑色或噪声图像），这有助于其更关注纹理判别。</p>
<p>For optimization, we use Adam [39] with $\beta_1 = 0.9, \beta_2 = 0.999$. We alternately update the generator and discriminator network until the model converges. We use two settings for our generator – one of them contains 16 residual blocks, with a capacity similar to that of SRGAN and the other is a deeper model with 23 RRDB blocks. We implement our models with the PyTorch framework and train them using NVIDIA Titan Xp GPUs.</p>
<p>为了优化，我们使用Adam[39]，其中$\beta_1 = 0.9， \beta_2 = 0.999$。我们交替更新生成器和判别器网络，直到模型收敛。我们为生成器使用了两种设置——其中一种包含16个残差块，能力类似于SRGAN，另一种是具有23个RRDB块的更深的模型。我们使用PyTorch框架实现我们的模型，并使用NVIDIA Titan Xp GPU对其进行训练。</p>
<h3 id="4-2-Data"><a href="#4-2-Data" class="headerlink" title="4.2 Data"></a>4.2 Data</h3><p>For training, we mainly use the DIV2K dataset [40], which is a high-quality (2K resolution) dataset for image restoration tasks. Beyond the training set of DIV2K that contains 800 images, we also seek for other datasets with rich and diverse textures for our training. To this end, we further use the Flickr2K dataset [41] consisting of 2650 2K high-resolution images collected on the Flickr website, and the OutdoorSceneTraining (OST) [17] dataset to enrich our training set. We empirically find that using this large dataset with richer textures helps the generator to produce more natural results, as shown in Fig. 8.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_8.jpeg" alt="Figure 8"></p>
<p>Fig. 8: Overall visual comparisons for showing the effects of each component in ESRGAN. Each column represents a model with its configurations in the top. The red sign indicates the main improvement compared with the previous model.</p>
<h3 id="4-2-数据"><a href="#4-2-数据" class="headerlink" title="4.2 数据"></a>4.2 数据</h3><p>对于训练，我们主要使用DIV2K数据集[40]，它是用于图像复原任务的高质量（2K分辨率）数据集。除了包含800张图像的DIV2K训练集外，我们也搜寻了其它具有丰富多样纹理的数据集进行训练。为此，我们进一步使用Flickr2K数据集[41]，包含Flickr网站上收集的2650张2K高分辨率图像，OutdoorSceneTraining(OST)[17]数据集来丰富我们的训练集。我们凭经验发现，使用具有丰富纹理的大型数据集有助于生成器产生更自然的结果，如图8所示。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_8.jpeg" alt="Figure 8"></p>
<p>图8：展示ESRGAN中每个组件效果的整体视觉比较。每一列表示一个模型，其配置在顶部。红色符号表示与前面模型相比的主要改进。</p>
<p>We train our models in RGB channels and augment the training dataset with random horizontal flips and 90 degree rotations. We evaluate our models on widely used benchmark datasets – Set5 [42], Set14 [43], BSD100 [44], Urban100 [45], and the PIRM self-validation dataset that is provided in the PIRM-SR Challenge.</p>
<p>我们在RGB通道训练模型，并通过随机水平翻转和90度旋转来增强训练集。我们在广泛使用的基准数据集——Set5[42]，Set14[43]，BSD100[44]，Urban100[45]以及PIRM-SR挑战赛提供的PIRM自验证数据上评估我们的模型。</p>
<h3 id="4-3-Qualitative-Results"><a href="#4-3-Qualitative-Results" class="headerlink" title="4.3 Qualitative Results"></a>4.3 Qualitative Results</h3><p>We compare our final models on several public benchmark datasets with state-ofthe-art PSNR-oriented methods including SRCNN [4], EDSR [20] and RCAN [12], and also with perceptual-driven approaches including SRGAN [1] and EnhanceNet [16]. Since there is no effective and standard metric for perceptual quality, we present some representative qualitative results in Fig. 7. PSNR (evaluated on the luminance channel in YCbCr color space) and the perceptual index used in the PIRM-SR Challenge are also provided for reference.</p>
<h3 id="4-3-定性结果"><a href="#4-3-定性结果" class="headerlink" title="4.3 定性结果"></a>4.3 定性结果</h3><p>我们将最终的模型与最新的面向PSNR的方法包括SRCNN[4]，EDSR[20]和RCAN[12]，以及感知驱动的方法包括在SRGAN[1]和EnhanceNet[16]在一些公开基准数据集上进行了比较。由于对于感知质量没有有效标准的度量标准，我们在图7中展示了一些具有代表性的结果，也提供了PSNR（在YCbCr颜色空间的亮度通道上评估）和PIRM-SR挑战赛中的感知指数供参考。</p>
<p>It can be observed from Fig. 7 that our proposed ESRGAN outperforms previous approaches in both sharpness and details. For instance, ESRGAN can produce sharper and more natural baboon’s whiskers and grass textures (see image 43074) than PSNR-oriented methods, which tend to generate blurry results, and than previous GAN-based methods, whose textures are unnatural and contain unpleasing noise. ESRGAN is capable of generating more detailed structures in building (see image 102061) while other methods either fail to produce enough details (SRGAN) or add undesired textures (EnhanceNet). Moreover, previous GAN-based methods sometimes introduce unpleasant artifacts, e.g., SRGAN adds wrinkles to the face. Our ESRGAN gets rid of these artifacts and produces natural results.</p>
<p>从图7可以看出，我们提出的ESRGAN在清晰度和细节方面都优于之前的方法。例如，与面向PSNR的方法（更趋向于产生模糊的结果）和以前的基于GAN的方法（纹理不自然并包含令人不快的噪声）相比，ESRGAN可以产生更清晰更自然的狒狒胡须和草的纹理（见图43074）。在建筑物中（见图102061），ESRGAN能够产生更详细的结构而其它的方法要么不能产生足够的细节(SRGAN)，要么添加不必要的纹理(EnhanceNet)。此外，以前基于GAN的方法有时会引入令人不快的伪影，例如SRGAN会在脸上添加皱纹。我们的ESRGAN除去了这些伪影并产生了自然的结果。</p>
<h3 id="4-4-Ablation-Study"><a href="#4-4-Ablation-Study" class="headerlink" title="4.4 Ablation Study"></a>4.4 Ablation Study</h3><p>In order to study the effects of each component in the proposed ESRGAN, we gradually modify the baseline SRGAN model and compare their differences. The overall visual comparison is illustrated in Fig. 8. Each column represents a model with its configurations shown in the top. The red sign indicates the main improvement compared with the previous model. A detailed discussion is provided as follows.</p>
<h3 id="4-4-消融研究"><a href="#4-4-消融研究" class="headerlink" title="4.4 消融研究"></a>4.4 消融研究</h3><p>为了研究提出的ESRGAN中每个组件的效果，我们逐渐修改基准的SRGAN模型并比较它们的差异。完整的视觉比较如图8所示。每一列表示一个模型，其配置在顶部。红色符号表明与前面模型相比的主要改进。详细讨论提供如下。</p>
<p><strong>BN removal</strong>. We first remove all BN layers for stable and consistent performance without artifacts. It does not decrease the performance but saves the computational resources and memory usage. For some cases, a slight improvement can be observed from the 2nd and 3rd columns in Fig. 8 (e.g., image 39). Furthermore, we observe that when a network is deeper and more complicated, the model with BN layers is more likely to introduce unpleasant artifacts. The examples can be found in the supplementary material.</p>
<p><strong>移除BN</strong>。为了稳定和没有伪影的一致性能，我们首先移除了所有的BN层。它不会降低性能但会节省计算资源和内存使用。在某些情况下，从图8中的第2列和第3列可以观察到轻微的改进（例如，图39）。此外，我们观察到当网络更深更复杂时，具有BN层的模型更可能引入令人不快的伪影。可以在补充材料中找到示例。</p>
<p><strong>Before activation in perceptual loss</strong>. We first demonstrate that using features before activation can result in more accurate brightness of reconstructed images. To eliminate the influences of textures and color, we filter the image with a Gaussian kernel and plot the histogram of its gray-scale counterpart. Fig. 9a shows the distribution of each brightness value. Using activated features skews the distribution to the left, resulting in a dimmer output while using features before activation leads to a more accurate brightness distribution closer to that of the ground-truth.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_9.jpeg" alt="Figure 9"></p>
<p>Fig. 9: Comparison between before activation and after activation.</p>
<p><strong>感知损失在激活之前</strong>。我们首先证实了使用激活之前的特征可以使重建图像的亮度更准确。为了消除纹理和颜色的影响，我们使用高斯核对图像进行了滤波并绘制了其对应灰度图像的直方图。图9a展示了每一个亮度值的分布。使用激活的特征会使分布偏向左，导致了较暗的输出，而使用激活之前的特征会得到更精确的亮度分布，更接近于真实图像的亮度分布。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_9.jpeg" alt="Figure 9"></p>
<p>图9：激活之前和激活之后的比较。</p>
<p>We can further observe that using features before activation helps to produce sharper edges and richer textures as shown in Fig. 9b (see bird feather) and Fig. 8 (see the 3rd and 4th columns), since the dense features before activation offer a stronger supervision than that a sparse activation could provide.</p>
<p>我们可以进一步观察到，使用激活之前的特征有助于产生更清晰的边缘和更丰富的纹理，如图9b（见鸟羽）和图8（见第三列和第四列）所示，因为与稀疏激活提供的特征相比，激活之前的密集特征能提供更强的监督。</p>
<p><strong>RaGAN</strong>. RaGAN uses an improved relativistic discriminator, which is shown to benefit learning sharper edges and more detailed textures. For example, in the 5th column of Fig. 8, the generated images are sharper with richer textures than those on their left (see the baboon, image 39 and image 43074).</p>
<p><strong>RaGAN</strong>。RaGAN使用改进的相对判别器，证明了其有利于学习更清晰的边缘和更细致的纹理。例如，在图8的第5列中，生成的图像比其左侧的图像更清晰，具有更丰富的纹理（见狒狒，图39和图43074）。</p>
<p><strong>Deeper network with RRDB</strong>. Deeper model with the proposed RRDB can further improve the recovered textures, especially for the regular structures like the roof of image 6 in Fig. 8, since the deep model has a strong representation capacity to capture semantic information. Also, we find that a deeper model can reduce unpleasing noises like image 20 in Fig. 8.</p>
<p><strong>具有RRDB的更深网络</strong>。具有提出的RRDB的更深模型可以进一步改善恢复的纹理，尤其是像图8中图像6的屋顶这样的常规结构，因为深度模型具有强大的表示能力来捕获语义信息。 我们也发现更深的模型可以减少像图8中图像20这样的令人不快的噪声。</p>
<p>In contrast to SRGAN, which claimed that deeper models are increasingly difficult to train, our deeper model shows its superior performance with easy training, thanks to the improvements mentioned above especially the proposed RRDB without BN layers.</p>
<p>与SRGAN声称的更深的模型越来越难训练相比，由于上述提供的改进尤其是提出的没有BN层的RRDB，我们更深的模型展示了它容易训练且优越性能。</p>
<h3 id="4-5-Network-Interpolation"><a href="#4-5-Network-Interpolation" class="headerlink" title="4.5 Network Interpolation"></a>4.5 Network Interpolation</h3><p>We compare the effects of network interpolation and image interpolation strategies in balancing the results of a PSNR-oriented model and GAN-based method. We apply simple linear interpolation on both the schemes. The interpolation parameter $\alpha$ is chosen from 0 to 1 with an interval of 0.2.</p>
<h3 id="4-5-网络插值"><a href="#4-5-网络插值" class="headerlink" title="4.5 网络插值"></a>4.5 网络插值</h3><p>我们比较了网络插值和图像插值策略在平衡面向PSNR模型与基于GAN方法的结果方面的作用。我们在这个两个方案中应用了简单的线性插值。插值参数$\alpha$从间隔为0.2的0-1之间选取。</p>
<p>As depicted in Fig. 10, the pure GAN-based method produces sharp edges and richer textures but with some unpleasant artifacts, while the pure PSNRoriented method outputs cartoon-style blurry images. By employing network interpolation, unpleasing artifacts are reduced while the textures are maintained. By contrast, image interpolation fails to remove these artifacts effectively.</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_10.jpeg" alt="Figure 10"></p>
<p>Fig. 10: The comparison between network interpolation and image interpolation.</p>
<p>如图10所示，单纯的基于GAN的方法会产生清晰的边缘和更丰富的纹理，但带有一些令人不快的伪影，而单纯的面向PSNR方法会输出卡通风格的模糊图像。通过采用网络插值，在减少令人不快的伪影的同时保持了纹理。相比之下，图像插值不能有效消除这些伪影。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_10.jpeg" alt="Figure 10"></p>
<p>图10：网络插值和图像插值的比较。</p>
<p>Interestingly, it is observed that the network interpolation strategy provides a smooth control of balancing perceptual quality and fidelity in Fig. 10.</p>
<p>有趣的是，在图10中观察到网络插值策略提供了对平衡感知质量和保真度的平滑控制。</p>
<h3 id="4-6-The-PIRM-SR-Challenge"><a href="#4-6-The-PIRM-SR-Challenge" class="headerlink" title="4.6 The PIRM-SR Challenge"></a>4.6 The PIRM-SR Challenge</h3><p>We take a variant of ESRGAN to participate in the PIRM-SR Challenge [3]. Specifically, we use the proposed ESRGAN with 16 residual blocks and also empirically make some modifications to cater to the perceptual index. 1) The MINC loss is used as a variant of perceptual loss, as discussed in Sec. 3.3. Despite the marginal gain on the perceptual index, we still believe that exploring perceptual loss that focuses on texture is crucial for SR. 2) Pristine dataset [24], which is used for learning the perceptual index, is also employed in our training; 3) a high weight of loss $L_1$ up to $\eta = 10$ is used due to the PSNR constraints; 4) we also use back projection [46] as post-processing, which can improve PSNR and sometimes lower the perceptual index.</p>
<h3 id="4-6-PIRM-SR挑战赛"><a href="#4-6-PIRM-SR挑战赛" class="headerlink" title="4.6 PIRM-SR挑战赛"></a>4.6 PIRM-SR挑战赛</h3><p>我们采用ESRGAN的一个变种来参加PIRM-SR挑战赛[3]。具体来说，我们使用提出的具有16个残差块的ESRGAN，并根据经验进行了一些修改来迎合感知指数。1）使用MINC损失作为感知损失的一个变种，如3.3节所述。尽管在感知指数上有边际收益，但我们仍认为采用专注于纹理的感知损失对于SR至关重要；2）我们的训练中也使用了Pristine数据集[24]来学习感知指数；3）由于PSNR约束，$L_1$损失的权重高达$\eta = 10$；4）我们也使用反向投射[46]作为后处理，其可以改善PSNR，有时会降低感知指数。</p>
<p>For other regions 1 and 2 that require a higher PSNR, we use image interpolation between the results of our ESRGAN and those of a PSNR-oriented method RCAN [12]. The image interpolation scheme achieves a lower perceptual index (lower is better) although we observed more visually pleasing results by using the network interpolation scheme. Our proposed ESRGAN model won the first place in the PIRM-SR Challenge (region 3) with the best perceptual index.</p>
<p>对于其它需要较高PSNR的区域1和2，我们在ESRGAN的结果和面向PSNR方法RCAN[12]的结果之间使用图像插值。尽管通过使用网络插值方案我们观察到了视觉上更令人满意的效果，但图像插值方案取得了较低的感知指数（越低越好）。我们提出的ESRGAN模型以最好的感知指数赢得了PIRM-SR挑战赛（区域3）的第一名。</p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>We have presented an ESRGAN model that achieves consistently better perceptual quality than previous SR methods. The method won the first place in the PIRM-SR Challenge in terms of the perceptual index. We have formulated a novel architecture containing several RDDB blocks without BN layers. In addition, useful techniques including residual scaling and smaller initialization are employed to facilitate the training of the proposed deep model. We have also introduced the use of relativistic GAN as the discriminator, which learns to judge whether one image is more realistic than another, guiding the generator to recover more detailed textures. Moreover, we have enhanced the perceptual loss by using the features before activation, which offer stronger supervision and thus restore more accurate brightness and realistic textures.</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>我们提出了一种ESRGAN模型，它比以前的SR方法始终取得更好的感知质量。就感知指数而言，该方法在PIRM-SR挑战赛中获得了第一名。我们构建了一种包含一些没有BN层的RDDB块的新颖架构。此外，采用了包括残差缩放和较小初始化的有用技术，以促进提出的深度模型的训练。我们还介绍了使用相对GAN作为判别器，其学习判断一张图像是否比另一张更真实，引导生成器恢复更详细的纹理。此外，我们通过使用激活之前的特征增强了感知损失，它提供了更强的监督，从而恢复了更精确的亮度和真实纹理。</p>
<p><strong>Acknowledgement</strong>. This work is supported by SenseTime Group Limited, the General Research Fund sponsored by the Research Grants Council of the Hong Kong SAR (CUHK 14241716, 14224316. 14209217), National Natural Science Foundation of China (U1613211) and Shenzhen Research Program (JCYJ20170818164704758, JCYJ20150925163005055).</p>
<p><strong>致谢</strong>。这项工作由商汤科技支持，香港特别行政区研究资助局（CUHK 14241716、14224316、14209217），中国国家自然科学基金（U1613211）和深圳研究计划（JCYJ20170818164704758，JCYJ20150925163005055）赞助。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p>Ledig,C.,Theis,L.,Husza ́r,F.,Caballero,J.,Cunningham,A.,Acosta,A.,Aitken, A., Tejani, A., Totz, J., Wang, Z., et al.: Photo-realistic single image super-resolution using a generative adversarial network. In: CVPR. (2017)</p>
</li>
<li><p>Jolicoeur-Martineau, A.: The relativistic discriminator: a key element missing from standard gan. arXiv preprint arXiv:1807.00734 (2018)</p>
</li>
<li><p>Blau, Y., Mechrez, R., Timofte, R., Michaeli, T., Zelnik-Manor, L.: The pirm challenge on perceptual super resolution. <a href="https://www.pirm2018.org/PIRM-SR" target="_blank" rel="external">https://www.pirm2018.org/PIRM-SR</a>. html (2018)</p>
</li>
<li><p>Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for image super-resolution. In: ECCV. (2014)</p>
</li>
<li><p>Kim, J., Kwon Lee, J., Mu Lee, K.: Accurate image super-resolution using very deep convolutional networks. In: CVPR. (2016)</p>
</li>
<li><p>Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Deep laplacian pyramid networks for fast and accurate super-resolution. In: CVPR. (2017)</p>
</li>
<li><p>Kim, J., Kwon Lee, J., Mu Lee, K.: Deeply-recursive convolutional network for image super-resolution. In: CVPR. (2016)</p>
</li>
<li><p>Tai, Y., Yang, J., Liu, X.: Image super-resolution via deep recursive residual network. In: CVPR. (2017)</p>
</li>
<li><p>Tai, Y., Yang, J., Liu, X., Xu, C.: Memnet: A persistent memory network for image restoration. In: ICCV. (2017)</p>
</li>
<li><p>Haris, M., Shakhnarovich, G., Ukita, N.: Deep backprojection networks for super- resolution. In: CVPR. (2018)</p>
</li>
<li><p>Zhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu, Y.: Residual dense network for image super-resolution. In: CVPR. (2018)</p>
</li>
<li><p>Zhang, Y., Li, K., Li, K., Wang, L., Zhong, B., Fu, Y.: Image super-resolution using very deep residual channel attention networks. In: ECCV. (2018)</p>
</li>
<li><p>Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: ECCV. (2016)</p>
</li>
<li><p>Bruna, J., Sprechmann, P., LeCun, Y.: Super-resolution with deep convolutional sufficient statistics. In: ICLR. (2015)</p>
</li>
<li><p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)</p>
</li>
<li><p>Sajjadi, M.S., Scho ̈lkopf, B., Hirsch, M.: Enhancenet: Single image super-resolution through automated texture synthesis. In: ICCV. (2017)</p>
</li>
<li><p>Wang, X., Yu, K., Dong, C., Loy, C.C.: Recovering realistic texture in image super-resolution by deep spatial feature transform. In: CVPR. (2018)</p>
</li>
<li><p>He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. (2016)</p>
</li>
<li><p>Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: ICMR. (2015)</p>
</li>
<li><p>Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M.: Enhanced deep residual networks for single image super-resolution. In: CVPRW. (2017)</p>
</li>
<li><p>Szegedy, C., Ioffe, S., Vanhoucke, V.: Inception-v4, inception-resnet and the impact of residual connections on learning. arXiv preprint arXiv:1602.07261 (2016)</p>
</li>
<li><p>Blau, Y., Michaeli, T.: The perception-distortion tradeoff. In: CVPR. (2017)</p>
</li>
<li><p>Ma, C., Yang, C.Y., Yang, X., Yang, M.H.: Learning a no-reference quality metric for single-image super-resolution. CVIU 158 (2017) 1–16</p>
</li>
<li><p>Mittal, A., Soundararajan, R., Bovik, A.C.: Making a completely blind image quality analyzer. IEEE Signal Process. Lett. 20(3) (2013) 209–212</p>
</li>
<li><p>Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convolutional networks. TPAMI 38(2) (2016) 295–307</p>
</li>
<li><p>Yu, K., Dong, C., Lin, L., Loy, C.C.: Crafting a toolchain for image restoration by deep reinforcement learning. In: CVPR. (2018)</p>
</li>
<li><p>Yuan, Y., Liu, S., Zhang, J., Zhang, Y., Dong, C., Lin, L.: Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks. In: CVPRW. (2018)</p>
</li>
<li><p>He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In: ICCV. (2015)</p>
</li>
<li><p>Gatys, L., Ecker, A.S., Bethge, M.: Texture synthesis using convolutional neural networks. In: NIPS. (2015)</p>
</li>
<li><p>Mechrez, R., Talmi, I., Shama, F., Zelnik-Manor, L.: Maintaining natural image statistics with the contextual loss. arXiv preprint arXiv:1803.04626 (2018)</p>
</li>
<li><p>Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. arXiv preprint arXiv:1701.07875 (2017)</p>
</li>
<li><p>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved training of wasserstein gans. In: NIPS. (2017)</p>
</li>
<li><p>Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957 (2018)</p>
</li>
<li><p>Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected convolutional networks. In: CVPR. (2017)</p>
</li>
<li><p>Nah, S., Kim, T.H., Lee, K.M.: Deep multi-scale convolutional neural network for dynamic scene deblurring. In: CVPR. (2017)</p>
</li>
<li><p>Zhang, K., Sun, M., Han, X., Yuan, X., Guo, L., Liu, T.: Residual networks of residual networks: Multilevel residual networks. IEEE Transactions on Circuits and Systems for Video Technology (2017)</p>
</li>
<li><p>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)</p>
</li>
<li><p>Bell, S., Upchurch, P., Snavely, N., Bala, K.: Material recognition in the wild with the materials in context database. In: CVPR. (2015)</p>
</li>
<li><p>Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICLR. (2015)</p>
</li>
<li><p>Agustsson, E., Timofte, R.: Ntire 2017 challenge on single image super-resolution: Dataset and study. In: CVPRW. (2017)</p>
</li>
<li><p>Timofte, R., Agustsson, E., Van Gool, L., Yang, M.H., Zhang, L., Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M., et al.: Ntire 2017 challenge on single image super-resolution: Methods and results. In: CVPRW. (2017)</p>
</li>
<li><p>Bevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In: BMVC, BMVA press (2012)</p>
</li>
<li><p>Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse-representations. In: International Conference on Curves and Surfaces, Springer (2010)</p>
</li>
<li><p>Martin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In: ICCV. (2001)</p>
</li>
<li><p>Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed self-exemplars. In: CVPR. (2015)</p>
</li>
<li><p>Timofte, R., Rothe, R., Van Gool, L.: Seven ways to improve example-based single image super resolution. In: CVPR. (2016)</p>
</li>
<li><p>Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural networks. In: International Conference on Artificial Intelligence and Statistics. (2010)</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      ESRGAN - Enhanced Super-Resolution Generative Adversarial Networks论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>ESRGAN - Enhanced Super-Resolution Generative Adversarial Networks论文翻译——中文版</title>
    <link href="http://noahsnail.com/2020/04/13/2020-04-13-ESRGAN%20-%20Enhanced%20Super-Resolution%20Generative%20Adversarial%20Networks%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2020/04/13/2020-04-13-ESRGAN - Enhanced Super-Resolution Generative Adversarial Networks论文翻译——中文版/</id>
    <published>2020-04-13T08:49:00.000Z</published>
    <updated>2020-05-22T03:33:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="ESRGAN-Enhanced-Super-Resolution-Generative-Adversarial-Networks"><a href="#ESRGAN-Enhanced-Super-Resolution-Generative-Adversarial-Networks" class="headerlink" title="ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks"></a>ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>超分辨率生成对抗网络(SR-GAN)[1]是一项开创性的工作，其能够在单图像超分辨率期间生成逼真的纹理。然而，虚幻的细节常常伴随讨厌的伪像。为了进一步增强视觉质量，我们充分研究了SRGAN的三个关键组成部分——网络架构、对抗损失和感知损失，并对每一个都进行了改进以取得增强的SRGAN(ESRGAN)。特别的是，我们引入了没有批归一化的Residual-in-Residual Dense Block(RRDB)作为基本的网络构架单元。此外，我们借鉴了相对GAN[2]中的思想，让判别器预测相对真实性而不是绝对值。最后，我们通过使用激活前的特征改进感知损失，这可以对亮度一致性和纹理复原提供更强的监督。得益于这些改进，相比于SRGAN，提出的ESRGAN一致地取得了更好的视觉质量、更多真实自然的纹理，并在PIRM2018-SR Challenge[3]中获得了第一名。源码地址：<a href="https://github.com/xinntao/ESRGAN" target="_blank" rel="external">https://github.com/xinntao/ESRGAN</a>。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>作为一个基本的低级视觉问题，单图像超分辨率(SISR)在研究领域和AI公司中引起了越来越多的关注。SISR目标是从一张低分辨率(LR)图像复原出一张高分辨率(HR)图像。从Dong等[4]提出SRCNN的开创性工作以来，深度卷积神经网络(CNN)方法带来了繁荣的发展。各种网络架构设计和训练策略持续地改善SR性能，尤其是峰值信噪比(PSNR)的值[5,6,7,1,8,9,10,11,12]。然而，这些面向PSNR的方法趋向于输出过于平滑的结果，缺少足够的高频细节，因为PSNR度量从根本上与人类观察者的主观评价[1]不符。</p>
<p>已经提出了一些感知驱动的方法来改进SR结果的视觉质量。例如，提出感知损失[13,14]来优化在特征空间而不是像素空间中的超分辨率模型。[1,16]引入生成对抗网络[15]到SR中以鼓励网络支持看起来更像自然图像的解。语义图像先验被进一步合并以改善恢复的纹理细节[17]。追寻视觉愉悦效果的方法中的里程碑之一是SRGAN[1]。基本模型是用残差块构建的[18]，并在GAN框架中使用感知损失来进行优化。通过所有这些技术，与面向PSNR的方法相比，SRGAN显著改善了重建的整体视觉质量。</p>
<p>然而，如图1所示，SRGAN结果与真实(GT)图像之间仍然存在明显的差距。在本研究中，我们重新审视SRGAN的关键组件，并在三个方面改进模型。首先，我们通过引入Residual-in-Residual Dense Block(RDDB)改进网络架构，该结构具有较高的能力且更容易训练。我们像[20]中一样也移除了批归一化(BN)[19]层，使用残差缩放[21,20]和更小的初始化来促进训练一个非常深的网络。其次，我们使用相对平均GAN(RaGAN)[2]来改进判别器，RaGAN学习判断“一张图像是否比另一张更真实”而不是“一张图像时真的还是假的”。我们的实验表明这个改进有助于生成器恢复更多的真实纹理细节。第三，我们提出了一种改进的感知损失，使用激活之前的VGG特征来代替SRGAN中激活之后的VGG特征。从经验上我们发现调整之后的感知损失提供了更清晰的边缘和视觉上更令人满意的结果，如4.4节所示。大量的实验表明增强SRGAN(称为ESRGAN)在清晰度和细节方面都始终优于最新的方法（见图1和图7）。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_1.jpeg" alt="Figure 1"></p>
<p>图1：SRGAN、提出的ESRGAN和实际的4倍超分辨率结果。ESRGAN在清晰度和细节方面优于SRGAN。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_7.jpeg" alt="Figure 7"></p>
<p>图7：ESRGAN的定性结果。ESRGAN生成了更自然的纹理，例如，动物皮毛，建筑物结构和草坪纹理，以及更少的令人不快的伪影，例如SRGAN中脸上的伪影。</p>
<p>我们采用ESRGAN的一个变种来参加PIRM-SR挑战赛[3]。这个挑战是第一个在[22]的基础上以察觉感知质量的方式评估性能的SR竞赛，[22]中作者声称失真和感知质量相互矛盾。感知质量是通过Ma分数[23]和NIQE[24]的非参考度量来判断的，即感知指数$=\frac {1} {2}((10−Ma)+NIQE)$。更低的感知指数表示更好的感知质量。</p>
<p>如图2所示，通过均方根误差(RMSE)的阈值，将感知失真平面分成三个区域，每个区域中取得最低感知指数的算法为区域冠军。我们主要关注区域3，因为我们旨在将感知质量提升到新的高度。由于上述的改进和4.6节中讨论的一些其它调整，我们提出的ESRGAN在PIRM-SR挑战赛（区域3）中以最好的感知指数赢得了第一名。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_2.jpeg" alt="Figure 2"></p>
<p>图2：PIRM自验证集上的感知失真平面。我们展示了EDSR[20]，RCAN[12]，EnhanceNet[16]以及提交的ESRGAN模型的基准线。蓝色的点通过图像插值生成。</p>
<p>为了平衡视觉质量和RMSE/PSNR，我们进一步提出了网络插值策略，其可以持续地调整重建风格和平滑度。另一种替代方案是图像插值，其直接逐像素地插值图像。我们采用这个策略来参加区域1和区域2。网络插值和图像插值策略以及它们的差异在3.4节中讨论。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>我们专注于解决SR问题的深度神经网络方法。作为开创性工作，Dong等[4,25]提出了SRCNN以端到端的方式来学习从LR到SR图像的映射，取得了优于之前工作的性能。后来，这个领域见证了各种网络架构，例如具有残差学习的神经网络[5]，拉普拉斯金字塔结构[6]，残差块[1]，递归学习[7,8]，密集连接网络[9]，深度反向投影[10]和残差密集网络[11]。具体来说，Lim等[20]通过移除残差块中不必要的BN层以及扩展模型尺寸提出了EDSR模型，取得了显著的改善。Zhang等[11]在SR中提出了使用有效的残差密集块，并且他们进一步开发了一个使用通道注意力[12]的更深网络，取得了最佳的PSNR性能。除了监督学习之外，也引入了其它的方法像强化学习[26]以及无监督学习[27]来解决一般的图像复原问题。</p>
<p>已经提出了一些方法来稳定训练非常深的模型。例如，开发残差路径来稳定训练并改善性能[18,5,12]。Szegedy等[21]首次采用残差缩放，也在EDSR中使用。对于一般的深度网络，He等[28]为没有BN的VGG风格的网络提出了一个鲁棒的初始化方法。为了便于训练更深的网络，我们也开发了一个简洁有效的残差套残差密集块，这有助于改善感知质量。</p>
<p>感知驱动的方法已经被提出用来改善SR结果的视觉质量。基于更接近于感知相似度[29,14]的想法提出感知损失[13]，通过最小化特征空间而不是像素空间的误差来增强视觉质量。通过使用专注于特征分布而不是只比较外观的目标函数，开发上下文损失[30]来生成具有自然图像统计的图像。Ledig等[1]提出SRGAN模型，使用感知损失和对抗损失来支持位于自然图像流形的输出。Sajjadi等[16]开发了类似的方法并进一步探索了局部纹理匹配损失。基于这些工作，Wang等[17]提出空间特征变换来有效地将语义先验合并到图像中并改进恢复的纹理。</p>
<p>在整个文献中，通常通过与GAN[15]的对抗训练来获得写实主义照片。最近有很多工作致力于开发更有效的GAN框架。WGAN[31]提出最小化Wasserstein距离的合理和有效近似，并通过权重修剪来正则化判别器。其它对判别器的正则化包括梯度修剪[32]和谱归一化[33]。开发的相对判别器[2]不仅提高了生成数据真实性的概率，而且同时降低了真实数据真实性的概率。在这项工作中，我们通过采用更有效的相对平均GAN来增强SRGAN。</p>
<p>SR通常通过几种广泛使用的失真测量方式来进行评估，例如PSNR和SSIM。然而，这些度量从根本上与人类观察者的主观评估不一致[1]。非参考度量通常用于感知质量评估，包括Ma的分数[23]和NIQE[24]，两者都用于PIRM-SR挑战赛中[3]计算感知指数。在最近的一项研究中，Blau等[22]发现失真和感知质量相互矛盾。</p>
<h2 id="3-提出的方法"><a href="#3-提出的方法" class="headerlink" title="3 提出的方法"></a>3 提出的方法</h2><p>我们的主要目标是提高SR的整体感知质量。在本节中，我们首先描述我们提出的网络架构，然后讨论判别器和感知损失的改进。最后，我们描述用于平衡感知质量和PSNR的网络插值策略。</p>
<h3 id="3-1-网络架构"><a href="#3-1-网络架构" class="headerlink" title="3.1 网络架构"></a>3.1 网络架构</h3><p>为了进一步改进SRGAN复原的图像质量，我们主要对生成器G的架构进行了两个修改：1）移除所有的BN层；2）用提出的残差套残差密集块(RRDB)替换原始的基本块，它结合了多层残差网络和密集连接，如图4所示。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_4.jpeg" alt="Figure 4"></p>
<p>图4：左：我们移除了SRGAN残差块中的BN层。右：RRDB块用在我们的更深模型中，$\beta$是残差尺度参数。</p>
<p>在不同的面向PSNR的任务（包括SR[20]和去模糊[35]）中，已经证实了移除BN层可以提高性能并降低计算复杂度。BN层在训练中使用一批数据的均值和方差对特征进行归一化，并在测试中使用整个训练集估计的均值和方差。当训练集和测试集的统计差别很大时，BN层趋向于引入令人不快的伪影并限制泛化能力。我们凭经验观察到，当网络较深且在GAN架构下训练时，BN层更可能带来伪影。这些伪影有时会在迭代中间和不同的设置下出现，违背了训练过程中对于稳定性能的需求。因此，我们为了稳定的训练和一致的性能移除了BN层。此外，移除BN层有助于提高泛化能力并降低计算复杂度及内存使用。</p>
<p>我们保留了SRGAN的高级架构设计（见图3），并使用了一个新颖的名为RRDB的基本块，如图4所示。基于观测，更多的层和连接总是可以提升性能[20,11,12]，与SRGAN中的原始残差块相比，提出的RRDB采用了更深更复杂的架构。具体地说，如图4所示，提出了的RRDB有残差套残差的结构，其中残差学习用在不同的级别中。[36]中提出的类似结构也适用于多级残差网络。然而，我们的RRDB与[36]的不同在于我们在主路径中使用了如[11]的密集块[34]，受益于密集连接其网络容量变得更高。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_3.jpeg" alt="Figure 3"></p>
<p>图3：我们采用SRResNet[1]的基本架构，大多数计算都在LR特征空间进行。我们可以为了更佳的性能选择或设计“基础块”（例如，残差块[18]，密集块[34]，RRDB）。</p>
<p>除了改进架构之外，我们也利用几种技术来促进训练非常深的网络：1）残差缩放[21,20]，例如在将残差加到主路径上之前，通过将其乘以一个0-1之间的常量来缩小残差以防止不稳定性；2）更小的初始化，因为我们凭经验发现当初始参数方差变得更小时，残差结构更容易训练。更多讨论可在<em>补充材料</em>中找到。</p>
<p>训练细节和提出网络的有效性将在第4节中介绍。</p>
<h3 id="3-2-相对判别器"><a href="#3-2-相对判别器" class="headerlink" title="3.2 相对判别器"></a>3.2 相对判别器</h3><p>除了改进生成器架构之外，我们还在相对GAN[2]的基础上增强了判断器。不同于SRGAN中的标注判别器$D$，$D$估算输入图像$x$是真实自然的概率，相对判别器尝试预测真实图像$x_r$比假图像$x_f$相对更真实的概率，如图5所示。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_5.jpeg" alt="Figure 5"></p>
<p>图5：标准判别器和相对判别器的差异。</p>
<p>具体来说，我们用相对平均判别器RaD[2]代替标准判别器，记为$D_{Ra}$。SRGAN中的标准判别器可表示为$D(x) = \sigma(C(x))$，其中$\sigma$是sigmoid函数，$C(x)$是非变换判别器输出。然后RaD用公式表示为$D_{Ra}(x_r, x_f) = \sigma(C(x_r) − \mathbb{E}_{x_f}[C(x_f)])$，其中$\mathbb{E}_{x_f}[\bullet]$表示对小批次中所有假数据取平均值的操作。然后判别器损失定义为：$$L^{Ra}_{D} =−\mathbb{E}_{x_r}[log(D_{Ra}(x_r, x_f))]−\mathbb{E}_{x_f}[1 - log(D_{Ra}(x_f, x_r))]. \tag{1}$$</p>
<p>生成器的对抗损失呈对称形式：$$L^{Ra}_{G} =−\mathbb{E}_{x_r}[1-log(D_{Ra}(x_r, x_f))]−\mathbb{E}_{x_f}[log(D_{Ra}(x_f, x_r))], \tag{2}$$</p>
<p>其中$x_f=G(x_i)$和$x_i$代表输入LR图像。可以看出，生成器的对抗损失包含$x_r$和$x_f$。因此，在对抗训练中，我们的生成器受益于生成数据和真实数据的梯度，而在SRGAN中仅生成部分起作用。在4.4节中，我们将展示判别器的这种修改有助于学习更清晰的边缘和更细致的纹理。</p>
<h3 id="3-3-感知损失"><a href="#3-3-感知损失" class="headerlink" title="3.3 感知损失"></a>3.3 感知损失</h3><p>通过约束激活之前的特征而不是SRGAN中实践的激活之后的特征，我们还开发了一种更有效的感知损失$L_{percep}$。</p>
<p>基于更接近感知相似[29,14]的想法，Johnson等[13]提出了感知损失并在SRGAN[1]中得到了扩展。之前的感知损失定义在预训练深度网络的激活层上，最小化两个激活特征之间的距离。与常规用法相反，我们提出使用激活层之前的特征，这将克服原始设计的两个缺点。首先，激活特征非常稀疏，尤其是在非常深的网络之后，如图6所示。例如，图像“狒狒”在VGG19-54层之后激活神经元的平均百分比只有$11.17\%$。稀疏的激活提供了弱监督，因此导致性能较差。其次，与真实图像相比，使用激活之后的特征也会导致重建亮度不一致，这将在4.4节中展示。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_6.jpeg" alt="Figure 6"></p>
<p>图6：图像“狒狒”激活之前和激活之后代表性的特征映射。随着网络加深，大多数激活之后的特征变得不活跃而激活之前的特征包含更多的信息。</p>
<p>因此，生成器的全部损失为：$$L_G = L_{percep} + \lambda L^{Ra}_G + \eta L_1 \tag{3}$$，其中$L_1 = \mathbb{E}_{x_i} || G(x_i) − y||_1$是内容损失，用来评估恢复图像$G(x_i)$和真实图像$y$之间的1范数距离，$\lambda, \eta$是平衡不同损失项的系数。</p>
<p>我们在PIRM-SR挑战赛中也探索了感知损失的变种。与采用图像分类训练的VGG网络的常用感知损失相比，我们为SR–MINC损失开发了一种更合适的感知损失。它是基于材料识别[38]的微调VGG网络，该网络注重于纹理而不是目标。尽管MINC损失带来的感知指数收益是微不足道的，但我们仍然认为，采用注重纹理的感知损失对于SR至关重要。</p>
<h3 id="3-4-网络插值"><a href="#3-4-网络插值" class="headerlink" title="3.4 网络插值"></a>3.4 网络插值</h3><p>为了去除基于GAN方法中讨厌的噪声同时保持好的感知质量，我们提出了一种弹性有效的策略——网络插值。具体来说，我们首先训练一个面向PSNR的网络$G_{PSNR}$，然后通过微调获得一个基于GAN的网络$G_{GAN}$。我们插值这两个网络的所有对应参数来取得插值模型$G_{INTERP}$，其参数为：$$\theta^{INTERP}_{G} = (1 − \alpha) \theta^{PSNR}_{G} + \alpha \theta^{GAN}_{G} \tag{4}$$ 其中$G_{INTERP}$, $G_{PSNR}$和$G_{GAN}$分别是$\theta^{INTERP}_{G}$, $\theta^{PSNR}_{G}$和$\theta^{GAN}_{G}$的参数，$\alpha \in [0, 1]$为插值参数。</p>
<p>提出的网络插值有两个优点。首先，插值模型对于任何合理的$\alpha$都能产生有意义的结果而不会产生伪影。其次，我们可以持续平衡感知质量和保真度都不必重新训练模型。</p>
<p>我们也探索了替代方法来平衡面向PSNR方法和基于GAN方法的影响。例如，可以直接插值它们的输出图像（逐像素）而不是网络参数。然而，这种方法不会在噪声和模糊之间取得良好的权衡，即插值图像或太模糊或带有伪影的噪声太大（见4.5节）。另一种方法是调整内容损失和对抗损失的权重，即方程3中的参数$\lambda$和$\eta$。但这种方法要求调整损失权重并微调网络，因此实现图像风格的连续控制代价很高。</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><h3 id="4-1-训练细节"><a href="#4-1-训练细节" class="headerlink" title="4.1 训练细节"></a>4.1 训练细节</h3><p>按照SRGAN[1]，所有实验在LR和HR图像间均以4倍的尺度系数进行。我们通过使用MATLAB双三次核函数对HR图像进行下采样来获得LR图像。最小批次大小设置为16。裁剪的HR图像块的空间大小为128×128。我们观察到，训练更深的网络可以从更大的批次大小中获益，因为扩大的感受野有助于捕获更多的语义信息。但是，这会花费更多的训练时间并消耗更多的计算资源。这种现象也可以在面向PSNR的方法中观察到（见补充材料）。</p>
<p>训练过程分为两个阶段。首先，我们训练一个具有L1损失的面向PSNR的模型。学习率初始化为$2 × 10^{−4}$，每$2 × 10^5$个小批次更新的衰减因子为2。然后，我们采用训练的面向PSNR的模型作为生成器的初始化。生成器训练使用等式3中的损失函数，$\lambda = 5×10^{−3}$，$\eta = 1×10^{−2}$。学习率设置为$1×10^{−4}$，在[50k, 100k, 200k, 300k]次迭代之后减半。使用逐像素损失进行预训练有助于基于GAN的方法获得视觉上更好的结果。原因是：1）它可以避免生成器不希望的局部最优；2）在预训练之后，最初判别器可以收到相对好的超分辨率图像而不是极端假的图像（黑色或噪声图像），这有助于其更关注纹理判别。</p>
<p>为了优化，我们使用Adam[39]，其中$\beta_1 = 0.9， \beta_2 = 0.999$。我们交替更新生成器和判别器网络，直到模型收敛。我们为生成器使用了两种设置——其中一种包含16个残差块，能力类似于SRGAN，另一种是具有23个RRDB块的更深的模型。我们使用PyTorch框架实现我们的模型，并使用NVIDIA Titan Xp GPU对其进行训练。</p>
<h3 id="4-2-数据"><a href="#4-2-数据" class="headerlink" title="4.2 数据"></a>4.2 数据</h3><p>对于训练，我们主要使用DIV2K数据集[40]，它是用于图像复原任务的高质量（2K分辨率）数据集。除了包含800张图像的DIV2K训练集外，我们也搜寻了其它具有丰富多样纹理的数据集进行训练。为此，我们进一步使用Flickr2K数据集[41]，包含Flickr网站上收集的2650张2K高分辨率图像，OutdoorSceneTraining(OST)[17]数据集来丰富我们的训练集。我们凭经验发现，使用具有丰富纹理的大型数据集有助于生成器产生更自然的结果，如图8所示。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_8.jpeg" alt="Figure 8"></p>
<p>图8：展示ESRGAN中每个组件效果的整体视觉比较。每一列表示一个模型，其配置在顶部。红色符号表示与前面模型相比的主要改进。</p>
<p>我们在RGB通道训练模型，并通过随机水平翻转和90度旋转来增强训练集。我们在广泛使用的基准数据集——Set5[42]，Set14[43]，BSD100[44]，Urban100[45]以及PIRM-SR挑战赛提供的PIRM自验证数据上评估我们的模型。</p>
<h3 id="4-3-定性结果"><a href="#4-3-定性结果" class="headerlink" title="4.3 定性结果"></a>4.3 定性结果</h3><p>我们将最终的模型与最新的面向PSNR的方法包括SRCNN[4]，EDSR[20]和RCAN[12]，以及感知驱动的方法包括在SRGAN[1]和EnhanceNet[16]在一些公开基准数据集上进行了比较。由于对于感知质量没有有效标准的度量标准，我们在图7中展示了一些具有代表性的结果，也提供了PSNR（在YCbCr颜色空间的亮度通道上评估）和PIRM-SR挑战赛中的感知指数供参考。</p>
<p>从图7可以看出，我们提出的ESRGAN在清晰度和细节方面都优于之前的方法。例如，与面向PSNR的方法（更趋向于产生模糊的结果）和以前的基于GAN的方法（纹理不自然并包含令人不快的噪声）相比，ESRGAN可以产生更清晰更自然的狒狒胡须和草的纹理（见图43074）。在建筑物中（见图102061），ESRGAN能够产生更详细的结构而其它的方法要么不能产生足够的细节(SRGAN)，要么添加不必要的纹理(EnhanceNet)。此外，以前基于GAN的方法有时会引入令人不快的伪影，例如SRGAN会在脸上添加皱纹。我们的ESRGAN除去了这些伪影并产生了自然的结果。</p>
<h3 id="4-4-消融研究"><a href="#4-4-消融研究" class="headerlink" title="4.4 消融研究"></a>4.4 消融研究</h3><p>为了研究提出的ESRGAN中每个组件的效果，我们逐渐修改基准的SRGAN模型并比较它们的差异。完整的视觉比较如图8所示。每一列表示一个模型，其配置在顶部。红色符号表明与前面模型相比的主要改进。详细讨论提供如下。</p>
<p><strong>移除BN</strong>。为了稳定和没有伪影的一致性能，我们首先移除了所有的BN层。它不会降低性能但会节省计算资源和内存使用。在某些情况下，从图8中的第2列和第3列可以观察到轻微的改进（例如，图39）。此外，我们观察到当网络更深更复杂时，具有BN层的模型更可能引入令人不快的伪影。可以在补充材料中找到示例。</p>
<p><strong>感知损失在激活之前</strong>。我们首先证实了使用激活之前的特征可以使重建图像的亮度更准确。为了消除纹理和颜色的影响，我们使用高斯核对图像进行了滤波并绘制了其对应灰度图像的直方图。图9a展示了每一个亮度值的分布。使用激活的特征会使分布偏向左，导致了较暗的输出，而使用激活之前的特征会得到更精确的亮度分布，更接近于真实图像的亮度分布。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_9.jpeg" alt="Figure 9"></p>
<p>图9：激活之前和激活之后的比较。</p>
<p>我们可以进一步观察到，使用激活之前的特征有助于产生更清晰的边缘和更丰富的纹理，如图9b（见鸟羽）和图8（见第三列和第四列）所示，因为与稀疏激活提供的特征相比，激活之前的密集特征能提供更强的监督。</p>
<p><strong>RaGAN</strong>。RaGAN使用改进的相对判别器，证明了其有利于学习更清晰的边缘和更细致的纹理。例如，在图8的第5列中，生成的图像比其左侧的图像更清晰，具有更丰富的纹理（见狒狒，图39和图43074）。</p>
<p><strong>具有RRDB的更深网络</strong>。具有提出的RRDB的更深模型可以进一步改善恢复的纹理，尤其是像图8中图像6的屋顶这样的常规结构，因为深度模型具有强大的表示能力来捕获语义信息。 我们也发现更深的模型可以减少像图8中图像20这样的令人不快的噪声。</p>
<p>与SRGAN声称的更深的模型越来越难训练相比，由于上述提供的改进尤其是提出的没有BN层的RRDB，我们更深的模型展示了它容易训练且优越性能。</p>
<h3 id="4-5-网络插值"><a href="#4-5-网络插值" class="headerlink" title="4.5 网络插值"></a>4.5 网络插值</h3><p>我们比较了网络插值和图像插值策略在平衡面向PSNR模型与基于GAN方法的结果方面的作用。我们在这个两个方案中应用了简单的线性插值。插值参数$\alpha$从间隔为0.2的0-1之间选取。</p>
<p>如图10所示，单纯的基于GAN的方法会产生清晰的边缘和更丰富的纹理，但带有一些令人不快的伪影，而单纯的面向PSNR方法会输出卡通风格的模糊图像。通过采用网络插值，在减少令人不快的伪影的同时保持了纹理。相比之下，图像插值不能有效消除这些伪影。</p>
<p><img src="http://noahsnail.com/images/esrgan/Figure_10.jpeg" alt="Figure 10"></p>
<p>图10：网络插值和图像插值的比较。</p>
<p>有趣的是，在图10中观察到网络插值策略提供了对平衡感知质量和保真度的平滑控制。</p>
<h3 id="4-6-PIRM-SR挑战赛"><a href="#4-6-PIRM-SR挑战赛" class="headerlink" title="4.6 PIRM-SR挑战赛"></a>4.6 PIRM-SR挑战赛</h3><p>我们采用ESRGAN的一个变种来参加PIRM-SR挑战赛[3]。具体来说，我们使用提出的具有16个残差块的ESRGAN，并根据经验进行了一些修改来迎合感知指数。1）使用MINC损失作为感知损失的一个变种，如3.3节所述。尽管在感知指数上有边际收益，但我们仍认为采用专注于纹理的感知损失对于SR至关重要；2）我们的训练中也使用了Pristine数据集[24]来学习感知指数；3）由于PSNR约束，$L_1$损失的权重高达$\eta = 10$；4）我们也使用反向投射[46]作为后处理，其可以改善PSNR，有时会降低感知指数。</p>
<p>对于其它需要较高PSNR的区域1和2，我们在ESRGAN的结果和面向PSNR方法RCAN[12]的结果之间使用图像插值。尽管通过使用网络插值方案我们观察到了视觉上更令人满意的效果，但图像插值方案取得了较低的感知指数（越低越好）。我们提出的ESRGAN模型以最好的感知指数赢得了PIRM-SR挑战赛（区域3）的第一名。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>我们提出了一种ESRGAN模型，它比以前的SR方法始终取得更好的感知质量。就感知指数而言，该方法在PIRM-SR挑战赛中获得了第一名。我们构建了一种包含一些没有BN层的RDDB块的新颖架构。此外，采用了包括残差缩放和较小初始化的有用技术，以促进提出的深度模型的训练。我们还介绍了使用相对GAN作为判别器，其学习判断一张图像是否比另一张更真实，引导生成器恢复更详细的纹理。此外，我们通过使用激活之前的特征增强了感知损失，它提供了更强的监督，从而恢复了更精确的亮度和真实纹理。</p>
<p><strong>致谢</strong>。这项工作由商汤科技支持，香港特别行政区研究资助局（CUHK 14241716、14224316、14209217），中国国家自然科学基金（U1613211）和深圳研究计划（JCYJ20170818164704758，JCYJ20150925163005055）赞助。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p>Ledig,C.,Theis,L.,Husza ́r,F.,Caballero,J.,Cunningham,A.,Acosta,A.,Aitken, A., Tejani, A., Totz, J., Wang, Z., et al.: Photo-realistic single image super-resolution using a generative adversarial network. In: CVPR. (2017)</p>
</li>
<li><p>Jolicoeur-Martineau, A.: The relativistic discriminator: a key element missing from standard gan. arXiv preprint arXiv:1807.00734 (2018)</p>
</li>
<li><p>Blau, Y., Mechrez, R., Timofte, R., Michaeli, T., Zelnik-Manor, L.: The pirm challenge on perceptual super resolution. <a href="https://www.pirm2018.org/PIRM-SR" target="_blank" rel="external">https://www.pirm2018.org/PIRM-SR</a>. html (2018)</p>
</li>
<li><p>Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for image super-resolution. In: ECCV. (2014)</p>
</li>
<li><p>Kim, J., Kwon Lee, J., Mu Lee, K.: Accurate image super-resolution using very deep convolutional networks. In: CVPR. (2016)</p>
</li>
<li><p>Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Deep laplacian pyramid networks for fast and accurate super-resolution. In: CVPR. (2017)</p>
</li>
<li><p>Kim, J., Kwon Lee, J., Mu Lee, K.: Deeply-recursive convolutional network for image super-resolution. In: CVPR. (2016)</p>
</li>
<li><p>Tai, Y., Yang, J., Liu, X.: Image super-resolution via deep recursive residual network. In: CVPR. (2017)</p>
</li>
<li><p>Tai, Y., Yang, J., Liu, X., Xu, C.: Memnet: A persistent memory network for image restoration. In: ICCV. (2017)</p>
</li>
<li><p>Haris, M., Shakhnarovich, G., Ukita, N.: Deep backprojection networks for super- resolution. In: CVPR. (2018)</p>
</li>
<li><p>Zhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu, Y.: Residual dense network for image super-resolution. In: CVPR. (2018)</p>
</li>
<li><p>Zhang, Y., Li, K., Li, K., Wang, L., Zhong, B., Fu, Y.: Image super-resolution using very deep residual channel attention networks. In: ECCV. (2018)</p>
</li>
<li><p>Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-resolution. In: ECCV. (2016)</p>
</li>
<li><p>Bruna, J., Sprechmann, P., LeCun, Y.: Super-resolution with deep convolutional sufficient statistics. In: ICLR. (2015)</p>
</li>
<li><p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)</p>
</li>
<li><p>Sajjadi, M.S., Scho ̈lkopf, B., Hirsch, M.: Enhancenet: Single image super-resolution through automated texture synthesis. In: ICCV. (2017)</p>
</li>
<li><p>Wang, X., Yu, K., Dong, C., Loy, C.C.: Recovering realistic texture in image super-resolution by deep spatial feature transform. In: CVPR. (2018)</p>
</li>
<li><p>He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. (2016)</p>
</li>
<li><p>Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: ICMR. (2015)</p>
</li>
<li><p>Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M.: Enhanced deep residual networks for single image super-resolution. In: CVPRW. (2017)</p>
</li>
<li><p>Szegedy, C., Ioffe, S., Vanhoucke, V.: Inception-v4, inception-resnet and the impact of residual connections on learning. arXiv preprint arXiv:1602.07261 (2016)</p>
</li>
<li><p>Blau, Y., Michaeli, T.: The perception-distortion tradeoff. In: CVPR. (2017)</p>
</li>
<li><p>Ma, C., Yang, C.Y., Yang, X., Yang, M.H.: Learning a no-reference quality metric for single-image super-resolution. CVIU 158 (2017) 1–16</p>
</li>
<li><p>Mittal, A., Soundararajan, R., Bovik, A.C.: Making a completely blind image quality analyzer. IEEE Signal Process. Lett. 20(3) (2013) 209–212</p>
</li>
<li><p>Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convolutional networks. TPAMI 38(2) (2016) 295–307</p>
</li>
<li><p>Yu, K., Dong, C., Lin, L., Loy, C.C.: Crafting a toolchain for image restoration by deep reinforcement learning. In: CVPR. (2018)</p>
</li>
<li><p>Yuan, Y., Liu, S., Zhang, J., Zhang, Y., Dong, C., Lin, L.: Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks. In: CVPRW. (2018)</p>
</li>
<li><p>He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In: ICCV. (2015)</p>
</li>
<li><p>Gatys, L., Ecker, A.S., Bethge, M.: Texture synthesis using convolutional neural networks. In: NIPS. (2015)</p>
</li>
<li><p>Mechrez, R., Talmi, I., Shama, F., Zelnik-Manor, L.: Maintaining natural image statistics with the contextual loss. arXiv preprint arXiv:1803.04626 (2018)</p>
</li>
<li><p>Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. arXiv preprint arXiv:1701.07875 (2017)</p>
</li>
<li><p>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved training of wasserstein gans. In: NIPS. (2017)</p>
</li>
<li><p>Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957 (2018)</p>
</li>
<li><p>Huang, G., Liu, Z., Weinberger, K.Q., van der Maaten, L.: Densely connected convolutional networks. In: CVPR. (2017)</p>
</li>
<li><p>Nah, S., Kim, T.H., Lee, K.M.: Deep multi-scale convolutional neural network for dynamic scene deblurring. In: CVPR. (2017)</p>
</li>
<li><p>Zhang, K., Sun, M., Han, X., Yuan, X., Guo, L., Liu, T.: Residual networks of residual networks: Multilevel residual networks. IEEE Transactions on Circuits and Systems for Video Technology (2017)</p>
</li>
<li><p>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)</p>
</li>
<li><p>Bell, S., Upchurch, P., Snavely, N., Bala, K.: Material recognition in the wild with the materials in context database. In: CVPR. (2015)</p>
</li>
<li><p>Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICLR. (2015)</p>
</li>
<li><p>Agustsson, E., Timofte, R.: Ntire 2017 challenge on single image super-resolution: Dataset and study. In: CVPRW. (2017)</p>
</li>
<li><p>Timofte, R., Agustsson, E., Van Gool, L., Yang, M.H., Zhang, L., Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M., et al.: Ntire 2017 challenge on single image super-resolution: Methods and results. In: CVPRW. (2017)</p>
</li>
<li><p>Bevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In: BMVC, BMVA press (2012)</p>
</li>
<li><p>Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse-representations. In: International Conference on Curves and Surfaces, Springer (2010)</p>
</li>
<li><p>Martin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In: ICCV. (2001)</p>
</li>
<li><p>Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed self-exemplars. In: CVPR. (2015)</p>
</li>
<li><p>Timofte, R., Rothe, R., Van Gool, L.: Seven ways to improve example-based single image super resolution. In: CVPR. (2016)</p>
</li>
<li><p>Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward neural networks. In: International Conference on Artificial Intelligence and Statistics. (2010)</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      ESRGAN - Enhanced Super-Resolution Generative Adversarial Networks论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文翻译——中文版</title>
    <link href="http://noahsnail.com/2020/04/10/2020-04-10-Photo-Realistic%20Single%20Image%20Super-Resolution%20Using%20a%20Generative%20Adversarial%20Network%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2020/04/10/2020-04-10-Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文翻译——中文版/</id>
    <published>2020-04-10T05:07:26.000Z</published>
    <updated>2020-05-21T08:18:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Photo-Realistic-Single-Image-Super-Resolution-Using-a-Generative-Adversarial-Network"><a href="#Photo-Realistic-Single-Image-Super-Resolution-Using-a-Generative-Adversarial-Network" class="headerlink" title="Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"></a>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>尽管使用更快更深的卷积神经网络在单图像超分辨率的准确性和速度方面取得了突破，但仍有一个主要问题尚未解决：当使用大的上采样系数进行超分辨率时，我们怎样来恢复更精细的纹理细节。基于优化的超分辨率方法的行为主要由目标函数的选择来决定。最近的工作主要专注于最小化均方重构误差。由此得出的评估结果具有很高的峰值信噪比，但它们通常缺乏高频细节，并且在感知上是不令人满意的，在某种意义上，它们在较高分辨率上没有满足期望的保真度。在本文中，我们提出了SRGAN，一种用于图像超分辨率(SR)的生成对抗网络(GAN)。据我们所知，这是第一个对于4倍上采样系数，能推断逼真自然图像的框架。为此，我们提出了一种感知损失函数，其由对抗损失和内容损失组成。对抗损失使用判别器网络将我们的解推向自然图像流形，判别器网络经过训练用以区分超分辨率图像和原始的逼真图像。此外，我们使用由感知相似性而不是像素空间相似性引起的内容损失。在公开的基准数据集上，我们的深度残差网络能从过度下采样图像中恢复出逼真的纹理。广泛的平均主观得分(MOS)测试显示，使用SRGAN可以显著提高感知质量。与任何最新方法获得的MOS得分相比，使用SRGAN获得的MOS得分更接近于原始高分辨率图像的MOS得分。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>从低分辨率(LR)图像来估计其对应高分辨率(HR)图像的高挑战性任务被称作超分辨率(SR)。SR在计算机视觉研究领域受到了广泛的关注并有大量应用[62, 70, 42]。</p>
<p>欠定SR问题的不适定特性对于大的上采样系数尤其显著，重建的SR图像中通常缺少纹理细节。有监督SR算法的优化目标通常是最小化恢复的HR图像和真实图像之间的均方误差(MSE)。最小化MSE即最大化峰值信噪比(PSNR)是方便的，这是用来评估和比较SR算法的常用方法[60]。然而，MSE(和PSNR)捕获感知相对差异(例如高级纹理细节)的能力是非常有限的，因为它们是基于像素级图像差异[59, 57, 25]定义的。这在图2中进行了说明，其中最高的PSNR不一定能反映出感知上更好的SR结果。超分辨率图像和原始图像之间的感知差异意味着恢复图像不如Ferwerda[15]中定义的逼真。</p>
<p><img src="http://noahsnail.com/images/srgan/srgan_figure_2.jpeg" alt="Figure 2"></p>
<p>图2：从左到右：双三次插值，优化MSE的深度残差网络，优化人感知更敏感损失的深度残差生成对抗网络，原始HR图像。对应的PSNR和SSIM显示在括号中。[4倍上采样]</p>
<p>在这项工作中我们提出了一种超分辨率生成对抗网络(SRGAN)，为此我们采用了具有跳跃连接的深度残差网络并舍弃了作为唯一优化目标的MSE。不同于以前的工作，我们定义了一种新的使用VGG网络[48, 32, 4]高级特征映射与判别器结合的感知损失，判别器会鼓励感知上更难与HR参考图像区分的解。图1中展示了一张示例逼真图像，其使用4倍上采样系数进行超分辨率。</p>
<p><img src="https://i.loli.net/2020/03/04/bmJQEHiYFfIgwhC.png" alt="Figure 1"></p>
<p>图1：超分辨率图像(左)是最难与原始图像(右)区分的. [4倍上采样]</p>
<h3 id="1-1-相关工作"><a href="#1-1-相关工作" class="headerlink" title="1.1. 相关工作"></a>1.1. 相关工作</h3><h4 id="1-1-1-图像超分辨率"><a href="#1-1-1-图像超分辨率" class="headerlink" title="1.1.1 图像超分辨率"></a>1.1.1 图像超分辨率</h4><p>最近的图像SR综述文章，包括Nasrollahi和Moeslund[42]或Yang等[60]。这里，我们将专注于单图像超分辨率(SISR)，不会进一步讨论从多张图像恢复HR图像的方法[3, 14]。</p>
<p>基于预测的方法是解决SISR的首批方法之一。虽然这些滤波方法可能非常快，例如线性，双三次或Lanczos[13]滤波，但它们简化了SISR问题，通常会产生纹理过于平滑的解。特别关注边缘保留的方法已经被提出[1, 38]。</p>
<p>更强大的方法旨在在低分辨率图像和高分辨率图像之间建立一个复杂映射，并且通常依赖于训练数据。许多基于样本对的方法依赖于LR训练图像块，其对应的HR图像块是已知的。早期的工作由Freeman等[17, 16]提出。与SR相关的方法起源于压缩感知[61, 11, 68]。在Glasner等[20]中作者利用图像内跨尺度图像块冗余来推动SR。Huang等[30]也采用了这种自相似范式，通过进一步允许小的变换和形状变化扩展了自字典。Gu等[24]提出了一种卷积稀疏编码方法通过处理整张图像而不是重叠图像块提高了一致性。</p>
<p>为了重建逼真的纹理细节同时避免边缘伪影，Tai等[51]将基于梯度轮廓先验[49]的边缘导向SR算法和基于学习的细节合成的优势相结合。张等[69]提出了一种多尺度字典来捕获不同尺度下相似图像块的冗余性。为了对地标图像进行超分辨率，Yue等[66]从网上采集了具有相似内容的相关HR图像，并提出了用于对齐的结构感知匹配标准。</p>
<p>邻域嵌入方法通过在低维流形中查找相似的LR训练图像块并结合它们对应的用于重建的HR图像块对LR图像块进行上采样[53, 54]。在Kim和Kwon[34]中，作者强调了邻域方法过拟合的趋势，并使用核岭回归构建了样本对的更通用映射。回归问题也可以通过高斯过程回归[26]，树[45]或随机森林[46]来解决。戴等[5]学习了大量特定图像块的回归器，并在测试中选择最合适的回归器。</p>
<p>最近基于卷积神经网络(CNN)的SR算法已经展现出了出色的性能。在Wang等[58]中，作者基于学习的迭代收缩和阈值算法(LISTA)将稀疏表示先验编码到他们的前馈神经架构中[22]。Dong等[8, 9]使用双三次插值对输入图像进行上采样，并端到端地训练了一个三层的全卷积网络，取得了最佳的SR性能。之后的研究表明网络可以直接学习到上采样滤波器，并可以在准确性和速度方面进一步提高性能[10, 47, 56]。借助深度循环神经网络(DRCN)，Kim等[33]提出了一种高性能架构，在考虑长期像素依赖的同时保持了较少的模型参数数量。与本文特别相关的是约翰逊等[32]和Bruna等[4]的工作，其依赖于更接近于感知相似的损失函数来恢复视觉上更具说服力的HR图像。</p>
<h4 id="1-1-2-卷积神经网络的设计"><a href="#1-1-2-卷积神经网络的设计" class="headerlink" title="1.1.2 卷积神经网络的设计"></a>1.1.2 卷积神经网络的设计</h4><p>随着Krizhevsky等[36]工作取得成功的同时，专门设计的CNN架构设置了许多计算机视觉问题的最新技术。</p>
<p>研究表明，更深的网络架构更难训练，但具有大幅提高网络准确性的潜力，因为其允许建模非常复杂的映射[48, 50]。为了有效训练这些更深的网络架构，批归一化[31]通常用来抵消内部协变量转移。对于SISR，更深的网络架构已经表现出了性能提高，例如，Kim等[33]构建了一个循环CNN并介绍了最新的结果。缓解深度CNN训练的另一种强大设计选择是最近介绍的残差块[28]和跳跃连接[29, 33]概念。跳跃连接减轻了建模恒等映射的网络架构，本质上恒等映射是不重要的，然而对于卷积核表示而言，这可能是有意义的。</p>
<p>SISR的背景下，研究表明学习上采样滤波器对于准确性和速度是有益的[10, 47, 56]。这是一种对Dong等[9]的改进，其中在将图片输入到CNN之前，采用双三次插值对LR观测进行上采样。</p>
<h4 id="1-1-3-损失函数"><a href="#1-1-3-损失函数" class="headerlink" title="1.1.3 损失函数"></a>1.1.3 损失函数</h4><p>逐像素的损失函数(例如MSE)在努力处理恢复损失的高频细节(例如纹理)中的内在不确定性：最小化MSE鼓励寻找合理解的逐像素平均，这通常是过平滑的，因此会得到较差的感知质量[41, 32, 12, 4]。图2中以相应的PSNR为例说明了不同感知质量的重建。我们在图3中说明了最小化MSE的问题，其中对多个具有高级纹理细节的潜在解进行平均从而创建一个平滑的重建。</p>
<p><img src="https://i.loli.net/2020/03/05/BG4NUtpQm8rPsgi.png" alt="Figure 3"></p>
<p>图3：自然图像流形图像块(红)，由MSE获得的超分辨率图像块(蓝)以及由GAN获得的超分辨率图像块(橙)。由于像素空间中可能解的逐像素平均，基于MSE的解似乎更平滑，而GAN将重建推向自然图像流形，产生了感知上更具说服力的解。</p>
<p>在Mathieu等[41]和Denton等[6]中，作者通过采用图像生成应用生成对抗网络(GANs)来解决这个问题。Yu和Porikli[65]通过判别器损失增大了逐像素的MSE损失来训练网络，这个网络使用较大的上采样系数(8×)·对人脸图像进行超分辨率。在Radford等[43]中GAN也用来进行无监督表示学习。Li和Wand[37]的风格转换以及Yeh等[63]的图像修复都描述了使用GAN学习一个流形到另一个流形映射的想法。Bruna等[4]在VGG19[48]特征空间以及散射网络中都最小化了方差。</p>
<p>Dosovitskiy和Brox使用基于神经网络特征空间中计算的欧式距离损失函数与对抗训练相结合。结果表明，提出的损失能够生成视觉上更好的图像并且可以用来解决解码非线性特征表示的不适定逆问题。与这个工作类似，Johnson等[32]和Bruna等[4]提出使用从预训练VGG网络中提取的特征来代替低级逐像素误差度量。具体来说，作者基于VGG19[48]网络提取的特征映射之间的欧式距离来构建损失函数。在超分辨率和艺术风格转换[18, 19]方面，都获得了感知上更具说服力的结果。最近，Li和Wand[37]还研究了在像素或VGG特征空间中对比和混合图像块的效果。</p>
<h3 id="1-2-贡献"><a href="#1-2-贡献" class="headerlink" title="1.2. 贡献"></a>1.2. 贡献</h3><p>GAN提供了一种强大的框架，其可以生成看起来真实、具有高感知质量的自然图像。GAN过程鼓励重建朝向有很大可能包含逼真图像的搜索空间区域，因此更接近图3中所示的自然图像流形。</p>
<p>本文中我们描述了第一个很深的ResNet[28, 29]架构，使用GAN概念形成了逼真SISR的感知损失函数。我们的主要贡献如下：</p>
<p>• 我们在大的上采样系数下(4×)为图像SR设置了最新的技术水平，并用PSNR、结构相似性(SSIM)以及MSE进行了度量，使用了为MSE优化的16块深度ResNet(SRResNet)。</p>
<p>• 我们提出了SRGAN，一种为新感知损失优化的基于GAN的网络。这里我们将基于MSE的内容损失替换为在VGG网络特征映射上计算的损失，其对于像素空间[37]的变化更具有不变性。</p>
<p>• 我们通过在三个公开基准数据集的图像上进行大量的平均主观得分(MOS)测试，确认了SRGAN是最新的技术，在使用较大的上采样系数(4×)进行逼真SR图像评估上具有很大优势。</p>
<p>我们将在第二节中描述网络架构和感知损失。第三节中提供在公开基准数据集上的定量评估和视觉插图。本文在第4节中进行了讨论，并在第5节中作了总结。</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h2><p>SISR的目标是根据低分辨率输入图像$I^{LR}$来估计高分辨率、超分辨率图像$I^{SR}$。这里$I^{HR}$是高分辨率图像，$I^{LR}$是其对应的低分辨率版本。高分辨率图像仅在训练中可获得。训练中，$I^{LR}$可以通过对$I^{HR}$应用高斯滤波，然后执行下采样系数为$r$的下采样操作得到。对于有$C$个颜色通道的图像，我们分别用大小为$W × H × C$的实值张量描述$I^{LR}$，用大小为$rW × rH × C$的实值张量描述$I^{HR}$、$I^{SR}$。</p>
<p>我们的最终目标是训练一个生成函数$G$，用来估算给定LR输入图像的对应HR图像。为此，我们训练了一个生成网络，参数为$\theta _G$的前馈CNN$G_{\theta_G}$。其中$\theta_G = {W_{1:L} ; b_{1:L} }$表示一个$L$层深度网络的权重和偏置，可以通过优化SR特定损失函数$l^{SR}$获得。对于训练图像$I^{HR}_n$， $n = 1, …, N_n$，及其对应的$I^{LR}_n$，$n = 1, …, N_n$，求解：</p>
<p>$$\hat\theta_G=\mathop{argmin}\limits_{\theta_G}\frac{1}{N}\sum^{N}_{n=1}l^{SR}(G_{\theta_G}(I^{LR}_n),I^{HR}_n)  \tag{1}$$</p>
<p>在这项工作中，我们将专门设计一个感知损失$l^{SR}$作为几种损失分量的加权组合，这些损失分量对恢复的SR图像的不同要求特性进行建模。单个损失函数在2.2节中有更详细的描述。</p>
<h3 id="2-1-对抗网络架构"><a href="#2-1-对抗网络架构" class="headerlink" title="2.1. 对抗网络架构"></a>2.1. 对抗网络架构</h3><p>按照Goodfellow等[21]，我们进一步定义了一个判别器网络$D_{\theta_D}$，我们对其与$G_{\theta_G}$进行交替优化来解决对抗最小-最大问题：</p>
<p>$$\mathop{min}\limits_{\theta_G}\mathop{max}\limits_{\theta_D}\mathbb{E}_{I^{HR}\sim p_{train}(I^{HR})}[logD_{\theta_D}(I^{HR})] + \mathbb{E}_{I^{LR}\sim p_{G}(I^{LR})}[log(1-D_{\theta_D}(G_{\theta_G}(I^{LR})))] \tag{2}$$</p>
<p>这个公式的总体思想是，它允许训练生成模型$G$，生成模型目的是欺骗具有辨别能力的判别器$D$，判别器被训练用来区分超分辨图像与真实图像。通过这种方法，我们的生成器可以学习创建与真实图像高度相似的解，因此很难被$D$分类。这鼓励了位于自然图像子空间，流形中的感知上更优的解。这与通过最小化逐像素的误差测量(例如MSE)获得的SR解形成鲜明的对比。</p>
<p>如图4所示，我们的深度生成器网络$G$的中心是$B$个含有恒等设计的残差块。受Johnson等[32]启发，我们采用了Gross和Wilber[23]提出的块设计。具体来说，我们使用了两个卷积层，其核大小为3×3，具有64层特征映射，其后是批归一化层[31]，使用ParametricReLU[27]作为激活函数。如Shi等[47]的提议，我们使用两个训练好的子像素卷积层来增加输入图像的分辨率。</p>
<p><img src="https://i.loli.net/2020/03/06/qo1Rgdac8mfeuzl.png" alt="Figure 4"></p>
<p>图4：生成器网络和判别器网络的架构，每个卷积层表明了对应的卷积核大小(k)，特征映射数量(n)和步长(s)。</p>
<p>为了从生成的SR样本中区分出真实的HR图像，我们训练了一个判别器网络。架构如图4所示。我们遵循Radford等[43]总结的架构指南，使用LeakyReLU激活(α=0.2)，在整个网络中避免使用最大池化。训练的判别器网络用来解决等式2中的最大化问题。它包含8个卷积层，其中3×3滤波器核的数量逐渐增加，与VGG网络一样[48]，从64个滤波器核增加到512个，增加了2倍。在每次特征数量加倍时，步长卷积用来降低图像分辨率。生成的512个特征映射之后是两个稠密层，最后的sigmoid激活用来获得样本分类的概率。</p>
<h3 id="2-2-感知损失函数"><a href="#2-2-感知损失函数" class="headerlink" title="2.2. 感知损失函数"></a>2.2. 感知损失函数</h3><p>感知损失函数$l^{SR}$的定义对于我们的生成器网络性能非常关键。虽然$l^{SR}$通常是基于MSE[9, 47]建模的，但我们在Johnson等[32]和Bruna等[4]的基础上进行了改进，设计了一个损失函数用来评估在感知相关特性方面的解。我们将感知损失构建为内容损失$l^{SR}_X$和对抗损失的加权和：</p>
<p>$$l^{SR}=\underbrace{\underbrace{l^{SR}_X}_{content\ loss} + \underbrace{10^{-3}l^{SR}_{Gen}}_{adversarial\ loss}}_{perceptual\ loss(for\ VGG\ based\ content\ loss)} \tag{3}$$</p>
<p>接下来我们描述内容损失$l^{SR}_X$和对抗损失$l^{SR}_{Gen}$的可能选择。</p>
<h4 id="2-2-1-内容损失"><a href="#2-2-1-内容损失" class="headerlink" title="2.2.1 内容损失"></a>2.2.1 内容损失</h4><p>逐像素的MSE损失计算如下：</p>
<p>$$l^{SR}_{MSE}=\frac {1} {r^2WH} \sum^{rW}_{x=1} \sum^{rH}_{y=1}(I^{HR}_{x,y} - G_{\theta_G}(I^{LR})_{x,y})^2 \tag{4}$$</p>
<p>对于图像SR，这是应用最广泛的优化目标，许多最新技术都依赖该目标[9, 47]。然而，虽然取得了特别高的PSNR，但MSE优化问题的解通常缺少高频内容，这会导致具有过于平滑纹理的解在感知上不令人满意（对比图2）。</p>
<p>在基于Gatys等[18]，Bruna等[4]和Johnson等[32]想法的基础上，我们构建并使用了更接近于感知相似性的损失函数，而不是依赖于逐像素损失。我们在Simonyan和Zisserman[48]中描述的预训练19层VGG网络的ReLU激活层的基础上定义了VGG损失。在给定的的VGG19网络中，我们用$\phi_{i,j}$指代在第i层池化层之前的第j层卷积(激活之后)获得的特征映射。我们使用重建图像$G_{\theta_G}(I^{LR})$的特征表示和参照图像$I^{HR}$之间的欧式距离来定义VGG损失：</p>
<p>$$l^{SR}_{VGG/i,j}=\frac {1} {W_{i,j}H_{i,j}}\sum^{W_{i,j}}_{x=1}\sum^{H_{i,j}}_{y=1}(\phi_{i,j}(I^{HR})_{x,y}-\phi_{i,j}(G_{\theta_G}(I^{HR}))_{x,y})^2 \tag{5}$$</p>
<p>这里$W_{i,j}$和$H_{i,j}$描述了VGG网络中各个特征映射的维度。</p>
<h4 id="2-2-2-对抗损失"><a href="#2-2-2-对抗损失" class="headerlink" title="2.2.2 对抗损失"></a>2.2.2 对抗损失</h4><p>除了目前为止描述的内容损失之外，我们也将GAN的生成组件添加到了感知损失中。通过设法欺骗判别器网络，这鼓励我们的网络支持位于自然图像流行上的解。基于判别器$D_{\theta_D}(G_{\theta_G}(I^{LR}))$在所有训练样本上的概率，生成损失$l^{SR}_{Gen}$定义为：</p>
<p>$$l^{SR}_{Gen}=\sum^N_{n=1}-logD_{\theta_D}(G_{\theta_G}(I^{LR})) \tag{6}$$</p>
<p>这里，$D_{\theta_D}(G_{\theta_G}(I^{LR}))$是重建图像$G_{\theta_G}(I^{LR})$为自然HR图像的概率。为了得到更好的梯度行为，我们对$-logD_{\theta_D}(G_{\theta_G}(I^{LR}))$进行最小化，而不是$log[1-logD_{\theta_D}(G_{\theta_G}(I^{LR}))]$ [21]。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><h3 id="3-1-数据和相似性度量"><a href="#3-1-数据和相似性度量" class="headerlink" title="3.1. 数据和相似性度量"></a>3.1. 数据和相似性度量</h3><p>我们在三个广泛使用的基准数据集Set5[2]，Set14[68]和BSD300的测试集BSD100[40]上进行实验。所有实验都在低分辨率和高分辨率图像之间以4倍的尺度因子执行。图像像素对应减少16倍。为了公平比较，所有报告的PSNR[dB]和SSIM[57]度量使用daala软件包，在中心裁剪的图像的y通道上进行计算，图像每个边界移除了4个像素宽的图像条。参考方法包括最近邻居，双三次，SRCNN[8]和SelfExSR[30]的超分辨图像是从Huang等[30]和Kim等的DRCN[33]的在线补充材料中获得的 。SRResNet(损失：$l^{SR}_{MSE}$和$l^{SR}_{VGG/2.2}$)和SRGAN变体得到的结果可在线获得。统计测试以成对的双侧威尔科克森符号秩检验和显著性检验进行，显著性水平为$p&lt;0.05$。</p>
<p>读者可能还对GitHub上独立开发的基于GAN的解决方案感兴趣。然而，它只能提供一组有限人脸图像上的实验结果，这是一个更受限且更轻松的任务。</p>
<h3 id="3-2-训练细节和参数"><a href="#3-2-训练细节和参数" class="headerlink" title="3.2. 训练细节和参数"></a>3.2. 训练细节和参数</h3><p>我们使用NVIDIA Tesla M40 GPU训练所有的网络，训练数据来自<strong>ImageNet</strong>数据集[44]中随机采样的35万张图像。这些图片不同于测试图片。我们使用双三次核对HR图像(BGR, $C=3$)进行下采样得到LR图像，下采样系数为$r=4$。对于每一份小批量数据，我们对不同的训练图像裁剪16个随机的96×96的HR子图像。注意我们可以对任意大小的图像应用生成器模型，因为它是全卷积的。我们使用Adam[35]，$\beta_{1}=0.9$来进行优化。SRResNet网络使用$10^{−4}$的学习率进行训练，更新迭代次数$10^6$。在训练实际的GAN时，为了避免不必要的局部最优值，我们采用预训练的基于MSE的SRResNet网络对生成器进行初始化。所有的SRGAN变种都以$10^{−4}$的学习率训练$10^5$次迭代，然后以$10^{−5}$的学习率再训练$10^5$次迭代。我们交替更新生成器和判别器网络，这等价于Goodfellow等[21]的$k=1$。我们的生成器网络有16个恒等($B=16$)残差块。测试期间，为了获得确定性地只依赖输入的输出，我们关闭了批归一化更新。我们的实现基于Theano[52]和Lasagne[7]。</p>
<h3 id="3-3-平均主观得分-MOS-测试"><a href="#3-3-平均主观得分-MOS-测试" class="headerlink" title="3.3. 平均主观得分(MOS)测试"></a>3.3. 平均主观得分(MOS)测试</h3><p>为了量化不同方法重建感知上令人信服的图像的能力，我们进行了MOS测试。具体来说，我们让26个评分员使用整数分1(质量差)到5(质量极好)对超分辨率图像进行打分。评分员对Set5，Set14和BSD100数据集上的每一张图像的12个版本进行了评分：最近邻(NN)，双三次，SRCNN[8]，SelfExSR[30]，DRCN[33]，ESPCN[47]，SRResNet-MSE，$SRResNet-VGG22^*$ ($*$没有在BSD100上评分)，$SRGAN-MSE^*$，$SRGAN-VGG22^*$，SRGAN-VGG54和原始HR图像。因此每一个评分员对随机呈现的1128个实例（19张图像的12个版本加上100张图像的9个版本）进行了评估。评分员对BSD300训练集的20张图像的NN（得分1）和HR（5）版本上进行了校准。在初步研究中，通过两次添加方法图像到更大的测试集中，我们评估了26个评分员在BSD100的10张图像子集上的校准程序和重测信度。我们发现了良好的可靠性，在相同图像的评分之间没有显著差异。评分员非常一致地将NN插值测试图像评分为1，原始HR图像评分为5（参加图5）。</p>
<p><img src="https://i.loli.net/2020/03/13/kxJ1thYLfnVSQy3.png" alt="Figure 5"></p>
<p>图5：<strong>BSD100</strong>上MOS得分的颜色编码分布。每一种方法使用2600个样本(100张图片×26个评估者)评估。均值显示为红色标记，bin以值$i$为中心(4倍上采样)。</p>
<p>进行的MOS测试的实验结果总结在表1，表2和图5中。</p>
<p>表1：SRResNet不同损失函数的性能和对抗网络在Set5和Set14上的基准数据。MOS得分明显比其它损失在对应类别上更高($p&lt;0.05$)。[4×上采样]</p>
<p><img src="https://i.loli.net/2020/03/30/SzHnxkJYQBTwZtb.png" alt="Table 1"></p>
<p>表2：NN，双三次，SRCNN[8]，SelfExSR[30]，DRCN[33]，ESPCN[47]，SRResNet，SRGAN-VGG54和原始HR在基准数据上的比较. 最高的度量(PSNR[dB]，SSIM，MOS)以粗体显示。[4×上采样]</p>
<p><img src="https://i.loli.net/2020/03/30/r2lFX3pA8Pb5zG4.png" alt="Table 2"></p>
<h3 id="3-4-内容损失研究"><a href="#3-4-内容损失研究" class="headerlink" title="3.4. 内容损失研究"></a>3.4. 内容损失研究</h3><p>对于基于GAN的网络，我们研究了感知损失中不同内容损失选择的影响。具体来说，对于下面的内容损失$l^{SR}_X$，我们研究了$l^{SR}=l^{SR}_X+10^{-3}l^{SR}_{Gen}$：</p>
<p>• SRGAN-MSE：$l^{SR}_{MSE}$，以标准MSE作为内容损失来研究对抗网络。</p>
<p>• SRGAN-VGG22：具有$\phi_{2,2}$的$l^{SR}_{VGG/2.2}$，表示更底层特征[67]的特征映射上定义的损失。</p>
<p>• SRGAN-VGG54：具有$\phi_{5,4}$的$l^{SR}_{VGG/5.4}$，来自较深网络层的更高层特征的特征映射上定义的损失，更可能集中在图像内容上[67, 64, 39]。在下文中，我们将此网络称为SRGAN。</p>
<p>对于两个损失$l^{SR}_{MSE}$(SRResNet-MSE)和$l^{SR}_{VGG/2.2}$(SRResNet-VGG22)，我们也对没有对抗组件的生成器网络性能进行了评估。我们将SRResNet-MSE称为SRResNet。在表1中总结了定量结果，图6中提供了直观的示例。即使结合对抗损失，MSE仍然提供了具有最高PSNR值的解，与视觉感知更敏感的损失组件取得的结果相比，其在感知上更平滑，更不令人信服。这是由基于MSE的内容损失和对抗损失之间的竞争引起的。我们进一步将少量基于SRGAN-MSE的重构中观测到的那些较小的重构结果，归因于那些相互竞争的目标。关于Set5上的MOS得分，我们不能确定一个对于SRResNet或SRGAN明显最好的损失函数。但是，考虑到Set14上的MOS得分，SRGAN-VGG54显著优于其它SRGAN和SRResNet变种。我们观察到一种趋势，与$\phi_{2,2}$相比，使用更高层的VGG特征映射$\phi_{5,4}$得到了更好的纹理细节，参见图6。</p>
<p><img src="https://i.loli.net/2020/04/01/h84ImQlWtJ2arjg.png" alt="Figure 6"></p>
<p>图6：SRResNet（左：a，b），SRGAN-MSE（左中：c，d），SRGAN-VGG2.2（中：e，f）和SRGAN-VGG54（右中：g，h）的重建结果以及相应的参考HR图像（右：i，j）。 [4倍上采样]</p>
<h3 id="3-5-最终网络的性能"><a href="#3-5-最终网络的性能" class="headerlink" title="3.5. 最终网络的性能"></a>3.5. 最终网络的性能</h3><p>我们比较了SRResNet、SRGAN、NN、双三次插值和四种最新方法的性能。定量结果总结在表2中，证实了SRResNet(考虑PSNR/SSIM)在三个基准数据集上确立了最新的技术水平。请注意，我们使用了一个公开可获得的框架进行评估，（参加3.1节），因此报告的值可能会与原始论文中报告的值略有不同。</p>
<p>我们进一步获得了BSD100数据集上SRGAN和所有其他方法的MOS评分。表2中展示的结果证实了SRGAN大幅度优于所有的参考方法，并为逼真图像SR确立了最新的技术水平。除了SRCNN和SelfExSR之外，BSD100上的MOS得分差异（参加表2）是非常显著的。所有收集的MOS得分分布总结在图5中。</p>
<h2 id="4-讨论和未来工作"><a href="#4-讨论和未来工作" class="headerlink" title="4. 讨论和未来工作"></a>4. 讨论和未来工作</h2><p>我们使用MOS测试证实了SRGAN优秀的感知性能。我们进一步表明，对于人类视觉系统[55]，标准的定量度量，例如PSNR和SSIM，不能捕获并准确评估的图像质量。这项工作的重点是超分辨率的感知质量而不是计算效率。与Shi等[47]相反，提出的模型未针对实时视频SR进行优化。然而，网络架构的初步试验表明，更窄的网络有可能在质量性能降低的情况下提供非常有效的替代方案。与Dong等[9]相反，我们发现更深的网络架构是有益的。我们推测ResNet设计对更深网络的性能有实质性影响。我们发现更深的网络(B&gt;16)可以进一步提升SRResNet的性能，但是以更长的训练和测试时间为代价。我们发现由于高频伪影的出现，更深网络的SRGAN变种越来越难训练。</p>
<p>当针对SR问题的逼真解决方案时，内容损失的选择是非常重要的，如图6所示。在这项工作中，我们发现$l^{SR}_{VGG/5.4}$取得了感知上最令人信服的结果，这归因于更深的网络层可能表示远离像素空间的更加抽象[67, 64, 39]特征。我们推测这些深层的特征映射单纯的注重内容而剩下的对抗损失注重纹理细节，这是没有对抗损失的超分辨率图像和逼真图像之间的主要差异。我们也注意到理想的损失函数取决于应用。例如，虚幻的更精细的细节可能不适合医疗引用或监控。感知上令人信服的文本或结构化场景[30]重建是具有挑战性的，是未来工作的一部分。内容损失函数的开发描述了图像空间内容，但对像素空间变化的不变性将进一步改善逼真的图像SR结果。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们描述了一个深度残差网络SRResNet，当广泛使用PSNR度量进行评估时，其在公共基准数据集上树立了最新的技术水平。我们强调了以PSNR为中心的超分辨率的一些限制，引入了SRGAN，其通过训练GAN增加了具有对抗损失的内容损失函数。使用广泛的MOS测试，我们证实了对于大的上采样系数(4×)，SRGAN重构比最新的参考方法得到的重构更逼真。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] J. Allebach and P. W. Wong. Edge-directed interpolation. In Proceedings of International Conference on Image Processing, volume 3, pages 707–710, 1996.</p>
<p>[2] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. BMVC, 2012.</p>
<p>[3] S. Borman and R. L. Stevenson. Super-Resolution from Image Sequences - A Review. Midwest Symposium on Circuits and Systems, pages 374–378, 1998.</p>
<p>[4] J. Bruna, P. Sprechmann, and Y. LeCun. Super-resolution with deep convolutional sufficient statistics. In International Conference on Learning Representations (ICLR), 2016.</p>
<p>[5] D. Dai, R. Timofte, and L. Van Gool. Jointly optimized regressors for image super-resolution. In Computer Graphics Forum, volume 34, pages 95–104, 2015.</p>
<p>[6] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems (NIPS), pages 1486–1494, 2015.</p>
<p>[7] S. Dieleman, J. Schluter, C. Raffel, E. Olson, S. K. Snderby, ¨D. Nouri, D. Maturana, M. Thoma, E. Battenberg, J. Kelly, J. D. Fauw, M. Heilman, diogo149, B. McFee, H. Weideman, takacsg84, peterderivaz, Jon, instagibbs, D. K. Rasul, CongLiu, Britefury, and J. Degrave. Lasagne: First release., 2015.</p>
<p>[8] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In European Conference on Computer Vision (ECCV), pages 184–199. Springer, 2014.</p>
<p>[9] C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(2):295–307, 2016.</p>
<p>[10] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-resolution convolutional neural network. In European Conference on Computer Vision (ECCV), pages 391–407. Springer, 2016.</p>
<p>[11] W. Dong, L. Zhang, G. Shi, and X. Wu. Image deblurring and superresolution by adaptive sparse domain selection and adaptive regularization. IEEE Transactions on Image Processing, 20(7):1838–1857, 2011.</p>
<p>[12] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on deep networks. In Advances in Neural Information Processing Systems (NIPS), pages 658–666, 2016.</p>
<p>[13] C. E. Duchon. Lanczos Filtering in One and Two Dimensions. In Journal of Applied Meteorology, volume 18, pages 1016–1022. 1979.</p>
<p>[14] S. Farsiu, M. D. Robinson, M. Elad, and P. Milanfar. Fast and robust multiframe super resolution. IEEE Transactions on Image Processing, 13(10):1327–1344, 2004.</p>
<p>[15] J. A. Ferwerda. Three varieties of realism in computer graphics. In Electronic Imaging, pages 290–297. International Society for Optics and Photonics, 2003.</p>
<p>[16] W. T. Freeman, T. R. Jones, and E. C. Pasztor. Example-based superresolution. IEEE Computer Graphics and Applications, 22(2):56–65, 2002.</p>
<p>[17] W. T. Freeman, E. C. Pasztor, and O. T. Carmichael. Learning lowlevel vision. International Journal of Computer Vision, 40(1):25–47, 2000.</p>
<p>[18] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 262–270, 2015.</p>
<p>[19] L. A. Gatys, A. S. Ecker, and M. Bethge. Image Style Transfer Using Convolutional Neural Networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2414–2423, 2016.</p>
<p>[20] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. In IEEE International Conference on Computer Vision (ICCV), pages 349–356, 2009.</p>
<p>[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2672–2680, 2014.</p>
<p>[22] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 399–406, 2010.</p>
<p>[23] S. Gross and M. Wilber. Training and investigating residual nets, online at <a href="http://torch.ch/blog/2016/02/04/resnets" target="_blank" rel="external">http://torch.ch/blog/2016/02/04/resnets</a>. html. 2016.</p>
<p>[24] S. Gu, W. Zuo, Q. Xie, D. Meng, X. Feng, and L. Zhang. Convolutional sparse coding for image super-resolution. In IEEE International Conference on Computer Vision (ICCV), pages 1823–1831. 2015.</p>
<p>[25] P. Gupta, P. Srivastava, S. Bhardwaj, and V. Bhateja. A modified psnr metric based on hvs for quality assessment of color images. In IEEE International Conference on Communication and Industrial Application (ICCIA), pages 1–4, 2011.</p>
<p>[26] H. He and W.-C. Siu. Single image super-resolution using gaussian process regression. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 449–456, 2011.</p>
<p>[27] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In IEEE International Conference on Computer Vision (ICCV), pages 1026–1034, 2015.</p>
<p>[28] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.</p>
<p>[29] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), pages 630–645. Springer, 2016.</p>
<p>[30] J. B. Huang, A. Singh, and N. Ahuja. Single image super-resolution from transformed self-exemplars. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5197–5206, 2015.</p>
<p>[31] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning (ICML), pages 448–456, 2015.</p>
<p>[32] J. Johnson, A. Alahi, and F. Li. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision (ECCV), pages 694–711. Springer, 2016.</p>
<p>[33] J. Kim, J. K. Lee, and K. M. Lee. Deeply-recursive convolutional network for image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</p>
<p>[34] K. I. Kim and Y. Kwon. Single-image super-resolution using sparse regression and natural image prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(6):1127–1133, 2010.</p>
<p>[35] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.</p>
<p>[36] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 1097–1105, 2012.</p>
<p>[37] C. Li and M. Wand. Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2479–2486, 2016.</p>
<p>[38] X. Li and M. T. Orchard. New edge-directed interpolation. IEEE Transactions on Image Processing, 10(10):1521–1527, 2001.</p>
<p>[39] A. Mahendran and A. Vedaldi. Visualizing deep convolutional neural networks using natural pre-images. International Journal of Computer Vision, pages 1–23, 2016.</p>
<p>[40] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In IEEE International Conference on Computer Vision (ICCV), volume 2, pages 416–423, 2001.</p>
<p>[41] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In International Conference on Learning Representations (ICLR), 2016.</p>
<p>[42] K. Nasrollahi and T. B. Moeslund. Super-resolution: A comprehensive survey. In Machine Vision and Applications, volume 25, pages 1423–1468. 2014.</p>
<p>[43] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations (ICLR), 2016.</p>
<p>[44] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, pages 1–42, 2014.</p>
<p>[45] J. Salvador and E. Perez-Pellitero. Naive bayes super-resolution ´forest. In IEEE International Conference on Computer Vision (ICCV), pages 325–333. 2015.</p>
<p>[46] S. Schulter, C. Leistner, and H. Bischof. Fast and accurate image upscaling with super-resolution forests. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3791–3799, 2015.</p>
<p>[47] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1874–1883, 2016.</p>
<p>[48] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015.</p>
<p>[49] J. Sun, J. Sun, Z. Xu, and H.-Y. Shum. Image super-resolution using gradient profile prior. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2008.</p>
<p>[50] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–9, 2015.</p>
<p>[51] Y.-W. Tai, S. Liu, M. S. Brown, and S. Lin. Super Resolution using Edge Prior and Single Image Detail Synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2400–2407, 2010.</p>
<p>[52] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688, 2016.</p>
<p>[53] R. Timofte, V. De, and L. Van Gool. Anchored neighborhood regression for fast example-based super-resolution. In IEEE International Conference on Computer Vision (ICCV), pages 1920–1927, 2013.</p>
<p>[54] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted anchored neighborhood regression for fast super-resolution. In Asian Conference on Computer Vision (ACCV), pages 111–126. Springer, 2014.</p>
<p>[55] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Minnen, J. Shor, and M. Covell. Full Resolution Image Compression with Recurrent Neural Networks. arXiv preprint arXiv:1608.05148, 2016.</p>
<p>[56] Y. Wang, L. Wang, H. Wang, and P. Li. End-to-End Image SuperResolution via Deep and Shallow Convolutional Networks. arXiv preprint arXiv:1607.07680, 2016.</p>
<p>[57] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004.</p>
<p>[58] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep networks for image super-resolution with sparse prior. In IEEE International Conference on Computer Vision (ICCV), pages 370–378, 2015.</p>
<p>[59] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multi-scale structural imilarity for image quality assessment. In IEEE Asilomar Conference on Signals, Systems and Computers, volume 2, pages 9–13, 2003.</p>
<p>[60] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-resolution: A benchmark. In European Conference on Computer Vision (ECCV), pages 372–386. Springer, 2014.</p>
<p>[61] J. Yang, J. Wright, T. Huang, and Y. Ma. Image super-resolution as sparse representation of raw image patches. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2008.</p>
<p>[62] Q. Yang, R. Yang, J. Davis, and D. Nister. Spatial-depth super resolution for range images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2007.</p>
<p>[63] R. Yeh, C. Chen, T. Y. Lim, M. Hasegawa-Johnson, and M. N. Do. Semantic Image Inpainting with Perceptual and Contextual Losses. arXiv preprint arXiv:1607.07539, 2016.</p>
<p>[64] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. Understanding Neural Networks Through Deep Visualization. In International Conference on Machine Learning - Deep Learning Workshop 2015, page 12, 2015.</p>
<p>[65] X. Yu and F. Porikli. Ultra-resolving face images by discriminative generative networks. In European Conference on Computer Vision (ECCV), pages 318–333. 2016.</p>
<p>[66] H. Yue, X. Sun, J. Yang, and F. Wu. Landmark image superresolution by retrieving web images. IEEE Transactions on Image Processing, 22(12):4865–4878, 2013.</p>
<p>[67] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision (ECCV), pages 818–833. Springer, 2014.</p>
<p>[68] R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In Curves and Surfaces, pages 711–730. Springer, 2012.</p>
<p>[69] K. Zhang, X. Gao, D. Tao, and X. Li. Multi-scale dictionary for single image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1114–1121, 2012.</p>
<p>[70] W. Zou and P. C. Yuen. Very Low Resolution Face Recognition in Parallel Environment . IEEE Transactions on Image Processing, 21:327–340, 2012.</p>
]]></content>
    
    <summary type="html">
    
      Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2020/04/10/2020-04-10-Photo-Realistic%20Single%20Image%20Super-Resolution%20Using%20a%20Generative%20Adversarial%20Network%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2020/04/10/2020-04-10-Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文翻译——中英文对照/</id>
    <published>2020-04-10T02:23:07.000Z</published>
    <updated>2020-05-21T08:18:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Photo-Realistic-Single-Image-Super-Resolution-Using-a-Generative-Adversarial-Network"><a href="#Photo-Realistic-Single-Image-Super-Resolution-Using-a-Generative-Adversarial-Network" class="headerlink" title="Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"></a>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image superresolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>尽管使用更快更深的卷积神经网络在单图像超分辨率的准确性和速度方面取得了突破，但仍有一个主要问题尚未解决：当使用大的上采样系数进行超分辨率时，我们怎样来恢复更精细的纹理细节。基于优化的超分辨率方法的行为主要由目标函数的选择来决定。最近的工作主要专注于最小化均方重构误差。由此得出的评估结果具有很高的峰值信噪比，但它们通常缺乏高频细节，并且在感知上是不令人满意的，在某种意义上，它们在较高分辨率上没有满足期望的保真度。在本文中，我们提出了SRGAN，一种用于图像超分辨率(SR)的生成对抗网络(GAN)。据我们所知，这是第一个对于4倍上采样系数，能推断逼真自然图像的框架。为此，我们提出了一种感知损失函数，其由对抗损失和内容损失组成。对抗损失使用判别器网络将我们的解推向自然图像流形，判别器网络经过训练用以区分超分辨率图像和原始的逼真图像。此外，我们使用由感知相似性而不是像素空间相似性引起的内容损失。在公开的基准数据集上，我们的深度残差网络能从过度下采样图像中恢复出逼真的纹理。广泛的平均主观得分(MOS)测试显示，使用SRGAN可以显著提高感知质量。与任何最新方法获得的MOS得分相比，使用SRGAN获得的MOS得分更接近于原始高分辨率图像的MOS得分。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>The highly challenging task of estimating a highresolution (HR) image from its low-resolution (LR) counterpart is referred to as super-resolution (SR). SR received substantial attention from within the computer vision research community and has a wide range of applications [62, 70, 42].</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>从低分辨率(LR)图像来估计其对应高分辨率(HR)图像的高挑战性任务被称作超分辨率(SR)。SR在计算机视觉研究领域受到了广泛的关注并有大量应用[62, 70, 42]。</p>
<p>The ill-posed nature of the underdetermined SR problem is particularly pronounced for high upscaling factors, for which texture detail in the reconstructed SR images is typically absent. The optimization target of supervised SR algorithms is commonly the minimization of the mean squared error (MSE) between the recovered HR image and the ground truth. This is convenient as minimizing MSE also maximizes the peak signal-to-noise ratio (PSNR), which is a common measure used to evaluate and compare SR algorithms [60]. However, the ability of MSE (and PSNR) to capture perceptually relevant differences, such as high texture detail, is very limited as they are defined based on pixel-wise image differences [59, 57, 25]. This is illustrated in Figure 2, where highest PSNR does not necessarily reflect the perceptually better SR result. The perceptual difference between the super-resolved and original image means that the recovered image is not photo-realistic as defined by Ferwerda [15].</p>
<p><img src="http://noahsnail.com/images/srgan/srgan_figure_2.jpeg" alt="Figure 2"></p>
<p>Figure 2: From left to right: bicubic interpolation, deep residual network optimized for MSE, deep residual generative adversarial network optimized for a loss more sensitive to human perception, original HR image. Corresponding PSNR and SSIM are shown in brackets. [4× upscaling]</p>
<p>欠定SR问题的不适定特性对于大的上采样系数尤其显著，重建的SR图像中通常缺少纹理细节。有监督SR算法的优化目标通常是最小化恢复的HR图像和真实图像之间的均方误差(MSE)。最小化MSE即最大化峰值信噪比(PSNR)是方便的，这是用来评估和比较SR算法的常用方法[60]。然而，MSE(和PSNR)捕获感知相对差异(例如高级纹理细节)的能力是非常有限的，因为它们是基于像素级图像差异[59, 57, 25]定义的。这在图2中进行了说明，其中最高的PSNR不一定能反映出感知上更好的SR结果。超分辨率图像和原始图像之间的感知差异意味着恢复图像不如Ferwerda[15]中定义的逼真。</p>
<p><img src="http://noahsnail.com/images/srgan/srgan_figure_2.jpeg" alt="Figure 2"></p>
<p>图2：从左到右：双三次插值，优化MSE的深度残差网络，优化人感知更敏感损失的深度残差生成对抗网络，原始HR图像。对应的PSNR和SSIM显示在括号中。[4倍上采样]</p>
<p>In this work we propose a super-resolution generative adversarial network (SRGAN) for which we employ a deep residual network (ResNet) with skip-connection and diverge from MSE as the sole optimization target. Different from previous works, we define a novel perceptual loss using high-level feature maps of the VGG network [48, 32, 4] combined with a discriminator that encourages solutions perceptually hard to distinguish from the HR reference images. An example photo-realistic image that was super-resolved with a 4× upscaling factor is shown in Figure 1.</p>
<p><img src="https://i.loli.net/2020/03/04/bmJQEHiYFfIgwhC.png" alt="Figure 1"></p>
<p>Figure 1: Super-resolved image (left) is almost indistinguishable from original (right). [4× upscaling]</p>
<p>在这项工作中我们提出了一种超分辨率生成对抗网络(SRGAN)，为此我们采用了具有跳跃连接的深度残差网络并舍弃了作为唯一优化目标的MSE。不同于以前的工作，我们定义了一种新的使用VGG网络[48, 32, 4]高级特征映射与判别器结合的感知损失，判别器会鼓励感知上更难与HR参考图像区分的解。图1中展示了一张示例逼真图像，其使用4倍上采样系数进行超分辨率。</p>
<p><img src="https://i.loli.net/2020/03/04/bmJQEHiYFfIgwhC.png" alt="Figure 1"></p>
<p>图1：超分辨率图像(左)是最难与原始图像(右)区分的. [4倍上采样]</p>
<h3 id="1-1-Related-work"><a href="#1-1-Related-work" class="headerlink" title="1.1. Related work"></a>1.1. Related work</h3><h4 id="1-1-1-Image-super-resolution"><a href="#1-1-1-Image-super-resolution" class="headerlink" title="1.1.1 Image super-resolution"></a>1.1.1 Image super-resolution</h4><p>Recent overview articles on image SR include Nasrollahi and Moeslund [42] or Yang et al. [60]. Here we will focus on single image super-resolution (SISR) and will not further discuss approaches that recover HR images from multiple images [3, 14].</p>
<h3 id="1-1-相关工作"><a href="#1-1-相关工作" class="headerlink" title="1.1. 相关工作"></a>1.1. 相关工作</h3><h4 id="1-1-1-图像超分辨率"><a href="#1-1-1-图像超分辨率" class="headerlink" title="1.1.1 图像超分辨率"></a>1.1.1 图像超分辨率</h4><p>最近的图像SR综述文章，包括Nasrollahi和Moeslund[42]或Yang等[60]。这里，我们将专注于单图像超分辨率(SISR)，不会进一步讨论从多张图像恢复HR图像的方法[3, 14]。</p>
<p>Prediction-based methods were among the first methods to tackle SISR. While these filtering approaches, e.g. linear, bicubic or Lanczos [13] filtering, can be very fast, they oversimplify the SISR problem and usually yield solutions with overly smooth textures. Methods that put particularly focus on edge-preservation have been proposed [1, 38].</p>
<p>基于预测的方法是解决SISR的首批方法之一。虽然这些滤波方法可能非常快，例如线性，双三次或Lanczos[13]滤波，但它们简化了SISR问题，通常会产生纹理过于平滑的解。特别关注边缘保留的方法已经被提出[1, 38]。</p>
<p>More powerful approaches aim to establish a complex mapping between low- and high-resolution image information and usually rely on training data. Many methods that are based on example-pairs rely on LR training patches for which the corresponding HR counterparts are known. Early work was presented by Freeman et al. [17, 16]. Related approaches to the SR problem originate in compressed sensing [61, 11, 68]. In Glasner et al. [20] the authors exploit patch redundancies across scales within the image to drive the SR. This paradigm of self-similarity is also employed in Huang et al. [30], where self dictionaries are extended by further allowing for small transformations and shape variations. Gu et al. [24] proposed a convolutional sparse coding approach that improves consistency by processing the whole image rather than overlapping patches.</p>
<p>更强大的方法旨在在低分辨率图像和高分辨率图像之间建立一个复杂映射，并且通常依赖于训练数据。许多基于样本对的方法依赖于LR训练图像块，其对应的HR图像块是已知的。早期的工作由Freeman等[17, 16]提出。与SR相关的方法起源于压缩感知[61, 11, 68]。在Glasner等[20]中作者利用图像内跨尺度图像块冗余来推动SR。Huang等[30]也采用了这种自相似范式，通过进一步允许小的变换和形状变化扩展了自字典。Gu等[24]提出了一种卷积稀疏编码方法通过处理整张图像而不是重叠图像块提高了一致性。</p>
<p>To reconstruct realistic texture detail while avoiding edge artifacts, Tai et al. [51] combine an edge-directed SR algorithm based on a gradient profile prior [49] with the benefits of learning-based detail synthesis. Zhang et al. [69] propose a multi-scale dictionary to capture redundancies of similar image patches at different scales. To super-resolve landmark images, Yue et al. [66] retrieve correlating HR images with similar content from the web and propose a structure-aware matching criterion for alignment.</p>
<p>为了重建逼真的纹理细节同时避免边缘伪影，Tai等[51]将基于梯度轮廓先验[49]的边缘导向SR算法和基于学习的细节合成的优势相结合。张等[69]提出了一种多尺度字典来捕获不同尺度下相似图像块的冗余性。为了对地标图像进行超分辨率，Yue等[66]从网上采集了具有相似内容的相关HR图像，并提出了用于对齐的结构感知匹配标准。</p>
<p>Neighborhood embedding approaches upsample a LR image patch by finding similar LR training patches in a low dimensional manifold and combining their corresponding HR patches for reconstruction [53, 54]. In Kim and Kwon [34] the authors emphasize the tendency of neighborhood approaches to overfit and formulate a more general map of example pairs using kernel ridge regression. The regression problem can also be solved with Gaussian process regression [26], trees [45] or Random Forests [46]. In Dai et al. [5] a multitude of patch-specific regressors is learned and the most appropriate regressors selected during testing.</p>
<p>邻域嵌入方法通过在低维流形中查找相似的LR训练图像块并结合它们对应的用于重建的HR图像块对LR图像块进行上采样[53, 54]。在Kim和Kwon[34]中，作者强调了邻域方法过拟合的趋势，并使用核岭回归构建了样本对的更通用映射。回归问题也可以通过高斯过程回归[26]，树[45]或随机森林[46]来解决。戴等[5]学习了大量特定图像块的回归器，并在测试中选择最合适的回归器。</p>
<p>Recently convolutional neural network (CNN) based SR algorithms have shown excellent performance. In Wang et al. [58] the authors encode a sparse representation prior into their feed-forward network architecture based on the learned iterative shrinkage and thresholding algorithm (LISTA) [22]. Dong et al. [8, 9] used bicubic interpolation to upscale an input image and trained a three layer deep fully convolutional network end-to-end to achieve state-of-the-art SR performance. Subsequently, it was shown that enabling the network to learn the upscaling filters directly can further increase performance both in terms of accuracy and speed [10, 47, 56]. With their deeply-recursive convolutional network (DRCN), Kim et al. [33] presented a highly performant architecture that allows for long-range pixel dependencies while keeping the number of model parameters small. Of particular relevance for our paper are the works by Johnson et al. [32] and Bruna et al. [4], who rely on a loss function closer to perceptual similarity to recover visually more convincing HR images.</p>
<p>最近基于卷积神经网络(CNN)的SR算法已经展现出了出色的性能。在Wang等[58]中，作者基于学习的迭代收缩和阈值算法(LISTA)将稀疏表示先验编码到他们的前馈神经架构中[22]。Dong等[8, 9]使用双三次插值对输入图像进行上采样，并端到端地训练了一个三层的全卷积网络，取得了最佳的SR性能。之后的研究表明网络可以直接学习到上采样滤波器，并可以在准确性和速度方面进一步提高性能[10, 47, 56]。借助深度循环神经网络(DRCN)，Kim等[33]提出了一种高性能架构，在考虑长期像素依赖的同时保持了较少的模型参数数量。与本文特别相关的是约翰逊等[32]和Bruna等[4]的工作，其依赖于更接近于感知相似的损失函数来恢复视觉上更具说服力的HR图像。</p>
<h4 id="1-1-2-Design-of-convolutional-neural-networks"><a href="#1-1-2-Design-of-convolutional-neural-networks" class="headerlink" title="1.1.2 Design of convolutional neural networks"></a>1.1.2 Design of convolutional neural networks</h4><p>The state of the art for many computer vision problems is meanwhile set by specifically designed CNN architectures following the success of the work by Krizhevsky et al. [36].</p>
<h4 id="1-1-2-卷积神经网络的设计"><a href="#1-1-2-卷积神经网络的设计" class="headerlink" title="1.1.2 卷积神经网络的设计"></a>1.1.2 卷积神经网络的设计</h4><p>随着Krizhevsky等[36]工作取得成功的同时，专门设计的CNN架构设置了许多计算机视觉问题的最新技术。</p>
<p>It was shown that deeper network architectures can be difficult to train but have the potential to substantially increase the network’s accuracy as they allow modeling mappings of very high complexity [48, 50]. To efficiently train these deeper network architectures, batch-normalization [31] is often used to counteract the internal co-variate shift. Deeper network architectures have also been shown to increase performance for SISR, e.g. Kim et al. [33] formulate a recursive CNN and present state-of-the-art results. Another powerful design choice that eases the training of deep CNNs is the recently introduced concept of residual blocks [28] and skip-connections [29, 33]. Skip-connections relieve the network architecture of modeling the identity mapping that is trivial in nature, however, potentially non-trivial to represent with convolutional kernels.</p>
<p>研究表明，更深的网络架构更难训练，但具有大幅提高网络准确性的潜力，因为其允许建模非常复杂的映射[48, 50]。为了有效训练这些更深的网络架构，批归一化[31]通常用来抵消内部协变量转移。对于SISR，更深的网络架构已经表现出了性能提高，例如，Kim等[33]构建了一个循环CNN并介绍了最新的结果。缓解深度CNN训练的另一种强大设计选择是最近介绍的残差块[28]和跳跃连接[29, 33]概念。跳跃连接减轻了建模恒等映射的网络架构，本质上恒等映射是不重要的，然而对于卷积核表示而言，这可能是有意义的。</p>
<p>In the context of SISR it was also shown that learning upscaling filters is beneficial in terms of accuracy and speed [10, 47, 56]. This is an improvement over Dong et al. [9] where bicubic interpolation is employed to upscale the LR observation before feeding the image to the CNN.</p>
<p>SISR的背景下，研究表明学习上采样滤波器对于准确性和速度是有益的[10, 47, 56]。这是一种对Dong等[9]的改进，其中在将图片输入到CNN之前，采用双三次插值对LR观测进行上采样。</p>
<h4 id="1-1-3-Loss-functions"><a href="#1-1-3-Loss-functions" class="headerlink" title="1.1.3 Loss functions"></a>1.1.3 Loss functions</h4><p>Pixel-wise loss functions such as MSE struggle to handle the uncertainty inherent in recovering lost high-frequency details such as texture: minimizing MSE encourages finding pixel-wise averages of plausible solutions which are typically overly-smooth and thus have poor perceptual quality [41, 32, 12, 4]. Reconstructions of varying perceptual quality are exemplified with corresponding PSNR in Figure 2. We illustrate the problem of minimizing MSE in Figure 3 where multiple potential solutions with high texture details are averaged to create a smooth reconstruction.</p>
<p><img src="https://i.loli.net/2020/03/05/BG4NUtpQm8rPsgi.png" alt="Figure 3"></p>
<p>Figure 3: Illustration of patches from the natural image manifold (red) and super-resolved patches obtained with MSE (blue) and GAN (orange). The MSE-based solution appears overly smooth due to the pixel-wise average of possible solutions in the pixel space, while GAN drives the reconstruction towards the natural image manifold producing perceptually more convincing solutions.</p>
<h4 id="1-1-3-损失函数"><a href="#1-1-3-损失函数" class="headerlink" title="1.1.3 损失函数"></a>1.1.3 损失函数</h4><p>逐像素的损失函数(例如MSE)在努力处理恢复损失的高频细节(例如纹理)中的内在不确定性：最小化MSE鼓励寻找合理解的逐像素平均，这通常是过平滑的，因此会得到较差的感知质量[41, 32, 12, 4]。图2中以相应的PSNR为例说明了不同感知质量的重建。我们在图3中说明了最小化MSE的问题，其中对多个具有高级纹理细节的潜在解进行平均从而创建一个平滑的重建。</p>
<p><img src="https://i.loli.net/2020/03/05/BG4NUtpQm8rPsgi.png" alt="Figure 3"></p>
<p>图3：自然图像流形图像块(红)，由MSE获得的超分辨率图像块(蓝)以及由GAN获得的超分辨率图像块(橙)。由于像素空间中可能解的逐像素平均，基于MSE的解似乎更平滑，而GAN将重建推向自然图像流形，产生了感知上更具说服力的解。</p>
<p>In Mathieu et al. [41] and Denton et al. [6] the authors tackled this problem by employing generative adversarial networks (GANs) [21] for the application of image generation. Yu and Porikli [65] augment pixel-wise MSE loss with a discriminator loss to train a network that super-resolves face images with large upscaling factors (8×). GANs were also used for unsupervised representation learning in Radford et al. [43]. The idea of using GANs to learn a mapping from one manifold to another is described by Li and Wand [37] for style transfer and Yeh et al. [63] for inpainting. Bruna et al. [4] minimize the squared error in the feature spaces of VGG19 [48] and scattering networks.</p>
<p>在Mathieu等[41]和Denton等[6]中，作者通过采用图像生成应用生成对抗网络(GANs)来解决这个问题。Yu和Porikli[65]通过判别器损失增大了逐像素的MSE损失来训练网络，这个网络使用较大的上采样系数(8×)·对人脸图像进行超分辨率。在Radford等[43]中GAN也用来进行无监督表示学习。Li和Wand[37]的风格转换以及Yeh等[63]的图像修复都描述了使用GAN学习一个流形到另一个流形映射的想法。Bruna等[4]在VGG19[48]特征空间以及散射网络中都最小化了方差。</p>
<p>Dosovitskiy and Brox [12] use loss functions based on Euclidean distances computed in the feature space of neural networks in combination with adversarial training. It is shown that the proposed loss allows visually superior image generation and can be used to solve the ill-posed inverse problem of decoding nonlinear feature representations. Similar to this work, Johnson et al. [32] and Bruna et al. [4] propose the use of features extracted from a pretrained VGG network instead of low-level pixel-wise error measures. Specifically the authors formulate a loss function based on the euclidean distance between feature maps extracted from the VGG19 [48] network. Perceptually more convincing results were obtained for both super-resolution and artistic style-transfer [18, 19]. Recently, Li and Wand [37] also investigated the effect of comparing and blending patches in pixel or VGG feature space.</p>
<p>Dosovitskiy和Brox使用基于神经网络特征空间中计算的欧式距离损失函数与对抗训练相结合。结果表明，提出的损失能够生成视觉上更好的图像并且可以用来解决解码非线性特征表示的不适定逆问题。与这个工作类似，Johnson等[32]和Bruna等[4]提出使用从预训练VGG网络中提取的特征来代替低级逐像素误差度量。具体来说，作者基于VGG19[48]网络提取的特征映射之间的欧式距离来构建损失函数。在超分辨率和艺术风格转换[18, 19]方面，都获得了感知上更具说服力的结果。最近，Li和Wand[37]还研究了在像素或VGG特征空间中对比和混合图像块的效果。</p>
<h3 id="1-2-Contribution"><a href="#1-2-Contribution" class="headerlink" title="1.2. Contribution"></a>1.2. Contribution</h3><p>GANs provide a powerful framework for generating plausible-looking natural images with high perceptual quality. The GAN procedure encourages the reconstructions to move towards regions of the search space with high probability of containing photo-realistic images and thus closer to the natural image manifold as shown in Figure 3.</p>
<h3 id="1-2-贡献"><a href="#1-2-贡献" class="headerlink" title="1.2. 贡献"></a>1.2. 贡献</h3><p>GAN提供了一种强大的框架，其可以生成看起来真实、具有高感知质量的自然图像。GAN过程鼓励重建朝向有很大可能包含逼真图像的搜索空间区域，因此更接近图3中所示的自然图像流形。</p>
<p>In this paper we describe the first very deep ResNet [28, 29] architecture using the concept of GANs to form a perceptual loss function for photo-realistic SISR. Our main contributions are:</p>
<p>• We set a new state of the art for image SR with high upscaling factors (4×) as measured by PSNR and structural similarity (SSIM) with our 16 blocks deep ResNet (SRResNet) optimized for MSE.</p>
<p>• We propose SRGAN which is a GAN-based network optimized for a new perceptual loss. Here we replace the MSE-based content loss with a loss calculated on feature maps of the VGG network [48], which are more invariant to changes in pixel space [37].</p>
<p>• We confirm with an extensive mean opinion score (MOS) test on images from three public benchmark datasets that SRGAN is the new state of the art, by a large margin, for the estimation of photo-realistic SR images with high upscaling factors (4×).</p>
<p>本文中我们描述了第一个很深的ResNet[28, 29]架构，使用GAN概念形成了逼真SISR的感知损失函数。我们的主要贡献如下：</p>
<p>• 我们在大的上采样系数下(4×)为图像SR设置了最新的技术水平，并用PSNR、结构相似性(SSIM)以及MSE进行了度量，使用了为MSE优化的16块深度ResNet(SRResNet)。</p>
<p>• 我们提出了SRGAN，一种为新感知损失优化的基于GAN的网络。这里我们将基于MSE的内容损失替换为在VGG网络特征映射上计算的损失，其对于像素空间[37]的变化更具有不变性。</p>
<p>• 我们通过在三个公开基准数据集的图像上进行大量的平均主观得分(MOS)测试，确认了SRGAN是最新的技术，在使用较大的上采样系数(4×)进行逼真SR图像评估上具有很大优势。</p>
<p>We describe the network architecture and the perceptual loss in Section 2. A quantitative evaluation on public benchmark datasets as well as visual illustrations are provided in Section 3. The paper concludes with a discussion in Section 4 and concluding remarks in Section 5.</p>
<p>我们将在第二节中描述网络架构和感知损失。第三节中提供在公开基准数据集上的定量评估和视觉插图。本文在第4节中进行了讨论，并在第5节中作了总结。</p>
<h2 id="2-Method"><a href="#2-Method" class="headerlink" title="2. Method"></a>2. Method</h2><p>In SISR the aim is to estimate a high-resolution, super-resolved image $I^{SR}$ from a low-resolution input image $I^{LR}$. Here $I^{LR}$ is the low-resolution version of its high-resolution counterpart $I^{HR}$. The high-resolution images are only available during training. In training, $I^{LR}$ is obtained by applying a Gaussian filter to $I^{HR}$ followed by a downsampling operation with downsampling factor $r$. For an image with $C$ color channels, we describe $I^{LR}$ by a real-valued tensor of size W × H × C and $I^{HR}$, $I^{SR}$ by $rW × rH × C$ respectively.</p>
<h2 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h2><p>SISR的目标是根据低分辨率输入图像$I^{LR}$来估计高分辨率、超分辨率图像$I^{SR}$。这里$I^{HR}$是高分辨率图像，$I^{LR}$是其对应的低分辨率版本。高分辨率图像仅在训练中可获得。训练中，$I^{LR}$可以通过对$I^{HR}$应用高斯滤波，然后执行下采样系数为$r$的下采样操作得到。对于有$C$个颜色通道的图像，我们分别用大小为$W × H × C$的实值张量描述$I^{LR}$，用大小为$rW × rH × C$的实值张量描述$I^{HR}$、$I^{SR}$。</p>
<p>Our ultimate goal is to train a generating function $G$ that estimates for a given LR input image its corresponding HR counterpart. To achieve this, we train a generator network as a feed-forward CNN $G_{\theta_G}$ parametrized by $\theta_G$. Here $\theta_G = {W_{1:L} ; b_{1:L} }$ denotes the weights and biases of a $L$-layer deep network and is obtained by optimizing a SR-specific loss function $l^{SR}$. For training images $I^{HR}_n$, $n = 1, …, N_n$ with corresponding $I^{LR}_n$, $n = 1, …, N_n$, we solve: </p>
<p>$$\hat\theta_G=\mathop{argmin}\limits_{\theta_G}\frac{1}{N}\sum^{N}_{n=1}l^{SR}(G_{\theta_G}(I^{LR}_n),I^{HR}_n)  \tag{1}$$</p>
<p>我们的最终目标是训练一个生成函数$G$，用来估算给定LR输入图像的对应HR图像。为此，我们训练了一个生成网络，参数为$\theta _G$的前馈CNN$G_{\theta_G}$。其中$\theta_G = {W_{1:L} ; b_{1:L} }$表示一个$L$层深度网络的权重和偏置，可以通过优化SR特定损失函数$l^{SR}$获得。对于训练图像$I^{HR}_n$， $n = 1, …, N_n$，及其对应的$I^{LR}_n$，$n = 1, …, N_n$，求解：</p>
<p>$$\hat\theta_G=\mathop{argmin}\limits_{\theta_G}\frac{1}{N}\sum^{N}_{n=1}l^{SR}(G_{\theta_G}(I^{LR}_n),I^{HR}_n)  \tag{1}$$</p>
<p>In this work we will specifically design a perceptual loss $l^{SR}$ as a weighted combination of several loss components that model distinct desirable characteristics of the recovered SR image. The individual loss functions are described in more detail in Section 2.2.</p>
<p>在这项工作中，我们将专门设计一个感知损失$l^{SR}$作为几种损失分量的加权组合，这些损失分量对恢复的SR图像的不同要求特性进行建模。单个损失函数在2.2节中有更详细的描述。</p>
<h3 id="2-1-Adversarial-network-architecture"><a href="#2-1-Adversarial-network-architecture" class="headerlink" title="2.1. Adversarial network architecture"></a>2.1. Adversarial network architecture</h3><p>Following Goodfellow et al. [21] we further define a discriminator network $D_{\theta_D}$ which we optimize in an alternating manner along with $G_{\theta_G}$ to solve the adversarial min-max problem: </p>
<p>$$\mathop{min}\limits_{\theta_G}\mathop{max}\limits_{\theta_D}\mathbb{E}_{I^{HR}\sim p_{train}(I^{HR})}[logD_{\theta_D}(I^{HR})] + \mathbb{E}_{I^{LR}\sim p_{G}(I^{LR})}[log(1-D_{\theta_D}(G_{\theta_G}(I^{LR})))] \tag{2}$$</p>
<p>The general idea behind this formulation is that it allows one to train a generative model $G$ with the goal of fooling a differentiable discriminator $D$ that is trained to distinguish super-resolved images from real images. With this approach our generator can learn to create solutions that are highly similar to real images and thus difficult to classify by $D$. This encourages perceptually superior solutions residing in the subspace, the manifold, of natural images. This is in contrast to SR solutions obtained by minimizing pixel-wise error measurements, such as the MSE.</p>
<h3 id="2-1-对抗网络架构"><a href="#2-1-对抗网络架构" class="headerlink" title="2.1. 对抗网络架构"></a>2.1. 对抗网络架构</h3><p>按照Goodfellow等[21]，我们进一步定义了一个判别器网络$D_{\theta_D}$，我们对其与$G_{\theta_G}$进行交替优化来解决对抗最小-最大问题：</p>
<p>$$\mathop{min}\limits_{\theta_G}\mathop{max}\limits_{\theta_D}\mathbb{E}_{I^{HR}\sim p_{train}(I^{HR})}[logD_{\theta_D}(I^{HR})] + \mathbb{E}_{I^{LR}\sim p_{G}(I^{LR})}[log(1-D_{\theta_D}(G_{\theta_G}(I^{LR})))] \tag{2}$$</p>
<p>这个公式的总体思想是，它允许训练生成模型$G$，生成模型目的是欺骗具有辨别能力的判别器$D$，判别器被训练用来区分超分辨图像与真实图像。通过这种方法，我们的生成器可以学习创建与真实图像高度相似的解，因此很难被$D$分类。这鼓励了位于自然图像子空间，流形中的感知上更优的解。这与通过最小化逐像素的误差测量(例如MSE)获得的SR解形成鲜明的对比。</p>
<p>At the core of our very deep generator network $G$, which is illustrated in Figure 4 are $B$ residual blocks with identical layout. Inspired by Johnson et al. [32] we employ the block layout proposed by Gross and Wilber [23]. Specifically, we use two convolutional layers with small 3×3 kernels and 64 feature maps followed by batch-normalization layers [31] and ParametricReLU [27] as the activation function. We increase the resolution of the input image with two trained sub-pixel convolution layers as proposed by Shi et al. [47].</p>
<p><img src="https://i.loli.net/2020/03/06/qo1Rgdac8mfeuzl.png" alt="Figure 4"></p>
<p>Figure 4: Architecture of Generator and Discriminator Network with corresponding kernel size (k), number of feature maps (n) and stride (s) indicated for each convolutional layer.</p>
<p>如图4所示，我们的深度生成器网络$G$的中心是$B$个含有恒等设计的残差块。受Johnson等[32]启发，我们采用了Gross和Wilber[23]提出的块设计。具体来说，我们使用了两个卷积层，其核大小为3×3，具有64层特征映射，其后是批归一化层[31]，使用ParametricReLU[27]作为激活函数。如Shi等[47]的提议，我们使用两个训练好的子像素卷积层来增加输入图像的分辨率。</p>
<p><img src="https://i.loli.net/2020/03/06/qo1Rgdac8mfeuzl.png" alt="Figure 4"></p>
<p>图4：生成器网络和判别器网络的架构，每个卷积层表明了对应的卷积核大小(k)，特征映射数量(n)和步长(s)。</p>
<p>To discriminate real HR images from generated SR samples we train a discriminator network. The architecture is shown in Figure 4. We follow the architectural guidelines summarized by Radford et al. [43] and use LeakyReLU activation (α = 0.2) and avoid max-pooling throughout the network. The discriminator network is trained to solve the maximization problem in Equation 2. It contains eight convolutional layers with an increasing number of 3 × 3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels as in the VGG network [48]. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. The resulting 512 feature maps are followed by two dense layers and a final sigmoid activation function to obtain a probability for sample classification.</p>
<p>为了从生成的SR样本中区分出真实的HR图像，我们训练了一个判别器网络。架构如图4所示。我们遵循Radford等[43]总结的架构指南，使用LeakyReLU激活(α=0.2)，在整个网络中避免使用最大池化。训练的判别器网络用来解决等式2中的最大化问题。它包含8个卷积层，其中3×3滤波器核的数量逐渐增加，与VGG网络一样[48]，从64个滤波器核增加到512个，增加了2倍。在每次特征数量加倍时，步长卷积用来降低图像分辨率。生成的512个特征映射之后是两个稠密层，最后的sigmoid激活用来获得样本分类的概率。</p>
<h3 id="2-2-Perceptual-loss-function"><a href="#2-2-Perceptual-loss-function" class="headerlink" title="2.2. Perceptual loss function"></a>2.2. Perceptual loss function</h3><p>The definition of our perceptual loss function $l^{SR}$ is critical for the performance of our generator network. While $l^{SR}$ is commonly modeled based on the MSE [9, 47], we improve on Johnson et al. [32] and Bruna et al. [4] and design a loss function that assesses a solution with respect to perceptually relevant characteristics. We formulate the perceptual loss as the weighted sum of a content loss $l^{SR}_X$ and an adversarial loss component as: </p>
<p>$$l^{SR}=\underbrace{\underbrace{l^{SR}_X}_{content\ loss} + \underbrace{10^{-3}l^{SR}_{Gen}}_{adversarial\ loss}}_{perceptual\ loss(for\ VGG\ based\ content\ loss)}\tag{3}$$</p>
<p>In the following we describe possible choices for the content loss $l^{SR}_X$ and the adversarial loss $l^{SR}_{Gen}$.</p>
<h3 id="2-2-感知损失函数"><a href="#2-2-感知损失函数" class="headerlink" title="2.2. 感知损失函数"></a>2.2. 感知损失函数</h3><p>感知损失函数$l^{SR}$的定义对于我们的生成器网络性能非常关键。虽然$l^{SR}$通常是基于MSE[9, 47]建模的，但我们在Johnson等[32]和Bruna等[4]的基础上进行了改进，设计了一个损失函数用来评估在感知相关特性方面的解。我们将感知损失构建为内容损失$l^{SR}_X$和对抗损失的加权和：</p>
<p>$$l^{SR}=\underbrace{\underbrace{l^{SR}_X}_{content\ loss} + \underbrace{10^{-3}l^{SR}_{Gen}}_{adversarial\ loss}}_{perceptual\ loss(for\ VGG\ based\ content\ loss)} \tag{3}$$</p>
<p>接下来我们描述内容损失$l^{SR}_X$和对抗损失$l^{SR}_{Gen}$的可能选择。</p>
<h4 id="2-2-1-Content-loss"><a href="#2-2-1-Content-loss" class="headerlink" title="2.2.1 Content loss"></a>2.2.1 Content loss</h4><p>The pixel-wise <strong>MSE loss</strong> is calculated as:</p>
<p>$$l^{SR}_{MSE}=\frac {1} {r^2WH} \sum^{rW}_{x=1} \sum^{rH}_{y=1}(I^{HR}_{x,y} - G_{\theta_G}(I^{LR})_{x,y})^2 \tag{4}$$</p>
<p>This is the most widely used optimization target for image SR on which many state-of-the-art approaches rely [9, 47]. However, while achieving particularly high PSNR, solutions of MSE optimization problems often lack high-frequency content which results in perceptually unsatisfying solutions with overly smooth textures (c.f. Figure 2).</p>
<h4 id="2-2-1-内容损失"><a href="#2-2-1-内容损失" class="headerlink" title="2.2.1 内容损失"></a>2.2.1 内容损失</h4><p>逐像素的MSE损失计算如下：</p>
<p>$$l^{SR}_{MSE}=\frac {1} {r^2WH} \sum^{rW}_{x=1} \sum^{rH}_{y=1}(I^{HR}_{x,y} - G_{\theta_G}(I^{LR})_{x,y})^2 \tag{4}$$</p>
<p>对于图像SR，这是应用最广泛的优化目标，许多最新技术都依赖该目标[9, 47]。然而，虽然取得了特别高的PSNR，但MSE优化问题的解通常缺少高频内容，这会导致具有过于平滑纹理的解在感知上不令人满意（对比图2）。</p>
<p>Instead of relying on pixel-wise losses we build on the ideas of Gatys et al. [18], Bruna et al. [4] and Johnson et al. [32] and use a loss function that is closer to perceptual similarity. We define the VGG loss based on the ReLU activation layers of the pre-trained 19 layer VGG network described in Simonyan and Zisserman [48]. With $\phi_{i,j}$ we indicate the feature map obtained by the j-th convolution (after activation) before the i-th maxpooling layer within the VGG19 network, which we consider given. We then define the VGG loss as the euclidean distance between the feature representations of a reconstructed image $G_{\theta_G}(I^{LR})$ and the reference image $I^{HR}$:</p>
<p>$$l^{SR}_{VGG/i,j}=\frac {1} {W_{i,j}H_{i,j}}\sum^{W_{i,j}}_{x=1}\sum^{H_{i,j}}_{y=1}(\phi_{i,j}(I^{HR})_{x,y}-\phi_{i,j}(G_{\theta_G}(I^{HR}))_{x,y})^2 \tag{5}$$</p>
<p>在基于Gatys等[18]，Bruna等[4]和Johnson等[32]想法的基础上，我们构建并使用了更接近于感知相似性的损失函数，而不是依赖于逐像素损失。我们在Simonyan和Zisserman[48]中描述的预训练19层VGG网络的ReLU激活层的基础上定义了VGG损失。在给定的的VGG19网络中，我们用$\phi_{i,j}$指代在第i层池化层之前的第j层卷积(激活之后)获得的特征映射。我们使用重建图像$G_{\theta_G}(I^{LR})$的特征表示和参照图像$I^{HR}$之间的欧式距离来定义VGG损失：</p>
<p>$$l^{SR}_{VGG/i,j}=\frac {1} {W_{i,j}H_{i,j}}\sum^{W_{i,j}}_{x=1}\sum^{H_{i,j}}_{y=1}(\phi_{i,j}(I^{HR})_{x,y}-\phi_{i,j}(G_{\theta_G}(I^{HR}))_{x,y})^2 \tag{5}$$</p>
<p>Here $W_{i,j}$ and $H_{i,j}$ describe the dimensions of the respective feature maps within the VGG network.</p>
<p>这里$W_{i,j}$和$H_{i,j}$描述了VGG网络中各个特征映射的维度。</p>
<h4 id="2-2-2-Adversarial-loss"><a href="#2-2-2-Adversarial-loss" class="headerlink" title="2.2.2 Adversarial loss"></a>2.2.2 Adversarial loss</h4><p>In addition to the content losses described so far, we also add the generative component of our GAN to the perceptual loss. This encourages our network to favor solutions that reside on the manifold of natural images, by trying to fool the discriminator network. The generative loss $l^{SR}_{Gen}$ is defined based on the probabilities of the discriminator $D_{\theta_D}(G_{\theta_G}(I^{LR}))$ over all training samples as:</p>
<p>$$l^{SR}_{Gen}=\sum^N_{n=1}-logD_{\theta_D}(G_{\theta_G}(I^{LR})) \tag{6}$$</p>
<h4 id="2-2-2-对抗损失"><a href="#2-2-2-对抗损失" class="headerlink" title="2.2.2 对抗损失"></a>2.2.2 对抗损失</h4><p>除了目前为止描述的内容损失之外，我们也将GAN的生成组件添加到了感知损失中。通过设法欺骗判别器网络，这鼓励我们的网络支持位于自然图像流行上的解。基于判别器$D_{\theta_D}(G_{\theta_G}(I^{LR}))$在所有训练样本上的概率，生成损失$l^{SR}_{Gen}$定义为：</p>
<p>$$l^{SR}_{Gen}=\sum^N_{n=1}-logD_{\theta_D}(G_{\theta_G}(I^{LR})) \tag{6}$$</p>
<p>Here, $D_{\theta_D}(G_{\theta_G}(I^{LR}))$ is the probability that the reconstructed image $G_{\theta_G}(I^{LR})$ is a natural HR image. For better gradient behavior we minimize $-logD_{\theta_D}(G_{\theta_G}(I^{LR}))$ instead of $log[1-logD_{\theta_D}(G_{\theta_G}(I^{LR}))]$ [21].</p>
<p>这里，$D_{\theta_D}(G_{\theta_G}(I^{LR}))$是重建图像$G_{\theta_G}(I^{LR})$为自然HR图像的概率。为了得到更好的梯度行为，我们对$-logD_{\theta_D}(G_{\theta_G}(I^{LR}))$进行最小化，而不是$log[1-logD_{\theta_D}(G_{\theta_G}(I^{LR}))]$ [21]。</p>
<h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><h3 id="3-1-Data-and-similarity-measures"><a href="#3-1-Data-and-similarity-measures" class="headerlink" title="3.1. Data and similarity measures"></a>3.1. Data and similarity measures</h3><p>We perform experiments on three widely used benchmark datasets Set5 [2], Set14 [68] and BSD100, the testing set of BSD300 [40]. All experiments are performed with a scale factor of 4× between low- and high-resolution images. This corresponds to a 16× reduction in image pixels. For fair comparison, all reported PSNR [dB] and SSIM [57] measures were calculated on the y-channel of center-cropped, removal of a 4-pixel wide strip from each border, images using the daala package. Super-resolved images for the reference methods, including nearest neighbor, bicubic, SRCNN [8] and SelfExSR [30], were obtained from online material supplementary to Huang et al. [30] and for DRCN from Kim et al. [33]. Results obtained with SRResNet (for losses: $l^{SR}_{MSE}$ and $l^{SR}_{VGG/2.2}$ ) and the SRGAN variants are available online. Statistical tests were performed as paired two-sided Wilcoxon signed-rank tests and significance determined at $p&lt;0.05$.</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><h3 id="3-1-数据和相似性度量"><a href="#3-1-数据和相似性度量" class="headerlink" title="3.1. 数据和相似性度量"></a>3.1. 数据和相似性度量</h3><p>我们在三个广泛使用的基准数据集Set5[2]，Set14[68]和BSD300的测试集BSD100[40]上进行实验。所有实验都在低分辨率和高分辨率图像之间以4倍的尺度因子执行。图像像素对应减少16倍。为了公平比较，所有报告的PSNR[dB]和SSIM[57]度量使用daala软件包，在中心裁剪的图像的y通道上进行计算，图像每个边界移除了4个像素宽的图像条。参考方法包括最近邻居，双三次，SRCNN[8]和SelfExSR[30]的超分辨图像是从Huang等[30]和Kim等的DRCN[33]的在线补充材料中获得的 。SRResNet(损失：$l^{SR}_{MSE}$和$l^{SR}_{VGG/2.2}$)和SRGAN变体得到的结果可在线获得。统计测试以成对的双侧威尔科克森符号秩检验和显著性检验进行，显著性水平为$p&lt;0.05$。</p>
<p>The reader may also be interested in an independently developed GAN-based solution on GitHub. However it only provides experimental results on a limited set of faces, which is a more constrained and easier task.</p>
<p>读者可能还对GitHub上独立开发的基于GAN的解决方案感兴趣。然而，它只能提供一组有限人脸图像上的实验结果，这是一个更受限且更轻松的任务。</p>
<h3 id="3-2-Training-details-and-parameters"><a href="#3-2-Training-details-and-parameters" class="headerlink" title="3.2. Training details and parameters"></a>3.2. Training details and parameters</h3><p>We trained all networks on a NVIDIA Tesla M40 GPU using a random sample of 350 thousand images from the <strong>ImageNet</strong> database [44]. These images are distinct from the testing images. We obtained the LR images by downsampling the HR images (BGR, $C=3$) using bicubic kernel with downsampling factor $r=4$. For each mini-batch we crop 16 random 96 × 96 HR sub images of distinct training images. Note that we can apply the generator model to images of arbitrary size as it is fully convolutional. For optimization we use Adam [35] with $\beta_{1}=0.9$. The SRResNet networks were trained with a learning rate of $10^{−4}$ and $10^6$ update iterations. We employed the trained MSE-based SRResNet network as initialization for the generator when training the actual GAN to avoid undesired local optima. All SRGAN variants were trained with $10^5$ update iterations at a learning rate of $10^{−4}$ and another $10^5$ iterations at a lower rate of $10^{−5}$. We alternate updates to the generator and discriminator network, which is equivalent to $k=1$ as used in Goodfellow et al. [21]. Our generator network has 16 identical ($B=16$) residual blocks. During test time we turn batch-normalization update off to obtain an output that deterministically depends only on the input [31]. Our implementation is based on Theano [52] and Lasagne [7].</p>
<h3 id="3-2-训练细节和参数"><a href="#3-2-训练细节和参数" class="headerlink" title="3.2. 训练细节和参数"></a>3.2. 训练细节和参数</h3><p>我们使用NVIDIA Tesla M40 GPU训练所有的网络，训练数据来自<strong>ImageNet</strong>数据集[44]中随机采样的35万张图像。这些图片不同于测试图片。我们使用双三次核对HR图像(BGR, $C=3$)进行下采样得到LR图像，下采样系数为$r=4$。对于每一份小批量数据，我们对不同的训练图像裁剪16个随机的96×96的HR子图像。注意我们可以对任意大小的图像应用生成器模型，因为它是全卷积的。我们使用Adam[35]，$\beta_{1}=0.9$来进行优化。SRResNet网络使用$10^{−4}$的学习率进行训练，更新迭代次数$10^6$。在训练实际的GAN时，为了避免不必要的局部最优值，我们采用预训练的基于MSE的SRResNet网络对生成器进行初始化。所有的SRGAN变种都以$10^{−4}$的学习率训练$10^5$次迭代，然后以$10^{−5}$的学习率再训练$10^5$次迭代。我们交替更新生成器和判别器网络，这等价于Goodfellow等[21]的$k=1$。我们的生成器网络有16个恒等($B=16$)残差块。测试期间，为了获得确定性地只依赖输入的输出，我们关闭了批归一化更新。我们的实现基于Theano[52]和Lasagne[7]。</p>
<h3 id="3-3-Mean-opinion-score-MOS-testing"><a href="#3-3-Mean-opinion-score-MOS-testing" class="headerlink" title="3.3. Mean opinion score (MOS) testing"></a>3.3. Mean opinion score (MOS) testing</h3><p>We have performed a MOS test to quantify the ability of different approaches to reconstruct perceptually convincing images. Specifically, we asked 26 raters to assign an integral score from 1 (bad quality) to 5 (excellent quality) to the super-resolved images. The raters rated 12 versions of each image on Set5, Set14 and BSD100: nearest neighbor (NN), bicubic, SRCNN [8], SelfExSR [30], DRCN [33], ESPCN [47], SRResNet-MSE, $SRResNet-VGG22^*$ ($*$ not rated on BSD100), $SRGAN-MSE^*$, $SRGAN-VGG22^*$, SRGAN-VGG54 and the original HR image. Each rater thus rated 1128 instances (12 versions of 19 images plus 9 versions of 100 images) that were presented in a randomized fashion. The raters were calibrated on the NN (score 1) and HR (5) versions of 20 images from the BSD300 training set. In a pilot study we assessed the calibration procedure and the test-retest reliability of 26 raters on a subset of 10 images from BSD100 by adding a method’s images twice to a larger test set. We found good reliability and no significant differences between the ratings of the identical images. Raters very consistently rated NN interpolated test images as 1 and the original HR images as 5 (c.f. Figure 5).</p>
<p><img src="https://i.loli.net/2020/03/13/kxJ1thYLfnVSQy3.png" alt="Figure 5"></p>
<p>Figure 5: Color-coded distribution of MOS scores on <strong>BSD100</strong>. For each method 2600 samples (100 images × 26 raters) were assessed. Mean shown as red marker, where the bins are centered around value $i$. [4× upscaling]</p>
<h3 id="3-3-平均主观得分-MOS-测试"><a href="#3-3-平均主观得分-MOS-测试" class="headerlink" title="3.3. 平均主观得分(MOS)测试"></a>3.3. 平均主观得分(MOS)测试</h3><p>为了量化不同方法重建感知上令人信服的图像的能力，我们进行了MOS测试。具体来说，我们让26个评分员使用整数分1(质量差)到5(质量极好)对超分辨率图像进行打分。评分员对Set5，Set14和BSD100数据集上的每一张图像的12个版本进行了评分：最近邻(NN)，双三次，SRCNN[8]，SelfExSR[30]，DRCN[33]，ESPCN[47]，SRResNet-MSE，$SRResNet-VGG22^*$ ($*$没有在BSD100上评分)，$SRGAN-MSE^*$，$SRGAN-VGG22^*$，SRGAN-VGG54和原始HR图像。因此每一个评分员对随机呈现的1128个实例（19张图像的12个版本加上100张图像的9个版本）进行了评估。评分员对BSD300训练集的20张图像的NN（得分1）和HR（5）版本上进行了校准。在初步研究中，通过两次添加方法图像到更大的测试集中，我们评估了26个评分员在BSD100的10张图像子集上的校准程序和重测信度。我们发现了良好的可靠性，在相同图像的评分之间没有显著差异。评分员非常一致地将NN插值测试图像评分为1，原始HR图像评分为5（参加图5）。</p>
<p><img src="https://i.loli.net/2020/03/13/kxJ1thYLfnVSQy3.png" alt="Figure 5"></p>
<p>图5：<strong>BSD100</strong>上MOS得分的颜色编码分布。每一种方法使用2600个样本(100张图片×26个评估者)评估。均值显示为红色标记，bin以值$i$为中心(4倍上采样)。</p>
<p>The experimental results of the conducted MOS tests are summarized in Table 1, Table 2 and Figure 5.</p>
<p>Table 1: Performance of different loss functions for SRResNet and the adversarial networks on Set5 and Set14 benchmark data. MOS score significantly higher ($p&lt;0.05$) than with other losses in that category. [4× upscaling]</p>
<p><img src="https://i.loli.net/2020/03/30/SzHnxkJYQBTwZtb.png" alt="Table 1"></p>
<p>Table 2: Comparison of NN, bicubic, SRCNN [8], SelfExSR [30], DRCN [33], ESPCN [47], SRResNet, SRGAN-VGG54 and the original HR on benchmark data. Highest measures (PSNR [dB], SSIM, MOS) in bold. [4× upscaling]</p>
<p><img src="https://i.loli.net/2020/03/30/r2lFX3pA8Pb5zG4.png" alt="Table 2"></p>
<p>进行的MOS测试的实验结果总结在表1，表2和图5中。</p>
<p>表1：SRResNet不同损失函数的性能和对抗网络在Set5和Set14上的基准数据。MOS得分明显比其它损失在对应类别上更高($p&lt;0.05$)。[4×上采样]</p>
<p><img src="https://i.loli.net/2020/03/30/SzHnxkJYQBTwZtb.png" alt="Table 1"></p>
<p>表2：NN，双三次，SRCNN[8]，SelfExSR[30]，DRCN[33]，ESPCN[47]，SRResNet，SRGAN-VGG54和原始HR在基准数据上的比较. 最高的度量(PSNR[dB]，SSIM，MOS)以粗体显示。[4×上采样]</p>
<p><img src="https://i.loli.net/2020/03/30/r2lFX3pA8Pb5zG4.png" alt="Table 2"></p>
<h3 id="3-4-Investigation-of-content-loss"><a href="#3-4-Investigation-of-content-loss" class="headerlink" title="3.4. Investigation of content loss"></a>3.4. Investigation of content loss</h3><p>We investigated the effect of different content loss choices in the perceptual loss for the GAN-based networks. Specifically we investigate $l^{SR}=l^{SR}_X+10^{-3}l^{SR}_{Gen}$ for the following content losses $l^{SR}_X$:</p>
<p>• SRGAN-MSE: $l^{SR}_{MSE}$, to investigate the adversarial network with the standard MSE as content loss.</p>
<p>• SRGAN-VGG22: $l^{SR}_{VGG/2.2}$ with $\phi_{2,2}$, a loss defined on feature maps representing lower-level features [67]. </p>
<p>• SRGAN-VGG54: $l^{SR}_{VGG/5.4}$ with $\phi_{5,4}$, a loss defined on feature maps of higher level features from deeper network layers with more potential to focus on the content of the images [67, 64, 39]. We refer to this network as SRGAN in the following.</p>
<h3 id="3-4-内容损失研究"><a href="#3-4-内容损失研究" class="headerlink" title="3.4. 内容损失研究"></a>3.4. 内容损失研究</h3><p>对于基于GAN的网络，我们研究了感知损失中不同内容损失选择的影响。具体来说，对于下面的内容损失$l^{SR}_X$，我们研究了$l^{SR}=l^{SR}_X+10^{-3}l^{SR}_{Gen}$：</p>
<p>• SRGAN-MSE：$l^{SR}_{MSE}$，以标准MSE作为内容损失来研究对抗网络。</p>
<p>• SRGAN-VGG22：具有$\phi_{2,2}$的$l^{SR}_{VGG/2.2}$，表示更底层特征[67]的特征映射上定义的损失。</p>
<p>• SRGAN-VGG54：具有$\phi_{5,4}$的$l^{SR}_{VGG/5.4}$，来自较深网络层的更高层特征的特征映射上定义的损失，更可能集中在图像内容上[67, 64, 39]。在下文中，我们将此网络称为SRGAN。</p>
<p>We also evaluate the performance of the generator network without adversarial component for the two losses $l^{SR}_{MSE}$ (SRResNet-MSE) and $l^{SR}_{VGG/2.2}$ (SRResNet-VGG22). We refer to SRResNet-MSE as SRResNet. Quantitative results are summarized in Table 1 and visual examples provided in Figure 6. Even combined with the adversarial loss, MSE provides solutions with the highest PSNR values that are, however, perceptually rather smooth and less convincing than results achieved with a loss component more sensitive to visual perception. This is caused by competition between the MSE-based content loss and the adversarial loss. We further attribute minor reconstruction artifacts, which we observed in a minority of SRGAN-MSE-based reconstructions, to those competing objectives. We could not determine a significantly best loss function for SRResNet or SRGAN with respect to MOS score on Set5. However, SRGAN-VGG54 significantly outperformed other SRGAN and SRResNet variants on Set14 in terms of MOS. We observed a trend that using the higher level VGG feature maps $\phi_{5,4}$ yields better texture detail when compared to $\phi_{2,2}$ (c.f. Figure 6).</p>
<p><img src="https://i.loli.net/2020/04/01/h84ImQlWtJ2arjg.png" alt="Figure 6"></p>
<p>Figure 6: SRResNet (left: a,b), SRGAN-MSE (middle left: c,d), SRGAN-VGG2.2 (middle: e,f) and SRGAN-VGG54 (middle right: g,h) reconstruction results and corresponding reference HR image (right: i,j). [4× upscaling]</p>
<p>对于两个损失$l^{SR}_{MSE}$(SRResNet-MSE)和$l^{SR}_{VGG/2.2}$(SRResNet-VGG22)，我们也对没有对抗组件的生成器网络性能进行了评估。我们将SRResNet-MSE称为SRResNet。在表1中总结了定量结果，图6中提供了直观的示例。即使结合对抗损失，MSE仍然提供了具有最高PSNR值的解，与视觉感知更敏感的损失组件取得的结果相比，其在感知上更平滑，更不令人信服。这是由基于MSE的内容损失和对抗损失之间的竞争引起的。我们进一步将少量基于SRGAN-MSE的重构中观测到的那些较小的重构结果，归因于那些相互竞争的目标。关于Set5上的MOS得分，我们不能确定一个对于SRResNet或SRGAN明显最好的损失函数。但是，考虑到Set14上的MOS得分，SRGAN-VGG54显著优于其它SRGAN和SRResNet变种。我们观察到一种趋势，与$\phi_{2,2}$相比，使用更高层的VGG特征映射$\phi_{5,4}$得到了更好的纹理细节，参见图6。</p>
<p><img src="https://i.loli.net/2020/04/01/h84ImQlWtJ2arjg.png" alt="Figure 6"></p>
<p>图6：SRResNet（左：a，b），SRGAN-MSE（左中：c，d），SRGAN-VGG2.2（中：e，f）和SRGAN-VGG54（右中：g，h）的重建结果以及相应的参考HR图像（右：i，j）。 [4倍上采样]</p>
<h3 id="3-5-Performance-of-the-final-networks"><a href="#3-5-Performance-of-the-final-networks" class="headerlink" title="3.5. Performance of the final networks"></a>3.5. Performance of the final networks</h3><p>We compare the performance of SRResNet and SRGAN to NN, bicubic interpolation, and four state-of-the-art methods. Quantitative results are summarized in Table 2 and confirm that SRResNet (in terms of PSNR/SSIM) sets a new state of the art on three benchmark datasets. Please note that we used a publicly available framework for evaluation (c.f. Section 3.1), reported values might thus slightly deviate from those reported in the original papers.</p>
<h3 id="3-5-最终网络的性能"><a href="#3-5-最终网络的性能" class="headerlink" title="3.5. 最终网络的性能"></a>3.5. 最终网络的性能</h3><p>我们比较了SRResNet、SRGAN、NN、双三次插值和四种最新方法的性能。定量结果总结在表2中，证实了SRResNet(考虑PSNR/SSIM)在三个基准数据集上确立了最新的技术水平。请注意，我们使用了一个公开可获得的框架进行评估，（参加3.1节），因此报告的值可能会与原始论文中报告的值略有不同。</p>
<p>We further obtained MOS ratings for SRGAN and all reference methods on BSD100. The results shown in Table 2 confirm that SRGAN outperforms all reference methods by a large margin and sets a new state of the art for photo-realistic image SR. All differences in MOS (c.f. Table 2) are highly significant on BSD100, except SRCNN vs. SelfExSR. The distribution of all collected MOS ratings is summarized in Figure 5.</p>
<p>我们进一步获得了BSD100数据集上SRGAN和所有其他方法的MOS评分。表2中展示的结果证实了SRGAN大幅度优于所有的参考方法，并为逼真图像SR确立了最新的技术水平。除了SRCNN和SelfExSR之外，BSD100上的MOS得分差异（参加表2）是非常显著的。所有收集的MOS得分分布总结在图5中。</p>
<h2 id="4-Discussion-and-future-work"><a href="#4-Discussion-and-future-work" class="headerlink" title="4. Discussion and future work"></a>4. Discussion and future work</h2><p>We confirmed the superior perceptual performance of SRGAN using MOS testing. We have further shown that standard quantitative measures such as PSNR and SSIM fail to capture and accurately assess image quality with respect to the human visual system [55]. The focus of this work was the perceptual quality of super-resolved images rather than computational efficiency. The presented model is, in contrast to Shi et al. [47], not optimized for video SR in real-time. However, preliminary experiments on the network architecture suggest that shallower networks have the potential to provide very efficient alternatives at a small reduction of qualitative performance. In contrast to Dong et al. [9], we found deeper network architectures to be beneficial. We speculate that the ResNet design has a substantial impact on the performance of deeper networks. We found that even deeper networks (B &gt; 16) can further increase the performance of SRResNet, however, come at the cost of longer training and testing times. We found SRGAN variants of deeper networks are increasingly difficult to train due to the appearance of high-frequency artifacts.</p>
<h2 id="4-讨论和未来工作"><a href="#4-讨论和未来工作" class="headerlink" title="4. 讨论和未来工作"></a>4. 讨论和未来工作</h2><p>我们使用MOS测试证实了SRGAN优秀的感知性能。我们进一步表明，对于人类视觉系统[55]，标准的定量度量，例如PSNR和SSIM，不能捕获并准确评估的图像质量。这项工作的重点是超分辨率的感知质量而不是计算效率。与Shi等[47]相反，提出的模型未针对实时视频SR进行优化。然而，网络架构的初步试验表明，更窄的网络有可能在质量性能降低的情况下提供非常有效的替代方案。与Dong等[9]相反，我们发现更深的网络架构是有益的。我们推测ResNet设计对更深网络的性能有实质性影响。我们发现更深的网络(B&gt;16)可以进一步提升SRResNet的性能，但是以更长的训练和测试时间为代价。我们发现由于高频伪影的出现，更深网络的SRGAN变种越来越难训练。</p>
<p>Of particular importance when aiming for photo-realistic solutions to the SR problem is the choice of the content loss as illustrated in Figure 6. In this work, we found $l^{SR}_{VGG/5.4}$ to yield the perceptually most convincing results, which we attribute to the potential of deeper network layers to represent features of higher abstraction [67, 64, 39] away from pixel space. We speculate that feature maps of these deeper layers focus purely on the content while leaving the adversarial loss focusing on texture details which are the main difference between the super-resolved images without the adversarial loss and photo-realistic images. We also note that the ideal loss function depends on the application. For example, approaches that hallucinate finer detail might be less suited for medical applications or surveillance. The perceptually convincing reconstruction of text or structured scenes [30] is challenging and part of future work. The development of content loss functions that describe image spatial content, but more invariant to changes in pixel space will further improve photo-realistic image SR results.</p>
<p>当针对SR问题的逼真解决方案时，内容损失的选择是非常重要的，如图6所示。在这项工作中，我们发现$l^{SR}_{VGG/5.4}$取得了感知上最令人信服的结果，这归因于更深的网络层可能表示远离像素空间的更加抽象[67, 64, 39]特征。我们推测这些深层的特征映射单纯的注重内容而剩下的对抗损失注重纹理细节，这是没有对抗损失的超分辨率图像和逼真图像之间的主要差异。我们也注意到理想的损失函数取决于应用。例如，虚幻的更精细的细节可能不适合医疗引用或监控。感知上令人信服的文本或结构化场景[30]重建是具有挑战性的，是未来工作的一部分。内容损失函数的开发描述了图像空间内容，但对像素空间变化的不变性将进一步改善逼真的图像SR结果。</p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>We have described a deep residual network SRResNet that sets a new state of the art on public benchmark datasets when evaluated with the widely used PSNR measure. We have highlighted some limitations of this PSNR-focused image super-resolution and introduced SRGAN, which augments the content loss function with an adversarial loss by training a GAN. Using extensive MOS testing, we have confirmed that SRGAN reconstructions for large upscaling factors (4×) are, by a considerable margin, more photo-realistic than reconstructions obtained with state-of-the-art reference methods.</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们描述了一个深度残差网络SRResNet，当广泛使用PSNR度量进行评估时，其在公共基准数据集上树立了最新的技术水平。我们强调了以PSNR为中心的超分辨率的一些限制，引入了SRGAN，其通过训练GAN增加了具有对抗损失的内容损失函数。使用广泛的MOS测试，我们证实了对于大的上采样系数(4×)，SRGAN重构比最新的参考方法得到的重构更逼真。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] J. Allebach and P. W. Wong. Edge-directed interpolation. In Proceedings of International Conference on Image Processing, volume 3, pages 707–710, 1996.</p>
<p>[2] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. BMVC, 2012.</p>
<p>[3] S. Borman and R. L. Stevenson. Super-Resolution from Image Sequences - A Review. Midwest Symposium on Circuits and Systems, pages 374–378, 1998.</p>
<p>[4] J. Bruna, P. Sprechmann, and Y. LeCun. Super-resolution with deep convolutional sufficient statistics. In International Conference on Learning Representations (ICLR), 2016.</p>
<p>[5] D. Dai, R. Timofte, and L. Van Gool. Jointly optimized regressors for image super-resolution. In Computer Graphics Forum, volume 34, pages 95–104, 2015.</p>
<p>[6] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems (NIPS), pages 1486–1494, 2015.</p>
<p>[7] S. Dieleman, J. Schluter, C. Raffel, E. Olson, S. K. Snderby, ¨D. Nouri, D. Maturana, M. Thoma, E. Battenberg, J. Kelly, J. D. Fauw, M. Heilman, diogo149, B. McFee, H. Weideman, takacsg84, peterderivaz, Jon, instagibbs, D. K. Rasul, CongLiu, Britefury, and J. Degrave. Lasagne: First release., 2015.</p>
<p>[8] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In European Conference on Computer Vision (ECCV), pages 184–199. Springer, 2014.</p>
<p>[9] C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(2):295–307, 2016.</p>
<p>[10] C. Dong, C. C. Loy, and X. Tang. Accelerating the super-resolution convolutional neural network. In European Conference on Computer Vision (ECCV), pages 391–407. Springer, 2016.</p>
<p>[11] W. Dong, L. Zhang, G. Shi, and X. Wu. Image deblurring and superresolution by adaptive sparse domain selection and adaptive regularization. IEEE Transactions on Image Processing, 20(7):1838–1857, 2011.</p>
<p>[12] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on deep networks. In Advances in Neural Information Processing Systems (NIPS), pages 658–666, 2016.</p>
<p>[13] C. E. Duchon. Lanczos Filtering in One and Two Dimensions. In Journal of Applied Meteorology, volume 18, pages 1016–1022. 1979.</p>
<p>[14] S. Farsiu, M. D. Robinson, M. Elad, and P. Milanfar. Fast and robust multiframe super resolution. IEEE Transactions on Image Processing, 13(10):1327–1344, 2004.</p>
<p>[15] J. A. Ferwerda. Three varieties of realism in computer graphics. In Electronic Imaging, pages 290–297. International Society for Optics and Photonics, 2003.</p>
<p>[16] W. T. Freeman, T. R. Jones, and E. C. Pasztor. Example-based superresolution. IEEE Computer Graphics and Applications, 22(2):56–65, 2002.</p>
<p>[17] W. T. Freeman, E. C. Pasztor, and O. T. Carmichael. Learning lowlevel vision. International Journal of Computer Vision, 40(1):25–47, 2000.</p>
<p>[18] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 262–270, 2015.</p>
<p>[19] L. A. Gatys, A. S. Ecker, and M. Bethge. Image Style Transfer Using Convolutional Neural Networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2414–2423, 2016.</p>
<p>[20] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. In IEEE International Conference on Computer Vision (ICCV), pages 349–356, 2009.</p>
<p>[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2672–2680, 2014.</p>
<p>[22] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 399–406, 2010.</p>
<p>[23] S. Gross and M. Wilber. Training and investigating residual nets, online at <a href="http://torch.ch/blog/2016/02/04/resnets" target="_blank" rel="external">http://torch.ch/blog/2016/02/04/resnets</a>. html. 2016.</p>
<p>[24] S. Gu, W. Zuo, Q. Xie, D. Meng, X. Feng, and L. Zhang. Convolutional sparse coding for image super-resolution. In IEEE International Conference on Computer Vision (ICCV), pages 1823–1831. 2015.</p>
<p>[25] P. Gupta, P. Srivastava, S. Bhardwaj, and V. Bhateja. A modified psnr metric based on hvs for quality assessment of color images. In IEEE International Conference on Communication and Industrial Application (ICCIA), pages 1–4, 2011.</p>
<p>[26] H. He and W.-C. Siu. Single image super-resolution using gaussian process regression. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 449–456, 2011.</p>
<p>[27] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In IEEE International Conference on Computer Vision (ICCV), pages 1026–1034, 2015.</p>
<p>[28] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.</p>
<p>[29] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), pages 630–645. Springer, 2016.</p>
<p>[30] J. B. Huang, A. Singh, and N. Ahuja. Single image super-resolution from transformed self-exemplars. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5197–5206, 2015.</p>
<p>[31] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning (ICML), pages 448–456, 2015.</p>
<p>[32] J. Johnson, A. Alahi, and F. Li. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision (ECCV), pages 694–711. Springer, 2016.</p>
<p>[33] J. Kim, J. K. Lee, and K. M. Lee. Deeply-recursive convolutional network for image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</p>
<p>[34] K. I. Kim and Y. Kwon. Single-image super-resolution using sparse regression and natural image prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(6):1127–1133, 2010.</p>
<p>[35] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.</p>
<p>[36] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 1097–1105, 2012.</p>
<p>[37] C. Li and M. Wand. Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2479–2486, 2016.</p>
<p>[38] X. Li and M. T. Orchard. New edge-directed interpolation. IEEE Transactions on Image Processing, 10(10):1521–1527, 2001.</p>
<p>[39] A. Mahendran and A. Vedaldi. Visualizing deep convolutional neural networks using natural pre-images. International Journal of Computer Vision, pages 1–23, 2016.</p>
<p>[40] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In IEEE International Conference on Computer Vision (ICCV), volume 2, pages 416–423, 2001.</p>
<p>[41] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In International Conference on Learning Representations (ICLR), 2016.</p>
<p>[42] K. Nasrollahi and T. B. Moeslund. Super-resolution: A comprehensive survey. In Machine Vision and Applications, volume 25, pages 1423–1468. 2014.</p>
<p>[43] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations (ICLR), 2016.</p>
<p>[44] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, pages 1–42, 2014.</p>
<p>[45] J. Salvador and E. Perez-Pellitero. Naive bayes super-resolution ´forest. In IEEE International Conference on Computer Vision (ICCV), pages 325–333. 2015.</p>
<p>[46] S. Schulter, C. Leistner, and H. Bischof. Fast and accurate image upscaling with super-resolution forests. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3791–3799, 2015.</p>
<p>[47] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1874–1883, 2016.</p>
<p>[48] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015.</p>
<p>[49] J. Sun, J. Sun, Z. Xu, and H.-Y. Shum. Image super-resolution using gradient profile prior. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2008.</p>
<p>[50] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–9, 2015.</p>
<p>[51] Y.-W. Tai, S. Liu, M. S. Brown, and S. Lin. Super Resolution using Edge Prior and Single Image Detail Synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2400–2407, 2010.</p>
<p>[52] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688, 2016.</p>
<p>[53] R. Timofte, V. De, and L. Van Gool. Anchored neighborhood regression for fast example-based super-resolution. In IEEE International Conference on Computer Vision (ICCV), pages 1920–1927, 2013.</p>
<p>[54] R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted anchored neighborhood regression for fast super-resolution. In Asian Conference on Computer Vision (ACCV), pages 111–126. Springer, 2014.</p>
<p>[55] G. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Minnen, J. Shor, and M. Covell. Full Resolution Image Compression with Recurrent Neural Networks. arXiv preprint arXiv:1608.05148, 2016.</p>
<p>[56] Y. Wang, L. Wang, H. Wang, and P. Li. End-to-End Image SuperResolution via Deep and Shallow Convolutional Networks. arXiv preprint arXiv:1607.07680, 2016.</p>
<p>[57] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004.</p>
<p>[58] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep networks for image super-resolution with sparse prior. In IEEE International Conference on Computer Vision (ICCV), pages 370–378, 2015.</p>
<p>[59] Z. Wang, E. P. Simoncelli, and A. C. Bovik. Multi-scale structural imilarity for image quality assessment. In IEEE Asilomar Conference on Signals, Systems and Computers, volume 2, pages 9–13, 2003.</p>
<p>[60] C.-Y. Yang, C. Ma, and M.-H. Yang. Single-image super-resolution: A benchmark. In European Conference on Computer Vision (ECCV), pages 372–386. Springer, 2014.</p>
<p>[61] J. Yang, J. Wright, T. Huang, and Y. Ma. Image super-resolution as sparse representation of raw image patches. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2008.</p>
<p>[62] Q. Yang, R. Yang, J. Davis, and D. Nister. Spatial-depth super resolution for range images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2007.</p>
<p>[63] R. Yeh, C. Chen, T. Y. Lim, M. Hasegawa-Johnson, and M. N. Do. Semantic Image Inpainting with Perceptual and Contextual Losses. arXiv preprint arXiv:1607.07539, 2016.</p>
<p>[64] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. Understanding Neural Networks Through Deep Visualization. In International Conference on Machine Learning - Deep Learning Workshop 2015, page 12, 2015.</p>
<p>[65] X. Yu and F. Porikli. Ultra-resolving face images by discriminative generative networks. In European Conference on Computer Vision (ECCV), pages 318–333. 2016.</p>
<p>[66] H. Yue, X. Sun, J. Yang, and F. Wu. Landmark image superresolution by retrieving web images. IEEE Transactions on Image Processing, 22(12):4865–4878, 2013.</p>
<p>[67] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European Conference on Computer Vision (ECCV), pages 818–833. Springer, 2014.</p>
<p>[68] R. Zeyde, M. Elad, and M. Protter. On single image scale-up using sparse-representations. In Curves and Surfaces, pages 711–730. Springer, 2012.</p>
<p>[69] K. Zhang, X. Gao, D. Tao, and X. Li. Multi-scale dictionary for single image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1114–1121, 2012.</p>
<p>[70] W. Zou and P. C. Yuen. Very Low Resolution Face Recognition in Parallel Environment . IEEE Transactions on Image Processing, 21:327–340, 2012.</p>
]]></content>
    
    <summary type="html">
    
      Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>SinGAN - Learning a Generative Model from a Single Natural Image论文翻译——中文版</title>
    <link href="http://noahsnail.com/2019/11/04/2019-11-04-SinGAN%20-%20Learning%20a%20Generative%20Model%20from%20a%20Single%20Natural%20Image%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2019/11/04/2019-11-04-SinGAN - Learning a Generative Model from a Single Natural Image论文翻译——中文版/</id>
    <published>2019-11-04T02:44:02.000Z</published>
    <updated>2019-11-05T07:54:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了SinGAN，一种可以从单张自然图像中学习到的无条件生成模型。我们的模型通过训练可以捕获图像中图像块的内部分布，然后能够生成高质量、多样性的样本，这些样本带有与训练图像相同的视觉内容。SinGAN包含一个全卷积GAN金字塔，每个GAN负责学习不同尺度图像的图像块分布。这可以生成具有任意大小和长宽比的新样本，这些样本具有显著的可变性，同时又保留了训练图像的整体结构和精细纹理。与之前的单图像GAN方案相比，我们的方法不局限于纹理图像，而且非条件的（即它从噪声中生成样本）。用户研究证明，生成的样本通常被混淆为真实图像。我们在各种图像处理任务中说明了SinGAN的实用性。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>生成对抗网络（GAN）[19]在建模视觉数据的高维分布方面取得了巨大飞跃。尤其是在特定类别数据集上（例如，面孔[33]，卧室[47]）训练的非条件GAN，在生成真实、高质量样本方面取得了显著的成功。然而，捕获具有多个对象类别（例如ImageNet[12]）的高度多样化数据集分布，仍然是一项重大挑战，并且通常需要另一个输入信号[6]来调整生成或针对特定任务训练模型（例如super-resolution[30]，inpainting[41]，retargeting[45]）。</p>
<p>我们利用GAN进入了一个新的领域——从单张自然图像学习的非条件生成。具体而言，我们证明了单张自然图像中图像块的内部统计通常可以携带足够的信息来学习一个强大的生成模型。我们的新单张图像生成模型SinGAN，可以让我们处理包含复杂结构和纹理的一般自然图像，而不需要依赖同类别图像数据集的存在。这是通过全卷积轻量级的GAN金字塔实现的，每个GAN负责捕获不同尺度的图像块分布。训练之后，SinGAN可以生成各种高质量的图像样本（任意尺寸），这些样本在语义上与训练图像类似，但包含新的对象配置和结构（图1）。</p>
<p><img src="https://i.loli.net/2019/11/05/GKriP9ez4Q3HLfc.png" alt="Figure 1"></p>
<p>图1：从单张训练图像学习到的图像生成。我们提出了SinGAN——一种新的在单张自然图像上训练的非条件生成模型。我们的模型在多个尺度上学习图像的图像块统计，使用专用的多尺度对抗训练方案；它可以用来生成新的真实图像样本，这些样本在保留原始图像块分布的同时创造了新的对象配置和结构。</p>
<p>长期以来，建模单张自然图像中图像块的内在分布被看做是许多计算机视觉任务中的有力先验。经典示例包括去噪[65]，去模糊[39]，超分辨率[18]，除雾[2，15]和图像编辑[37，21，9，11，50]。在这些工作中，最密切相关的是[48]，其中定义并优化了双向图像块相似性度量，用来确保处理后的图像块与原始图像块相同。受这些工作的激励，这里我们证明了SinGAN可以用在一个简单统一的学习框架内来解决一系列图像处理任务，包括图像的绘制，编辑，协调，超分辨率和从单张图像生成动画。在所有这些任务中，我们的模型都生成了高质量的结果，并保留了训练图像的内在图像块统计（图2和我们的项目网页）。所有的任务都可以通过同一个生成网络实现，而无需任何额外信息或除了原始训练图像外的进一步训练。</p>
<p><img src="https://i.loli.net/2019/11/05/CzBDYoIU6Pm8dWO.png" alt="Figure 2"></p>
<p>图2：图像处理。SinGAN可用于各种图像处理任务，包括：将绘画（剪贴画）转换为真实照片，重新排列和编辑图像中的对象，将新对象协调为图像，图像超分辨率以及从单张图像输入创建动画。在所有这些情况下，我们的模型仅观察训练图像（第一行），并且对于所有应用以相同的方式进行训练，而无需进行架构更改或进一步调整（请参见第4节）。</p>
]]></content>
    
    <summary type="html">
    
      SinGAN - Learning a Generative Model from a Single Natural Image论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>SinGAN - Learning a Generative Model from a Single Natural Image论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2019/11/04/2019-11-04-SinGAN%20-%20Learning%20a%20Generative%20Model%20from%20a%20Single%20Natural%20Image%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2019/11/04/2019-11-04-SinGAN - Learning a Generative Model from a Single Natural Image论文翻译——中英文对照/</id>
    <published>2019-11-04T02:43:09.000Z</published>
    <updated>2019-11-06T03:14:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="SinGAN-Learning-a-Generative-Model-from-a-Single-Natural-Image"><a href="#SinGAN-Learning-a-Generative-Model-from-a-Single-Natural-Image" class="headerlink" title="SinGAN: Learning a Generative Model from a Single Natural Image"></a>SinGAN: Learning a Generative Model from a Single Natural Image</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了SinGAN，一种可以从单张自然图像中学习到的无条件生成模型。我们的模型通过训练可以捕获图像中图像块的内部分布，然后能够生成高质量、多样性的样本，这些样本带有与训练图像相同的视觉内容。SinGAN包含一个全卷积GAN金字塔，每个GAN负责学习不同尺度图像的图像块分布。这可以生成具有任意大小和长宽比的新样本，这些样本具有显著的可变性，同时又保留了训练图像的整体结构和精细纹理。与之前的单图像GAN方案相比，我们的方法不局限于纹理图像，而且非条件的（即它从噪声中生成样本）。用户研究证明，生成的样本通常被混淆为真实图像。我们在各种图像处理任务中说明了SinGAN的实用性。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Generative Adversarial Nets (GANs) [19] have made a dramatic leap in modeling high dimensional distributions of visual data. In particular, unconditional GANs have shown remarkable success in generating realistic, high quality samples when trained on class specific datasets (e.g., faces [33], bedrooms[47]). However, capturing the distribution of highly diverse datasets with multiple object classes (e.g. ImageNet [12]), is still considered a major challenge and often requires conditioning the generation on another input signal [6] or training the model for a specific task (e.g. super-resolution [30], inpainting [41], retargeting [45]).</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>生成对抗网络（GAN）[19]在建模视觉数据的高维分布方面取得了巨大飞跃。尤其是在特定类别数据集上（例如，面孔[33]，卧室[47]）训练的非条件GAN，在生成真实、高质量样本方面取得了显著的成功。然而，捕获具有多个对象类别（例如ImageNet[12]）的高度多样化数据集分布，仍然是一项重大挑战，并且通常需要另一个输入信号[6]来调整生成或针对特定任务训练模型（例如super-resolution[30]，inpainting[41]，retargeting[45]）。</p>
<p>Here, we take the use of GANs into a new realm – unconditional generation learned from a single natural image. Specifically, we show that the internal statistics of patches within a single natural image typically carry enough information for learning a powerful generative model. SinGAN, our new single image generative model, allows us to deal with general natural images that contain complex structures and textures, without the need to rely on the existence of a database of images from the same class. This is achieved by a pyramid of fully convolutional light-weight GANs, each is responsible for capturing the distribution of patches at a different scale. Once trained, SinGAN can produce diverse high quality image samples (of arbitrary dimensions), which semantically resemble the training image, yet contain new object configurations and structures(Fig. 1).</p>
<p><img src="https://i.loli.net/2019/11/05/GKriP9ez4Q3HLfc.png" alt="Figure 1"></p>
<p>Figure 1: Image generation learned from a single training image. We propose SinGAN–a new unconditional generative model trained on a single natural image. Our model learns the image’s patch statistics across multiple scales, using a dedicated multi-scale adversarial training scheme; it can then be used to generate new realistic image samples that preserve the original patch distribution while creating new object configurations and structures.</p>
<p>我们利用GAN进入了一个新的领域——从单张自然图像学习的非条件生成。具体而言，我们证明了单张自然图像中图像块的内部统计通常可以携带足够的信息来学习一个强大的生成模型。我们的新单张图像生成模型SinGAN，可以让我们处理包含复杂结构和纹理的一般自然图像，而不需要依赖同类别图像数据集的存在。这是通过全卷积轻量级的GAN金字塔实现的，每个GAN负责捕获不同尺度的图像块分布。训练之后，SinGAN可以生成各种高质量的图像样本（任意尺寸），这些样本在语义上与训练图像类似，但包含新的对象配置和结构（图1）。</p>
<p><img src="https://i.loli.net/2019/11/05/GKriP9ez4Q3HLfc.png" alt="Figure 1"></p>
<p>图1：从单张训练图像学习到的图像生成。我们提出了SinGAN——一种新的在单张自然图像上训练的非条件生成模型。我们的模型在多个尺度上学习图像的图像块统计，使用专用的多尺度对抗训练方案；它可以用来生成新的真实图像样本，这些样本在保留原始图像块分布的同时创造了新的对象配置和结构。</p>
<p>Modeling the internal distribution of patches within a single natural image has been long recognized as a powerful prior in many computer vision tasks [64]. Classical examples include denoising [65], deblurring [39], super resolution [18], dehazing [2, 15], and image editing [37, 21, 9, 11, 50]. The most closley related work in this context is [48], where a bidirectional patch similarity measure is defined and optimized to guarantee that the patches of an image after manipulation are the same as the original ones. Motivated by these works, here we show how SinGAN can be used within a simple unified learning framework to solve a variety of image manipulation tasks, including paint-to-image, editing, harmonization, superresolution, and animation from a single image. In all these cases, our model produces high quality results that preserve the internal patch statistics of the training image (see Fig. 2 and our project webpage). All tasks are achieved with the same generative network, without any additional information or further training beyond the original training image.</p>
<p><img src="https://i.loli.net/2019/11/05/CzBDYoIU6Pm8dWO.png" alt="Figure 2"></p>
<p>Figure 2: Image manipulation. SinGAN can be used in various image manipulation tasks, including: transforming a paint (clipart) into a realistic photo, rearranging and editing objects in the image, harmonizing a new object into an image, image super-resolution and creating an animation from a single input. In all these cases, our model observes only the training image (first row) and is trained in the same manner for all applications, with no architectural changes or further tuning (see Sec. 4).</p>
<p>长期以来，建模单张自然图像中图像块的内在分布被看做是许多计算机视觉任务中的有力先验。经典示例包括去噪[65]，去模糊[39]，超分辨率[18]，除雾[2，15]和图像编辑[37，21，9，11，50]。在这些工作中，最密切相关的是[48]，其中定义并优化了双向图像块相似性度量，用来确保处理后的图像块与原始图像块相同。受这些工作的激励，这里我们证明了SinGAN可以用在一个简单统一的学习框架内来解决一系列图像处理任务，包括图像的绘制，编辑，协调，超分辨率和从单张图像生成动画。在所有这些任务中，我们的模型都生成了高质量的结果，并保留了训练图像的内在图像块统计（图2和我们的项目网页）。所有的任务都可以通过同一个生成网络实现，而无需任何额外信息或除了原始训练图像外的进一步训练。</p>
<p><img src="https://i.loli.net/2019/11/05/CzBDYoIU6Pm8dWO.png" alt="Figure 2"></p>
<p>图2：图像处理。SinGAN可用于各种图像处理任务，包括：将绘画（剪贴画）转换为真实照片，重新排列和编辑图像中的对象，将新对象协调为图像，图像超分辨率以及从单张图像输入创建动画。在所有这些情况下，我们的模型仅观察训练图像（第一行），并且对于所有应用以相同的方式进行训练，而无需进行架构更改或进一步调整（请参见第4节）。</p>
<h3 id="1-1-Related-Work"><a href="#1-1-Related-Work" class="headerlink" title="1.1. Related Work"></a>1.1. Related Work</h3><p><strong>Single image deep models</strong> Several recent works proposed to “overfit” a deep model to a single training example [51, 60, 46, 7, 1]. However, these methods are designed for specific tasks (e.g., super resolution [46], texture expansion [60]). Shocher et al. [44, 45] were the first to introduce an internal GAN based model for a single natural image, and illustrated it in the context of retargeting. However, their generation is conditioned on an input image (i.e., mapping images to images) and is not used to draw random samples. In contrast, our framework is purely generative (i.e. maps noise to image samples), and thus suits many different image manipulation tasks. Unconditional single image GANs have been explored only in the context of texture generation [3, 27, 31]. These models do not generate meaningful samples when trained on non-texture images (Fig. 3). Our method, on the other hand, is not restricted to texture and can handle general natural images (e.g., Fig. 1).</p>
<h3 id="1-1-相关工作"><a href="#1-1-相关工作" class="headerlink" title="1.1. 相关工作"></a>1.1. 相关工作</h3><p><strong>单图像深度模型</strong>最近的一些工作提出将深度模型“过拟合”单个训练样本[51，60，46，7，1]。然而，这些方法是为特定的任务（例如，超分辨率[46]，纹理扩展[60]）设计的。Shocher等[44，45]是第一个为单张自然图像引入以GAN为基础的内部模型的，</p>
<p>最近的一些工作提出将深度模型“过度拟合”为单个训练示例[51、60、46、7、1]。 但是，这些方法是为特定任务设计的。 Shocher等。 [44，45]是第一个为单个自然图像引入基于GAN的内部模型的，并在重新定位的情况下进行了说明。 但是，它们的生成取决于输入图像（即，将图像映射到图像），并且不用于绘制随机样本。 相反，我们的框架是纯粹生成的（即将噪声映射到图像样本），因此适合许多不同的图像处理任务。 仅在纹理生成的背景下研究了无条件的单图像GAN [3，27，31]。 当在非纹理图像上训练时，这些模型不会生成有意义的样本（图3）。 另一方面，我们的方法不限于纹理，并且可以处理一般的自然图像（例如，图1）。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Yuki M Asano, Christian Rupprecht, and Andrea Vedaldi. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. 2</p>
<p>[2] Yuval Bahat and Michal Irani. Blind dehazing using internal patch recurrence. In 2016 IEEE International Conference on Computational Photography (ICCP), pages 1–9. IEEE, 2016. 1</p>
<p>[3] Urs Bergmann, Nikolay Jetchev, and Roland Vollgraf. Learning texture manifolds with the periodic spatial GAN. arXiv preprint arXiv:1705.06566, 2017. 2, 4</p>
<p>[4] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on perceptual image super-resolution. In European Conference on Computer Vision Workshops, pages 334–355. Springer, 2018. 8</p>
<p>[5] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6228–6237, 2018. 8</p>
<p>[6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 1</p>
<p>[7] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. arXiv preprint arXiv:1808.07371, 2018. 2</p>
<p>[8] Wengling Chen and James Hays. Sketchygan: towards diverse and realistic sketch to image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9416–9425, 2018. 2</p>
<p>[9] Taeg Sang Cho, Moshe Butman, Shai Avidan, and William T Freeman. The patch transform and its applications to image editing. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2008. 1</p>
<p>[10] Tali Dekel, Chuang Gan, Dilip Krishnan, Ce Liu, and William T Freeman. Sparse, smart contours to represent and edit images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3511–3520, 2018. 2</p>
<p>[11] Tali Dekel, Tomer Michaeli, Michal Irani, and William T Freeman. Revealing and modifying non-local variations in a single image. ACM Transactions on Graphics (TOG), 34(6):227, 2015. 1</p>
<p>[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. 1</p>
<p>[13] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pages 1486–1494, 2015. 3</p>
<p>[14] Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pages 569–593. Springer, 1992. 6</p>
<p>[15] Gilad Freedman and Raanan Fattal. Image and video upscaling from local self-examples. ACM Transactions on Graphics (TOG), 30(2):12, 2011. 1</p>
<p>[16] Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In Advances in neural information processing systems, pages 262–270, 2015. 2</p>
<p>[17] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414–2423, 2016. 7, 8</p>
<p>[18] Daniel Glasner, Shai Bagon, and Michal Irani. Superresolution from a single image. In 2009 IEEE 12th International Conference on Computer Vision (ICCV), pages 349–356. IEEE, 2009. 1, 7</p>
<p>[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680, 2014. 1</p>
<p>[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5767–5777, 2017. 4</p>
<p>[21] Kaiming He and Jian Sun. Statistics of patch offsets for image completion. In European Conference on Computer Vision, pages 16–29. Springer, 2012. 1</p>
<p>[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 4</p>
<p>[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637, 2017. 5, 6</p>
<p>[24] Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5077–5086, 2017. 3</p>
<p>[25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 4</p>
<p>[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint, 2017. 3, 4, 5</p>
<p>[27] Nikolay Jetchev, Urs Bergmann, and Roland Vollgraf. Texture synthesis with spatial generative adversarial networks. Workshop on Adversarial Training, NIPS, 2016. 2, 4</p>
<p>[28] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 3</p>
<p>[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. arXiv preprint arXiv:1812.04948, 2018. 3</p>
<p>[30] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photorealistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4681–4690, 2017. 1, 7, 8</p>
<p>[31] Chuan Li and Michael Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. In European Conference on Computer Vision, pages 702–716. Springer, 2016. 2, 3, 4</p>
<p>[32] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 136–144, 2017. 7</p>
<p>[33] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages 3730–3738, 2015. 1</p>
<p>[34] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep painterly harmonization. arXiv preprint arXiv:1804.03189, 2018. 8</p>
<p>[35] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In null, page 416. IEEE, 2001. 4, 8</p>
<p>[36] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. arXiv preprint arXiv:1511.05440, 2015. 4</p>
<p>[37] Roey Mechrez, Eli Shechtman, and Lihi Zelnik-Manor. Saliency driven image manipulation. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1368–1376. IEEE, 2018. 1</p>
<p>[38] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The contextual loss for image transformation with non-aligned data. In Proceedings of the European Conference on Computer Vision (ECCV), pages 768–783, 2018. 7, 8</p>
<p>[39] Tomer Michaeli and Michal Irani. Blind deblurring using internal patch recurrence. In European Conference on Computer Vision, pages 783–798. Springer, 2014. 1</p>
<p>[40] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a completely blind image quality analyzer. IEEE Signal Processing Letters, 20(3):209–212, 2013. 7, 8</p>
<p>[41] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2536–2544, 2016. 1</p>
<p>[42] Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, and Jose M Alvarez. Invertible conditional GANs for image editing. arXiv preprint arXiv:1611.06355, 2016. 2</p>
<p>[43] Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. Scribbler: Controlling deep image synthesis with sketch and color. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5400–5409, 2017. 2</p>
<p>[44] Assaf Shocher, Shai Bagon, Phillip Isola, and Michal Irani. Ingan: Capturing and remapping the “DNA” of a natural image. arXiv preprint arXiv: arXiv:1812.00231, 2018. 2</p>
<p>[45] Assaf Shocher, Shai Bagon, Phillip Isola, and Michal Irani. InGAN: Capturing and Remapping the “DNA” of a Natural Image. International Conference on Computer Vision (ICCV), 2019. 1, 2</p>
<p>[46] Assaf Shocher, Nadav Cohen, and Michal Irani. Zero-Shot Super-Resolution using Deep Internal Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3118–3126, 2018. 2, 7</p>
<p>[47] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European Conference on Computer Vision, pages 746–760. Springer, 2012. 1</p>
<p>[48] Denis Simakov, Yaron Caspi, Eli Shechtman, and Michal Irani. Summarizing visual data using bidirectional similarity. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8. IEEE, 2008. 1</p>
<p>[49] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015. 6</p>
<p>[50] Tal Tlusty, Tomer Michaeli, Tali Dekel, and Lihi ZelnikManor. Modifying non-local variations across multiple views. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6276–6285, 2018. 1</p>
<p>[51] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2, 7</p>
<p>[52] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. arXiv preprint arXiv:1711.11585, 2017. 2, 3</p>
<p>[53] Xiaolong Wang and Abhinav Gupta. Generative image modeling using style and structure adversarial networks. 2016. 2</p>
<p>[54] Wenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. Texturegan: Controlling deep image synthesis with texture patches. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2</p>
<p>[55] Xuemiao Xu, Liang Wan, Xiaopei Liu, Tien-Tsin Wong, Liansheng Wang, and Chi-Sing Leung. Animating animal motion from still. ACM Transactions on Graphics (TOG), 27(5):117, 2008. 8</p>
<p>[56] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5505–5514, 2018. 2</p>
<p>[57] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing, 26(7):3142–3155, 2017. 4</p>
<p>[58] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European conference on computer vision, pages 649–666. Springer, 2016. 5</p>
<p>[59] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. In Advances in neural information processing systems, pages 487–495, 2014. 4, 6</p>
<p>[60] Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Non-stationary texture synthesis by adversarial expansion. arXiv preprint arXiv:1805.04487, 2018. 2<br>[61] Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision (ECCV), pages 597–613. Springer, 2016. 2</p>
<p>[62] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In IEEE International Conference on Computer Vision, 2017. 2, 4</p>
<p>[63] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems, pages 465–476, 2017. 4</p>
<p>[64] Maria Zontak and Michal Irani. Internal statistics of a single natural image. In CVPR 2011, pages 977–984. IEEE, 2011. 1</p>
<p>[65] Maria Zontak, Inbar Mosseri, and Michal Irani. Separating signal from noise using patch recurrence across scales. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1195–1202, 2013. 1</p>
]]></content>
    
    <summary type="html">
    
      SinGAN - Learning a Generative Model from a Single Natural Image论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux删除重复文件</title>
    <link href="http://noahsnail.com/2019/10/28/2019-10-28-Linux%E5%88%A0%E9%99%A4%E9%87%8D%E5%A4%8D%E6%96%87%E4%BB%B6/"/>
    <id>http://noahsnail.com/2019/10/28/2019-10-28-Linux删除重复文件/</id>
    <published>2019-10-28T01:44:53.000Z</published>
    <updated>2019-10-28T02:01:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在Linux系统处理数据时，经常会遇到删除重复文件的问题。例如，在进行图片分类任务时，希望删除训练数据中的重复图片。在Linux系统中，存在一个<code>fdupes</code>命令可以查找并删除重复文件。</p>
<h2 id="2-Fdupes介绍"><a href="#2-Fdupes介绍" class="headerlink" title="2. Fdupes介绍"></a>2. Fdupes介绍</h2><p>Fdupes是Adrian Lopez用C语言编写的Linux实用程序，它能够在给定的目录和子目录集中找到重复文件，Fdupes通过比较文件的MD5签名然后进行字节比较来识别重复文件。其比较顺序为：</p>
<p>大小比较 &gt; 部分MD5签名比较 &gt; 完整MD5签名比较 &gt; 字节比较</p>
<h2 id="3-安装fdupes"><a href="#3-安装fdupes" class="headerlink" title="3. 安装fdupes"></a>3. 安装fdupes</h2><p>以CentOS系统为例，<code>fdupes</code>的安装命令为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install -y fdupes</div></pre></td></tr></table></figure>
<h2 id="4-fdupes的使用"><a href="#4-fdupes的使用" class="headerlink" title="4. fdupes的使用"></a>4. fdupes的使用</h2><p>删除重复文件，并且不需要询问用户：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ fdupes -dN [folder_name]</div></pre></td></tr></table></figure>
<p>其中，<code>-d</code>参数表示保留一个文件，并删除其它重复文件，<code>-N</code>与<code>-d</code>一起使用，表示保留第一个重复文件并删除其它重复文件，不需要提示用户。</p>
<p>使用说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">$ fdupes -h</div><div class="line">Usage: fdupes [options] DIRECTORY...</div><div class="line"></div><div class="line"> -r --recurse           for every directory given follow subdirectories</div><div class="line">                        encountered within</div><div class="line"> -R --recurse:          for each directory given after this option follow</div><div class="line">                        subdirectories encountered within (note the &apos;:&apos; at</div><div class="line">                        the end of the option, manpage for more details)</div><div class="line"> -s --symlinks          follow symlinks</div><div class="line"> -H --hardlinks         normally, when two or more files point to the same</div><div class="line">                        disk area they are treated as non-duplicates; this</div><div class="line">                        option will change this behavior</div><div class="line"> -n --noempty           exclude zero-length files from consideration</div><div class="line"> -A --nohidden          exclude hidden files from consideration</div><div class="line"> -f --omitfirst         omit the first file in each set of matches</div><div class="line"> -1 --sameline          list each set of matches on a single line</div><div class="line"> -S --size              show size of duplicate files</div><div class="line"> -m --summarize         summarize dupe information</div><div class="line"> -q --quiet             hide progress indicator</div><div class="line"> -d --delete            prompt user for files to preserve and delete all</div><div class="line">                        others; important: under particular circumstances,</div><div class="line">                        data may be lost when using this option together</div><div class="line">                        with -s or --symlinks, or when specifying a</div><div class="line">                        particular directory more than once; refer to the</div><div class="line">                        fdupes documentation for additional information</div><div class="line"> -N --noprompt          together with --delete, preserve the first file in</div><div class="line">                        each set of duplicates and delete the rest without</div><div class="line">                        prompting the user</div><div class="line"> -I --immediate         delete duplicates as they are encountered, without</div><div class="line">                        grouping into sets; implies --noprompt</div><div class="line"> -p --permissions       don&apos;t consider files with different owner/group or</div><div class="line">                        permission bits as duplicates</div><div class="line"> -o --order=BY          select sort order for output and deleting; by file</div><div class="line">                        modification time (BY=&apos;time&apos;; default), status</div><div class="line">                        change time (BY=&apos;ctime&apos;), or filename (BY=&apos;name&apos;)</div><div class="line"> -i --reverse           reverse order while sorting</div><div class="line"> -v --version           display fdupes version</div><div class="line"> -h --help              display this help message</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://www.tecmint.com/fdupes-find-and-delete-duplicate-files-in-linux/" target="_blank" rel="external">https://www.tecmint.com/fdupes-find-and-delete-duplicate-files-in-linux/</a></li>
<li><a href="https://www.howtoing.com/fdupes-find-and-delete-duplicate-files-in-linux" target="_blank" rel="external">https://www.howtoing.com/fdupes-find-and-delete-duplicate-files-in-linux</a></li>
<li><a href="http://www.runoob.com/linux/linux-comm-who.html" target="_blank" rel="external">http://www.runoob.com/linux/linux-comm-who.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Linux删除重复文件
    
    </summary>
    
      <category term="Linux" scheme="http://noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux的set命令</title>
    <link href="http://noahsnail.com/2019/07/24/2019-07-24-Linux%E7%9A%84set%E5%91%BD%E4%BB%A4/"/>
    <id>http://noahsnail.com/2019/07/24/2019-07-24-Linux的set命令/</id>
    <published>2019-07-24T01:23:32.000Z</published>
    <updated>2019-07-24T05:06:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-set命令介绍"><a href="#1-set命令介绍" class="headerlink" title="1. set命令介绍"></a>1. set命令介绍</h2><p><code>set</code>命令主要用来设置shell，在编写shell脚本时，使用<code>set</code>命令能设置shell的执行方式，根据需求不同，采用的参数设置也不同。<code>set</code>命令也用来显示系统中已存在的shell变量以及设置新的shell变量。</p>
<h2 id="2-set命令的常用参数及作用"><a href="#2-set命令的常用参数及作用" class="headerlink" title="2. set命令的常用参数及作用"></a>2. set命令的常用参数及作用</h2><ul>
<li>set</li>
</ul>
<p>不带参数的<code>set</code>命令用来显示环境变量。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">root@3500f62fe5ae:/workspace<span class="comment"># set</span></div><div class="line">BASH=/bin/bash</div><div class="line">BASHOPTS=checkwinsize:cmdhist:complete_fullquote:expand_aliases:extquote:force_fignore:histappend:hostcomplete:interactive_comments:progcomp:promptvars:sourcepath</div><div class="line">BASH_ALIASES=()</div><div class="line">BASH_ARGC=()</div><div class="line">BASH_ARGV=()</div><div class="line">BASH_CMDS=()</div><div class="line">BASH_LINENO=()</div><div class="line">BASH_SOURCE=()</div><div class="line">BASH_VERSINFO=([0]=<span class="string">"4"</span> [1]=<span class="string">"3"</span> [2]=<span class="string">"48"</span> [3]=<span class="string">"1"</span> [4]=<span class="string">"release"</span> [5]=<span class="string">"x86_64-pc-linux-gnu"</span>)</div><div class="line">BASH_VERSION=<span class="string">'4.3.48(1)-release'</span></div><div class="line">COLUMNS=236</div><div class="line">CUDA_HOME=/usr/<span class="built_in">local</span>/cuda</div><div class="line">CUDA_PKG_VERSION=10-0=10.0.130-1</div><div class="line">CUDA_VERSION=10.0.130</div><div class="line">CUDNN_VERSION=7.6.0.64</div><div class="line">...</div></pre></td></tr></table></figure>
<ul>
<li>set -e</li>
</ul>
<p><code>-e</code>参数表示只要shell脚本中发生错误，即命令返回值不等于0，则停止执行并退出shell。<code>set -e</code>在shell脚本中经常使用。默认情况下，shell脚本碰到错误会报错，但会继续执行后面的命令。</p>
<p><code>test.sh</code>脚本内容如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/usr/bin/env bash</span></div><div class="line"><span class="built_in">set</span> <span class="_">-e</span></div><div class="line"></div><div class="line">hello</div><div class="line"><span class="built_in">echo</span> <span class="string">"Hello set"</span></div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@3500f62fe5ae:/workspace<span class="comment"># sh test.sh</span></div><div class="line">test.sh: 4: test.sh: hello: not found</div></pre></td></tr></table></figure>
<p>注：<code>set +e</code>表示关闭-e选项，<code>set -e</code>表示重新打开-e选项。</p>
<ul>
<li>set -u</li>
</ul>
<p><code>-u</code>参数表示shell脚本执行时如果遇到不存在的变量会报错并停止执行。默认不加<code>-u</code>参数的情况下，shell脚本遇到不存在的变量不会报错，会继续执行。</p>
<p><code>test.sh</code>脚本内容如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/usr/bin/env bash</span></div><div class="line"><span class="built_in">echo</span> <span class="variable">$test</span></div><div class="line"></div><div class="line"><span class="built_in">set</span> -u</div><div class="line"><span class="built_in">echo</span> <span class="variable">$hello</span></div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">root@3500f62fe5ae:/workspace<span class="comment"># sh test.sh</span></div><div class="line"></div><div class="line">test.sh: 5: test.sh: hello: parameter not <span class="built_in">set</span></div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://www.ruanyifeng.com/blog/2017/11/bash-set.html" target="_blank" rel="external">http://www.ruanyifeng.com/blog/2017/11/bash-set.html</a></li>
<li><a href="http://www.runoob.com/linux/linux-comm-set.html" target="_blank" rel="external">http://www.runoob.com/linux/linux-comm-set.html</a></li>
<li><a href="http://man.linuxde.net/set" target="_blank" rel="external">http://man.linuxde.net/set</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Linux的set命令
    
    </summary>
    
      <category term="Linux" scheme="http://noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Python调用C</title>
    <link href="http://noahsnail.com/2019/05/27/2019-05-27-Python%E8%B0%83%E7%94%A8C/"/>
    <id>http://noahsnail.com/2019/05/27/2019-05-27-Python调用C/</id>
    <published>2019-05-27T06:05:37.000Z</published>
    <updated>2019-05-28T07:01:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>众所周知，Python语言简单、易学、开源、具有丰富的库，Python的第一个编译器是用C语言实现的。但Python的缺点也非常明显，最让人诟病的就是Python的性能问题。因此，为了提高程序的运行效率，通常会将程序的关键部分使用C或C++重写，编译成动态链接库，然后在Python（CPython）中进行调用。运行环境：Ubuntu 16.04、Python 2.7、Python 3.5。</p>
<h2 id="2-Python-C扩展"><a href="#2-Python-C扩展" class="headerlink" title="2. Python C扩展"></a>2. Python C扩展</h2><h3 id="2-1-普通C函数"><a href="#2-1-普通C函数" class="headerlink" title="2.1 普通C函数"></a>2.1 普通C函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">void hello()</div><div class="line">&#123;</div><div class="line">	printf(&quot;Hello World!\n&quot;);</div><div class="line">&#125;</div><div class="line"></div><div class="line">int add(int a, int b)</div><div class="line">&#123;</div><div class="line">	return a + b;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="2-2-Python-C扩展"><a href="#2-2-Python-C扩展" class="headerlink" title="2.2 Python C扩展"></a>2.2 Python C扩展</h3><p>Python扩展模块由以下几部分组成：</p>
<ul>
<li>头文件<python.h></python.h></li>
<li>调用的C函数</li>
<li>模块方法表</li>
<li>模块初始化函数</li>
</ul>
<p>具体实现<code>demo.c</code>如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 包含Python头文件</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Python.h&gt;</span></span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// 兼容Python3</span></div><div class="line"><span class="meta">#<span class="meta-keyword">if</span> PY_MAJOR_VERSION &gt;= 3</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTHON3</span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// hello函数实现</span></div><div class="line"><span class="function"><span class="keyword">static</span> PyObject* <span class="title">hello</span><span class="params">(PyObject *self, PyObject *args)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World\n"</span>);</div><div class="line">    <span class="keyword">return</span> Py_None;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// add函数实现</span></div><div class="line"><span class="function"><span class="keyword">static</span> PyObject* <span class="title">add</span><span class="params">(PyObject *self, PyObject *args)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> a, b;</div><div class="line">    <span class="keyword">if</span>(!PyArg_ParseTuple(args, <span class="string">"ii"</span>, &amp;a, &amp;b))</div><div class="line">    &#123;</div><div class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> Py_BuildValue(<span class="string">"i"</span>, a + b);</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// 模块方法表</span></div><div class="line"><span class="keyword">static</span> PyMethodDef TwoMethods[] = &#123;</div><div class="line">    &#123; <span class="string">"hello"</span>, hello, METH_NOARGS, <span class="string">"Print Hello"</span> &#125;,</div><div class="line">    &#123; <span class="string">"add"</span>, add, METH_VARARGS, <span class="string">"Add two integers"</span>&#125;,</div><div class="line">    &#123; <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span> &#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> PYTHON3</span></div><div class="line"><span class="comment">// Python3模块定义结构体</span></div><div class="line"><span class="keyword">static</span> <span class="keyword">struct</span> PyModuleDef testModule = &#123;</div><div class="line">	PyModuleDef_HEAD_INIT,</div><div class="line">	<span class="string">"testModule"</span>,</div><div class="line">	<span class="string">"Test Module"</span>,</div><div class="line">	<span class="number">-1</span>,</div><div class="line">	TwoMethods</div><div class="line">&#125;;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// Python3模块初始化函数</span></div><div class="line"><span class="function">PyMODINIT_FUNC <span class="title">PyInit_demo</span><span class="params">(<span class="keyword">void</span>)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">return</span> PyModule_Create(&amp;testModule);</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line"><span class="comment">// Python2模块初始化函数</span></div><div class="line"><span class="function">PyMODINIT_FUNC <span class="title">initdemo</span><span class="params">(<span class="keyword">void</span>)</span></span></div><div class="line">&#123;</div><div class="line">    Py_InitModule(<span class="string">"demo"</span>, TwoMethods);</div><div class="line">&#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>
<h3 id="2-3-编译并测试"><a href="#2-3-编译并测试" class="headerlink" title="2.3 编译并测试"></a>2.3 编译并测试</h3><p>编写<code>setup.py</code>文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</div><div class="line"></div><div class="line">demo = Extension(<span class="string">'demo'</span>, sources = [<span class="string">'demo.c'</span>])</div><div class="line"></div><div class="line">setup(name = <span class="string">'C extension module'</span>, version = <span class="string">'1.0'</span>, description = <span class="string">'This is a demo'</span>, ext_modules = [demo])</div></pre></td></tr></table></figure>
<p>生成动态链接库的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#python2</span></div><div class="line">$ python setup.py build_ext --inplace</div><div class="line">running build_ext</div><div class="line">building <span class="string">'demo'</span> extension</div><div class="line">x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security <span class="_">-f</span>PIC -I/usr/include/python2.7 -c demo.c -o build/temp.linux-x86_64-2.7/demo.o</div><div class="line">x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-Bsymbolic-functions -Wl,-z,relro -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/demo.o -o /workspace/python-c/demo.so</div><div class="line"></div><div class="line"><span class="comment">#python3</span></div><div class="line">$ python3 setup.py build_ext --inplace</div><div class="line">running build_ext</div><div class="line">building <span class="string">'demo'</span> extension</div><div class="line">x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 <span class="_">-f</span>PIC -I/usr/include/python3.5m -c demo.c -o build/temp.linux-x86_64-3.5/demo.o</div><div class="line">x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.5/demo.o -o /workspace/python-c/demo.cpython-35m-x86_64-linux-gnu.so</div></pre></td></tr></table></figure>
<p><code>hello</code>，<code>add</code>函数测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from demo import hello, add</div><div class="line">&gt;&gt;&gt; hello()</div><div class="line">Hello World</div><div class="line">&gt;&gt;&gt; add(2, 3)</div><div class="line">5</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://www.cnblogs.com/vamei/archive/2013/02/06/2892628.html" target="_blank" rel="external">https://www.cnblogs.com/vamei/archive/2013/02/06/2892628.html</a></li>
<li><a href="https://www.yanxurui.cc/posts/python/2017-06-18-3-ways-of-calling-c-functions-from-python/" target="_blank" rel="external">https://www.yanxurui.cc/posts/python/2017-06-18-3-ways-of-calling-c-functions-from-python/</a></li>
<li><a href="https://swe.mirsking.com/languages/python/pythoncallcplusplus" target="_blank" rel="external">https://swe.mirsking.com/languages/python/pythoncallcplusplus</a></li>
<li><a href="https://www.jianshu.com/p/cd28e8b0cce1" target="_blank" rel="external">https://www.jianshu.com/p/cd28e8b0cce1</a></li>
<li><a href="https://docs.python.org/2.7/extending/extending.html" target="_blank" rel="external">https://docs.python.org/2.7/extending/extending.html</a></li>
<li><a href="https://docs.python.org/2.7/extending/building.html" target="_blank" rel="external">https://docs.python.org/2.7/extending/building.html</a></li>
<li><a href="https://tutorialedge.net/python/python-c-extensions-tutorial/" target="_blank" rel="external">https://tutorialedge.net/python/python-c-extensions-tutorial/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python调用C
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络中卷积运算的前向传播与反向传播推导</title>
    <link href="http://noahsnail.com/2019/05/21/2019-05-21-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%8E%A8%E5%AF%BC/"/>
    <id>http://noahsnail.com/2019/05/21/2019-05-21-卷积神经网络中的前向传播与反向传播推导/</id>
    <published>2019-05-21T07:22:56.000Z</published>
    <updated>2019-05-21T10:07:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; [简书](<a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">http://www.jianshu.com/users/7731e83f3a4e/latest_articles</a></p>
<h2 id="0-必备基础知识"><a href="#0-必备基础知识" class="headerlink" title="0. 必备基础知识"></a>0. 必备基础知识</h2><ul>
<li><p>卷积以及卷积的运算过程</p>
</li>
<li><p>微分相关知识，包括求偏导及链式法则</p>
</li>
</ul>
<h2 id="1-卷积运算的前向传播"><a href="#1-卷积运算的前向传播" class="headerlink" title="1. 卷积运算的前向传播"></a>1. 卷积运算的前向传播</h2><p>数学符号定义：</p>
<p>输入：</p>
<p><img src="https://i.loli.net/2019/05/21/5ce3ac602662d57192.png" height="150" width="300" alt="Input"></p>
<p>卷积核：</p>
<p><img src="https://i.loli.net/2019/05/21/5ce3ad4205d3476167.png" alt="Filter"></p>
<p>输出：</p>
<p><img src="https://i.loli.net/2019/05/21/5ce3aed9a5c9071482.png" alt="Output"></p>
<p>卷积运算：</p>
<p><img src="https://i.loli.net/2019/05/21/5ce3af11b3ece18152.png" alt="Convolution"></p>
<p><img src="https://i.loli.net/2019/05/21/5ce3b144a817112712.png" alt="Convolution"></p>
<p>定义损失函数，将损失函数定义为输出的和，这样方便反向传播计算的演示：</p>
<p><img src="https://i.loli.net/2019/05/21/5ce3b372458cb80571.png" alt="Loss Function"></p>
<p>从<code>X -&gt; Y -&gt; L</code>的过程是卷积运算的前向传播过程，为了简化这个过程，这里忽略了偏置项<code>b</code>以及卷积之后的激活函数。</p>
<h2 id="2-卷积运算的反向传播"><a href="#2-卷积运算的反向传播" class="headerlink" title="2. 卷积运算的反向传播"></a>2. 卷积运算的反向传播</h2><ul>
<li>计算损失函数<code>L</code>对输出<code>Y</code>的梯度</li>
</ul>
<p><img src="https://i.loli.net/2019/05/21/5ce3b615700b843254.png" alt="Derivation"></p>
<ul>
<li>计算输入<code>X</code>的梯度</li>
</ul>
<p><img src="https://i.loli.net/2019/05/21/5ce3c06b1ee5f25969.png" height="150" width="400" alt="Derivation"></p>
<p>计算其中每一项的梯度：</p>
<p><img src="https://i.loli.net/2019/05/21/5ce3c0b14ad7c70260.png" height="600" width="600" alt="Chain Rule"></p>
<ul>
<li>计算卷积核<code>W</code>的梯度</li>
</ul>
<p><img src="https://i.loli.net/2019/05/21/5ce3ca45f1d0114908.png" alt="Derivation"></p>
<p>计算其中每一项的梯度：</p>
<p><img src="https://i.loli.net/2019/05/21/5ce3cb1d9fee066058.png" alt="Chain Rule"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://plantsandbuildings.github.io/machine-learning/misc/math/2018/04/28/a-ground-up-c++-convnet-that-scores-0.973-on-the-kaggle-digit-recognizer-challenge.html" target="_blank" rel="external">https://plantsandbuildings.github.io/machine-learning/misc/math/2018/04/28/a-ground-up-c++-convnet-that-scores-0.973-on-the-kaggle-digit-recognizer-challenge.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      C++源文件到可执行程序
    
    </summary>
    
      <category term="数学之美" scheme="http://noahsnail.com/categories/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>C++源文件到可执行程序</title>
    <link href="http://noahsnail.com/2019/05/14/2019-05-14-C++%E6%BA%90%E6%96%87%E4%BB%B6%E5%88%B0%E5%8F%AF%E6%89%A7%E8%A1%8C%E7%A8%8B%E5%BA%8F/"/>
    <id>http://noahsnail.com/2019/05/14/2019-05-14-C++源文件到可执行程序/</id>
    <published>2019-05-14T08:58:23.000Z</published>
    <updated>2019-05-17T01:33:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>C++程序从源代码到可执行程序是一个复杂的过程，其流程为：<code>源代码 --&gt; 预处理 --&gt; 编译 --&gt; 优化 --&gt; 汇编 --&gt; 链接 --&gt; 可执行文件</code>，本文以一段C++代码为例，按执行顺序来描述这个过程。</p>
<h2 id="2-源代码"><a href="#2-源代码" class="headerlink" title="2. 源代码"></a>2. 源代码</h2><p>源代码文件分为两个，<code>hello.h</code>、<code>hello.cpp</code>和<code>main.cpp</code>，代码如下：</p>
<ul>
<li><p>hello.hpp</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> HELLO_HPP_</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> HELLO_HPP_</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>
</li>
<li><p>hello.cpp</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"hello.hpp"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Hello, world!"</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>main.cpp</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"hello.hpp"</span></span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</div><div class="line">    hello();</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3-预处理"><a href="#3-预处理" class="headerlink" title="3. 预处理"></a>3. 预处理</h2><p>预处理是指C++程序源代码在编译之前，由预处理器（Preprocessor）对C++程序源代码进行的处理。在这个阶段，预处理器会处理以<code>#</code>开头的命令，处理完成之后会生成一个不包含预处理命令的纯C++文件，常见的预处理有：文件包含(#inlcude)、条件编译(<code>#ifndef #ifdef #endif</code>)、提供编译信息(<code>#pragma</code>)、宏替换(<code>#define</code>)等。</p>
<p>使用<code>g++</code>预处理<code>main.cpp</code>的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ -E main.cpp -o main.ii</div></pre></td></tr></table></figure>
<p><code>-E</code>参数表示预处理后即停止，不进行编译，预处理后的代码送往标准输出，<code>-o</code>指定输出文件。输出文件<code>main.ii</code>的内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 1 &quot;main.cpp&quot;</div><div class="line"># 1 &quot;&lt;built-in&gt;&quot;</div><div class="line"># 1 &quot;&lt;command-line&gt;&quot;</div><div class="line"># 1 &quot;/usr/include/stdc-predef.h&quot; 1 3 4</div><div class="line"># 1 &quot;&lt;command-line&gt;&quot; 2</div><div class="line"># 1 &quot;main.cpp&quot;</div><div class="line"># 1 &quot;hello.hpp&quot; 1</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">void hello();</div><div class="line"># 2 &quot;main.cpp&quot; 2</div><div class="line"></div><div class="line">int main(int argc, char *argv[]) &#123;</div><div class="line">    hello();</div><div class="line">    return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="4-编译"><a href="#4-编译" class="headerlink" title="4. 编译"></a>4. 编译</h2><p>在编译过程中，编译器主要作语法检查和词法分析。通过词法分析和语法分析，在确认所有的指令都符合语法规则之后，将其翻译成等价的中间代码表示或汇编代码。</p>
<p>编译<code>main.ii</code>的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ -S main.ii</div></pre></td></tr></table></figure>
<p><code>-S</code>参数表示编译后即停止，不进行汇编。对于每个输入的非汇编语言文件，输出文件是汇编语言文件。输出文件<code>main.s</code>的内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">        .file   &quot;main.cpp&quot;</div><div class="line">        .text</div><div class="line">        .globl  main</div><div class="line">        .type   main, @function</div><div class="line">main:</div><div class="line">.LFB0:</div><div class="line">        .cfi_startproc</div><div class="line">        pushq   %rbp</div><div class="line">        .cfi_def_cfa_offset 16</div><div class="line">        .cfi_offset 6, -16</div><div class="line">        movq    %rsp, %rbp</div><div class="line">        .cfi_def_cfa_register 6</div><div class="line">        subq    $16, %rsp</div><div class="line">        movl    %edi, -4(%rbp)</div><div class="line">        movq    %rsi, -16(%rbp)</div><div class="line">        call    _Z5hellov</div><div class="line">        movl    $0, %eax</div><div class="line">        leave</div><div class="line">        .cfi_def_cfa 7, 8</div><div class="line">        ret</div><div class="line">        .cfi_endproc</div><div class="line">.LFE0:</div><div class="line">        .size   main, .-main</div><div class="line">        .ident  &quot;GCC: (GNU) 4.8.5 20150623 (Red Hat 4.8.5-16)&quot;</div><div class="line">        .section        .note.GNU-stack,&quot;&quot;,@progbits</div></pre></td></tr></table></figure>
<h2 id="5-优化"><a href="#5-优化" class="headerlink" title="5. 优化"></a>5. 优化</h2><p>优化是在编译过程中最重要的，也是最难的。它不仅与编译技术本身有关，而且跟机器的硬件环境也有很大的关系。优化可在编译的不同阶段进行，一类优化是对中间代码的优化，这类优化不依赖于具体的计算机，另一类优化是对目标代码的优化，这类优化与机器的硬件环境有关。</p>
<p><code>g++</code>编译器的编译优化参数为<code>-O</code>，分为四级，分别为<code>-O0</code>、<code>-O1</code>、<code>-O2</code>、<code>-O3</code>，默认为<code>-O0</code>。各级优化后的结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"># 默认优化，-O0</div><div class="line">[root@localhost:/workspace] $: g++ -c main.cpp hello.cpp</div><div class="line">[root@localhost:/workspace] $: nm -C main.o</div><div class="line">                 U __cxa_atexit</div><div class="line">                 U __dso_handle</div><div class="line">000000000000007a t _GLOBAL__sub_I__Z5hellov</div><div class="line">0000000000000022 T main</div><div class="line">000000000000003d t __static_initialization_and_destruction_0(int, int)</div><div class="line">0000000000000000 T hello()</div><div class="line">                 U std::ostream::operator&lt;&lt;(std::ostream&amp; (*)(std::ostream&amp;))</div><div class="line">                 U std::ios_base::Init::Init()</div><div class="line">                 U std::ios_base::Init::~Init()</div><div class="line">                 U std::cout</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::endl&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)</div><div class="line">0000000000000000 b std::__ioinit</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::operator&lt;&lt; &lt;std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*)</div><div class="line"></div><div class="line"></div><div class="line"># 优化级别-O1</div><div class="line">[root@localhost:/workspace] $: g++ -c -O1 main.cpp hello.cpp</div><div class="line">[root@localhost:/workspace] $: nm -C main.o</div><div class="line">                 U __cxa_atexit</div><div class="line">                 U __dso_handle</div><div class="line">000000000000007d t _GLOBAL__sub_I__Z5hellov</div><div class="line">000000000000006a T main</div><div class="line">0000000000000000 T hello()</div><div class="line">                 U std::ctype&lt;char&gt;::_M_widen_init() const</div><div class="line">                 U std::ostream::put(char)</div><div class="line">                 U std::ostream::flush()</div><div class="line">                 U std::ios_base::Init::Init()</div><div class="line">                 U std::ios_base::Init::~Init()</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::__ostream_insert&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*, long)</div><div class="line">                 U std::__throw_bad_cast()</div><div class="line">                 U std::cout</div><div class="line">0000000000000000 b std::__ioinit</div><div class="line"></div><div class="line"># 优化级别-O2</div><div class="line">[root@localhost:/workspace] $: g++ -c -O2 main.cpp hello.cpp</div><div class="line">[root@localhost:/workspace] $: nm -C main.o</div><div class="line">                 U __cxa_atexit</div><div class="line">                 U __dso_handle</div><div class="line">0000000000000010 t _GLOBAL__sub_I__Z5hellov</div><div class="line">0000000000000000 T main</div><div class="line">0000000000000000 T hello()</div><div class="line">                 U std::ctype&lt;char&gt;::_M_widen_init() const</div><div class="line">                 U std::ostream::put(char)</div><div class="line">                 U std::ostream::flush()</div><div class="line">                 U std::ios_base::Init::Init()</div><div class="line">                 U std::ios_base::Init::~Init()</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::__ostream_insert&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*, long)</div><div class="line">                 U std::__throw_bad_cast()</div><div class="line">                 U std::cout</div><div class="line">0000000000000000 b std::__ioinit</div></pre></td></tr></table></figure>
<h2 id="6-汇编"><a href="#6-汇编" class="headerlink" title="6. 汇编"></a>6. 汇编</h2><p>汇编是把汇编语言代码翻译成目标机器指令的过程。</p>
<p>编译<code>main.s</code>的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ -c main.s</div></pre></td></tr></table></figure>
<p><code>-c</code>参数表示编译或汇编源文件，但是不作连接，编译器输出对应于源文件的目标文件。输出文件为<code>main.o</code>，使用<code>nm -C main.o</code>命令来查看文件内容，文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">0000000000000000 T main</div><div class="line">                 U hello()</div></pre></td></tr></table></figure>
<h2 id="7-链接"><a href="#7-链接" class="headerlink" title="7. 链接"></a>7. 链接</h2><p>链接是将目标文件、启动代码、库文件链接成可执行文件的过程，得到的文件可以直接执行。经过汇编之后生成的目标文件<code>main.o</code>是不可以直接执行的。链接命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ main.o -o main</div><div class="line">main.o: In function `main&apos;:</div><div class="line">main.cpp:(.text+0x10): undefined reference to `hello()&apos;</div><div class="line">collect2: error: ld returned 1 exit status</div></pre></td></tr></table></figure>
<p>从上面可以看出，只链接<code>main.o</code>文件会报错，这是因为<code>main.cpp</code>引用了<code>hello.cpp</code>中定义的函数<code>hello</code>，因此需要链接文件<code>hello.cpp</code>才能生成可执行程序。重复上述过程，生成<code>hello.o</code>，链接两个文件的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ main.o hello.o -o main</div></pre></td></tr></table></figure>
<p>经过链接，多个文件被链接成了单一的可执行文件<code>main</code>，执行<code>main</code>程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: ./main</div><div class="line">Hello, world!</div></pre></td></tr></table></figure>
<h4 id="7-1-静态链接库"><a href="#7-1-静态链接库" class="headerlink" title="7.1 静态链接库"></a>7.1 静态链接库</h4><p>除了直接链接多个目标文件之外，还可以通过链接静态库生成可执行文件。静态链接库是编译器生成的一系列对象文件的集合，库中的成员包括普通函数，类定义，类的对象实例等。静态链接是指把要调用的函数或者过程链接到可执行文件中，成为可执行文件的一部分。可执行文件生成之后，就不再需要静态链接库，即编译后的可执行程序不需要外部函数库的支持。但如果静态链接库发生改变，则可执行程序需要重新编译。静态链接库属于编译时链接。</p>
<p>我们再添加两个<code>static.hpp</code>，<code>static.cpp</code>，并修改<code>main.cpp</code>，内容如下：</p>
<ul>
<li><p><code>static.hpp</code>文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> STATIC_HPP_</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> STATIC_HPP_</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">test</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>
</li>
<li><p><code>static.cpp</code>文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"static.hpp"</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"static lib"</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p><code>main.cpp</code>文件：</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">extern</span> <span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">extern</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123; </div><div class="line">    hello();</div><div class="line">    test();</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>编译汇编<code>hello.cpp</code>、<code>static.cpp</code>之后可以得到两个文件<code>hello.o</code>和<code>static.o</code>，linux系统中的命令<code>ar</code>，可以将多个目标文件打包成为一个单独的文件，这个文件被称为静态库。生成静态库的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: ar -r libstatic.a hello.o static.o</div><div class="line">ar: creating libstatic.a</div></pre></td></tr></table></figure>
<p>查看<code>libstatic.a</code>的内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: nm -C libstatic.a</div><div class="line"></div><div class="line">hello.o:</div><div class="line">                 U __cxa_atexit</div><div class="line">                 U __dso_handle</div><div class="line">000000000000005f t _GLOBAL__sub_I__Z5hellov</div><div class="line">0000000000000022 t __static_initialization_and_destruction_0(int, int)</div><div class="line">0000000000000000 T hello()</div><div class="line">                 U std::ostream::operator&lt;&lt;(std::ostream&amp; (*)(std::ostream&amp;))</div><div class="line">                 U std::ios_base::Init::Init()</div><div class="line">                 U std::ios_base::Init::~Init()</div><div class="line">                 U std::cout</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::endl&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)</div><div class="line">0000000000000000 b std::__ioinit</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::operator&lt;&lt; &lt;std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*)</div><div class="line"></div><div class="line">static.o:</div><div class="line">                 U __cxa_atexit</div><div class="line">                 U __dso_handle</div><div class="line">000000000000005f t _GLOBAL__sub_I__Z4testv</div><div class="line">0000000000000022 t __static_initialization_and_destruction_0(int, int)</div><div class="line">0000000000000000 T test()</div><div class="line">                 U std::ostream::operator&lt;&lt;(std::ostream&amp; (*)(std::ostream&amp;))</div><div class="line">                 U std::ios_base::Init::Init()</div><div class="line">                 U std::ios_base::Init::~Init()</div><div class="line">                 U std::cout</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::endl&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)</div><div class="line">0000000000000000 b std::__ioinit</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::operator&lt;&lt; &lt;std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*)</div></pre></td></tr></table></figure>
<p>通过静态链接库生成可执行程序<code>main</code>并执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ main.o libstatic.a -o main</div><div class="line">[root@localhost:/workspace] $: ./main</div><div class="line">Hello, world!</div><div class="line">static lib</div></pre></td></tr></table></figure>
<p>另一种命令方式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ -L ./ main.cpp -lstatic -o main</div></pre></td></tr></table></figure>
<p>Linux静态库的命名惯例是名字以三个字母<code>lib</code>开头并以後缀<code>.a</code>结束。所有的系统库都采用这种命名惯例，并且它允许通过<code>-l(ell)</code>选项来简写命令行中的库名。<code>-lstatic</code>中的<code>-l</code>是要求编译器在系统库目录下查找<code>static</code>库，<code>static</code>是<code>libstatic.a</code>的简写。<code>-L</code>参数用来指定要具体的查找目录，如果缺少这个参数，则只会在系统库目录下查找<code>static</code>，会报错。错误如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ main.cpp -lstatic -o ltest</div><div class="line">/usr/bin/ld: cannot find -lstatic</div></pre></td></tr></table></figure>
<h4 id="7-2-共享库"><a href="#7-2-共享库" class="headerlink" title="7.2 共享库"></a>7.2 共享库</h4><p>共享库（Windows叫动态链接库）是编译器以一种特殊的方式生成的对象文件的集合。对象文件模块中所有地址（变量引用或函数调用）都是相对而不是绝对的，这使得共享模块可以在程序的运行过程中被动态地调用和执行。共享库属于运行时链接。当使用共享库时，只要共享库的接口不变，共享库修改之后，不需要重新编译可执行程序。</p>
<p>创建<code>dynamic.cpp</code>，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#include &lt;iostream&gt;</div><div class="line">using namespace std;</div><div class="line"></div><div class="line">void test() &#123;</div><div class="line">    cout &lt;&lt; &quot;dynamic lib&quot; &lt;&lt; endl;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>编译<code>hello.cpp</code>和<code>dynamic.cpp</code>，<code>-fpic</code>表示生成的对象模块采用浮动（可重定位）地址，<code>pic</code>是位置无关代码（position independent code）的缩写。：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ -c -fpic hello.cpp static.cpp</div></pre></td></tr></table></figure>
<p>使用<code>-fpic</code>与不使用<code>-fpic</code>生成的目标文件<code>hello.o</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"># 使用-fpic</div><div class="line">                 U __cxa_atexit</div><div class="line">                 U __dso_handle</div><div class="line">                 U _GLOBAL_OFFSET_TABLE_</div><div class="line">0000000000000076 t _GLOBAL__sub_I_hello.cpp</div><div class="line">000000000000002e t __static_initialization_and_destruction_0(int, int)</div><div class="line">0000000000000000 T hello()</div><div class="line">                 U std::ostream::operator&lt;&lt;(std::ostream&amp; (*)(std::ostream&amp;))</div><div class="line">                 U std::ios_base::Init::Init()</div><div class="line">                 U std::ios_base::Init::~Init()</div><div class="line">                 U std::cout</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::endl&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)</div><div class="line">0000000000000000 b std::__ioinit</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::operator&lt;&lt; &lt;std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*)</div><div class="line"></div><div class="line"># 不使用-fpic</div><div class="line">                 U __cxa_atexit</div><div class="line">                 U __dso_handle</div><div class="line">000000000000005f t _GLOBAL__sub_I__Z5hellov</div><div class="line">0000000000000022 t __static_initialization_and_destruction_0(int, int)</div><div class="line">0000000000000000 T hello()</div><div class="line">                 U std::ostream::operator&lt;&lt;(std::ostream&amp; (*)(std::ostream&amp;))</div><div class="line">                 U std::ios_base::Init::Init()</div><div class="line">                 U std::ios_base::Init::~Init()</div><div class="line">                 U std::cout</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::endl&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)</div><div class="line">0000000000000000 b std::__ioinit</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::operator&lt;&lt; &lt;std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*)</div></pre></td></tr></table></figure>
<p>创建共享库<code>dynamic.so</code>，<code>-shared</code>表示生成共享目标文件。：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ -shared hello.o dynamic.o -o libdynamic.so</div></pre></td></tr></table></figure>
<p>编译<code>main.cpp</code>并链接共享库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: g++ main.cpp libdynamic.so -o main</div></pre></td></tr></table></figure>
<p>执行<code>main</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: ./main</div><div class="line">./main: error while loading shared libraries: dynamic.so: cannot open shared object file: No such file or directory</div></pre></td></tr></table></figure>
<p>报错是因为当前工作目录可能不在共享库的查找路径中，因此需要将当前目录添加到环境变量<code>LD_LIBRARY_PATH</code>中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@localhost:/workspace] $: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:./</div><div class="line">[root@localhost:/workspace] $: ./main</div><div class="line">Hello, world!</div><div class="line">dynamic lib</div></pre></td></tr></table></figure>
<p>查看链接静态库和共享库生成的两个可执行<code>main</code>文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div></pre></td><td class="code"><pre><div class="line"># 共享库</div><div class="line">[root@localhost:/workspace] $: nm -C main</div><div class="line">000000000060103c B __bss_start</div><div class="line">000000000060103c b completed.6354</div><div class="line">0000000000601038 D __data_start</div><div class="line">0000000000601038 W data_start</div><div class="line">0000000000400650 t deregister_tm_clones</div><div class="line">00000000004006c0 t __do_global_dtors_aux</div><div class="line">0000000000600dd8 t __do_global_dtors_aux_fini_array_entry</div><div class="line">00000000004007b8 R __dso_handle</div><div class="line">0000000000600de8 d _DYNAMIC</div><div class="line">000000000060103c D _edata</div><div class="line">0000000000601040 B _end</div><div class="line">00000000004007a4 T _fini</div><div class="line">00000000004006e0 t frame_dummy</div><div class="line">0000000000600dd0 t __frame_dummy_init_array_entry</div><div class="line">00000000004008e8 r __FRAME_END__</div><div class="line">0000000000601000 d _GLOBAL_OFFSET_TABLE_</div><div class="line">                 w __gmon_start__</div><div class="line">00000000004005a8 T _init</div><div class="line">0000000000600dd8 t __init_array_end</div><div class="line">0000000000600dd0 t __init_array_start</div><div class="line">00000000004007b0 R _IO_stdin_used</div><div class="line">                 w _ITM_deregisterTMCloneTable</div><div class="line">                 w _ITM_registerTMCloneTable</div><div class="line">0000000000600de0 d __JCR_END__</div><div class="line">0000000000600de0 d __JCR_LIST__</div><div class="line">                 w _Jv_RegisterClasses</div><div class="line">00000000004007a0 T __libc_csu_fini</div><div class="line">0000000000400730 T __libc_csu_init</div><div class="line">                 U __libc_start_main@@GLIBC_2.2.5</div><div class="line">000000000040070d T main</div><div class="line">0000000000400680 t register_tm_clones</div><div class="line">0000000000400620 T _start</div><div class="line">0000000000601040 D __TMC_END__</div><div class="line">                 U test()</div><div class="line">                 U hello()</div><div class="line"></div><div class="line"></div><div class="line"># 静态库</div><div class="line">[root@localhost:/workspace] $: nm -C main</div><div class="line">000000000060105c B __bss_start</div><div class="line">0000000000601170 b completed.6354</div><div class="line">                 U __cxa_atexit@@GLIBC_2.2.5</div><div class="line">0000000000601058 D __data_start</div><div class="line">0000000000601058 W data_start</div><div class="line">00000000004007b0 t deregister_tm_clones</div><div class="line">0000000000400820 t __do_global_dtors_aux</div><div class="line">0000000000600de8 t __do_global_dtors_aux_fini_array_entry</div><div class="line">0000000000400a08 R __dso_handle</div><div class="line">0000000000600df8 d _DYNAMIC</div><div class="line">000000000060105c D _edata</div><div class="line">0000000000601178 B _end</div><div class="line">00000000004009f4 T _fini</div><div class="line">0000000000400840 t frame_dummy</div><div class="line">0000000000600dd0 t __frame_dummy_init_array_entry</div><div class="line">0000000000400c40 r __FRAME_END__</div><div class="line">0000000000601000 d _GLOBAL_OFFSET_TABLE_</div><div class="line">0000000000400960 t _GLOBAL__sub_I__Z4testv</div><div class="line">00000000004008ec t _GLOBAL__sub_I__Z5hellov</div><div class="line">                 w __gmon_start__</div><div class="line">00000000004006d0 T _init</div><div class="line">0000000000600de8 t __init_array_end</div><div class="line">0000000000600dd0 t __init_array_start</div><div class="line">0000000000400a00 R _IO_stdin_used</div><div class="line">                 w _ITM_deregisterTMCloneTable</div><div class="line">                 w _ITM_registerTMCloneTable</div><div class="line">0000000000600df0 d __JCR_END__</div><div class="line">0000000000600df0 d __JCR_LIST__</div><div class="line">                 w _Jv_RegisterClasses</div><div class="line">00000000004009f0 T __libc_csu_fini</div><div class="line">0000000000400980 T __libc_csu_init</div><div class="line">                 U __libc_start_main@@GLIBC_2.2.5</div><div class="line">000000000040086d T main</div><div class="line">00000000004007e0 t register_tm_clones</div><div class="line">0000000000400780 T _start</div><div class="line">0000000000601060 D __TMC_END__</div><div class="line">00000000004008af t __static_initialization_and_destruction_0(int, int)</div><div class="line">0000000000400923 t __static_initialization_and_destruction_0(int, int)</div><div class="line">0000000000400901 T test()</div><div class="line">000000000040088d T hello()</div><div class="line">                 U std::ostream::operator&lt;&lt;(std::ostream&amp; (*)(std::ostream&amp;))@@GLIBCXX_3.4</div><div class="line">                 U std::ios_base::Init::Init()@@GLIBCXX_3.4</div><div class="line">                 U std::ios_base::Init::~Init()@@GLIBCXX_3.4</div><div class="line">0000000000601060 B std::cout@@GLIBCXX_3.4</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::endl&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)@@GLIBCXX_3.4</div><div class="line">0000000000601171 b std::__ioinit</div><div class="line">0000000000601172 b std::__ioinit</div><div class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::operator&lt;&lt; &lt;std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char const*)@@GLIBCXX_3.4</div></pre></td></tr></table></figure>
<h2 id="8-可执行文件"><a href="#8-可执行文件" class="headerlink" title="8. 可执行文件"></a>8. 可执行文件</h2><p>可执行文件指的是可以由操作系统进行加载执行的文件。在不同的操作系统环境下，可执行程序的呈现方式不一样。例如上面生成的<code>main</code>就是Linux系统下的可执行文件，windows系统下的可执行文件一般为<code>*.exe</code>。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://wiki.ubuntu.org.cn/Compiling_Cpp" target="_blank" rel="external">https://wiki.ubuntu.org.cn/Compiling_Cpp</a></li>
<li><a href="https://tech.meituan.com/2015/01/22/linker.html" target="_blank" rel="external">https://tech.meituan.com/2015/01/22/linker.html</a></li>
<li><a href="http://notes.maxwi.com/3416/06/05/source-to-program/" target="_blank" rel="external">http://notes.maxwi.com/3416/06/05/source-to-program/</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2014/11/compiler.html" target="_blank" rel="external">http://www.ruanyifeng.com/blog/2014/11/compiler.html</a></li>
<li><a href="https://blog.csdn.net/zhengqijun_/article/details/51881149" target="_blank" rel="external">https://blog.csdn.net/zhengqijun_/article/details/51881149</a></li>
<li><a href="https://www.cnblogs.com/Goldworm/archive/2012/05/21/2511910.html" target="_blank" rel="external">https://www.cnblogs.com/Goldworm/archive/2012/05/21/2511910.html</a></li>
<li><a href="https://juejin.im/entry/5c0d23b35188253b7e7480db" target="_blank" rel="external">https://juejin.im/entry/5c0d23b35188253b7e7480db</a></li>
<li><a href="https://www.zhihu.com/question/280665935" target="_blank" rel="external">https://www.zhihu.com/question/280665935</a></li>
<li><a href="http://www.shanghai.ws/gnu/gcc_1.htm" target="_blank" rel="external">http://www.shanghai.ws/gnu/gcc_1.htm</a></li>
<li><a href="https://wiki.ubuntu.org.cn/Compiling_C" target="_blank" rel="external">https://wiki.ubuntu.org.cn/Compiling_C</a></li>
<li><a href="https://www.cnblogs.com/sunsky303/p/7731911.html" target="_blank" rel="external">https://www.cnblogs.com/sunsky303/p/7731911.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      C++源文件到可执行程序
    
    </summary>
    
      <category term="Linux" scheme="http://noahsnail.com/categories/Linux/"/>
    
    
      <category term="C++" scheme="http://noahsnail.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>U-Net - Convolutional Networks for Biomedical Image Segmentation论文翻译——中文版</title>
    <link href="http://noahsnail.com/2019/05/13/2019-05-13-U-Net-Convolutional%20Networks%20for%20Biomedical%20Image%20Segmentation%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2019/05/13/2019-05-13-U-Net-Convolutional Networks for Biomedical Image Segmentation论文翻译——中文版/</id>
    <published>2019-05-13T07:57:03.000Z</published>
    <updated>2020-04-10T09:48:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation"><a href="#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="U-Net: Convolutional Networks for Biomedical Image Segmentation"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>许多人都赞同深度网络的成功训练需要大量标注的训练样本。在本文中，我们提出了一种网络及训练策略，它依赖于大量使用数据增强，以便更有效地使用获得的标注样本。这个架构包括捕获上下文的收缩路径和能够精确定位的对称扩展路径。我们证明了这种网络可以从非常少的图像进行端到端训练，并且优于之前的ISBI赛挑战赛的最好方法（滑动窗口卷积网络），ISBI赛挑战赛主要是在电子显微镜堆叠中进行神经元结构分割。使用在透射光显微镜图像（相位衬度和DIC）上训练的相同网络，我们在这些类别中大幅度地赢得了2015年ISBI细胞追踪挑战赛。而且，网络速度很快。在最新的GPU上，分割一张512x512的图像不到一秒钟。网络的完整实现（基于Caffe）和预训练网络可在<a href="http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net" target="_blank" rel="external">http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net</a>上获得。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在过去两年，深度卷积网络在许多视觉识别任务中的表现都优于当前的最新技术，例如[7,3]。虽然卷积网络已经存在了很长时间[8]，但由于可用训练集的大小和所考虑网络的规模，它们的成功受到了限制。Krizhevsky等人[7]的突破是通过大型网络在ImageNet数据集上的监督训练实现的，其中大型网络有8个网络层和数百万参数，ImageNet数据集包含百万张训练图像。从那时起，即使更大更深的网络也已经得到了训练[12]。</p>
<p>卷积网络的典型用途是分类任务，其中图像输出是单个的类别标签。然而，在许多视觉任务中，尤其是在生物医学图像处理中，期望的输出应该包括位置，即类别标签应该分配给每个像素。此外，生物医学任务中通常无法获得数千张训练图像。因此，Ciresan等人[1]在滑动窗口设置中训练网络，通过提供像素周围局部区域（patch）作为输入来预测每个像素的类别标签。首先，这个网络可以定位。其次，局部块方面的训练数据远大于训练图像的数量。由此产生的网络大幅度地赢得了ISBI 2012EM分割挑战赛。</p>
<p>显然，Ciresan等人[1]的策略有两个缺点。首先，它非常慢，因为必须为每个图像块单独运行网络，并且由于图像块重叠而存在大量冗余。其次，定位准确性与上下文的使用之间存在着权衡。较大的图像块需要更多的最大池化层，从而降低了定位精度，而较小的图像块则允许网络只能看到很少的上下文。许多最近的方法[11，4]提出了一种分类器输出，其考虑了来自多个层的特征。同时具有良好的定位和上下文的使用是可能的。</p>
<p>在本文中，我们构建了一个更优雅的架构，即所谓的“全卷积网络”[9]。我们对这种架构进行了修改和扩展，使得它只需很少的训练图像就可以取得更精确的分割; 参见图1。[9]中的主要思想是通过连续层补充通常的收缩网络，其中的池化运算符由上采样运算符替换。因此，这些层增加了输出的分辨率。为了进行定位，来自收缩路径的高分辨率特征与上采样输出相结合。然后，后续卷积层可以基于该信息学习组装更精确的输出。</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbd198243fa74325.png" alt="Figure 1"></p>
<p>图1. U-net架构（最低分辨率为32x32像素的示例）。每个蓝色框对应于一张多通道特征映射。通道数在框的顶部。<code>x-y</code>尺寸提供在框的左下边。白框表示复制的特征映射。箭头表示不同的操作。</p>
<p>我们架构中的一个重要修改是在上采样部分中我们还有大量的特征通道，这些通道允许网络将上下文信息传播到具有更高分辨率的层。因此，扩展路径或多或少地与收缩路径对称，并产生U形结构。网络没有任何全连接层，并且仅使用每个卷积的有效部分，即分割映射仅包含在输入图像中可获得完整上下文的像素。该策略允许通过重叠图像区策略无缝分割任意大小的图像（参见图2）。为了预测图像边界区域中的像素，通过镜像输入图像来外推缺失的上下文。这种图像块策略对于将网络应用于大的图像非常重要，否则分辨率将受到GPU内存的限制。</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbd68de246d25163.png" alt="Figure 2"></p>
<p>图2. 重叠图像块策略可以无缝分割任意大小的图像（EM堆叠中的神经元结构分割）。分割的预测在黄色区域，要求蓝色区域的图像数据作为输入。缺失的输入数据通过镜像外推。</p>
<p>对于我们的任务，可用的训练数据非常少，我们通过对可用的训练图像应用弹性变形来使用更多的数据增强。这允许网络学习这种变形的不变性，而不需要在标注图像语料库中看到这些变形。 这在生物医学分割中尤其重要，因为变形曾经是组织中最常见的变化，并且可以有效地模拟真实的变形。Dosovitskiy等人[2]在无监督特征学习的领域内已经证明了数据增强在学习不变性中的价值。</p>
<p>许多细胞分割任务中的另一个挑战是分离同类的接触目标，见图3。为此，我们建议使用加权损失，其中接触单元之间的分离背景标签在损失函数中获得较大的权重。</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbdb9fd656556641.png" alt="Figure 3"></p>
<p>图3. 用DIC（差异干涉对比）显微镜记录玻璃上的HeLa细胞。（a）原始图像。（b）覆盖的实际分割。不同的颜色表示不同的HeLa细胞实例。（c）生成分割掩码（白色：前景，黑色：背景）。（d）以像素损失权重的映射来迫使网络学习边界像素。</p>
<p>由此产生的网络适用于各种生物医学分割问题。在本文中，我们展示了EM堆叠中神经元结构的分割结果（从ISBI 2012开始的持续竞赛），其中我们的表现优于Ciresan等人[1]的网络。此外，我们展示了2015 ISBI细胞追踪挑战赛光学显微镜图像中的细胞分割结果。我们在两个最具挑战性的2D透射光数据集上以巨大的优势赢得了比赛。</p>
<h2 id="2-网络架构"><a href="#2-网络架构" class="headerlink" title="2 网络架构"></a>2 网络架构</h2><p>网络架构如图1所示。它由一个收缩路径（左侧）和一个扩展路径（右侧）组成。收缩路径遵循卷积网络的典型架构。它包括重复使用两个3x3卷积（无填充卷积），每个卷积后跟一个线性修正单元（ReLU）和一个2x2最大池化操作，步长为2的下采样。在每个下采样步骤中，我们将特征通道的数量加倍。扩展路径中的每一步都包括特征映射的上采样，然后进行2x2卷积（“向上卷积”），将特征通道数量减半，与来自收缩路径的相应裁剪特征映射串联，然后是两个3x3卷积，每个卷积后面接ReLU。由于每一次卷积都会丢失边界像素，因此裁剪是必要的。在最后一层，使用1x1卷积将每个64分量特征向量映射到所需数量的类别上。网络总共有23个卷积层。</p>
<p>为了允许输出分割映射的无缝平铺（参见图2），选择输入的图像块大小非常重要，这样所有的2x2最大池化操作都可以应用在具有偶数x和偶数y大小的层上。</p>
<h2 id="3-训练"><a href="#3-训练" class="headerlink" title="3 训练"></a>3 训练</h2><p>使用输入图像及其相应的分割映射来训练带有随机梯度下降的网络，网络是采用Caffe[6]实现的。由于无填充卷积，输出图像比输入少恒定的边界宽度。为了最小化开销并最大限度地利用GPU内存，我们倾向于在大批量数据大小的情况下使用大的输入图像块，从而将批量数据大小减少到单张图像。因此，我们使用高动量（0.99），使得大量先前看到的训练样本确定当前优化步骤中的更新。</p>
<p>能量函数由最终的特征映射上逐像素soft-max与交叉熵损失函数结合计算而成。soft-max定义为$p_k(\mathbf {x})=exp(a_k(\mathbf {x}))/ (\sum^{K}_{k’=1}exp(a_{k’}(\mathbf {x})))$，其中$a_k(\mathbf {x})$表示特征通道$k$中在像素位置$\mathbf {x} \in \Omega$上的激活，$\Omega \subset \mathbb {Z}^2$。$K$是类别数量，$p_k(\mathbf{x})$是近似的最大化函数，即，对于有最大激活$a_k(\mathbf{x})$的$k$，$p_k(\mathbf{x}) \approx 1$，对于其它的$k$有$p_k(\mathbf{x}) \approx 0$。交叉熵在每个位置上使用$$E=\sum_{x \in \Omega} \omega(\mathbf{x})log(p_{l(x)}(\mathbf{x})) \tag{1}$$来惩罚$p_{l(x)}(\mathbf{x})$与$1$的偏差。其中，$l:\Omega \to {1,…,K}$是每个像素的真实标签，$\omega:\Omega \to \mathbb{R}$是训练中我们引入的用来赋予某些像素更多权重的权重图。</p>
<p>我们为每一个真实分割预先计算了权重图，以补偿训练集里某个类别的像素的不同频率，并且迫使网络学习我们在相邻细胞间的引入小的分割边界（参见图3c和d）。</p>
<p>分割边界使用形态学操作来计算。然后将权重图计算为$$\omega(\mathbf{x})=\omega_c(\mathbf{x})+\omega_{0} \bullet exp(-\frac {(d_1(\mathbf{x}) + d_2(\mathbf{x}))^2} {2\sigma^2}) \tag{2}$$ 其中，$\omega_c:\Omega \to \mathbb{R}$是用来平衡类频率的权重图，$d_1:\Omega \to \mathbb{R}$表示到最近细胞边界的距离，$d_2:\Omega \to \mathbb{R}$表示到次近细胞边界的距离。在我们的实验中，设置$\omega_{0}=10$，$\sigma \approx 5$个像素。</p>
<p>在具有许多卷积层和通过网络的不同路径的深度网络中，权重的良好初始化非常重要。否则，网络的某些部分可能会进行过多的激活，而其他部分永远不会起作用。理想情况下，初始化权重应该是自适应的，以使网络中的每个特征映射都具有近似的单位方差。对于具有我们架构（交替卷积和ReLU层）的网络，可以通过从标准偏差为$\sqrt{2/N}$的高斯分布中绘制初始化权重来实现，其中$N$表示一个神经元[5]传入结点的数量。例如，对于前一层中3x3卷积和64个特征通道，$N=9·64=576$。</p>
<h3 id="3-1-数据增强"><a href="#3-1-数据增强" class="headerlink" title="3.1 数据增强"></a>3.1 数据增强</h3><p>当只有少量训练样本可用时，对于教网络学习所需的不变性和鲁棒性而言，数据增强至关重要。对于显微镜图像，我们主要需要平移和旋转不变性，以及对形变和灰度值变化的鲁棒性。尤其是训练样本的随机弹性形变似乎是训练具有很少标注图像的分割网络的关键概念。我们使用在3x3粗糙网格上的随机位移矢量来生成平滑形变。从具有10个像素标准偏差的高斯分布中采样位移。然后使用双三次插值计算每个像素的位移。收缩路径末端的丢弃层执行进一步隐式数据增强。</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><p>我们演示了U-Net在三个不同分割任务中的应用。第一个任务是在电子显微记录中分割神经元结构。图2显示了数据集样本以及获得的分割。我们在补充材料中提供了完整的结果。数据集由EM分割挑战赛[14]提供，该挑战始于ISBI 2012，目前仍在接受新的贡献。训练数据是一组来自果蝇一龄幼虫腹神经索（VNC）的连续切片透射电镜的30张图像（512x512像素）。每张图像都有一个对应的细胞（白色）和膜（黑色）完整标注的实际分割图。该测试集是公开可获得的，但其分割图像是保密的。评估可以通过将预测的膜概率图发送给组织者来获得。通过在10个不同级别对映射进行阈值化并计算“弯曲误差”，“兰德误差”和“像素误差”来进行评估[14]。</p>
<p>U-Net（输入数据7个旋转版本的平均）在没有任何进一步的“前”或后处理的情况下实现了0.0003529的“弯曲误差”（新的最佳分数，参见表1）和0.0382的“兰德误差”。</p>
<p>表1：EM分割挑战<a href="march 6th, 2015">14</a>的排名，按warping error排序。</p>
<p><img src="https://i.loli.net/2020/04/10/9ta2VNOnuQ1PDmo.png" alt="Table 1"></p>
<p>这比Ciresan等人[1]的滑动窗口卷积网络结果要好得多，其最佳提交的弯曲误差为0.000420，兰德误差为0.0504。 就兰德误差而言，在该数据集上唯一表现更好的算法，其使用了应用到Ciresan等[1]概率图上的针对数据集的非常特定后处理方法。</p>
<p>我们还将U-Net应用于光学显微图像中的细胞分割任务。这个分割任务是ISBI细胞跟踪挑战赛2014和2015的一部分[10,13]。第一个数据集“PhC-U373”包含在聚丙烯酰亚胺基质上通过相衬显微技术记录的多形性胶质母细胞瘤U373细胞（参见图4a，b和补充材料）。它包含35个部分标注的训练图像。这里，我们取得了$92\%$的平均IOU（“并集上的交集”），明显好于$83\%$的次优算法（参见表2）。第二个数据集“DIC-HeLa”是通过微分干涉相差(DIC)显微镜记录的平板玻璃上的HeLa细胞（请参见图3，图4c，d和补充材料）。它包含20个部分标注的训练图像。这里，我们取得了$77.5\%$的平均IOU，明显好于$46\%$的次优算法。</p>
<p><img src="https://i.loli.net/2020/04/10/njlqCkAUh2Q1woP.png" alt="Fingure 4"></p>
<p>图4：ISBI细胞跟踪挑战赛上的结果。(a)“PhC-U373”数据集的一张输入图像的一部分。(b)分割结果（蓝绿图像块）和实际结果（黄色边框）。(c)“DIC-HeLa”数据集的输入图像。(d)分割结果（随机颜色的图像块）和实际结果（黄色边框）。</p>
<p>表2：ISBI细胞跟踪挑战赛2015上的分割结果(IOU)。</p>
<p><img src="https://i.loli.net/2020/04/10/pom8VDJe12uW6LT.png" alt="Table 2"></p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>U-Net架构在截然不同的生物医疗分割应用中取得了非常好的性能。由于具有弹性形变的数据增强，它仅需要非常少的标注图像，并且在NVidia Titan GPU (6 GB)上仅需要10个小时的合理训练时间。我们提供了完整的基于Caffe[6]的实现以及训练之后的网络。我们确信U-Net架构可以很轻松地应用到更多的任务上。</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>这项研究得到了德国联邦和州政府卓越计划(EXC 294)和BMBF(Fkz 0316185B)的支持。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p>Ciresan, D.C., Gambardella, L.M., Giusti, A., Schmidhuber, J.: Deep neural networks segment neuronal membranes in electron microscopy images. In: NIPS. pp. 2852–2860 (2012)</p>
</li>
<li><p>Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative unsupervised feature learning with convolutional neural networks. In: NIPS (2014)</p>
</li>
<li><p>Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014)</p>
</li>
<li><p>Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation and fine-grained localization (2014), arXiv:1411.5752 [cs.CV]</p>
</li>
<li><p>He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015), arXiv:1502.01852 [cs.CV]</p>
</li>
<li><p>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Ca↵e: Convolutional architecture for fast feature embedding (2014), arXiv:1408.5093 [cs.CV]</p>
</li>
<li><p>Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. pp. 1106–1114 (2012)</p>
</li>
<li><p>LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural Computation 1(4), 541–551 (1989)</p>
</li>
<li><p>Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation (2014), arXiv:1411.4038 [cs.CV]</p>
</li>
<li><p>Maska, M., (…), de Solorzano, C.O.: A benchmark for comparison of cell tracking algorithms. Bioinformatics 30, 1609–1617 (2014)</p>
</li>
<li><p>Seyedhosseini, M., Sajjadi, M., Tasdizen, T.: Image segmentation with cascaded hierarchical models and logistic disjunctive normal networks. In: Computer Vision (ICCV), 2013 IEEE International Conference on. pp. 2168–2175 (2013)</p>
</li>
<li><p>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2014), arXiv:1409.1556 [cs.CV]</p>
</li>
<li><p>WWW: Web page of the cell tracking challenge, <a href="http://www.codesolorzano.com/celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html" target="_blank" rel="external">http://www.codesolorzano.com/celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html</a></p>
</li>
<li><p>WWW: Web page of the em segmentation challenge, <a href="http://brainiac2.mit.edu/isbi_challenge/" target="_blank" rel="external">http://brainiac2.mit.edu/isbi_challenge/</a></p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      U-Net - Convolutional Networks for Biomedical Image Segmentation论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>U-Net - Convolutional Networks for Biomedical Image Segmentation论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2019/05/13/2019-05-13-U-Net-Convolutional%20Networks%20for%20Biomedical%20Image%20Segmentation%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2019/05/13/2019-05-13-U-Net-Convolutional Networks for Biomedical Image Segmentation论文翻译——中英文对照/</id>
    <published>2019-05-13T07:56:22.000Z</published>
    <updated>2020-04-13T01:32:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation"><a href="#U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="U-Net: Convolutional Networks for Biomedical Image Segmentation"></a>U-Net: Convolutional Networks for Biomedical Image Segmentation</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at <a href="http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net" target="_blank" rel="external">http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net</a>.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>许多人都赞同深度网络的成功训练需要大量标注的训练样本。在本文中，我们提出了一种网络及训练策略，它依赖于大量使用数据增强，以便更有效地使用获得的标注样本。这个架构包括捕获上下文的收缩路径和能够精确定位的对称扩展路径。我们证明了这种网络可以从非常少的图像进行端到端训练，并且优于之前的ISBI赛挑战赛的最好方法（滑动窗口卷积网络），ISBI赛挑战赛主要是在电子显微镜堆叠中进行神经元结构分割。使用在透射光显微镜图像（相位衬度和DIC）上训练的相同网络，我们在这些类别中大幅度地赢得了2015年ISBI细胞追踪挑战赛。而且，网络速度很快。在最新的GPU上，分割一张512x512的图像不到一秒钟。网络的完整实现（基于Caffe）和预训练网络可在<a href="http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net" target="_blank" rel="external">http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net</a>上获得。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>In the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [7,3]. While convolutional networks have already existed for a long time [8], their success was limited due to the size of the available training sets and the size of the considered networks. The breakthrough by Krizhevsky et al. [7] was due to supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [12].</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在过去两年，深度卷积网络在许多视觉识别任务中的表现都优于当前的最新技术，例如[7,3]。虽然卷积网络已经存在了很长时间[8]，但由于可用训练集的大小和所考虑网络的规模，它们的成功受到了限制。Krizhevsky等人[7]的突破是通过大型网络在ImageNet数据集上的监督训练实现的，其中大型网络有8个网络层和数百万参数，ImageNet数据集包含百万张训练图像。从那时起，即使更大更深的网络也已经得到了训练[12]。</p>
<p>The typical use of convolutional networks is on classification tasks, where the output to an image is a single class label. However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e., a class label is supposed to be assigned to each pixel. Moreover, thousands of training images are usually beyond reach in biomedical tasks. Hence, Ciresan et al. [1] trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel as input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin.</p>
<p>卷积网络的典型用途是分类任务，其中图像输出是单个的类别标签。然而，在许多视觉任务中，尤其是在生物医学图像处理中，期望的输出应该包括位置，即类别标签应该分配给每个像素。此外，生物医学任务中通常无法获得数千张训练图像。因此，Ciresan等人[1]在滑动窗口设置中训练网络，通过提供像素周围局部区域（patch）作为输入来预测每个像素的类别标签。首先，这个网络可以定位。其次，局部块方面的训练数据远大于训练图像的数量。由此产生的网络大幅度地赢得了ISBI 2012EM分割挑战赛。</p>
<p>Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context. Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11,4] proposed a classifier output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time.</p>
<p>显然，Ciresan等人[1]的策略有两个缺点。首先，它非常慢，因为必须为每个图像块单独运行网络，并且由于图像块重叠而存在大量冗余。其次，定位准确性与上下文的使用之间存在着权衡。较大的图像块需要更多的最大池化层，从而降低了定位精度，而较小的图像块则允许网络只能看到很少的上下文。许多最近的方法[11，4]提出了一种分类器输出，其考虑了来自多个层的特征。同时具有良好的定位和上下文的使用是可能的。</p>
<p>In this paper, we build upon a more elegant architecture, the so-called “fully convolutional network” [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1. The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise output based on this information.</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbd198243fa74325.png" alt="Figure 1"></p>
<p>Fig. 1. U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.</p>
<p>在本文中，我们构建了一个更优雅的架构，即所谓的“全卷积网络”[9]。我们对这种架构进行了修改和扩展，使得它只需很少的训练图像就可以取得更精确的分割; 参见图1。[9]中的主要思想是通过连续层补充通常的收缩网络，其中的池化运算符由上采样运算符替换。因此，这些层增加了输出的分辨率。为了进行定位，来自收缩路径的高分辨率特征与上采样输出相结合。然后，后续卷积层可以基于该信息学习组装更精确的输出。</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbd198243fa74325.png" alt="Figure 1"></p>
<p>图1. U-net架构（最低分辨率为32x32像素的示例）。每个蓝色框对应于一张多通道特征映射。通道数在框的顶部。<code>x-y</code>尺寸提供在框的左下边。白框表示复制的特征映射。箭头表示不同的操作。</p>
<p>One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see Figure 2). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbd68de246d25163.png" alt="Figure 2"></p>
<p>Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring</p>
<p>我们架构中的一个重要修改是在上采样部分中我们还有大量的特征通道，这些通道允许网络将上下文信息传播到具有更高分辨率的层。因此，扩展路径或多或少地与收缩路径对称，并产生U形结构。网络没有任何全连接层，并且仅使用每个卷积的有效部分，即分割映射仅包含在输入图像中可获得完整上下文的像素。该策略允许通过重叠图像区策略无缝分割任意大小的图像（参见图2）。为了预测图像边界区域中的像素，通过镜像输入图像来外推缺失的上下文。这种图像块策略对于将网络应用于大的图像非常重要，否则分辨率将受到GPU内存的限制。</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbd68de246d25163.png" alt="Figure 2"></p>
<p>图2. 重叠图像块策略可以无缝分割任意大小的图像（EM堆叠中的神经元结构分割）。分割的预测在黄色区域，要求蓝色区域的图像数据作为输入。缺失的输入数据通过镜像外推。</p>
<p>As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simulated e ciently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning.</p>
<p>对于我们的任务，可用的训练数据非常少，我们通过对可用的训练图像应用弹性变形来使用更多的数据增强。这允许网络学习这种变形的不变性，而不需要在标注图像语料库中看到这些变形。 这在生物医学分割中尤其重要，因为变形曾经是组织中最常见的变化，并且可以有效地模拟真实的变形。Dosovitskiy等人[2]在无监督特征学习的领域内已经证明了数据增强在学习不变性中的价值。</p>
<p>Another challenge in many cell segmentation tasks is the separation of touching objects of the same class; see Figure 3. To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function.</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbdb9fd656556641.png" alt="Figure 3"></p>
<p>Fig. 3. HeLa cells on glass recorded with DIC (differential interference contrast) microscopy. (a) raw image. (b) overlay with ground truth segmentation. Different colors indicate di↵erent instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels.</p>
<p>许多细胞分割任务中的另一个挑战是分离同类的接触目标，见图3。为此，我们建议使用加权损失，其中接触单元之间的分离背景标签在损失函数中获得较大的权重。</p>
<p><img src="https://i.loli.net/2019/05/15/5cdbdb9fd656556641.png" alt="Figure 3"></p>
<p>图3. 用DIC（差异干涉对比）显微镜记录玻璃上的HeLa细胞。（a）原始图像。（b）覆盖的实际分割。不同的颜色表示不同的HeLa细胞实例。（c）生成分割掩码（白色：前景，黑色：背景）。（d）以像素损失权重的映射来迫使网络学习边界像素。</p>
<p>The resulting network is applicable to various biomedical segmentation problems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we outperformed the network of Ciresan et al. [1]. Furthermore, we show results for cell segmentation in light microscopy images from the ISBI cell tracking challenge 2015. Here we won with a large margin on the two most challenging 2D transmitted light datasets.</p>
<p>由此产生的网络适用于各种生物医学分割问题。在本文中，我们展示了EM堆叠中神经元结构的分割结果（从ISBI 2012开始的持续竞赛），其中我们的表现优于Ciresan等人[1]的网络。此外，我们展示了2015 ISBI细胞追踪挑战赛光学显微镜图像中的细胞分割结果。我们在两个最具挑战性的2D透射光数据集上以巨大的优势赢得了比赛。</p>
<h2 id="2-Network-Architecture"><a href="#2-Network-Architecture" class="headerlink" title="2 Network Architecture"></a>2 Network Architecture</h2><p>The network architecture is illustrated in Figure 1. It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.</p>
<h2 id="2-网络架构"><a href="#2-网络架构" class="headerlink" title="2 网络架构"></a>2 网络架构</h2><p>网络架构如图1所示。它由一个收缩路径（左侧）和一个扩展路径（右侧）组成。收缩路径遵循卷积网络的典型架构。它包括重复使用两个3x3卷积（无填充卷积），每个卷积后跟一个线性修正单元（ReLU）和一个2x2最大池化操作，步长为2的下采样。在每个下采样步骤中，我们将特征通道的数量加倍。扩展路径中的每一步都包括特征映射的上采样，然后进行2x2卷积（“向上卷积”），将特征通道数量减半，与来自收缩路径的相应裁剪特征映射串联，然后是两个3x3卷积，每个卷积后面接ReLU。由于每一次卷积都会丢失边界像素，因此裁剪是必要的。在最后一层，使用1x1卷积将每个64分量特征向量映射到所需数量的类别上。网络总共有23个卷积层。</p>
<p>To allow a seamless tiling of the output segmentation map (see Figure 2), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size.</p>
<p>为了允许输出分割映射的无缝平铺（参见图2），选择输入的图像块大小非常重要，这样所有的2x2最大池化操作都可以应用在具有偶数x和偶数y大小的层上。</p>
<h2 id="3-Training"><a href="#3-Training" class="headerlink" title="3 Training"></a>3 Training</h2><p>The input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caffe [6]. Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.</p>
<h2 id="3-训练"><a href="#3-训练" class="headerlink" title="3 训练"></a>3 训练</h2><p>使用输入图像及其相应的分割映射来训练带有随机梯度下降的网络，网络采用Caffe[6]实现。由于无填充卷积，输出图像比输入少恒定的边界宽度。为了最小化开销并最大限度地利用GPU内存，我们倾向于在大批量数据大小的情况下使用大的输入图像块，从而将批量数据大小减少到单张图像。因此，我们使用高动量值（0.99），大量先前看到的训练样本来决定当前优化步骤中的更新。</p>
<p>The energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross entropy loss function. The soft-max is defined as $p_k(\mathbf {x})=exp(a_k(\mathbf {x}))/ (\sum^{K}_{k’=1}exp(a_{k’}(\mathbf {x})))$ where $a_k(\mathbf {x})$ denotes the activation in feature channel $k$ at the pixel position $\mathbf {x} \in \Omega$ with $\Omega \subset \mathbb {Z}^2$. $K$ is the number of classes and $p_k(\mathbf{x})$ is the approximated maximum-function. I.e. $p_k(\mathbf{x}x) \approx 1$ for the $k$ that has the maximum activation $a_k(\mathbf{x})$ and $p_k(\mathbf{x}) \approx 0$ for all other $k$. The cross entropy then penalizes at each position the deviation of $p_{l(x)}(\mathbf{x})$ from $1$ using $$E=\sum_{x \in \Omega} \omega(\mathbf{x})log(p_{l(x)}(\mathbf{x})) \tag{1}$$ where $l:\Omega \to {1,…,K}$ is the true label of each pixel and $\omega:\Omega \to \mathbb{R}$ is a weight map that we introduced to give some pixels more importance in the training.</p>
<p>能量函数由最终的特征映射上逐像素soft-max与交叉熵损失函数结合计算而成。soft-max定义为$p_k(\mathbf {x})=exp(a_k(\mathbf {x}))/ (\sum^{K}_{k’=1}exp(a_{k’}(\mathbf {x})))$，其中$a_k(\mathbf {x})$表示特征通道$k$中在像素位置$\mathbf {x} \in \Omega$上的激活，$\Omega \subset \mathbb {Z}^2$。$K$是类别数量，$p_k(\mathbf{x})$是近似的最大化函数，即，对于有最大激活$a_k(\mathbf{x})$的$k$，$p_k(\mathbf{x}) \approx 1$，对于其它的$k$有$p_k(\mathbf{x}) \approx 0$。交叉熵在每个位置上使用$$E=\sum_{x \in \Omega} \omega(\mathbf{x})log(p_{l(x)}(\mathbf{x})) \tag{1}$$来惩罚$p_{l(x)}(\mathbf{x})$与$1$的偏差。其中，$l:\Omega \to {1,…,K}$是每个像素的真实标签，$\omega:\Omega \to \mathbb{R}$是训练中我们引入的用来赋予某些像素更多权重的权重图。</p>
<p>We pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels from a certain class in the training data set, and to force the network to learn the small separation borders that we introduce between touching cells (See Figure 3c and d).</p>
<p>我们为每一个真实分割预先计算了权重图，以补偿训练集里某个类别的像素的不同频率，并且迫使网络学习我们在相邻细胞间的引入小的分割边界（参见图3c和d）。</p>
<p>The separation border is computed using morphological operations. The weight map is then computed as $$\omega(\mathbf{x})=\omega_c(\mathbf{x})+\omega_{0} \bullet exp(-\frac {(d_1(\mathbf{x}) + d_2(\mathbf{x}))^2} {2\sigma^2}) \tag{2}$$ where $\omega_c:\Omega \to \mathbb{R}$ is the weight map to balance the class frequencies,  $d_1:\Omega \to \mathbb{R}$ denotes the distance to the border of the nearest cell and $d_2:\Omega \to \mathbb{R}$ the distance to the border of the second nearest cell. In our experiments we set $\omega_{0}=10$ and $\sigma \approx 5$ pixels.</p>
<p>分割边界使用形态学操作来计算。然后将权重图计算为$$\omega(\mathbf{x})=\omega_c(\mathbf{x})+\omega_{0} \bullet exp(-\frac {(d_1(\mathbf{x}) + d_2(\mathbf{x}))^2} {2\sigma^2}) \tag{2}$$ 其中，$\omega_c:\Omega \to \mathbb{R}$是用来平衡类频率的权重图，$d_1:\Omega \to \mathbb{R}$表示到最近细胞边界的距离，$d_2:\Omega \to \mathbb{R}$表示到次近细胞边界的距离。在我们的实验中，设置$\omega_{0}=10$，$\sigma \approx 5$个像素。</p>
<p>In deep networks with many convolutional layers and different paths through the network, a good initialization of the weights is extremely important. Otherwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. For a network with our architecture (alternating convolution and ReLU layers) this can be achieved by drawing the initial weights from a Gaussian distribution with a standard deviation of $\sqrt{2/N}$, where $N$ denotes the number of incoming nodes of one neuron [5]. E.g. for a 3x3 convolution and 64 feature channels in the previous layer $N=9·64=576$.</p>
<p>在具有许多卷积层和通过网络的不同路径的深度网络中，权重的良好初始化非常重要。否则，网络的某些部分可能会进行过多的激活，而其他部分永远不会起作用。理想情况下，初始化权重应该是自适应的，以使网络中的每个特征映射都具有近似的单位方差。对于具有我们架构（交替卷积和ReLU层）的网络，可以通过从标准偏差为$\sqrt{2/N}$的高斯分布中绘制初始化权重来实现，其中$N$表示一个神经元[5]传入结点的数量。例如，对于前一层中3x3卷积和64个特征通道，$N=9·64=576$。</p>
<h3 id="3-1-Data-Augmentation"><a href="#3-1-Data-Augmentation" class="headerlink" title="3.1 Data Augmentation"></a>3.1 Data Augmentation</h3><p>Data augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.</p>
<h3 id="3-1-数据增强"><a href="#3-1-数据增强" class="headerlink" title="3.1 数据增强"></a>3.1 数据增强</h3><p>当只有少量训练样本可用时，对于教网络学习所需的不变性和鲁棒性而言，数据增强至关重要。对于显微镜图像，我们主要需要平移和旋转不变性，以及对形变和灰度值变化的鲁棒性。尤其是训练样本的随机弹性形变似乎是训练具有很少标注图像的分割网络的关键概念。我们使用在3x3粗糙网格上的随机位移矢量来生成平滑形变。从具有10个像素标准偏差的高斯分布中采样位移。然后使用双三次插值计算每个像素的位移。收缩路径末端的丢弃层执行进一步隐式数据增强。</p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h2><p>We demonstrate the application of the u-net to three different segmentation tasks. The first task is the segmentation of neuronal structures in electron microscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its segmentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 different levels and computation of the “warping error”, the “Rand error” and the “pixel error” [14].</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><p>我们演示了U-Net在三个不同分割任务中的应用。第一个任务是在电子显微记录中分割神经元结构。图2显示了数据集样本以及获得的分割。我们在补充材料中提供了完整的结果。数据集由EM分割挑战赛[14]提供，该挑战始于ISBI 2012，目前仍在接受新的贡献。训练数据是一组来自果蝇一龄幼虫腹神经索（VNC）的连续切片透射电镜的30张图像（512x512像素）。每张图像都有一个对应的细胞（白色）和膜（黑色）完整标注的实际分割图。该测试集是公开可获得的，但其分割图像是保密的。评估可以通过将预测的膜概率图发送给组织者来获得。通过在10个不同级别对映射进行阈值化并计算“弯曲误差”，“兰德误差”和“像素误差”来进行评估[14]。</p>
<p>The u-net (averaged over 7 rotated versions of the input data) achieves without any further <code>pre-</code> or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a  of 0.0382.</p>
<p>Table 1. Ranking on the EM segmentation challenge [14] (march 6th, 2015), sorted by warping error.</p>
<p><img src="https://i.loli.net/2020/04/10/9ta2VNOnuQ1PDmo.png" alt="Table 1"></p>
<p>U-Net（输入数据7个旋转版本的平均）在没有任何进一步的“前”或后处理的情况下实现了0.0003529的“弯曲误差”（新的最佳分数，参见表1）和0.0382的“兰德误差”。</p>
<p>表1：EM分割挑战<a href="march 6th, 2015">14</a>的排名，按warping error排序。</p>
<p><img src="https://i.loli.net/2020/04/10/9ta2VNOnuQ1PDmo.png" alt="Table 1"></p>
<p>This is significantly better than the sliding-window convolutional network result by Ciresan et al. , whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing algorithms on this data set use highly data set specific post-processing methods applied to the probability map of Ciresan et al. [1].</p>
<p>这比Ciresan等人[1]的滑动窗口卷积网络结果要好得多，其最佳提交的弯曲误差为0.000420，兰德误差为0.0504。 就兰德误差而言，在该数据集上唯一表现更好的算法，其使用了应用到Ciresan等[1]概率图上的针对数据集的非常特定后处理方法。</p>
<p>We also applied the u-net to a cell segmentation task in light microscopic images. This segmenation task is part of the ISBI cell tracking challenge 2014 and 2015 [10,13]. The first data set “PhC-U373” contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy (see Figure 4a,b and Supp. Material). It contains 35 partially annotated training images. Here we achieve an average IOU (“intersection over union”) of $92\%$, which is significantly better than the second best algorithm with $83\%$ (see Table 2). The second data set “DIC-HeLa” are HeLa cells on a flat glass recorded by differential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d and Supp. Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is significantly better than the second best algorithm with $46\%$.</p>
<p><img src="https://i.loli.net/2020/04/10/njlqCkAUh2Q1woP.png" alt="Fingure 4"></p>
<p>Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the “PhC-U373” data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the “DIC-HeLa” data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border).</p>
<p>Table 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015.</p>
<p><img src="https://i.loli.net/2020/04/10/pom8VDJe12uW6LT.png" alt="Table 2"></p>
<p>我们还将U-Net应用于光学显微图像中的细胞分割任务。这个分割任务是ISBI细胞跟踪挑战赛2014和2015的一部分[10,13]。第一个数据集“PhC-U373”包含在聚丙烯酰亚胺基质上通过相衬显微技术记录的多形性胶质母细胞瘤U373细胞（参见图4a，b和补充材料）。它包含35个部分标注的训练图像。这里，我们取得了$92\%$的平均IOU（“并集上的交集”），明显好于$83\%$的次优算法（参见表2）。第二个数据集“DIC-HeLa”是通过微分干涉相差(DIC)显微镜记录的平板玻璃上的HeLa细胞（请参见图3，图4c，d和补充材料）。它包含20个部分标注的训练图像。这里，我们取得了$77.5\%$的平均IOU，明显好于$46\%$的次优算法。</p>
<p><img src="https://i.loli.net/2020/04/10/njlqCkAUh2Q1woP.png" alt="Fingure 4"></p>
<p>图4：ISBI细胞跟踪挑战赛上的结果。(a)“PhC-U373”数据集的一张输入图像的一部分。(b)分割结果（蓝绿图像块）和实际结果（黄色边框）。(c)“DIC-HeLa”数据集的输入图像。(d)分割结果（随机颜色的图像块）和实际结果（黄色边框）。</p>
<p>表2：ISBI细胞跟踪挑战赛2015上的分割结果(IOU)。</p>
<p><img src="https://i.loli.net/2020/04/10/pom8VDJe12uW6LT.png" alt="Table 2"></p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>The u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic deformations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Caffe[6]-based implementation and the trained networks. We are sure that the u-net architecture can be applied easily to many more tasks.</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>U-Net架构在截然不同的生物医疗分割应用中取得了非常好的性能。由于具有弹性形变的数据增强，它仅需要非常少的标注图像，并且在NVidia Titan GPU (6 GB)上仅需要10个小时的合理训练时间。我们提供了完整的基于Caffe[6]的实现以及训练之后的网络。我们确信U-Net架构可以很轻松地应用到更多的任务上。</p>
<h2 id="Acknowlegements"><a href="#Acknowlegements" class="headerlink" title="Acknowlegements"></a>Acknowlegements</h2><p>This study was supported by the Excellence Initiative of the German Federal and State governments (EXC 294) and by the BMBF (Fkz 0316185B).</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>这项研究得到了德国联邦和州政府卓越计划(EXC 294)和BMBF(Fkz 0316185B)的支持。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p>Ciresan, D.C., Gambardella, L.M., Giusti, A., Schmidhuber, J.: Deep neural networks segment neuronal membranes in electron microscopy images. In: NIPS. pp. 2852–2860 (2012)</p>
</li>
<li><p>Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative unsupervised feature learning with convolutional neural networks. In: NIPS (2014)</p>
</li>
<li><p>Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014)</p>
</li>
<li><p>Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation and fine-grained localization (2014), arXiv:1411.5752 [cs.CV]</p>
</li>
<li><p>He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015), arXiv:1502.01852 [cs.CV]</p>
</li>
<li><p>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Ca↵e: Convolutional architecture for fast feature embedding (2014), arXiv:1408.5093 [cs.CV]</p>
</li>
<li><p>Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. pp. 1106–1114 (2012)</p>
</li>
<li><p>LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural Computation 1(4), 541–551 (1989)</p>
</li>
<li><p>Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation (2014), arXiv:1411.4038 [cs.CV]</p>
</li>
<li><p>Maska, M., (…), de Solorzano, C.O.: A benchmark for comparison of cell tracking algorithms. Bioinformatics 30, 1609–1617 (2014)</p>
</li>
<li><p>Seyedhosseini, M., Sajjadi, M., Tasdizen, T.: Image segmentation with cascaded hierarchical models and logistic disjunctive normal networks. In: Computer Vision (ICCV), 2013 IEEE International Conference on. pp. 2168–2175 (2013)</p>
</li>
<li><p>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2014), arXiv:1409.1556 [cs.CV]</p>
</li>
<li><p>WWW: Web page of the cell tracking challenge, <a href="http://www.codesolorzano.com/celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html" target="_blank" rel="external">http://www.codesolorzano.com/celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html</a></p>
</li>
<li><p>WWW: Web page of the em segmentation challenge, <a href="http://brainiac2.mit.edu/isbi_challenge/" target="_blank" rel="external">http://brainiac2.mit.edu/isbi_challenge/</a></p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      U-Net - Convolutional Networks for Biomedical Image Segmentation论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux登录信息查询</title>
    <link href="http://noahsnail.com/2019/05/09/2019-05-09-Linux%E7%99%BB%E5%BD%95%E4%BF%A1%E6%81%AF%E6%9F%A5%E8%AF%A2/"/>
    <id>http://noahsnail.com/2019/05/09/2019-05-09-Linux登录信息查询/</id>
    <published>2019-05-09T03:34:26.000Z</published>
    <updated>2019-05-09T07:18:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-Linux登录日志的存储"><a href="#1-Linux登录日志的存储" class="headerlink" title="1. Linux登录日志的存储"></a>1. Linux登录日志的存储</h2><p>在Linux系统中，登录日志主要存储在三个文件中，<code>/var/log/wtmp</code>，<code>/var/run/utmp</code>，<code>/var/log/lastlog</code>。常用的查询命令有<code>w</code>，<code>who</code>，<code>last</code>，<code>users</code>，<code>lastlog</code>等。</p>
<h2 id="2-w命令"><a href="#2-w命令" class="headerlink" title="2. w命令"></a>2. w命令</h2><ul>
<li><code>w</code>命令可用于显示当前登录系统的用户信息。</li>
<li>执行这项指令可查询目前登录系统的用户有哪些人，以及正在执行的程序。</li>
<li>单独执行<code>w</code>指令会显示所有的用户，也可以指定用户名称，仅显示某位用户的相关信息。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root@0b6987cc587f /workspace $ w</div><div class="line">19:30  up 48 days,  3:05, 2 users, load averages: 2.11 2.11 2.14</div><div class="line">USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT</div><div class="line">root     pts/2    192.168.0.1      Wed14    0.00s  0.08s  0.00s w</div><div class="line">root     pts/16   192.168.0.1      Wed11    5:20   0.08s  0.08s -bash</div></pre></td></tr></table></figure>
<p><code>w</code>命令显示的对应信息如下：</p>
<p>当前时间，系统启动到现在的时间，登录用户的数目，系统在最近1秒、5秒和15秒的平均负载。</p>
<p>USER: 登录帐号<br>TTY : 终端名称<br>FROM: 远程主机名<br>LOGIN@: 登录时间<br>IDLE: 空闲时间<br>JCPU: 该TTY终端连接的所有进程的占用时间<br>PCPU: 当前进程(即w项中显示的)的占用时间<br>WHAT: 当前正在运行进程的命令行</p>
<h2 id="3-who命令"><a href="#3-who命令" class="headerlink" title="3. who命令"></a>3. who命令</h2><p><code>who</code>命令用于显示系统中有哪些登录用户。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">root@0b6987cc587f /workspace $ who</div><div class="line">root   pts/2        2019-05-08 19:14 (192.168.0.1)</div><div class="line">root   pts/16       2019-05-08 18:39 (192.168.0.1)</div></pre></td></tr></table></figure>
<p><code>who</code>命令显示的对应信息如下：</p>
<p>登录帐号，终端名称，日期和时间，用户登录IP地址。</p>
<p><code>who am i</code>用来查看当前登陆者的信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@0b6987cc587f /workspace $ who am i</div><div class="line">root   pts/2        2019-05-08 19:14 (192.168.0.1)</div></pre></td></tr></table></figure>
<h2 id="4-last命令"><a href="#4-last命令" class="headerlink" title="4. last命令"></a>4. last命令</h2><p><code>last</code>命令用于显示用户最近登录信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">root@0b6987cc587f /workspace $ last -n 3</div><div class="line">root   pts/21       192.168.0.1    Thu May  9 12:01 - 19:00  (00:00)</div><div class="line">root   pts/6        192.168.0.1    Wed May  8 18:30   still logged in</div><div class="line">root   pts/2        192.168.0.1    Wed May  8 18:14   still logged in</div><div class="line"></div><div class="line">wtmp begins Sun Dec 30 19:10:00 2018</div></pre></td></tr></table></figure>
<p><code>last</code>命令显示的对应信息如下：</p>
<p>用户名称，终端名称，远程主机名，日志活动发生时间，括号中的数字表示连接持续了多少小时和分钟。</p>
<h2 id="5-users命令"><a href="#5-users命令" class="headerlink" title="5. users命令"></a>5. users命令</h2><p><code>users</code>命令用于显示当前登录系统的所有用户列表。每个用户名对应一个登录会话。如果一个用户有不止一个登录会话，则用户名将显示相同的次数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@0b6987cc587f /workspace $ users</div><div class="line">root root</div></pre></td></tr></table></figure>
<h2 id="6-lastlog命令"><a href="#6-lastlog命令" class="headerlink" title="6. lastlog命令"></a>6. lastlog命令</h2><p><code>lastlog</code>命令用于显示系统中所有用户最近一次的登录信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">root@0b6987cc587f /workspace $ lastlog</div><div class="line">Username         Port     From             Latest</div><div class="line">root             pts/6    192.168.0.1      Sun Apr 28 18:38:20 +0800 2019</div><div class="line">daemon                                     **Never logged in**</div><div class="line">bin                                        **Never logged in**</div><div class="line">sys                                        **Never logged in**</div><div class="line">sync                                       **Never logged in**</div><div class="line">games                                      **Never logged in**</div><div class="line">man                                        **Never logged in**</div><div class="line">lp                                         **Never logged in**</div><div class="line">mail                                       **Never logged in**</div><div class="line">news                                       **Never logged in**</div><div class="line">uucp                                       **Never logged in**</div><div class="line">proxy                                      **Never logged in**</div><div class="line">www-data                                   **Never logged in**</div><div class="line">backup                                     **Never logged in**</div><div class="line">list                                       **Never logged in**</div><div class="line">irc                                        **Never logged in**</div><div class="line">gnats                                      **Never logged in**</div><div class="line">nobody                                     **Never logged in**</div><div class="line">systemd-timesync                           **Never logged in**</div><div class="line">systemd-network                            **Never logged in**</div><div class="line">systemd-resolve                            **Never logged in**</div><div class="line">systemd-bus-proxy                           **Never logged in**</div><div class="line">_apt                                       **Never logged in**</div><div class="line">sshd                                       **Never logged in**</div></pre></td></tr></table></figure>
<h2 id="7-ac命令"><a href="#7-ac命令" class="headerlink" title="7. ac命令"></a>7. ac命令</h2><p><code>ac</code>命令计算所有用户总的连接时间，默认单位是小时，基于<code>/var/log/wtmp</code>文件统计。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@0b6987cc587f /workspace $ ac</div><div class="line">total     4595.16</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://www.runoob.com/linux/linux-comm-w.html" target="_blank" rel="external">http://www.runoob.com/linux/linux-comm-w.html</a></li>
<li><a href="http://www.runoob.com/linux/linux-comm-who.html" target="_blank" rel="external">http://www.runoob.com/linux/linux-comm-who.html</a></li>
<li><a href="https://linux.cn/article-2437-1.html" target="_blank" rel="external">https://linux.cn/article-2437-1.html</a></li>
<li><a href="http://www.runoob.com/linux/linux-comm-last.html" target="_blank" rel="external">http://www.runoob.com/linux/linux-comm-last.html</a></li>
<li><a href="http://man.linuxde.net/users" target="_blank" rel="external">http://man.linuxde.net/users</a></li>
<li><a href="http://man.linuxde.net/lastlog" target="_blank" rel="external">http://man.linuxde.net/lastlog</a></li>
<li><a href="https://cnbin.github.io/blog/2015/06/26/linux-ac-ming-ling/" target="_blank" rel="external">https://cnbin.github.io/blog/2015/06/26/linux-ac-ming-ling/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Linux登录信息查询
    
    </summary>
    
      <category term="Linux" scheme="http://noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode 139. Word Break</title>
    <link href="http://noahsnail.com/2019/05/06/2019-05-06-Leetcode-139--Word-Break/"/>
    <id>http://noahsnail.com/2019/05/06/2019-05-06-Leetcode-139--Word-Break/</id>
    <published>2019-05-06T03:15:37.000Z</published>
    <updated>2019-05-13T07:00:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-Description"><a href="#1-Description" class="headerlink" title="1. Description"></a>1. Description</h2><p><img src="https://i.loli.net/2019/05/06/5ccfa5fc5a3e8.png" alt="Word Break"></p>
<h2 id="2-Solution"><a href="#2-Solution" class="headerlink" title="2. Solution"></a>2. Solution</h2><ul>
<li>Version 1</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Solution &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">wordBreak</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; wordDict)</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span>(s.size() == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; flags(s.size(), <span class="number">-1</span>);</div><div class="line">        <span class="built_in">unordered_set</span>&lt;<span class="built_in">string</span>&gt; dict(wordDict.begin(), wordDict.end());</div><div class="line">        <span class="keyword">return</span> split(s, dict, flags, <span class="number">0</span>);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">split</span><span class="params">(<span class="built_in">string</span>&amp; s, <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt;&amp; dict, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; flags, <span class="keyword">int</span> start)</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span>(start == s.size()) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">if</span>(flags[start] != <span class="number">-1</span>) &#123;</div><div class="line">            <span class="keyword">return</span> flags[start];</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = start + <span class="number">1</span>; i &lt;= s.size(); i++) &#123;</div><div class="line">            <span class="built_in">string</span> temp = s.substr(start, i - start);</div><div class="line">            <span class="keyword">if</span>(dict.find(temp) != dict.end() &amp;&amp; split(s, dict, flags, i)) &#123;</div><div class="line">                flags[start] = <span class="number">1</span>;</div><div class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        flags[start] = <span class="number">0</span>;</div><div class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<ul>
<li>Version 2</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Solution &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">wordBreak</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; wordDict)</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span>(s.size() == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt; dict(wordDict.begin(), wordDict.end());</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; flag(s.size() + <span class="number">1</span>, <span class="literal">false</span>);</div><div class="line">        flag[<span class="number">0</span>] = <span class="literal">true</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= s.size(); i++) &#123;</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++) &#123;</div><div class="line">                <span class="keyword">if</span>(flag[j] &amp;&amp; dict.find(s.substr(j, i - j)) != dict.end()) &#123;</div><div class="line">                    flag[i] = <span class="literal">true</span>;</div><div class="line">                    <span class="keyword">break</span>;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> flag[s.size()];</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<ul>
<li>Version 3</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Solution &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">wordBreak</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; wordDict)</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span>(s.size() == <span class="number">0</span>) &#123;</div><div class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">set</span>&lt;<span class="built_in">string</span>&gt; dict(wordDict.begin(), wordDict.end());</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; flag(s.size() + <span class="number">1</span>, <span class="literal">false</span>);</div><div class="line">        flag[<span class="number">0</span>] = <span class="literal">true</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= s.size(); i++) &#123;</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--) &#123;</div><div class="line">                <span class="keyword">if</span>(flag[j] &amp;&amp; dict.find(s.substr(j, i - j)) != dict.end()) &#123;</div><div class="line">                    flag[i] = <span class="literal">true</span>;</div><div class="line">                    <span class="keyword">break</span>;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> flag[s.size()];</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="https://leetcode.com/problems/word-break/" target="_blank" rel="external">https://leetcode.com/problems/word-break/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Leetcode 139. Word Break
    
    </summary>
    
      <category term="基础算法" scheme="http://noahsnail.com/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://noahsnail.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
