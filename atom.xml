<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://noahsnail.com/"/>
  <updated>2018-05-03T10:13:24.000Z</updated>
  <id>http://noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux的uptime命令</title>
    <link href="http://noahsnail.com/2018/05/03/2018-05-03-Linux%E7%9A%84uptime%E5%91%BD%E4%BB%A4/"/>
    <id>http://noahsnail.com/2018/05/03/2018-05-03-Linux的uptime命令/</id>
    <published>2018-05-03T09:46:00.000Z</published>
    <updated>2018-05-03T10:13:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-uptime命令介绍"><a href="#1-uptime命令介绍" class="headerlink" title="1. uptime命令介绍"></a>1. uptime命令介绍</h2><p><code>uptime</code>是Linux系统最基本的统计命令。<code>uptime</code>会提供一些我们要用到的不同基本信息：</p>
<ul>
<li>当前时间</li>
<li>系统运行的天数、小时数、分钟数</li>
<li>当前登录到系统上的用户数</li>
<li>一分钟、五分钟、十五分钟的平均负载</li>
</ul>
<h2 id="2-uptime用法"><a href="#2-uptime用法" class="headerlink" title="2. uptime用法"></a>2. uptime用法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ uptime</div><div class="line">18:12  up 70 days,  8:35, 14 users, load averages: 1.89 2.07 2.31</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Linux命令行与shell脚本编程</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Linux的uptime命令
    
    </summary>
    
      <category term="Linux" scheme="http://noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux的vmstat命令</title>
    <link href="http://noahsnail.com/2018/05/03/2018-05-03-Linux%E7%9A%84vmstat%E5%91%BD%E4%BB%A4/"/>
    <id>http://noahsnail.com/2018/05/03/2018-05-03-Linux的vmstat命令/</id>
    <published>2018-05-03T08:46:23.000Z</published>
    <updated>2018-05-03T10:42:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-vmstat命令介绍"><a href="#1-vmstat命令介绍" class="headerlink" title="1. vmstat命令介绍"></a>1. vmstat命令介绍</h2><p><code>vmstat</code>命令主要用来提取系统信息，其会生成一个详尽的系统内存和CPU使用情况报告。</p>
<h2 id="2-vmstat的符号含义介绍"><a href="#2-vmstat的符号含义介绍" class="headerlink" title="2. vmstat的符号含义介绍"></a>2. vmstat的符号含义介绍</h2><table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>r</td>
<td>等待CPU时间的进程数</td>
</tr>
<tr>
<td>b</td>
<td>处于不可中断休眠中的进程数</td>
</tr>
<tr>
<td>swpd</td>
<td>使用的虚拟内存总量（单位：MB）</td>
</tr>
<tr>
<td>free</td>
<td>空闲的物理内存总量（单位：MB）</td>
</tr>
<tr>
<td>buff</td>
<td>用作缓冲区的内存总量（单位：MB）</td>
</tr>
<tr>
<td>cache</td>
<td>用作高速缓存的内存总量（单位：MB）</td>
</tr>
<tr>
<td>si</td>
<td>从磁盘交换进来的内存总量（单位：MB）</td>
</tr>
<tr>
<td>so</td>
<td>交换到磁盘的内存总量（单位：MB）</td>
</tr>
<tr>
<td>bi</td>
<td>从块设备受到的块数</td>
</tr>
<tr>
<td>bo</td>
<td>发送给块设备的块数</td>
</tr>
<tr>
<td>in</td>
<td>每秒的CPU中断次数</td>
</tr>
<tr>
<td>cs</td>
<td>每秒的CPU上下文切换数</td>
</tr>
<tr>
<td>us</td>
<td>用于执行非内核代码的CPU时间所占的百分比</td>
</tr>
<tr>
<td>sy</td>
<td>用于执行内核代码的CPU时间所占的百分比</td>
</tr>
<tr>
<td>id</td>
<td>处于空闲状态的CPU时间所占的百分比</td>
</tr>
<tr>
<td>wa</td>
<td>用于等待I/O的CPU时间所占的百分比</td>
</tr>
<tr>
<td>st</td>
<td>虚拟机偷取的时间所占的百分比</td>
</tr>
</tbody>
</table>
<h2 id="3-vmstat用法"><a href="#3-vmstat用法" class="headerlink" title="3. vmstat用法"></a>3. vmstat用法</h2><p>第一次运行<code>vmstat</code>命令，它会显示自上次重启以来负载的平均负载值。用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ vmstat</div><div class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</div><div class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</div><div class="line"> 2  0   2508 144238112      0 98689096    0    0     3    13    0    0  2  6 92  0  0</div></pre></td></tr></table></figure>
<p><code>vmstat</code>工具通常使用两个数字参数来显示系统信息，第一个参数是采样时间间隔数，单位是秒，第二个参数是采样次数。用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ vmstat 2 5</div><div class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</div><div class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</div><div class="line">41  0   2508 143143008      0 98875408    0    0     3    13    0    0  2  6 92  0  0</div><div class="line"> 1  0   2508 143974560      0 98875736    0    0     0     0 37405 41706  8 26 66  0  0</div><div class="line"> 5  0   2508 143964080      0 98875184    0    0     0     0 41914 50578  9 30 61  0  0</div><div class="line">40  0   2508 143142720      0 98875656    0    0     0    92 40721 44492  8 28 64  0  0</div><div class="line"> 1  0   2508 143964080      0 98876032    0    0     0     0 35374 42967  8 23 69  0  0</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Linux命令行与shell脚本编程</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Linux的vmstat命令
    
    </summary>
    
      <category term="Linux" scheme="http://noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="http://noahsnail.com/2018/05/02/2018-05-02-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>http://noahsnail.com/2018/05/02/2018-05-02-正则表达式/</id>
    <published>2018-05-02T09:56:41.000Z</published>
    <updated>2018-05-03T01:43:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-正则表达式"><a href="#1-正则表达式" class="headerlink" title="1. 正则表达式"></a>1. 正则表达式</h2><p>正则表达式(Regular Expression)描述了一种字符串匹配模式，主要用来检索、替换匹配某种模式的字符串。</p>
<h2 id="2-语法"><a href="#2-语法" class="headerlink" title="2. 语法"></a>2. 语法</h2><ul>
<li>^</li>
</ul>
<p><code>^</code>表示字符串的开始，例：<code>^Th</code>表示以<code>Th</code>开头的字符串，可匹配<code>The</code>，<code>There</code>等。</p>
<ul>
<li>$</li>
</ul>
<p><code>$</code>表示字符串的结束，例：<code>ch$</code>表示以<code>ch</code>结尾的字符串，可匹配<code>match</code>，<code>watch</code>等。</p>
<ul>
<li><p>*</p>
</li>
<li><p>?</p>
</li>
<li><p>+</p>
</li>
<li><p>.</p>
</li>
<li><p>\d</p>
</li>
<li><p>\w</p>
</li>
<li></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;font color=blue&gt;内容&lt;/font&gt;</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://juejin.im/entry/59a651116fb9a024844938b5" target="_blank" rel="external">https://juejin.im/entry/59a651116fb9a024844938b5</a></p>
]]></content>
    
    <summary type="html">
    
      正则表达式
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Feature Pyramid Networks for Object Detection论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2018/03/20/2018-03-20-Feature%20Pyramid%20Networks%20for%20Object%20Detection%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2018/03/20/2018-03-20-Feature Pyramid Networks for Object Detection论文翻译——中英文对照/</id>
    <published>2018-03-20T10:30:55.000Z</published>
    <updated>2018-05-02T09:33:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Feature-Pyramid-Networks-for-Object-Detection"><a href="#Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="Feature Pyramid Networks for Object Detection"></a>Feature Pyramid Networks for Object Detection</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>特征金字塔是识别系统中用于检测不同尺度目标的基本组件。但最近的深度学习目标检测器已经避免了金字塔表示，部分原因是它们是计算和内存密集型的。在本文中，我们利用深度卷积网络内在的多尺度、金字塔分级来构造具有很少额外成本的特征金字塔。开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。此外，我们的方法可以在GPU上以6FPS运行，因此是多尺度目标检测的实用和准确的解决方案。代码将公开发布。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Recognizing objects at vastly different scales is a fundamental challenge in computer vision. Feature pyramids built upon image pyramids (for short we call these featurized image pyramids) form the basis of a standard solution [1] (Fig. 1(a)). These pyramids are scale-invariant in the sense that an object’s scale change is offset by shifting its level in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-79414c80444765b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>Figure 1. (a) Using an image pyramid to build a feature pyramid. Features are computed on each of the image scales independently, which is slow. (b) Recent detection systems have opted to use only single scale features for faster detection. (c) An alternative is to reuse the pyramidal feature hierarchy computed by a ConvNet as if it were a featurized image pyramid. (d) Our proposed Feature Pyramid Network (FPN) is fast like (b) and (c), but more accurate. In this figure, feature maps are indicate by blue outlines and thicker outlines denote semantically stronger features.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>识别不同尺度的目标是计算机视觉中的一个基本挑战。建立在图像金字塔之上的特征金字塔（我们简称为特征化图像金字塔）构成了标准解决方案的基础[1]（图1（a））。这些金字塔是尺度不变的，因为目标的尺度变化是通过在金字塔中移动它的层级来抵消的。直观地说，该属性使模型能够通过在位置和金字塔等级上扫描模型来检测大范围尺度内的目标。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-79414c80444765b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1。（a）使用图像金字塔构建特征金字塔。每个图像尺度上的特征都是独立计算的，速度很慢。（b）最近的检测系统选择只使用单一尺度特征进行更快的检测。（c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。（d）我们提出的特征金字塔网络（FPN）与（b）和（c）类似，但更准确。在该图中，特征映射用蓝色轮廓表示，较粗的轮廓表示语义上较强的特征。</p>
<p>Featurized image pyramids were heavily used in the era of hand-engineered features [5, 25]. They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (e.g., 10 scales per octave). For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) [19, 20]. Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)). But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on featurized image pyramids (e.g., [16, 35]). The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.</p>
<p>特征化图像金字塔在手工设计的时代被大量使用[5，25]。它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave含义参考SIFT特征）。对于识别任务，工程特征大部分已经被深度卷积网络（ConvNets）[19，20]计算的特征所取代。除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。但即使有这种鲁棒性，金字塔仍然需要得到最准确的结果。在ImageNet[33]和COCO[21]检测挑战中，最近的所有排名靠前的输入都使用了针对特征化图像金字塔的多尺度测试（例如[16，35]）。对图像金字塔的每个层次进行特征化的主要优势在于它产生了多尺度的特征表示，其中所有层次上在语义上都很强，包括高分辨率层。</p>
<p>Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times [11]), making this approach impractical for real applications. Moreover, training deep networks end-to-end on an image pyramid is infeasible in terms of memory, and so, if exploited, image pyramids are used only at test time [15, 11, 16, 35], which creates an inconsistency between train/test-time inference. For these reasons, Fast and Faster R-CNN [11, 29] opt to not use featurized image pyramids under default settings.</p>
<p>尽管如此，特征化图像金字塔的每个层次都具有明显的局限性。推断时间显著增加（例如，四倍[11]），使得这种方法在实际应用中不切实际。此外，在图像金字塔上端对端地训练深度网络在内存方面是不可行的，所以如果被采用，图像金字塔仅在测试时被使用[15，11，16，35]，这造成了训练/测试时推断的不一致性。出于这些原因，Fast和Faster R-CNN[11，29]选择在默认设置下不使用特征化图像金字塔。</p>
<p>However, image pyramids are not the only way to compute a multi-scale feature representation. A deep ConvNet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape. This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p>
<p>但是，图像金字塔并不是计算多尺度特征表示的唯一方法。深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。这种网内特征层级产生不同空间分辨率的特征映射，但引入了由不同深度引起的较大的语义差异。高分辨率映射具有损害其目标识别表示能力的低级特征。</p>
<p>The Single Shot Detector (SSD) [22] is one of the first attempts at using a ConvNet’s pyramidal feature hierarchy as if it were a featurized image pyramid (Fig. 1(c)). Ideally, the SSD-style pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost. But to avoid using low-level features SSD foregoes reusing already computed layers and instead builds the pyramid starting from high up in the network (e.g., conv4_3 of VGG nets [36]) and then by adding several new layers. Thus it misses the opportunity to reuse the higher-resolution maps of the feature hierarchy. We show that these are important for detecting small objects.</p>
<p>单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。理想情况下，SSD风格的金字塔将重用正向传递中从不同层中计算的多尺度特征映射，因此是零成本的。但为了避免使用低级特征，SSD放弃重用已经计算好的图层，而从网络中的最高层开始构建金字塔（例如，VGG网络的conv4_3[36]），然后添加几个新层。因此它错过了重用特征层级的更高分辨率映射的机会。我们证明这些对于检测小目标很重要。</p>
<p>The goal of this paper is to naturally leverage the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales. To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections (Fig. 1(d)). The result is a feature pyramid that has rich semantics at all levels and is built quickly from a single input image scale. In other words, we show how to create in-network feature pyramids that can be used to replace featurized image pyramids without sacrificing representational power, speed, or memory.</p>
<p>本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。为了实现这个目标，我们依赖于一种结构，它将低分辨率，具有高分辨率的强大语义特征，语义上的弱特征通过自顶向下的路径和横向连接相结合（图1（d））。其结果是一个特征金字塔，在所有级别都具有丰富的语义，并且可以从单个输入图像尺度上进行快速构建。换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</p>
<p>Similar architectures adopting top-down and skip connections are popular in recent research [28, 17, 8, 26]. Their goals are to produce a single high-level feature map of a fine resolution on which the predictions are to be made (Fig. 2 top). On the contrary, our method leverages the architecture as a feature pyramid where predictions (e.g., object detections) are independently made on each level (Fig. 2 bottom). Our model echoes a featurized image pyramid, which has not been explored in these works.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e1ee021eac1b88be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>Figure 2. Top: a top-down architecture with skip connections, where predictions are made on the finest level (e.g., [28]). Bottom: our model that has a similar structure but leverages it as a feature pyramid, with predictions made independently at all levels.</p>
<p>最近的研究[28，17，8，26]中流行采用自顶向下和跳跃连接的类似架构。他们的目标是生成具有高分辨率的单个高级特征映射，并在其上进行预测（图2顶部）。相反，我们的方法利用这个架构作为特征金字塔，其中预测（例如目标检测）在每个级别上独立进行（图2底部）。我们的模型反映了一个特征化的图像金字塔，这在这些研究中还没有探索过。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e1ee021eac1b88be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2。顶部：带有跳跃连接的自顶向下的架构，在最好的级别上进行预测（例如，[28]）。底部：我们的模型具有类似的结构，但将其用作特征金字塔，并在各个层级上独立进行预测。</p>
<p>We evaluate our method, called a Feature Pyramid Network (FPN), in various systems for detection and segmentation [11, 29, 27]. Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on FPN and a basic Faster R-CNN detector [29], surpassing all existing heavily-engineered single-model entries of competition winners. In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16]. Our method is also easily extended to mask proposals and improves both instance segmentation AR and speed over state-of-the-art methods that heavily depend on image pyramids.</p>
<p>我们评估了我们称为特征金字塔网络（FPN）的方法，其在各种系统中用于检测和分割[11，29，27]。没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。我们的方法也很容易扩展掩模提议，改进实例分隔AR，加速严重依赖图像金字塔的最先进方法。</p>
<p>In addition, our pyramid structure can be trained end-to-end with all scales and is used consistently at train/test time, which would be memory-infeasible using image pyramids. As a result, FPNs are able to achieve higher accuracy than all existing state-of-the-art methods. Moreover, this improvement is achieved without increasing testing time over the single-scale baseline. We believe these advances will facilitate future research and applications. Our code will be made publicly available.</p>
<p>另外，我们的金字塔结构可以通过所有尺度进行端对端培训，并且在训练/测试时一致地使用，这在使用图像金字塔时是内存不可行的。因此，FPN能够比所有现有的最先进方法获得更高的准确度。此外，这种改进是在不增加单尺度基准测试时间的情况下实现的。我们相信这些进展将有助于未来的研究和应用。我们的代码将公开发布。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Hand-engineered features and early neural networks.</strong> SIFT features [25] were originally extracted at scale-space extrema and used for feature point matching. HOG features [5], and later SIFT features as well, were computed densely over entire image pyramids. These HOG and SIFT pyramids have been used in numerous works for image classification, object detection, human pose estimation, and more. There has also been significant interest in computing featurized image pyramids quickly. Dollar et al.[6] demonstrated fast pyramid computation by first computing a sparsely sampled (in scale) pyramid and then interpolating missing levels. Before HOG and SIFT, early work on face detection with ConvNets [38, 32] computed shallow networks over image pyramids to detect faces across scales.</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>手工设计特征和早期神经网络</strong>。SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。HOG特征[5]，以及后来的SIFT特征，都是在整个图像金字塔上密集计算的。这些HOG和SIFT金字塔已在许多工作中得到了应用，用于图像分类，目标检测，人体姿势估计等。这对快速计算特征化图像金字塔也很有意义。Dollar等人[6]通过先计算一个稀疏采样（尺度）金字塔，然后插入缺失的层级，从而演示了快速金字塔计算。在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</p>
<p><strong>Deep ConvNet object detectors</strong>. With the development of modern deep ConvNets [19], object detectors like OverFeat [34] and R-CNN [12] showed dramatic improvements in accuracy. OverFeat adopted a strategy similar to early neural network face detectors by applying a ConvNet as a sliding window detector on an image pyramid. R-CNN adopted a region proposal-based strategy [37] in which each proposal was scale-normalized before classifying with a ConvNet. SPPnet [15] demonstrated that such region-based detectors could be applied much more efficiently on feature maps extracted on a single image scale. Recent and more accurate detection methods like Fast R-CNN [11] and Faster R-CNN [29] advocate using features computed from a single scale, because it offers a good trade-off between accuracy and speed. Multi-scale detection, however, still performs better, especially for small objects.</p>
<p><strong>Deep ConvNet目标检测器</strong>。随着现代深度卷积网络[19]的发展，像OverFeat[34]和R-CNN[12]这样的目标检测器在精度上显示出了显著的提高。OverFeat采用了一种类似于早期神经网络人脸检测器的策略，通过在图像金字塔上应用ConvNet作为滑动窗口检测器。R-CNN采用了基于区域提议的策略[37]，其中每个提议在用ConvNet进行分类之前都进行了尺度归一化。SPPnet[15]表明，这种基于区域的检测器可以更有效地应用于在单个图像尺度上提取的特征映射。最近更准确的检测方法，如Fast R-CNN[11]和Faster R-CNN[29]提倡使用从单一尺度计算出的特征，因为它提供了精确度和速度之间的良好折衷。然而，多尺度检测性能仍然更好，特别是对于小型目标。</p>
<p><strong>Methods using multiple layers</strong>. A number of recent approaches improve detection and segmentation by using different layers in a ConvNet. FCN [24] sums partial scores for each category over multiple scales to compute semantic segmentations. Hypercolumns [13] uses a similar method for object instance segmentation. Several other approaches (HyperNet [18], ParseNet [23], and ION [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features. SSD [22] and MS-CNN [3] predict objects at multiple layers of the feature hierarchy without combining features or scores.</p>
<p><strong>使用多层的方法</strong>。一些最近的方法通过使用ConvNet中的不同层来改进检测和分割。FCN[24]将多个尺度上的每个类别的部分分数相加以计算语义分割。Hypercolumns[13]使用类似的方法进行目标实例分割。在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。SSD[22]和MS-CNN[3]可预测特征层级中多个层的目标，而不需要组合特征或分数。</p>
<p>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation. Ghiasi et al. [8] present a Laplacian pyramid presentation for FCNs to progressively refine segmentation. Although these methods adopt architectures with pyramidal shapes, they are unlike featurized image pyramids [5, 7, 34] where predictions are made independently at all levels, see Fig. 2. In fact, for the pyramidal architecture in Fig. 2 (top), image pyramids are still needed to recognize objects across multiple scales [28].</p>
<p>最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。尽管这些方法采用的是金字塔形状的架构，但它们不同于特征化的图像金字塔[5，7，34]，其中所有层次上的预测都是独立进行的，参见图2。事实上，对于图2（顶部）中的金字塔结构，图像金字塔仍然需要跨多个尺度上识别目标[28]。</p>
<h2 id="3-Feature-Pyramid-Networks"><a href="#3-Feature-Pyramid-Networks" class="headerlink" title="3. Feature Pyramid Networks"></a>3. Feature Pyramid Networks</h2><p>Our goal is to leverage a ConvNet’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout. The resulting Feature Pyramid Network is general-purpose and in this paper we focus on sliding window proposers (Region Proposal Network, RPN for short) [29] and region-based detectors (Fast R-CNN) [11]. We also generalize FPNs to instance segmentation proposals in Sec.6.</p>
<h2 id="3-特征金字塔网络"><a href="#3-特征金字塔网络" class="headerlink" title="3. 特征金字塔网络"></a>3. 特征金字塔网络</h2><p>我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。在第6节中我们还将FPN泛化到实例细分提议。</p>
<p>Our method takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures (e.g., [19, 36, 16]), and in this paper we present results using ResNets [16]. The construction of our pyramid involves a bottom-up pathway, a top-down pathway, and lateral connections, as introduced in the following.</p>
<p>我们的方法以任意大小的单尺度图像作为输入，并以全卷积的方式输出多层适当大小的特征映射。这个过程独立于主卷积体系结构（例如[19，36，16]），在本文中，我们呈现了使用ResNets[16]的结果。如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</p>
<p><strong>Bottom-up pathway</strong>. The bottom-up pathway is the feed-forward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. There are often many layers producing output maps of the same size and we say these layers are in the same network stage. For our feature pyramid, we define one pyramid level for each stage. We choose the output of the last layer of each stage as our reference set of feature maps, which we will enrich to create our pyramid. This choice is natural since the deepest layer of each stage should have the strongest features.</p>
<p><strong>自下而上的路径</strong>。自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。通常有许多层产生相同大小的输出映射，并且我们认为这些层位于相同的网络阶段。对于我们的特征金字塔，我们为每个阶段定义一个金字塔层。我们选择每个阶段的最后一层的输出作为我们的特征映射参考集，我们将丰富它来创建我们的金字塔。这种选择是自然的，因为每个阶段的最深层应具有最强大的特征。</p>
<p>Specifically, for ResNets [16] we use the feature activations output by each stage’s last residual block. We denote the output of these last residual blocks as $\lbrace C_2 , C_3 , C_4 , C_5 \rbrace$ for conv2, conv3, conv4, and conv5 outputs, and note that they have strides of {4, 8, 16, 32} pixels with respect to the input image. We do not include conv1 into the pyramid due to its large memory footprint.</p>
<p>具体而言，对于ResNets[16]，我们使用每个阶段的最后一个残差块输出的特征激活。对于conv2，conv3，conv4和conv5输出，我们将这些最后残差块的输出表示为$\lbrace C_2, C_3, C_4, C_5 \rbrace$，并注意相对于输入图像它们的步长为{4，8，16，32}个像素。由于其庞大的内存占用，我们不会将conv1纳入金字塔。</p>
<p><strong>Top-down pathway and lateral connections</strong>. The top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.</p>
<p><strong>自顶向下的路径和横向连接</strong>。自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。这些特征随后通过来自自下而上路径上的特征经由横向连接进行增强。每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</p>
<p>Fig. 3 shows the building block that constructs our top-down feature maps. With a coarser-resolution feature map, we upsample the spatial resolution by a factor of 2 (using nearest neighbor upsampling for simplicity). The upsampled map is then merged with the corresponding bottom-up map (which undergoes a 1×1 convolutional layer to reduce channel dimensions) by element-wise addition. This process is iterated until the finest resolution map is generated. To start the iteration, we simply attach a 1×1 convolutional layer on $C_5$ to produce the coarsest resolution map. Finally, we append a 3 × 3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling. This final set of feature maps is called $\lbrace P_2 , P_3 , P_4 , P_5 \rbrace$, corresponding to $\lbrace C_2, C_3, C_4, C_5 \rbrace$ that are respectively of the same spatial sizes.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a51d06a2a94bfec4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>Figure 3. A building block illustrating the lateral connection and the top-down pathway, merged by addition.</p>
<p>图3显示了建造我们的自顶向下特征映射的构建块。使用较粗糙分辨率的特征映射，我们将空间分辨率上采样为2倍（为了简单起见，使用最近邻上采样）。然后通过按元素相加，将上采样映射与相应的自下而上映射（其经过1×1卷积层来减少通道维度）合并。迭代这个过程，直到生成最佳分辨率映射。为了开始迭代，我们只需在$C_5$上添加一个1×1卷积层来生成最粗糙分辨率映射。最后，我们在每个合并的映射上添加一个3×3卷积来生成最终的特征映射，这是为了减少上采样的混叠效应。这个最终的特征映射集称为$\lbrace P_2 , P_3 , P_4 , P_5 \rbrace$，对应于$\lbrace C_2, C_3, C_4, C_5 \rbrace$，分别具有相同的空间大小。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a51d06a2a94bfec4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3。构建模块说明了横向连接和自顶向下路径，通过加法合并。</p>
<p>Because all levels of the pyramid use shared classifiers/regressors as in a traditional featurized image pyramid, we fix the feature dimension (numbers of channels, denoted as $d$) in all the feature maps. We set $d=256$ in this paper and thus all extra convolutional layers have 256-channel outputs. There are no non-linearities in these extra layers, which we have empirically found to have minor impacts.</p>
<p>由于金字塔的所有层都像传统的特征图像金字塔一样使用共享分类器/回归器，因此我们在所有特征映射中固定特征维度（通道数记为$d$）。我们在本文中设置$d=256$，因此所有额外的卷积层都有256个通道的输出。在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</p>
<p>Simplicity is central to our design and we have found that our model is robust to many design choices. We have experimented with more sophisticated blocks (e.g., using multi-layer residual blocks [16] as the connections) and observed marginally better results. Designing better connection modules is not the focus of this paper, so we opt for the simple design described above.</p>
<p>简洁性是我们设计的核心，我们发现我们的模型对许多设计选择都很鲁棒。我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。设计更好的连接模块并不是本文的重点，所以我们选择上述的简单设计。</p>
<h2 id="4-Applications"><a href="#4-Applications" class="headerlink" title="4. Applications"></a>4. Applications</h2><p>Our method is a generic solution for building feature pyramids inside deep ConvNets. In the following we adopt our method in RPN [29] for bounding box proposal generation and in Fast R-CNN [11] for object detection. To demonstrate the simplicity and effectiveness of our method, we make minimal modifications to the original systems of [29, 11] when adapting them to our feature pyramid.</p>
<h2 id="4-应用"><a href="#4-应用" class="headerlink" title="4. 应用"></a>4. 应用</h2><p>我们的方法是在深度ConvNets内部构建特征金字塔的通用解决方案。在下面，我们采用我们的方法在RPN[29]中进行边界框提议生成，并在Fast R-CNN[11]中进行目标检测。为了证明我们方法的简洁性和有效性，我们对[29，11]的原始系统进行最小修改，使其适应我们的特征金字塔。</p>
<h3 id="4-1-Feature-Pyramid-Networks-for-RPN"><a href="#4-1-Feature-Pyramid-Networks-for-RPN" class="headerlink" title="4.1. Feature Pyramid Networks for RPN"></a>4.1. Feature Pyramid Networks for RPN</h3><p>RPN [29] is a sliding-window class-agnostic object detector. In the original RPN design, a small subnetwork is evaluated on dense 3×3 sliding windows, on top of a single-scale convolutional feature map, performing object/non-object binary classification and bounding box regression. This is realized by a 3×3 convolutional layer followed by two sibling 1×1 convolutions for classification and regression, which we refer to as a network head. The object/non-object criterion and bounding box regression target are defined with respect to a set of reference boxes called anchors [29]. The anchors are of multiple pre-defined scales and aspect ratios in order to cover objects of different shapes.</p>
<h3 id="4-1-RPN的特征金字塔网络"><a href="#4-1-RPN的特征金字塔网络" class="headerlink" title="4.1. RPN的特征金字塔网络"></a>4.1. RPN的特征金字塔网络</h3><p>RPN[29]是一个滑动窗口类不可知的目标检测器。在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。这是通过一个3×3的卷积层实现的，后面跟着两个用于分类和回归的1×1兄弟卷积，我们称之为网络头部。目标/非目标标准和边界框回归目标的定义是关于一组称为锚点的参考框的[29]。这些锚点具有多个预定义的尺度和长宽比，以覆盖不同形状的目标。</p>
<p>We adapt RPN by replacing the single-scale feature map with our FPN. We attach a head of the same design (3×3 conv and two sibling 1×1 convs) to each level on our feature pyramid. Because the head slides densely over all locations in all pyramid levels, it is not necessary to have multi-scale anchors on a specific level. Instead, we assign anchors of a single scale to each level. Formally, we define the anchors to have areas of $\lbrace 32^2 , 64^2 , 128^2 , 256^2 , 512^2 \rbrace$ pixels on $\lbrace P_2, P_3, P_4, P_5, P_6 \rbrace$ respectively. As in [29] we also use anchors of multiple aspect ratios $\lbrace 1:2, 1:1, 2:1 \rbrace$ at each level. So in total there are 15 anchors over the pyramid.</p>
<p>我们通过用我们的FPN替换单尺度特征映射来适应RPN。我们在我们的特征金字塔的每个层级上附加一个相同设计的头部（3x3 conv和两个1x1兄弟convs）。由于头部在所有金字塔等级上的所有位置密集滑动，所以不需要在特定层级上具有多尺度锚点。相反，我们为每个层级分配单尺度的锚点。在形式上，我们定义锚点$\lbrace P_2, P_3, P_4, P_5, P_6 \rbrace$分别具有$\lbrace 32^2 , 64^2 , 128^2 , 256^2 , 512^2 \rbrace$个像素的面积。正如在[29]中，我们在每个层级上也使用了多个长宽比$\lbrace 1:2, 1:1, 2:1 \rbrace$的锚点。所以在金字塔上总共有十五个锚点。</p>
<p>We assign training labels to the anchors based on their Intersection-over-Union (IoU) ratios with ground-truth bounding boxes as in [29]. Formally, an anchor is assigned a positive label if it has the highest IoU for a given ground-truth box or an IoU over 0.7 with any ground-truth box, and a negative label if it has IoU lower than 0.3 for all ground-truth boxes. Note that scales of ground-truth boxes are not explicitly used to assign them to the levels of the pyramid; instead, ground-truth boxes are associated with anchors, which have been assigned to pyramid levels. As such, we introduce no extra rules in addition to those in [29].</p>
<p>如[29]，我们根据锚点和实际边界框的交并比（IoU）比例将训练标签分配给锚点。形式上，如果一个锚点对于一个给定的实际边界框具有最高的IoU或者与任何实际边界框的IoU超过0.7，则给其分配一个正标签，如果其与所有实际边界框的IoU都低于0.3，则为其分配一个负标签。请注意，实际边界框的尺度并未明确用于将它们分配到金字塔的层级；相反，实际边界框与已经分配给金字塔等级的锚点相关联。因此，除了[29]中的内容外，我们不引入额外的规则。</p>
<p>We note that the parameters of the heads are shared across all feature pyramid levels; we have also evaluated the alternative without sharing parameters and observed similar accuracy. The good performance of sharing parameters indicates that all levels of our pyramid share similar semantic levels. This advantage is analogous to that of using a featurized image pyramid, where a common head classifier can be applied to features computed at any image scale.</p>
<p>我们注意到头部的参数在所有特征金字塔层级上共享；我们也评估了替代方案，没有共享参数并且观察到相似的准确性。共享参数的良好性能表明我们的金字塔的所有层级共享相似的语义级别。这个优点类似于使用特征图像金字塔的优点，其中可以将常见头部分类器应用于在任何图像尺度下计算的特征。</p>
<p>With the above adaptations, RPN can be naturally trained and tested with our FPN, in the same fashion as in [29]. We elaborate on the implementation details in the experiments.</p>
<p>通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。我们在实验中详细说明实施细节。</p>
<h3 id="4-2-Feature-Pyramid-Networks-for-Fast-R-CNN"><a href="#4-2-Feature-Pyramid-Networks-for-Fast-R-CNN" class="headerlink" title="4.2. Feature Pyramid Networks for Fast R-CNN"></a>4.2. Feature Pyramid Networks for Fast R-CNN</h3><p>Fast R-CNN [11] is a region-based object detector in which Region-of-Interest (RoI) pooling is used to extract features. Fast R-CNN is most commonly performed on a single-scale feature map. To use it with our FPN, we need to assign RoIs of different scales to the pyramid levels.</p>
<p>We view our feature pyramid as if it were produced from an image pyramid. Thus we can adapt the assignment strategy of region-based detectors [15, 11] in the case when they are run on image pyramids. Formally, we assign an RoI of width $w$ and height $h$ (on the input image to the network) to the level $P_k$ of our feature pyramid by:</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 1984.</p>
<p>[2] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016.</p>
<p>[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection. In ECCV, 2016.</p>
<p>[4] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive fully convolutional networks. In ECCV, 2016.</p>
<p>[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.</p>
<p>[6] P. Dollar, R. Appel, S. Belongie, and P. Perona. Fast feature pyramids for object detection. TPAMI, 2014.</p>
<p>[7] P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 2010.</p>
<p>[8] G.GhiasiandC.C.Fowlkes.Laplacianpyramidreconstruction and refinement for semantic segmentation. In ECCV, 2016.</p>
<p>[9] S. Gidaris and N. Komodakis. Object detection via a multi-region &amp; semantic segmentation-aware CNN model. In ICCV, 2015.</p>
<p>[10] S. Gidaris and N. Komodakis. Attend refine repeat: Active box proposal generation via in-out localization. In BMVC, 2016.</p>
<p>[11] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.</p>
<p>[13] B.Hariharan,P.Arbelaez,R.Girshick,andJ.Malik.Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015.</p>
<p>[14] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn. arXiv:1703.06870, 2017.</p>
<p>[15] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.</p>
<p>[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>[17] S. Honari, J. Yosinski, P. Vincent, and C. Pal. Recombinator networks: Learning coarse-to-fine feature aggregation. In CVPR, 2016.</p>
<p>[18] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards accurate region proposal generation and joint object detection. In CVPR, 2016.</p>
<p>[19] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.</p>
<p>[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[22] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot multibox detector. In ECCV, 2016.</p>
<p>[23] W. Liu, A. Rabinovich, and A. C. Berg. ParseNet: Looking wider to see better. In ICLR workshop, 2016.</p>
<p>[24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[25] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.</p>
<p>[26] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In ECCV, 2016.</p>
<p>[27] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to segment object candidates. In NIPS, 2015.</p>
<p>[28] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár. Learning to refine object segments. In ECCV, 2016.</p>
<p>[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. PAMI, 2016.</p>
<p>[31] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image segmentation. In MIC- CAI, 2015.</p>
<p>[32] H. Rowley, S. Baluja, and T. Kanade. Human face detection in visual scenes. Technical Report CMU-CS-95-158R, Carnegie Mellon University, 1995.</p>
<p>[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.</p>
<p>[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[35] A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object detectors with online hard example mining. In CVPR, 2016.</p>
<p>[36] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[37] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.</p>
<p>[38] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE Proc. on Vision, Image, and Signal Processing, 1994.</p>
<p>[39] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC, 2016.</p>
<p>[40] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chintala, and P. Dollár. A multipath network for object detection. In BMVC, 2016. 10</p>
]]></content>
    
    <summary type="html">
    
      Feature Pyramid Networks for Object Detection论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Feature Pyramid Networks for Object Detection论文翻译——中文版</title>
    <link href="http://noahsnail.com/2018/03/20/2018-03-20-Feature%20Pyramid%20Networks%20for%20Object%20Detection%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2018/03/20/2018-03-20-Feature Pyramid Networks for Object Detection论文翻译——中文版/</id>
    <published>2018-03-20T10:30:15.000Z</published>
    <updated>2018-04-25T10:47:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Feature-Pyramid-Networks-for-Object-Detection"><a href="#Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="Feature Pyramid Networks for Object Detection"></a>Feature Pyramid Networks for Object Detection</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>特征金字塔是识别系统中用于检测不同尺度目标的基本组件。但最近的深度学习目标检测器已经避免了金字塔表示，部分原因是它们是计算和内存密集型的。在本文中，我们利用深度卷积网络内在的多尺度、金字塔分级来构造具有很少额外成本的特征金字塔。开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。此外，我们的方法可以在GPU上以6FPS运行，因此是多尺度目标检测的实用和准确的解决方案。代码将公开发布。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>识别不同尺度的目标是计算机视觉中的一个基本挑战。建立在图像金字塔之上的特征金字塔（我们简称为特征化图像金字塔）构成了标准解决方案的基础[1]（图1（a））。这些金字塔是尺度不变的，因为目标的尺度变化是通过在金字塔中移动它的层级来抵消的。直观地说，该属性使模型能够通过在位置和金字塔等级上扫描模型来检测大范围尺度内的目标。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-79414c80444765b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1。（a）使用图像金字塔构建特征金字塔。每个图像尺度上的特征都是独立计算的，速度很慢。（b）最近的检测系统选择只使用单一尺度特征进行更快的检测。（c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。（d）我们提出的特征金字塔网络（FPN）与（b）和（c）类似，但更准确。在该图中，特征映射用蓝色轮廓表示，较粗的轮廓表示语义上较强的特征。</p>
<p>特征化图像金字塔在手工设计的时代被大量使用[5，25]。它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave参考SIFT特征）。对于识别任务，工程特征大部分已经被深度卷积网络（ConvNets）[19，20]计算的特征所取代。除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。但即使有这种鲁棒性，金字塔仍然需要得到最准确的结果。在ImageNet[33]和COCO[21]检测挑战中，最近的所有排名靠前的输入都使用了针对特征化图像金字塔的多尺度测试（例如[16，35]）。对图像金字塔的每个层次进行特征化的主要优势在于它产生了多尺度的特征表示，其中所有层次上在语义上都很强，包括高分辨率层。</p>
<p>尽管如此，特征化图像金字塔的每个层次都具有明显的局限性。推断时间显著增加（例如，四倍[11]），使得这种方法在实际应用中不切实际。此外，在图像金字塔上端对端地训练深度网络在内存方面是不可行的，所以如果被采用，图像金字塔仅在测试时被使用[15，11，16，35]，这造成了训练/测试时推断的不一致性。出于这些原因，Fast和Faster R-CNN[11，29]选择在默认设置下不使用特征化图像金字塔。</p>
<p>但是，图像金字塔并不是计算多尺度特征表示的唯一方法。深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。这种网内特征层级产生不同空间分辨率的特征映射，但引入了由不同深度引起的较大的语义差异。高分辨率映射具有损害其目标识别表示能力的低级特征。</p>
<p>单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。理想情况下，SSD风格的金字塔将重用正向传递中从不同层中计算的多尺度特征映射，因此是零成本的。但为了避免使用低级特征，SSD放弃重用已经计算好的图层，而从网络中的最高层开始构建金字塔（例如，VGG网络的conv4_3[36]），然后添加几个新层。因此它错过了重用特征层级的更高分辨率映射的机会。我们证明这些对于检测小目标很重要。</p>
<p>本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。为了实现这个目标，我们依赖于一种结构，它将低分辨率，具有高分辨率的强大语义特征，语义上的弱特征通过自顶向下的路径和横向连接相结合（图1（d））。其结果是一个特征金字塔，在所有级别都具有丰富的语义，并且可以从单个输入图像尺度上进行快速构建。换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</p>
<p>最近的研究[28，17，8，26]中流行采用自顶向下和跳跃连接的类似架构。他们的目标是生成具有高分辨率的单个高级特征映射，并在其上进行预测（图2顶部）。相反，我们的方法利用这个架构作为特征金字塔，其中预测（例如目标检测）在每个级别上独立进行（图2底部）。我们的模型反映了一个特征化的图像金字塔，这在这些研究中还没有探索过。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e1ee021eac1b88be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2。顶部：带有跳跃连接的自顶向下的架构，在最好的级别上进行预测（例如，[28]）。底部：我们的模型具有类似的结构，但将其用作特征金字塔，并在各个层级上独立进行预测。</p>
<p>我们评估了我们称为特征金字塔网络（FPN）的方法，其在各种系统中用于检测和分割[11，29，27]。没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。我们的方法也很容易扩展掩模提议，改进实例分隔AR，加速严重依赖图像金字塔的最先进方法。</p>
<p>另外，我们的金字塔结构可以通过所有尺度进行端对端培训，并且在训练/测试时一致地使用，这在使用图像金字塔时是内存不可行的。因此，FPN能够比所有现有的最先进方法获得更高的准确度。此外，这种改进是在不增加单尺度基准测试时间的情况下实现的。我们相信这些进展将有助于未来的研究和应用。我们的代码将公开发布。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>手工设计特征和早期神经网络</strong>。SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。HOG特征[5]，以及后来的SIFT特征，都是在整个图像金字塔上密集计算的。这些HOG和SIFT金字塔已在许多工作中得到了应用，用于图像分类，目标检测，人体姿势估计等。这对快速计算特征化图像金字塔也很有意义。Dollar等人[6]通过先计算一个稀疏采样（尺度）金字塔，然后插入缺失的层级，从而演示了快速金字塔计算。在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</p>
<p><strong>Deep ConvNet目标检测器</strong>。随着现代深度卷积网络[19]的发展，像OverFeat[34]和R-CNN[12]这样的目标检测器在精度上显示出了显著的提高。OverFeat采用了一种类似于早期神经网络人脸检测器的策略，通过在图像金字塔上应用ConvNet作为滑动窗口检测器。R-CNN采用了基于区域提议的策略[37]，其中每个提议在用ConvNet进行分类之前都进行了尺度归一化。SPPnet[15]表明，这种基于区域的检测器可以更有效地应用于在单个图像尺度上提取的特征映射。最近更准确的检测方法，如Fast R-CNN[11]和Faster R-CNN[29]提倡使用从单一尺度计算出的特征，因为它提供了精确度和速度之间的良好折衷。然而，多尺度检测性能仍然更好，特别是对于小型目标。</p>
<p><strong>使用多层的方法</strong>。一些最近的方法通过使用ConvNet中的不同层来改进检测和分割。FCN[24]将多个尺度上的每个类别的部分分数相加以计算语义分割。Hypercolumns[13]使用类似的方法进行目标实例分割。在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。SSD[22]和MS-CNN[3]可预测特征层级中多个层的目标，而不需要组合特征或分数。</p>
<p>最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。尽管这些方法采用的是金字塔形状的架构，但它们不同于特征化的图像金字塔[5，7，34]，其中所有层次上的预测都是独立进行的，参见图2。事实上，对于图2（顶部）中的金字塔结构，图像金字塔仍然需要跨多个尺度上识别目标[28]。</p>
<h2 id="3-特征金字塔网络"><a href="#3-特征金字塔网络" class="headerlink" title="3. 特征金字塔网络"></a>3. 特征金字塔网络</h2><p>我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。在第6节中我们还将FPN泛化到实例细分提议。</p>
<p>我们的方法以任意大小的单尺度图像作为输入，并以全卷积的方式输出多层适当大小的特征映射。这个过程独立于主卷积体系结构（例如[19，36，16]），在本文中，我们呈现了使用ResNets[16]的结果。如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</p>
<p><strong>自下而上的路径</strong>。自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。通常有许多层产生相同大小的输出映射，并且我们认为这些层位于相同的网络阶段。对于我们的特征金字塔，我们为每个阶段定义一个金字塔层。我们选择每个阶段的最后一层的输出作为我们的特征映射参考集，我们将丰富它来创建我们的金字塔。这种选择是自然的，因为每个阶段的最深层应具有最强大的特征。</p>
<p>具体而言，对于ResNets[16]，我们使用每个阶段的最后一个残差块输出的特征激活。对于conv2，conv3，conv4和conv5输出，我们将这些最后残差块的输出表示为$\lbrace C_2 , C_3 , C_4 , C_5 \rbrace$，并注意相对于输入图像它们的步长为{4，8，16，32}个像素。由于其庞大的内存占用，我们不会将conv1纳入金字塔。</p>
<p><strong>自顶向下的路径和横向连接</strong>。自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。这些特征随后通过来自自下而上路径上的特征经由横向连接进行增强。每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</p>
<p>图3显示了建造我们的自顶向下特征映射的构建块。使用较粗糙分辨率的特征映射，我们将空间分辨率上采样为2倍（为了简单起见，使用最近邻上采样）。然后通过按元素相加，将上采样映射与相应的自下而上映射（其经过1×1卷积层来减少通道维度）合并。迭代这个过程，直到生成最佳分辨率映射。为了开始迭代，我们只需在$C_5$上添加一个1×1卷积层来生成最粗糙分辨率映射。最后，我们在每个合并的映射上添加一个3×3卷积来生成最终的特征映射，这是为了减少上采样的混叠效应。这个最终的特征映射集称为$\lbrace P_2 , P_3 , P_4 , P_5 \rbrace$，对应于$\lbrace C_2, C_3, C_4, C_5 \rbrace$，分别具有相同的空间大小。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a51d06a2a94bfec4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3。构建模块说明了横向连接和自顶向下路径，通过加法合并。</p>
<p>由于金字塔的所有层都像传统的特征图像金字塔一样使用共享分类器/回归器，因此我们在所有特征映射中固定特征维度（通道数记为$d$）。我们在本文中设置$d=256$，因此所有额外的卷积层都有256个通道的输出。在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</p>
<p>简洁性是我们设计的核心，我们发现我们的模型对许多设计选择都很鲁棒。我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。设计更好的连接模块并不是本文的重点，所以我们选择上述的简单设计。</p>
<h2 id="4-应用"><a href="#4-应用" class="headerlink" title="4. 应用"></a>4. 应用</h2><p>我们的方法是在深度ConvNets内部构建特征金字塔的通用解决方案。在下面，我们采用我们的方法在RPN[29]中进行边界框提议生成，并在Fast R-CNN[11]中进行目标检测。为了证明我们方法的简洁性和有效性，我们对[29，11]的原始系统进行最小修改，使其适应我们的特征金字塔。</p>
<h3 id="4-1-RPN的特征金字塔网络"><a href="#4-1-RPN的特征金字塔网络" class="headerlink" title="4.1. RPN的特征金字塔网络"></a>4.1. RPN的特征金字塔网络</h3><p>RPN[29]是一个滑动窗口类不可知的目标检测器。在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。这是通过一个3×3的卷积层实现的，后面跟着两个用于分类和回归的1×1兄弟卷积，我们称之为网络头部。目标/非目标标准和边界框回归目标的定义是关于一组称为锚点的参考框的[29]。这些锚点具有多个预定义的尺度和长宽比，以覆盖不同形状的目标。</p>
<p>我们通过用我们的FPN替换单尺度特征映射来适应RPN。我们在我们的特征金字塔的每个层级上附加一个相同设计的头部（3x3 conv和两个1x1兄弟convs）。由于头部在所有金字塔等级上的所有位置密集滑动，所以不需要在特定层级上具有多尺度锚点。相反，我们为每个层级分配单尺度的锚点。在形式上，我们定义锚点$\lbrace P_2, P_3, P_4, P_5, P_6 \rbrace$分别具有$\lbrace 32^2 , 64^2 , 128^2 , 256^2 , 512^2 \rbrace$个像素的面积。正如在[29]中，我们在每个层级上也使用了多个长宽比$\lbrace 1:2, 1:1, 2:1 \rbrace$的锚点。所以在金字塔上总共有十五个锚点。</p>
<p>如[29]，我们根据锚点和实际边界框的交并比（IoU）比例将训练标签分配给锚点。形式上，如果一个锚点对于一个给定的实际边界框具有最高的IoU或者与任何实际边界框的IoU超过0.7，则给其分配一个正标签，如果其与所有实际边界框的IoU都低于0.3，则为其分配一个负标签。请注意，实际边界框的尺度并未明确用于将它们分配到金字塔的层级；相反，实际边界框与已经分配给金字塔等级的锚点相关联。因此，除了[29]中的内容外，我们不引入额外的规则。</p>
<p>我们注意到头部的参数在所有特征金字塔层级上共享；我们也评估了替代方案，没有共享参数并且观察到相似的准确性。共享参数的良好性能表明我们的金字塔的所有层级共享相似的语义级别。这个优点类似于使用特征图像金字塔的优点，其中可以将常见头部分类器应用于在任何图像尺度下计算的特征。</p>
<p>通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。我们在实验中详细说明实施细节。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 1984.</p>
<p>[2] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016.</p>
<p>[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection. In ECCV, 2016.</p>
<p>[4] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive fully convolutional networks. In ECCV, 2016.</p>
<p>[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.</p>
<p>[6] P. Dollar, R. Appel, S. Belongie, and P. Perona. Fast feature pyramids for object detection. TPAMI, 2014.</p>
<p>[7] P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 2010.</p>
<p>[8] G.GhiasiandC.C.Fowlkes.Laplacianpyramidreconstruction and refinement for semantic segmentation. In ECCV, 2016.</p>
<p>[9] S. Gidaris and N. Komodakis. Object detection via a multi-region &amp; semantic segmentation-aware CNN model. In ICCV, 2015.</p>
<p>[10] S. Gidaris and N. Komodakis. Attend refine repeat: Active box proposal generation via in-out localization. In BMVC, 2016.</p>
<p>[11] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.</p>
<p>[13] B.Hariharan,P.Arbelaez,R.Girshick,andJ.Malik.Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015.</p>
<p>[14] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn. arXiv:1703.06870, 2017.</p>
<p>[15] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.</p>
<p>[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>[17] S. Honari, J. Yosinski, P. Vincent, and C. Pal. Recombinator networks: Learning coarse-to-fine feature aggregation. In CVPR, 2016.</p>
<p>[18] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards accurate region proposal generation and joint object detection. In CVPR, 2016.</p>
<p>[19] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.</p>
<p>[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[22] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot multibox detector. In ECCV, 2016.</p>
<p>[23] W. Liu, A. Rabinovich, and A. C. Berg. ParseNet: Looking wider to see better. In ICLR workshop, 2016.</p>
<p>[24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[25] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.</p>
<p>[26] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In ECCV, 2016.</p>
<p>[27] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to segment object candidates. In NIPS, 2015.</p>
<p>[28] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár. Learning to refine object segments. In ECCV, 2016.</p>
<p>[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. PAMI, 2016.</p>
<p>[31] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image segmentation. In MIC- CAI, 2015.</p>
<p>[32] H. Rowley, S. Baluja, and T. Kanade. Human face detection in visual scenes. Technical Report CMU-CS-95-158R, Carnegie Mellon University, 1995.</p>
<p>[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.</p>
<p>[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[35] A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object detectors with online hard example mining. In CVPR, 2016.</p>
<p>[36] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[37] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.</p>
<p>[38] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE Proc. on Vision, Image, and Signal Processing, 1994.</p>
<p>[39] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC, 2016.</p>
<p>[40] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chintala, and P. Dollár. A multipath network for object detection. In BMVC, 2016. 10</p>
]]></content>
    
    <summary type="html">
    
      Feature Pyramid Networks for Object Detection论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的优化方法</title>
    <link href="http://noahsnail.com/2018/03/19/2018-03-19-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>http://noahsnail.com/2018/03/19/2018-03-19-深度学习中的优化方法/</id>
    <published>2018-03-19T03:33:03.000Z</published>
    <updated>2018-03-20T02:01:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-优化算法"><a href="#1-优化算法" class="headerlink" title="1. 优化算法"></a>1. 优化算法</h2><h2 id="2-SGD"><a href="#2-SGD" class="headerlink" title="2. SGD"></a>2. SGD</h2><h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3. Momentum"></a>3. Momentum</h2><h2 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4. Adam"></a>4. Adam</h2><h2 id="5-Adamgrad"><a href="#5-Adamgrad" class="headerlink" title="5. Adamgrad"></a>5. Adamgrad</h2>]]></content>
    
    <summary type="html">
    
      深度学习中的优化方法
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(三)——丢弃法(gluon)</title>
    <link href="http://noahsnail.com/2018/03/15/2018-03-15-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%89)%E2%80%94%E2%80%94%E4%B8%A2%E5%BC%83%E6%B3%95(gluon)/"/>
    <id>http://noahsnail.com/2018/03/15/2018-03-15-动手学深度学习(三)——丢弃法(gluon)/</id>
    <published>2018-03-15T10:53:32.000Z</published>
    <updated>2018-03-15T10:55:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="定义模型并添加丢弃层"><a href="#定义模型并添加丢弃层" class="headerlink" title="定义模型并添加丢弃层"></a>定义模型并添加丢弃层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义模型</span></div><div class="line">net = nn.Sequential()</div><div class="line"><span class="comment"># 丢弃概率</span></div><div class="line">drop_prob1 = <span class="number">0.2</span></div><div class="line">drop_prob2 = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 添加层</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    <span class="comment"># 将输入数据展开</span></div><div class="line">    net.add(nn.Flatten())</div><div class="line">    <span class="comment"># 第一个全连接层</span></div><div class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</div><div class="line">    <span class="comment"># 添加丢弃层</span></div><div class="line">    net.add(nn.Dropout(drop_prob1))</div><div class="line">    <span class="comment"># 第二个全连接层</span></div><div class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</div><div class="line">    <span class="comment"># 添加丢弃层</span></div><div class="line">    net.add(nn.Dropout(drop_prob2))</div><div class="line">    <span class="comment"># 定义输出层</span></div><div class="line">    net.add(nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 初始化模型参数</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="读取数据并训练"><a href="#读取数据并训练" class="headerlink" title="读取数据并训练"></a>读取数据并训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 加载数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div><div class="line"></div><div class="line"><span class="comment"># 优化</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 梯度反向传播</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 更新梯度</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 记录训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 记录训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 计算测试准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.817475, Train acc 0.697349, Test acc 0.778145
Epoch 1. Loss: 0.515098, Train acc 0.810831, Test acc 0.847456
Epoch 2. Loss: 0.458402, Train acc 0.833450, Test acc 0.823918
Epoch 3. Loss: 0.419452, Train acc 0.846554, Test acc 0.862079
Epoch 4. Loss: 0.396483, Train acc 0.854067, Test acc 0.874499
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(三)——丢弃法(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(三)——丢弃法(从零开始)</title>
    <link href="http://noahsnail.com/2018/03/14/2018-03-14-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%89)%E2%80%94%E2%80%94%E4%B8%A2%E5%BC%83%E6%B3%95(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/03/14/2018-03-14-动手学深度学习(三)——丢弃法(从零开始)/</id>
    <published>2018-03-14T11:02:04.000Z</published>
    <updated>2018-03-15T10:43:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="丢弃法的概念"><a href="#丢弃法的概念" class="headerlink" title="丢弃法的概念"></a>丢弃法的概念</h2><p>在现代神经网络中，我们所指的丢弃法，通常是对输入层或者隐含层做以下操作：</p>
<ul>
<li>随机选择一部分该层的输出作为丢弃元素；</li>
<li>把丢弃元素乘以0；</li>
<li>把非丢弃元素拉伸。</li>
</ul>
<h2 id="丢弃法的实现"><a href="#丢弃法的实现" class="headerlink" title="丢弃法的实现"></a>丢弃法的实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy, SGD</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 实现dropout</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_probability)</span>:</span></div><div class="line">    <span class="comment"># 计算保留数据的比例</span></div><div class="line">    keep_probability = <span class="number">1</span> - drop_probability</div><div class="line">    <span class="comment"># 确保drop_probability的输入合法</span></div><div class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= keep_probability &lt;= <span class="number">1</span></div><div class="line">    <span class="comment"># 丢弃所有元素</span></div><div class="line">    <span class="keyword">if</span> keep_probability == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span> X.zeros_like()</div><div class="line">    <span class="comment"># 随机生成一个相同纬度的矩阵, 根据随机值和keep_probability的对比确定是否丢弃该元素</span></div><div class="line">    mask = nd.random.uniform(<span class="number">0</span>, <span class="number">1.0</span>, X.shape, ctx=X.context) &lt; keep_probability</div><div class="line">    <span class="comment"># 保证 E[dropout(X)] == X, 对剩下的数据进行缩放</span></div><div class="line">    scale = <span class="number">1</span> / keep_probability</div><div class="line">    <span class="keyword">return</span> mask * X * scale</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试dropout</span></div><div class="line">A = nd.arange(<span class="number">20</span>).reshape((<span class="number">5</span>,<span class="number">4</span>))</div><div class="line">dropout(A, <span class="number">0.0</span>)</div></pre></td></tr></table></figure>
<pre><code>[[  0.   1.   2.   3.]
 [  4.   5.   6.   7.]
 [  8.   9.  10.  11.]
 [ 12.  13.  14.  15.]
 [ 16.  17.  18.  19.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dropout(A, <span class="number">1.0</span>)</div></pre></td></tr></table></figure>
<pre><code>[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dropout(A, <span class="number">0.5</span>)</div></pre></td></tr></table></figure>
<pre><code>[[  0.   2.   4.   0.]
 [  8.   0.  12.   0.]
 [ 16.  18.   0.   0.]
 [  0.   0.   0.  30.]
 [  0.  34.  36.   0.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><h2 id="丢弃法的本质"><a href="#丢弃法的本质" class="headerlink" title="丢弃法的本质"></a>丢弃法的本质</h2><p>一般来说，在集成学习里，我们可以对训练数据集有放回地采样若干次并分别训练若干个不同的分类器；测试时，把这些分类器的结果集成一下作为最终分类结果。事实上，丢弃法在模拟集成学习。丢弃法实质上是对每一个这样的数据集分别训练一个原神经网络子集的分类器。与一般的集成学习不同，这里每个原神经网络子集的分类器用的是同一套参数。因此丢弃法只是在模拟集成学习。使用丢弃法的神经网络实质上是对输入层和隐含层的参数做了正则化：学到的参数使得原神经网络不同子集在训练数据上都尽可能表现良好。</p>
<h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 加载数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="含两个隐藏层的多层感知机"><a href="#含两个隐藏层的多层感知机" class="headerlink" title="含两个隐藏层的多层感知机"></a>含两个隐藏层的多层感知机</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型输入大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"></div><div class="line"><span class="comment"># 模型输出大小</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 第一个隐藏层节点数量</span></div><div class="line">num_hidden1 = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 第二个隐藏层节点数量</span></div><div class="line">num_hidden2 = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 随机数据时的标准差</span></div><div class="line">weight_scale = <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 第一个隐藏层权重</span></div><div class="line">W1 = nd.random_normal(shape=(num_inputs, num_hidden1), scale=weight_scale)</div><div class="line"><span class="comment"># 第一个隐藏层偏置</span></div><div class="line">b1 = nd.zeros(num_hidden1)</div><div class="line"></div><div class="line"><span class="comment"># 第二个隐藏层权重</span></div><div class="line">W2 = nd.random_normal(shape=(num_hidden1, num_hidden2), scale=weight_scale)</div><div class="line"><span class="comment"># 第二个隐藏层偏置</span></div><div class="line">b2 = nd.zeros(num_hidden2)</div><div class="line"></div><div class="line"><span class="comment"># 输出层权重</span></div><div class="line">W3 = nd.random_normal(shape=(num_hidden2, num_outputs), scale=weight_scale)</div><div class="line"><span class="comment"># 输出层偏置</span></div><div class="line">b3 = nd.zeros(num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W1, b1, W2, b2, W3, b3]</div><div class="line"></div><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="定义包含丢弃层的模型"><a href="#定义包含丢弃层的模型" class="headerlink" title="定义包含丢弃层的模型"></a>定义包含丢弃层的模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 第一个隐藏层的丢弃概率</span></div><div class="line">drop_prob1 = <span class="number">0.2</span></div><div class="line"><span class="comment"># 第二个隐藏层的丢弃概率</span></div><div class="line">drop_prob2 = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</div><div class="line">    <span class="comment"># 第一层全连接</span></div><div class="line">    h1 = nd.relu(nd.dot(X, W1) + b1)</div><div class="line">    <span class="comment"># 在第一层全连接后添加丢弃层</span></div><div class="line">    h1 = dropout(h1, drop_prob1)</div><div class="line">    <span class="comment"># 第二层全连接</span></div><div class="line">    h2 = nd.relu(nd.dot(h1, W2) + b2)</div><div class="line">    <span class="comment"># 在第二层全连接后添加丢弃层</span></div><div class="line">    h2 = dropout(h2, drop_prob2)</div><div class="line">    <span class="comment"># 返回输出</span></div><div class="line">    <span class="keyword">return</span> nd.dot(h2, W3) + b3</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># 定义学习率</span></div><div class="line">learning_rate = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 梯度反向传播</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># SGD更新梯度</span></div><div class="line">        SGD(params, learning_rate / batch_size)</div><div class="line">        <span class="comment"># 记录训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 记录训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 计算测试准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 1.221062, Train acc 0.528746, Test acc 0.754006
Epoch 1. Loss: 0.598503, Train acc 0.774890, Test acc 0.813101
Epoch 2. Loss: 0.499490, Train acc 0.818493, Test acc 0.840244
Epoch 3. Loss: 0.457343, Train acc 0.832699, Test acc 0.835036
Epoch 4. Loss: 0.426575, Train acc 0.846070, Test acc 0.849159
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(三)——丢弃法(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——正则化(gluon)</title>
    <link href="http://noahsnail.com/2018/03/09/2018-03-09-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96(gluon)/"/>
    <id>http://noahsnail.com/2018/03/09/2018-03-09-动手学深度学习(二)——正则化(gluon)/</id>
    <published>2018-03-09T10:07:25.000Z</published>
    <updated>2018-03-09T10:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="高维线性回归数据集"><a href="#高维线性回归数据集" class="headerlink" title="高维线性回归数据集"></a>高维线性回归数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">random.seed(<span class="number">2</span>)</div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># 输入数据特征维度</span></div><div class="line">num_inputs = <span class="number">200</span></div><div class="line"></div><div class="line"><span class="comment"># 实际权重</span></div><div class="line">true_w = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 实际偏置</span></div><div class="line">true_b = <span class="number">0.05</span></div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">X = nd.random_normal(shape=(num_train + num_test, num_inputs))</div><div class="line">y = nd.dot(X, true_w) + true_b</div><div class="line"></div><div class="line"><span class="comment"># 添加随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据和测试数据</span></div><div class="line">X_train, X_test = X[:num_train, :], X[num_train:, :]</div><div class="line">y_train, y_test = y[:num_train], y[num_train:]</div></pre></td></tr></table></figure>
<h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># 创建数据集</span></div><div class="line">dataset_train = gluon.data.ArrayDataset(X_train, y_train)</div><div class="line"></div><div class="line"><span class="comment"># 读取数据</span></div><div class="line">data_iter = gluon.data.DataLoader(dataset_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 损失函数</span></div><div class="line">square_loss = gluon.loss.L2Loss()</div><div class="line"></div><div class="line"><span class="comment"># 测试</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(net, X, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> square_loss(net(X), y).mean().asscalar()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(weight_decay)</span>:</span></div><div class="line">    <span class="comment"># 定义训练的迭代周期</span></div><div class="line">    epochs = <span class="number">10</span></div><div class="line">    <span class="comment"># 定义学习率</span></div><div class="line">    learning_rate = <span class="number">0.005</span></div><div class="line">    <span class="comment"># 定义网络</span></div><div class="line">    net = gluon.nn.Sequential()</div><div class="line">    <span class="keyword">with</span> net.name_scope():</div><div class="line">        net.add(gluon.nn.Dense(<span class="number">1</span>))</div><div class="line">    <span class="comment">#net.collect_params().initialize(mx.init.Normal(sigma=1))</span></div><div class="line">    <span class="comment"># 初始化网络参数</span></div><div class="line">    net.initialize(mx.init.Normal(sigma=<span class="number">1</span>))</div><div class="line">    <span class="comment"># SGD训练, 使用权重衰减代替L2正则化</span></div><div class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: learning_rate, <span class="string">'wd'</span>: weight_decay&#125;)</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">            <span class="comment"># 记录梯度</span></div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                <span class="comment"># 计算预测值</span></div><div class="line">                output = net(data)</div><div class="line">                <span class="comment"># 计算损失</span></div><div class="line">                loss = square_loss(output, label)</div><div class="line">            <span class="comment"># 反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新权重</span></div><div class="line">            trainer.step(batch_size)</div><div class="line">            <span class="comment"># 训练损失</span></div><div class="line">            train_loss.append(test(net, X_train, y_train))</div><div class="line">            <span class="comment"># 测试损失</span></div><div class="line">            test_loss.append(test(net, X_test, y_test))</div><div class="line">    <span class="comment"># 绘制图像</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>,<span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> (<span class="string">'learned w[:10]:'</span>, net[<span class="number">0</span>].weight.data()[:,:<span class="number">10</span>], <span class="string">'\nlearned b:'</span>, net[<span class="number">0</span>].bias.data())</div></pre></td></tr></table></figure>
<h2 id="训练模型并观察过拟合"><a href="#训练模型并观察过拟合" class="headerlink" title="训练模型并观察过拟合"></a>训练模型并观察过拟合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-bd038b77c2ef488a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Overfitting"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 1.04817235 -0.02568591  0.86764944  0.29322273  0.01006198 -0.56152564
    0.38436413 -0.3084037  -2.32450151  0.03733355]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;\nlearned b:&apos;, 
 [ 0.79914868]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="使用Gluon的正则化"><a href="#使用Gluon的正则化" class="headerlink" title="使用Gluon的正则化"></a>使用Gluon的正则化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">5</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8d88c219f34f10ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Normal"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 0.00107634 -0.00052574  0.00450234 -0.00110544 -0.00683913 -0.00181657
   -0.00530634  0.00512847 -0.00742552 -0.00058494]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;\nlearned b:&apos;, 
 [ 0.00449433]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="可用权重衰减代替L2正则化的原因"><a href="#可用权重衰减代替L2正则化的原因" class="headerlink" title="可用权重衰减代替L2正则化的原因"></a>可用权重衰减代替L2正则化的原因</h2><p><img src="http://upload-images.jianshu.io/upload_images/3232548-c9da036502d1949d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="推导"></p>
<p>注：图片来自Gluon社区。</p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——正则化(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——正则化(从零开始)</title>
    <link href="http://noahsnail.com/2018/03/08/2018-03-08-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/03/08/2018-03-08-动手学深度学习(二)——正则化(从零开始)/</id>
    <published>2018-03-08T11:09:49.000Z</published>
    <updated>2018-03-08T11:12:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="高维线性回归"><a href="#高维线性回归" class="headerlink" title="高维线性回归"></a>高维线性回归</h2><p>使用线性函数$y = 0.05 + \sum_{i = 1}^p 0.01x_i +  \text{noise}$生成数据样本，噪音服从均值0和标准差为0.01的正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">random.seed(<span class="number">2</span>)</div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div></pre></td></tr></table></figure>
<h2 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># 输入数据特征维度</span></div><div class="line">num_inputs = <span class="number">200</span></div><div class="line"></div><div class="line"><span class="comment"># 实际权重</span></div><div class="line">true_w = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 实际偏置</span></div><div class="line">true_b = <span class="number">0.05</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成数据</span></div><div class="line">X = nd.random_normal(shape=(num_train + num_test, num_inputs))</div><div class="line">y = nd.dot(X, true_w) + true_b</div><div class="line"></div><div class="line"><span class="comment"># 添加随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据和测试数据</span></div><div class="line">X_train, X_test = X[:num_train, :], X[num_train:, :]</div><div class="line">y_train, y_test = y[:num_train], y[num_train:]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># 通过yield进行数据读取</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(num_examples)</span>:</span></div><div class="line">    <span class="comment"># 产生样本的索引</span></div><div class="line">    idx = list(range(num_examples))</div><div class="line">    <span class="comment"># 将索引随机打乱</span></div><div class="line">    random.shuffle(idx)</div><div class="line">    <span class="comment"># 迭代一个epoch</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</div><div class="line">        <span class="comment"># 依次取出样本的索引, 这种实现方式在num_examples/batch_size不能整除时也适用</span></div><div class="line">        j = nd.array(idx[i:min((i + batch_size), num_examples)])</div><div class="line">        <span class="comment"># 根据提供的索引取元素</span></div><div class="line">        <span class="keyword">yield</span> nd.take(X, j), nd.take(y, j)</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 随机初始化权重w</span></div><div class="line">    w = nd.random_normal(shape=(num_inputs, <span class="number">1</span>))</div><div class="line">    <span class="comment"># 偏置b初始化为0</span></div><div class="line">    b = nd.zeros((<span class="number">1</span>,))</div><div class="line">    <span class="comment"># w, b放入list里</span></div><div class="line">    params = [w, b]</div><div class="line"></div><div class="line">    <span class="comment"># 需要计算反向传播, 添加自动求导</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        param.attach_grad()</div><div class="line">    <span class="keyword">return</span> params</div></pre></td></tr></table></figure>
<h2 id="L-2-范数正则化"><a href="#L-2-范数正则化" class="headerlink" title="$L_2$ 范数正则化"></a>$L_2$ 范数正则化</h2><p>在训练时最小化函数为：$\text{loss} + \lambda \sum_{p \in \textrm{params}}|p|_2^2。$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># L2范数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2_penalty</span><span class="params">(w, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> ((w**<span class="number">2</span>).sum() + b ** <span class="number">2</span>) / <span class="number">2</span></div></pre></td></tr></table></figure>
<h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, w, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</div><div class="line"></div><div class="line"><span class="comment"># 损失函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span><span class="params">(yhat, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> (yhat - y.reshape(yhat.shape)) ** <span class="number">2</span> / <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># 梯度下降</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        param[:] = param - lr * param.grad / batch_size</div><div class="line"></div><div class="line"><span class="comment"># 测试</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(net, params, X, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> square_loss(net(X, *params), y).mean().asscalar()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(_lambda)</span>:</span></div><div class="line">    <span class="comment"># 定义训练的迭代周期</span></div><div class="line">    epochs = <span class="number">10</span></div><div class="line">    <span class="comment"># 定义学习率</span></div><div class="line">    learning_rate = <span class="number">0.005</span></div><div class="line">    <span class="comment"># 初始化参数</span></div><div class="line">    w, b = params = init_params()</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter(num_train):</div><div class="line">            <span class="comment"># 记录梯度</span></div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                 <span class="comment"># 计算预测值</span></div><div class="line">                output = net(data, *params)</div><div class="line">                <span class="comment"># 计算loss</span></div><div class="line">                loss = square_loss(output, label) + _lambda * L2_penalty(*params)</div><div class="line">            <span class="comment"># 反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新梯度</span></div><div class="line">            sgd(params, learning_rate, batch_size)</div><div class="line">            <span class="comment"># 训练损失</span></div><div class="line">            train_loss.append(test(net, params, X_train, y_train))</div><div class="line">            <span class="comment"># 测试损失</span></div><div class="line">            test_loss.append(test(net, params, X_test, y_test))</div><div class="line">            </div><div class="line">    <span class="comment"># 绘制损失图像</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> <span class="string">'learned w[:10]:'</span>, w[:<span class="number">10</span>].T, <span class="string">'learned b:'</span>, b</div></pre></td></tr></table></figure>
<h1 id="观察过拟合"><a href="#观察过拟合" class="headerlink" title="观察过拟合"></a>观察过拟合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-bd038b77c2ef488a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Overfitting"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 1.04817176 -0.02568593  0.86764956  0.29322267  0.01006179 -0.56152576
    0.38436398 -0.30840367 -2.32450151  0.03733355]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;learned b:&apos;, 
 [ 0.79914856]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="使用正则化"><a href="#使用正则化" class="headerlink" title="使用正则化"></a>使用正则化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">5</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8d88c219f34f10ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Normal"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 0.00107633 -0.00052574  0.00450233 -0.00110545 -0.0068391  -0.00181657
   -0.00530632  0.00512845 -0.00742549 -0.00058495]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;learned b:&apos;, 
 [ 0.00449432]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——正则化(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——欠拟合和过拟合</title>
    <link href="http://noahsnail.com/2018/03/01/2018-03-01-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>http://noahsnail.com/2018/03/01/2018-03-01-动手学深度学习(二)——欠拟合和过拟合/</id>
    <published>2018-03-01T10:49:47.000Z</published>
    <updated>2018-03-08T11:09:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h1 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h1><h3 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h3><p>机器学习模型在训练数据集上表现出的误差叫做训练误差，在任意一个测试数据样本上表现出的误差的期望值叫做泛化误差。</p>
<p>统计学习理论的一个假设是：训练数据集和测试数据集里的每一个数据样本都是从同一个概率分布中相互独立地生成出的（独立同分布假设）。</p>
<p>一个重要结论是：训练误差的降低不一定意味着泛化误差的降低。机器学习既需要降低训练误差，又需要降低泛化误差。</p>
<h3 id="欠拟合和过拟合-1"><a href="#欠拟合和过拟合-1" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h3><ul>
<li>欠拟合：机器学习模型无法得到较低训练误差。</li>
<li>过拟合：机器学习模型的训练误差远小于其在测试数据集上的误差。</li>
</ul>
<h3 id="模型的选择"><a href="#模型的选择" class="headerlink" title="模型的选择"></a>模型的选择</h3><p>模型拟合能力和误差之间的关系如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f50a8a6737e3927b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="error_model_complexity.png"></p>
<h3 id="训练数据集的大小"><a href="#训练数据集的大小" class="headerlink" title="训练数据集的大小"></a>训练数据集的大小</h3><p>一般来说，如果训练数据集过小，特别是比模型参数数量更小时，过拟合更容易发生。除此之外，泛化误差不会随训练数据集里样本数量增加而增大。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-7e957140a2d8242c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="model_vs_data.png"></p>
<h3 id="多项式拟合"><a href="#多项式拟合" class="headerlink" title="多项式拟合"></a>多项式拟合</h3><p>给定一个<strong>标量</strong>数据点集合<code>x</code>和对应的标量目标值<code>y</code>，多项式拟合的目标是找一个K阶多项式，其由向量<code>w</code>和位移<code>b</code>组成，来最好地近似每个样本<code>x</code>和<code>y</code>。用数学符号来表示就是我们将学<code>w</code>和<code>b</code>来预测</p>
<p>$$\hat{y} = b + \sum_{k=1}^K x^k w_k$$</p>
<p>并以平方误差为损失函数，一阶多项式拟合又叫线性拟合。</p>
<h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><p>使用二阶多项式来生成每一个数据样本，$y=1.2x−3.4x^2+5.6x^3+5.0+noise$，噪音服从均值0和标准差为0.1的正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">100</span></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"><span class="comment"># 多项式权重</span></div><div class="line">true_w = [<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>]</div><div class="line"><span class="comment"># 多项式偏置</span></div><div class="line">true_b = <span class="number">5.0</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成随机数据x</span></div><div class="line">x = nd.random.normal(shape=(num_train + num_test, <span class="number">1</span>))</div><div class="line"><span class="comment"># 计算x的多项式值</span></div><div class="line">X = nd.concat(x, nd.power(x, <span class="number">2</span>), nd.power(x, <span class="number">3</span>))</div><div class="line"><span class="comment"># 计算y</span></div><div class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_w[<span class="number">2</span>] * X[:, <span class="number">2</span>] + true_b</div><div class="line"><span class="comment"># 查看数据</span></div><div class="line">(<span class="string">'x:'</span>, x[:<span class="number">5</span>], <span class="string">'X:'</span>, X[:<span class="number">5</span>], <span class="string">'y:'</span>, y[:<span class="number">5</span>])</div></pre></td></tr></table></figure>
<pre><code>(200L,)
</code></pre><h2 id="定义训练和测试步骤"><a href="#定义训练和测试步骤" class="headerlink" title="定义训练和测试步骤"></a>定义训练和测试步骤</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义训练过程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X_train, X_test, y_train, y_test)</span>:</span></div><div class="line">    <span class="comment"># 定义线性回归模型</span></div><div class="line">    net = gluon.nn.Sequential()</div><div class="line">    <span class="keyword">with</span> net.name_scope():</div><div class="line">        net.add(gluon.nn.Dense(<span class="number">1</span>))</div><div class="line">    <span class="comment"># 权重初始化</span></div><div class="line">    net.initialize()</div><div class="line">    <span class="comment"># 学习率</span></div><div class="line">    learning_rate = <span class="number">0.01</span></div><div class="line">    <span class="comment"># 迭代周期</span></div><div class="line">    epochs = <span class="number">100</span></div><div class="line">    <span class="comment"># 训练的批数据大小</span></div><div class="line">    batch_size = min(<span class="number">10</span>, y_train.shape[<span class="number">0</span>])</div><div class="line">    <span class="comment"># 创建训练数据集</span></div><div class="line">    dataset_train = gluon.data.ArrayDataset(X_train, y_train)</div><div class="line">    <span class="comment"># 读取数据</span></div><div class="line">    data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 训练方法SGD</span></div><div class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: learning_rate&#125;)</div><div class="line">    <span class="comment"># 定义损失函数</span></div><div class="line">    square_loss = gluon.loss.L2Loss()</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="comment"># 进行训练</span></div><div class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter_train:</div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                <span class="comment"># 进行预测</span></div><div class="line">                output = net(data)</div><div class="line">                <span class="comment"># 计算预测值与实际值之间的损失</span></div><div class="line">                loss = square_loss(output, label)</div><div class="line">            <span class="comment"># 损失进行反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新权重</span></div><div class="line">            trainer.step(batch_size)</div><div class="line">        <span class="comment"># 保存训练损失</span></div><div class="line">        train_loss.append(square_loss(net(X_train), y_train).mean().asscalar())</div><div class="line">        <span class="comment"># 保存测试损失</span></div><div class="line">        test_loss.append(square_loss(net(X_test), y_test).mean().asscalar())</div><div class="line">    <span class="comment"># 绘制损失</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>,<span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> (<span class="string">'learned weight'</span>, net[<span class="number">0</span>].weight.data(), <span class="string">'learned bias'</span>, net[<span class="number">0</span>].bias.data())</div></pre></td></tr></table></figure>
<h2 id="三阶多项式拟合（正常）"><a href="#三阶多项式拟合（正常）" class="headerlink" title="三阶多项式拟合（正常）"></a>三阶多项式拟合（正常）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(X[:num_train, :], X[num_train:, :], y[:num_train], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-4500dba51617c3ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 1.22117233 -3.39606118  5.59531116]]
 &lt;NDArray 1x3 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [ 4.98550272]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="线性拟合（欠拟合）"><a href="#线性拟合（欠拟合）" class="headerlink" title="线性拟合（欠拟合）"></a>线性拟合（欠拟合）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(x[:num_train, :], x[num_train:, :], y[:num_train], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cac456292c149670.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 19.74101448]]
 &lt;NDArray 1x1 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [-0.23861444]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="训练量不足（过拟合）"><a href="#训练量不足（过拟合）" class="headerlink" title="训练量不足（过拟合）"></a>训练量不足（过拟合）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(X[<span class="number">0</span>:<span class="number">2</span>, :], X[num_train:, :], y[<span class="number">0</span>:<span class="number">2</span>], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cc94f0d732ebd573.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 3.10832024 -0.740421    4.85165691]]
 &lt;NDArray 1x3 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [ 0.29450524]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>训练误差的降低并不一定意味着泛化误差的降低。</li>
<li>欠拟合和过拟合都是需要尽量避免的。我们要注意模型的选择和训练量的大小。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——欠拟合和过拟合
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——多层感知机(gluon)</title>
    <link href="http://noahsnail.com/2018/02/28/2018-02-28-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(gluon)/"/>
    <id>http://noahsnail.com/2018/02/28/2018-02-28-动手学深度学习(二)——多层感知机(gluon)/</id>
    <published>2018-02-28T08:42:14.000Z</published>
    <updated>2018-02-28T08:42:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 按顺序堆叠网络层</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># name_scope作用, 方便管理参数名称</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    net.add(gluon.nn.Flatten())</div><div class="line">    net.add(gluon.nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</div><div class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 参数初始化</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练器和优化方法</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.819048, Train acc 0.728666, Test acc 0.768530
Epoch 1. Loss: 0.550646, Train acc 0.808644, Test acc 0.823618
Epoch 2. Loss: 0.488554, Train acc 0.829210, Test acc 0.845553
Epoch 3. Loss: 0.457407, Train acc 0.839493, Test acc 0.842448
Epoch 4. Loss: 0.438059, Train acc 0.845486, Test acc 0.852063
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——多层感知机(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——多层感知机(从零开始)</title>
    <link href="http://noahsnail.com/2018/02/27/2018-02-27-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/02/27/2018-02-27-动手学深度学习(二)——多层感知机(从零开始)/</id>
    <published>2018-02-27T10:35:46.000Z</published>
    <updated>2018-02-27T10:35:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy, SGD</div></pre></td></tr></table></figure>
<h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输入数据大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"><span class="comment"># 输出数据大小, 分为10类</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 隐藏单元个数</span></div><div class="line">num_hidden = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 正态分布的标准差</span></div><div class="line">weight_scale = <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化输入层权重</span></div><div class="line">W1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale)</div><div class="line">b1 = nd.zeros(num_hidden)</div><div class="line"></div><div class="line"><span class="comment"># 随机初始化隐藏层权重</span></div><div class="line">W2 = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale)</div><div class="line">b2 = nd.zeros(num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W1, b1, W2, b2]</div><div class="line"></div><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 激活函数使用ReLU, relu(x)=max(x,0)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.maximum(X, <span class="number">0</span>)</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># 输入数据重排</span></div><div class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</div><div class="line">    <span class="comment"># 计算激活值</span></div><div class="line">    h1 = relu(nd.dot(X, W1) + b1)</div><div class="line">    <span class="comment"># 计算输出</span></div><div class="line">    output = nd.dot(h1, W2) + b2</div><div class="line">    <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment">## 定义学习率</span></div><div class="line">learning_rate = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        SGD(params, learning_rate/batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 1.042064, Train acc 0.630976, Test acc 0.776142
Epoch 1. Loss: 0.601578, Train acc 0.788862, Test acc 0.815204
Epoch 2. Loss: 0.525148, Train acc 0.816556, Test acc 0.835136
Epoch 3. Loss: 0.486619, Train acc 0.829427, Test acc 0.833033
Epoch 4. Loss: 0.459395, Train acc 0.836104, Test acc 0.835136
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——多层感知机(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——逻辑回归(gluon)</title>
    <link href="http://noahsnail.com/2018/02/22/2018-02-22-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(gluon)/"/>
    <id>http://noahsnail.com/2018/02/22/2018-02-22-动手学深度学习(一)——逻辑回归(gluon)/</id>
    <published>2018-02-22T09:58:07.000Z</published>
    <updated>2018-02-22T09:58:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div></pre></td></tr></table></figure>
<h2 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="定义和初始化模型"><a href="#定义和初始化模型" class="headerlink" title="定义和初始化模型"></a>定义和初始化模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个空的模型</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># name_scope作用, 方便管理参数命名</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    <span class="comment"># 加入一个平铺层, 其会将输入数据平铺为batch_size*?维</span></div><div class="line">    net.add(gluon.nn.Flatten())</div><div class="line">    <span class="comment"># 加入一个全连接层, 输出为10类</span></div><div class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 参数初始化</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练器和优化方法</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.793821, Train acc 0.744107, Test acc 0.786659
Epoch 1. Loss: 0.575076, Train acc 0.809879, Test acc 0.820112
Epoch 2. Loss: 0.530560, Train acc 0.822583, Test acc 0.831731
Epoch 3. Loss: 0.506161, Train acc 0.829728, Test acc 0.835837
Epoch 4. Loss: 0.488752, Train acc 0.834769, Test acc 0.834135
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——逻辑回归(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——逻辑回归(从零开始)</title>
    <link href="http://noahsnail.com/2018/02/09/2018-02-09-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/02/09/2018-02-09-动手学深度学习(一)——逻辑回归(从零开始)/</id>
    <published>2018-02-09T06:23:27.000Z</published>
    <updated>2018-02-09T07:18:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<p>注意：mxnet随机种子设为1时，loss一直为nan，经测试，种子为2时，jupyter-notebook有时会出现nan，但在命令行执行python文件多次都不会出现nan。</p>
<h2 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 数据预处理</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label)</span>:</span></div><div class="line">    <span class="keyword">return</span> data.astype(<span class="string">'float32'</span>) / <span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 加载训练数据</span></div><div class="line">mnist_train = gluon.data.vision.FashionMNIST(train=<span class="keyword">True</span>, transform=transform)</div><div class="line"></div><div class="line"><span class="comment"># 加载测试数据</span></div><div class="line">mnist_test = gluon.data.vision.FashionMNIST(train=<span class="keyword">False</span>, transform=transform)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 取出单条数据</span></div><div class="line">data, label = mnist_train[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="comment"># 查看数据</span></div><div class="line">(<span class="string">'example shape: '</span>, data.shape, <span class="string">'label:'</span>, label)</div></pre></td></tr></table></figure>
<pre><code>(&apos;example shape: &apos;, (28L, 28L, 1L), &apos;label:&apos;, 2.0)
</code></pre><h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 显示图像</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span><span class="params">(images)</span>:</span></div><div class="line">    <span class="comment"># 获得图像的数量</span></div><div class="line">    n = images.shape[<span class="number">0</span>]</div><div class="line">    <span class="comment"># 绘制子图</span></div><div class="line">    _, figs = plt.subplots(<span class="number">1</span>, n, figsize=(<span class="number">15</span>, <span class="number">15</span>))</div><div class="line">    <span class="comment"># 遍历绘制图像</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(n):</div><div class="line">        <span class="comment"># 显示图像</span></div><div class="line">        figs[i].imshow(images[i].reshape((<span class="number">28</span>, <span class="number">28</span>)).asnumpy())</div><div class="line">        </div><div class="line">        <span class="comment"># 显示灰度图</span></div><div class="line">        <span class="comment">#figs[i].imshow(images[i].reshape((28, 28)).asnumpy(), cmap="gray")</span></div><div class="line">        </div><div class="line">        <span class="comment"># 不显示坐标轴</span></div><div class="line">        figs[i].axes.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">        figs[i].axes.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    plt.show()</div><div class="line">    </div><div class="line"><span class="comment"># 获取图像对应的文本标签</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text_labels</span><span class="params">(labels)</span>:</span></div><div class="line">    <span class="comment"># 图像标签对应的文本</span></div><div class="line">    text_labels = [</div><div class="line">        <span class="string">'t-shirt'</span>, <span class="string">'trouser'</span>, <span class="string">'pullover'</span>, <span class="string">'dress,'</span>, <span class="string">'coat'</span>,</div><div class="line">        <span class="string">'sandal'</span>, <span class="string">'shirt'</span>, <span class="string">'sneaker'</span>, <span class="string">'bag'</span>, <span class="string">'ankle boot'</span></div><div class="line">    ]</div><div class="line">    <span class="comment"># 返回图像标签对应的文本</span></div><div class="line">    <span class="keyword">return</span> [text_labels[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</div><div class="line"></div><div class="line"><span class="comment"># 取出训练集的前9条数据</span></div><div class="line">data, label = mnist_train[<span class="number">0</span>:<span class="number">9</span>]</div><div class="line"></div><div class="line"><span class="comment"># 显示数据</span></div><div class="line">show_images(data)</div><div class="line"><span class="comment"># 显示标签</span></div><div class="line"><span class="keyword">print</span> get_text_labels(label)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8cb1872a2eba78be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure"></p>
<pre><code>[&apos;pullover&apos;, &apos;ankle boot&apos;, &apos;shirt&apos;, &apos;t-shirt&apos;, &apos;dress,&apos;, &apos;coat&apos;, &apos;coat&apos;, &apos;sandal&apos;, &apos;coat&apos;]
</code></pre><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 读取训练数据</span></div><div class="line">train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># 读取测试数据</span></div><div class="line">test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输入数据大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"><span class="comment"># 输出数据大小, 分为10类</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化权重</span></div><div class="line">W = nd.random_normal(shape=(num_inputs, num_outputs))</div><div class="line"><span class="comment"># 随机初始化偏置</span></div><div class="line">b = nd.random_normal(shape=num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W, b]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义softmax</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># 计算e^x</span></div><div class="line">    exp = nd.exp(X)</div><div class="line">    <span class="comment"># 假设exp是矩阵，这里对行进行求和，并要求保留axis 1, 就是返回 (nrows, 1) 形状的矩阵</span></div><div class="line">    partition = exp.sum(axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 对exp进行归一化</span></div><div class="line">    <span class="keyword">return</span> exp / partition</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试softmax</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化数据</span></div><div class="line">X = nd.random_normal(shape=(<span class="number">2</span>, <span class="number">5</span>))</div><div class="line"><span class="comment"># 求softmax</span></div><div class="line">X_prob = softmax(X)</div><div class="line"><span class="comment"># 输出结果</span></div><div class="line"><span class="keyword">print</span> X_prob</div><div class="line"><span class="comment"># 对每行概率求和, 如果和为1, 说明没问题</span></div><div class="line"><span class="keyword">print</span> X_prob.sum(axis=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<pre><code>[[ 0.06774449  0.5180918   0.1474141   0.11459844  0.1521512 ]
 [ 0.23102701  0.47666225  0.10536087  0.09706162  0.08988826]]
&lt;NDArray 2x5 @cpu(0)&gt;

[ 1.  1.]
&lt;NDArray 2 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义模型</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> softmax(nd.dot(X.reshape((<span class="number">-1</span>, num_inputs)), W) + b)</div></pre></td></tr></table></figure>
<h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(yhat, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> -nd.pick(nd.log(yhat), y)</div></pre></td></tr></table></figure>
<h2 id="计算精度"><a href="#计算精度" class="headerlink" title="计算精度"></a>计算精度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算分类准确率</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>) == label).asscalar()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 评估整个数据集的分类精度</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net)</span>:</span></div><div class="line">    <span class="comment"># 最终的准确率</span></div><div class="line">    acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 遍历测试数据集</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</div><div class="line">        output = net(data)</div><div class="line">        <span class="comment"># 累加准确率</span></div><div class="line">        acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 返回平均准确率</span></div><div class="line">    <span class="keyword">return</span> acc / len(data_iterator)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试随机初始化参数的分类准确率</span></div><div class="line">evaluate_accuracy(test_data, net)</div></pre></td></tr></table></figure>
<pre><code>0.15156249999999999
</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义SGD</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></div><div class="line">    <span class="comment"># 对参数进行梯度下降</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        <span class="comment"># 这样写不会创建新的param, 而是会写在原来的param里, 新的param没有梯度</span></div><div class="line">        param[:] = param - lr * param.grad</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 学习率</span></div><div class="line">learning_rate = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代巡礼</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 将梯度做平均，这样学习率会对batch size不那么敏感, 避免学习率与batch_size耦合</span></div><div class="line">        SGD(params, learning_rate / batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 3.725043, Train acc 0.487916, Test acc 0.602344
Epoch 1. Loss: 1.965692, Train acc 0.632812, Test acc 0.666895
Epoch 2. Loss: 1.630362, Train acc 0.677349, Test acc 0.691016
Epoch 3. Loss: 1.450728, Train acc 0.701690, Test acc 0.710352
Epoch 4. Loss: 1.329035, Train acc 0.717996, Test acc 0.720410
</code></pre><ul>
<li>learning_rate = 1</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Epoch 0. Loss: 18.803541, Train acc 0.081366, Test acc 0.085840</div><div class="line">Epoch 1. Loss: 18.806381, Train acc 0.081394, Test acc 0.085840</div><div class="line">Epoch 2. Loss: 18.795504, Train acc 0.081449, Test acc 0.085840</div><div class="line">Epoch 3. Loss: 18.798995, Train acc 0.081505, Test acc 0.085840</div><div class="line">Epoch 4. Loss: 18.794157, Train acc 0.081505, Test acc 0.085840</div></pre></td></tr></table></figure>
<ul>
<li>learning_rate = 0.01</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Epoch 0. Loss: 10.406917, Train acc 0.115165, Test acc 0.169629</div><div class="line">Epoch 1. Loss: 5.512807, Train acc 0.246349, Test acc 0.304590</div><div class="line">Epoch 2. Loss: 3.911078, Train acc 0.373232, Test acc 0.412695</div><div class="line">Epoch 3. Loss: 3.218969, Train acc 0.454189, Test acc 0.471191</div><div class="line">Epoch 4. Loss: 2.855459, Train acc 0.502371, Test acc 0.509082</div></pre></td></tr></table></figure>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 从测试集中取数据</span></div><div class="line">data, label = mnist_test[<span class="number">0</span>:<span class="number">9</span>]</div><div class="line"><span class="comment"># 显示图片及真实标签</span></div><div class="line">show_images(data)</div><div class="line"><span class="keyword">print</span> <span class="string">'True labels: '</span></div><div class="line"><span class="keyword">print</span> get_text_labels(label)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 预测标签</span></div><div class="line">predicted_labels = net(data).argmax(axis=<span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">'Predicted labels: '</span></div><div class="line"><span class="keyword">print</span> get_text_labels(predicted_labels.asnumpy())</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3d41ebd81c95b691.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure"></p>
<pre><code>True labels: 
[&apos;t-shirt&apos;, &apos;trouser&apos;, &apos;pullover&apos;, &apos;pullover&apos;, &apos;dress,&apos;, &apos;pullover&apos;, &apos;bag&apos;, &apos;shirt&apos;, &apos;sandal&apos;]
Predicted labels: 
[&apos;shirt&apos;, &apos;trouser&apos;, &apos;pullover&apos;, &apos;pullover&apos;, &apos;dress,&apos;, &apos;bag&apos;, &apos;bag&apos;, &apos;shirt&apos;, &apos;sandal&apos;]
</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>FashionMNIST<br><a href="https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/vision.html#FashionMNIST" target="_blank" rel="external">https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/vision.html#FashionMNIST</a>  </li>
</ul>
<h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——逻辑回归(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python中的__all__</title>
    <link href="http://noahsnail.com/2018/02/08/2018-02-08-Python%E4%B8%AD%E7%9A%84__all__/"/>
    <id>http://noahsnail.com/2018/02/08/2018-02-08-Python中的__all__/</id>
    <published>2018-02-08T08:19:15.000Z</published>
    <updated>2018-02-08T08:43:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-动机"><a href="#1-动机" class="headerlink" title="1. 动机"></a>1. 动机</h2><p>今天看MXNet的gluon源码时发现<code>mxnet.gluon.data.vision</code>有<code>__all__</code>如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">__all__ = [&apos;MNIST&apos;, &apos;FashionMNIST&apos;, &apos;CIFAR10&apos;, &apos;CIFAR100&apos;,</div><div class="line">           &apos;ImageRecordDataset&apos;, &apos;ImageFolderDataset&apos;]</div></pre></td></tr></table></figure>
<h2 id="2-作用"><a href="#2-作用" class="headerlink" title="2. 作用"></a>2. 作用</h2><p><code>__all__</code>是一个字符串list，用来定义模块中对于<code>from XXX import *</code>时要对外导出的符号，即要暴露的借口，但它只对<code>import *</code>起作用，对<code>from XXX import XXX</code>不起作用。</p>
<h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3. 测试"></a>3. 测试</h2><p><code>all.py</code>文件时要导出的模块，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">__all__ = [&apos;x&apos;, &apos;y&apos;, &apos;test&apos;]</div><div class="line"></div><div class="line">x = 2</div><div class="line">y = 3</div><div class="line">z = 4</div><div class="line"></div><div class="line">def test():</div><div class="line">	print(&apos;test&apos;)</div></pre></td></tr></table></figure>
<ul>
<li>测试文件一</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from foo import *</div><div class="line">print(&apos;x: &apos;, x)</div><div class="line"></div><div class="line">print(&apos;y: &apos;, y)</div><div class="line">print(&apos;z: &apos;, z)</div><div class="line"></div><div class="line">test()</div></pre></td></tr></table></figure>
<ul>
<li>运行结果：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x:  2</div><div class="line">y:  3</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;test.py&quot;, line 6, in &lt;module&gt;</div><div class="line">    print(&apos;z: &apos;, z)</div><div class="line">NameError: name &apos;z&apos; is not defined</div></pre></td></tr></table></figure>
<ul>
<li>测试文件二</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from foo import *</div><div class="line">from foo import z</div><div class="line">print(&apos;x: &apos;, x)</div><div class="line"></div><div class="line">print(&apos;y: &apos;, y)</div><div class="line">print(&apos;z: &apos;, z)</div><div class="line"></div><div class="line">test()</div></pre></td></tr></table></figure>
<ul>
<li>运行结果</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x:  2</div><div class="line">y:  3</div><div class="line">z:  4</div><div class="line">test</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python" target="_blank" rel="external">https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python中的__all__
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>使用Docker搭建Anaconda Python3.6的练习环境</title>
    <link href="http://noahsnail.com/2018/02/08/2018-02-08-%E4%BD%BF%E7%94%A8Docker%E6%90%AD%E5%BB%BAAnaconda%20Python3.6%E7%9A%84%E7%BB%83%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    <id>http://noahsnail.com/2018/02/08/2018-02-08-使用Docker搭建Anaconda Python3.6的练习环境/</id>
    <published>2018-02-08T07:34:43.000Z</published>
    <updated>2018-02-08T08:18:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>最近在看Python 3的相关内容，由于电脑里已经装了Anaconda 2.7，因此就在Docker里搭建了一个Anaconda Python3.6的练习环境。Dockerfile如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">FROM nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04</div><div class="line">MAINTAINER Tyan &lt;tyan.liu.git@gmail.com&gt;</div><div class="line"></div><div class="line"></div><div class="line"># Install basic dependencies</div><div class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</div><div class="line">        build-essential \</div><div class="line">        cmake \</div><div class="line">        git \</div><div class="line">        wget \</div><div class="line">        libopencv-dev \</div><div class="line">        libsnappy-dev \</div><div class="line">        python-dev \</div><div class="line">        python-pip \</div><div class="line">        tzdata \</div><div class="line">        vim</div><div class="line"></div><div class="line"></div><div class="line"># Install anaconda for python 3.6</div><div class="line">RUN wget --quiet https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh -O ~/anaconda.sh &amp;&amp; \</div><div class="line">    /bin/bash ~/anaconda.sh -b -p /opt/conda &amp;&amp; \</div><div class="line">    rm ~/anaconda.sh &amp;&amp; \</div><div class="line">    echo &quot;export PATH=/opt/conda/bin:$PATH&quot; &gt;&gt; ~/.bashrc</div><div class="line"></div><div class="line"></div><div class="line"># Set timezone</div><div class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</div><div class="line"></div><div class="line"></div><div class="line"># Set locale</div><div class="line">ENV LANG C.UTF-8 LC_ALL=C.UTF-8</div><div class="line"></div><div class="line"></div><div class="line"># Initialize workspace</div><div class="line">RUN mkdir /workspace</div><div class="line">WORKDIR /workspace</div></pre></td></tr></table></figure>
<p>构建docker image命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker build -t python:3.6 .</div></pre></td></tr></table></figure>
<p>运行docker image命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -ti --rm python:3.6</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      使用Docker搭建Anaconda Python3.6的练习环境
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>nohup python缓存问题</title>
    <link href="http://noahsnail.com/2018/02/07/2018-02-07-nohup%20python%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98/"/>
    <id>http://noahsnail.com/2018/02/07/2018-02-07-nohup python缓存问题/</id>
    <published>2018-02-07T09:08:12.000Z</published>
    <updated>2018-02-08T07:28:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>深度学习用python跑数据时，经常会用到<code>nohup</code>命令，通常的命令格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup python [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<p>如果没有指定输出文件，<code>nohup</code>会将输出放到<code>nohup.out</code>文件中，但在程序运行过程中<code>nohup.out</code>文件中不能实时的看到python的输出，原因是python的输出有缓冲。</p>
<p>解决方案如下：</p>
<ul>
<li>方案一</li>
</ul>
<p>使用<code>-u</code>参数，使python输出不进行缓冲，命令格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup python -u [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<ul>
<li>方案二</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export PYTHONUNBUFFERED=1</div><div class="line">nohup python [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://stackoverflow.com/questions/12919980/nohup-is-not-writing-log-to-output-file" target="_blank" rel="external">https://stackoverflow.com/questions/12919980/nohup-is-not-writing-log-to-output-file</a></p>
]]></content>
    
    <summary type="html">
    
      nohup python缓存问题
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——线性回归(gluon)</title>
    <link href="http://noahsnail.com/2018/02/06/2018-02-06-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(gluon)/"/>
    <id>http://noahsnail.com/2018/02/06/2018-02-06-动手学深度学习(一)——线性回归(gluon)/</id>
    <published>2018-02-06T10:59:38.000Z</published>
    <updated>2018-02-09T06:31:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 导入mxnet的gluon, ndarray, autograd</span></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">1</span>)</div><div class="line">random.seed(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据的维度</span></div><div class="line">num_inputs = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># 训练数据的样本数量</span></div><div class="line">num_examples = <span class="number">1000</span></div><div class="line"></div><div class="line"><span class="comment"># 实际的权重w</span></div><div class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</div><div class="line"></div><div class="line"><span class="comment"># 实际的偏置b</span></div><div class="line">true_b = <span class="number">4.2</span></div><div class="line"></div><div class="line"><span class="comment"># 随机生成均值为0, 方差为1, 服从正态分布的训练数据X, </span></div><div class="line">X = nd.random_normal(shape=(num_examples, num_inputs))</div><div class="line"></div><div class="line"><span class="comment"># 根据X, w, b生成对应的输出y</span></div><div class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_b </div><div class="line"></div><div class="line"><span class="comment"># 给y加上随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div></pre></td></tr></table></figure>
<h2 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 绘制数据的散点图 </span></div><div class="line">plt.scatter(X[:, <span class="number">1</span>].asnumpy(), y.asnumpy())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1ddbc52187302d98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="数据展示"></p>
<h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练时的批数据大小</span></div><div class="line">batch_size = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 创建数据集</span></div><div class="line">dataset = gluon.data.ArrayDataset(X, y)</div><div class="line"></div><div class="line"><span class="comment"># 读取数据</span></div><div class="line">data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 查看数据</span></div><div class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">    <span class="keyword">print</span> data, label</div><div class="line">    <span class="keyword">break</span></div></pre></td></tr></table></figure>
<pre><code>[[-2.11255503  0.61242002]
 [ 2.18546367 -0.48856559]
 [ 0.91085583  0.38985687]
 [-0.56097323  1.44421673]
 [ 0.31765923 -1.75729597]
 [-0.57738042  2.03963804]
 [-0.91808975  0.64181799]
 [-0.20269176  0.21012937]
 [-0.22549874  0.19895147]
 [ 1.42844415  0.06982213]]
&lt;NDArray 10x2 @cpu(0)&gt; 
[ -2.11691356  10.22533131   4.70613146  -1.82755637  10.82125568
  -3.88111711   0.17608714   3.07074499   3.06542921   6.82972908]
&lt;NDArray 10 @cpu(0)&gt;
</code></pre><h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个空的模型</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># 加入一个Dense层</span></div><div class="line">net.add(gluon.nn.Dense(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">square_loss = gluon.loss.L2Loss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.01</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练的迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</div><div class="line">    <span class="comment"># 总的loss</span></div><div class="line">    total_loss = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算预测值</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算loss</span></div><div class="line">            loss = square_loss(output, label)</div><div class="line">        <span class="comment"># 根据loss进行反向传播计算梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 更新权重, batch_size用来进行梯度平均</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 计算总的loss</span></div><div class="line">        total_loss += nd.sum(loss).asscalar()</div><div class="line">        </div><div class="line">    <span class="keyword">print</span> <span class="string">"Epoch %d, average loss: %f"</span> % (epoch, total_loss/num_examples)</div></pre></td></tr></table></figure>
<pre><code>Epoch 0, average loss: 7.403182
Epoch 1, average loss: 0.854247
Epoch 2, average loss: 0.099864
Epoch 3, average loss: 0.011887
Epoch 4, average loss: 0.001479
</code></pre><h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><p>ArrayDataset<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.ArrayDataset" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.ArrayDataset</a>  </p>
</li>
<li><p>DataLoader<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataload#mxnet.gluon.data.DataLoader" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataload#mxnet.gluon.data.DataLoader</a>  </p>
</li>
<li><p>Sequential<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=gluon.nn.sequential#mxnet.gluon.nn.Sequential" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=gluon.nn.sequential#mxnet.gluon.nn.Sequential</a>  </p>
</li>
<li><p>L2Loss<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html?highlight=l2loss#mxnet.gluon.loss.L2Loss" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/loss.html?highlight=l2loss#mxnet.gluon.loss.L2Loss</a>  </p>
</li>
<li><p>Trainer<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=trainer#mxnet.gluon.Trainer" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=trainer#mxnet.gluon.Trainer</a>  </p>
</li>
</ul>
<h2 id="代码地址-1"><a href="#代码地址-1" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——线性回归(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Model Zoo</title>
    <link href="http://noahsnail.com/2018/02/02/2018-02-02-Caffe%20Model%20Zoo/"/>
    <id>http://noahsnail.com/2018/02/02/2018-02-02-Caffe Model Zoo/</id>
    <published>2018-02-02T10:59:55.000Z</published>
    <updated>2018-02-02T10:59:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Caffe有许多分类的预训练模型及网络结构，我自己训练过的模型总结在Github上，基本上涵盖了大部分的分类模型，包括AlexNet，VGG，GoogLeNet，Inception系列，ResNet，SENet，DenseNet，SqueezeNet。</p>
<p>其中会碰到不少坑，例如VGG给的结构已经太旧了，需要根据新版本Caffe的进行修改，DenseNet训练有些地方需要修改等。鉴于以上原因，我自己整理了一个Caffe Model Zoo，都是已经使用Caffe训练过模型的。</p>
<p>Github地址：<a href="https://github.com/SnailTyan/caffe-model-zoo" target="_blank" rel="external">https://github.com/SnailTyan/caffe-model-zoo</a></p>
]]></content>
    
    <summary type="html">
    
      Caffe Model Zoo
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
