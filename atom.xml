<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="noahsnail.com/"/>
  <updated>2017-08-03T03:10:09.000Z</updated>
  <id>noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Yolo-Darknet的安装和使用</title>
    <link href="noahsnail.com/2017/08/02/2017-8-2-Yolo-Darknet%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>noahsnail.com/2017/08/02/2017-8-2-Yolo-Darknet的安装和使用/</id>
    <published>2017-08-02T03:02:30.000Z</published>
    <updated>2017-08-03T03:10:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Yolo-Darknet介绍"><a href="#1-Yolo-Darknet介绍" class="headerlink" title="1. Yolo-Darknet介绍"></a>1. Yolo-Darknet介绍</h2><p>YOLO是基于深度学习方法的端到端实时目标检测系统，目前有三个版本，Yolo-v1，Yolo-9000，Yolo-v2。Darknet是Yolo的实现，但Darknet不仅包含Yolo的实现，还包括其它内容。</p>
<h2 id="2-Darknet安装"><a href="#2-Darknet安装" class="headerlink" title="2. Darknet安装"></a>2. Darknet安装</h2><p>安装过程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># 代码下载</div><div class="line">git clone https://github.com/pjreddie/darknet.git</div><div class="line"> </div><div class="line"># 修改Makefile</div><div class="line">cd darknet</div><div class="line">sed -i &apos;1s/GPU=0/GPU=1/&apos; Makefile</div><div class="line">sed -i &apos;2s/CUDNN=0/CUDNN=1/&apos; Makefile</div><div class="line">sed -i &apos;3s/OPENCV=0/OPENCV=1/&apos; Makefile</div><div class="line"> </div><div class="line"># 安装</div><div class="line">make</div><div class="line"> </div><div class="line"># 下载预训练的模型</div><div class="line">wget https://pjreddie.com/media/files/yolo.weights</div><div class="line">wget https://pjreddie.com/media/files/tiny-yolo-voc.weights</div><div class="line">wget http://pjreddie.com/media/files/yolov1.weights</div><div class="line">wget http://pjreddie.com/media/files/tiny-yolo.weights</div><div class="line">wget http://pjreddie.com/media/files/tiny-coco.weights</div><div class="line">wget http://pjreddie.com/media/files/yolo-coco.weights</div></pre></td></tr></table></figure>
<h2 id="3-Yolo-v2用法"><a href="#3-Yolo-v2用法" class="headerlink" title="3. Yolo-v2用法"></a>3. Yolo-v2用法</h2><ul>
<li>使用预训练的模型进行目标检测</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/predictions.jpg" alt="Result"></p>
<ul>
<li>输入图像名称进行检测</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">$ ./darknet detect cfg/yolo.cfg yolo.weights</div><div class="line"># 输入 data/horses.jpg</div><div class="line"># 执行结果如下:</div><div class="line">layer     filters    size              input                output</div><div class="line">    0 conv     32  3 x 3 / 1   608 x 608 x   3   -&gt;   608 x 608 x  32</div><div class="line">    1 max          2 x 2 / 2   608 x 608 x  32   -&gt;   304 x 304 x  32</div><div class="line">    2 conv     64  3 x 3 / 1   304 x 304 x  32   -&gt;   304 x 304 x  64</div><div class="line">    3 max          2 x 2 / 2   304 x 304 x  64   -&gt;   152 x 152 x  64</div><div class="line">    4 conv    128  3 x 3 / 1   152 x 152 x  64   -&gt;   152 x 152 x 128</div><div class="line">    5 conv     64  1 x 1 / 1   152 x 152 x 128   -&gt;   152 x 152 x  64</div><div class="line">    6 conv    128  3 x 3 / 1   152 x 152 x  64   -&gt;   152 x 152 x 128</div><div class="line">    7 max          2 x 2 / 2   152 x 152 x 128   -&gt;    76 x  76 x 128</div><div class="line">    8 conv    256  3 x 3 / 1    76 x  76 x 128   -&gt;    76 x  76 x 256</div><div class="line">    9 conv    128  1 x 1 / 1    76 x  76 x 256   -&gt;    76 x  76 x 128</div><div class="line">   10 conv    256  3 x 3 / 1    76 x  76 x 128   -&gt;    76 x  76 x 256</div><div class="line">   11 max          2 x 2 / 2    76 x  76 x 256   -&gt;    38 x  38 x 256</div><div class="line">   12 conv    512  3 x 3 / 1    38 x  38 x 256   -&gt;    38 x  38 x 512</div><div class="line">   13 conv    256  1 x 1 / 1    38 x  38 x 512   -&gt;    38 x  38 x 256</div><div class="line">   14 conv    512  3 x 3 / 1    38 x  38 x 256   -&gt;    38 x  38 x 512</div><div class="line">   15 conv    256  1 x 1 / 1    38 x  38 x 512   -&gt;    38 x  38 x 256</div><div class="line">   16 conv    512  3 x 3 / 1    38 x  38 x 256   -&gt;    38 x  38 x 512</div><div class="line">   17 max          2 x 2 / 2    38 x  38 x 512   -&gt;    19 x  19 x 512</div><div class="line">   18 conv   1024  3 x 3 / 1    19 x  19 x 512   -&gt;    19 x  19 x1024</div><div class="line">   19 conv    512  1 x 1 / 1    19 x  19 x1024   -&gt;    19 x  19 x 512</div><div class="line">   20 conv   1024  3 x 3 / 1    19 x  19 x 512   -&gt;    19 x  19 x1024</div><div class="line">   21 conv    512  1 x 1 / 1    19 x  19 x1024   -&gt;    19 x  19 x 512</div><div class="line">   22 conv   1024  3 x 3 / 1    19 x  19 x 512   -&gt;    19 x  19 x1024</div><div class="line">   23 conv   1024  3 x 3 / 1    19 x  19 x1024   -&gt;    19 x  19 x1024</div><div class="line">   24 conv   1024  3 x 3 / 1    19 x  19 x1024   -&gt;    19 x  19 x1024</div><div class="line">   25 route  16</div><div class="line">   26 conv     64  1 x 1 / 1    38 x  38 x 512   -&gt;    38 x  38 x  64</div><div class="line">   27 reorg              / 2    38 x  38 x  64   -&gt;    19 x  19 x 256</div><div class="line">   28 route  27 24</div><div class="line">   29 conv   1024  3 x 3 / 1    19 x  19 x1280   -&gt;    19 x  19 x1024</div><div class="line">   30 conv    425  1 x 1 / 1    19 x  19 x1024   -&gt;    19 x  19 x 425</div><div class="line">   31 detection</div><div class="line">mask_scale: Using default &apos;1.000000&apos;</div><div class="line">Loading weights from yolo.weights...Done!</div><div class="line">Enter Image Path: data/horses.jpg</div><div class="line">data/horses.jpg: Predicted in 0.030211 seconds.</div><div class="line">horse: 46%</div><div class="line">horse: 59%</div><div class="line">horse: 91%</div><div class="line"> </div><div class="line">(predictions:31): Gtk-WARNING **: cannot open display:</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/horse-predictions.jpg" alt="result"></p>
<ul>
<li>设置检测阈值</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg -thresh 0.1</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/thres-predictions.jpg" alt="result"></p>
<ul>
<li>检测视频</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet detector demo cfg/coco.data cfg/yolo.cfg yolo.weights &lt;video file&gt;</div></pre></td></tr></table></figure>
<p>参考资料</p>
<ol>
<li><p><a href="https://pjreddie.com/darknet/install/" target="_blank" rel="external">https://pjreddie.com/darknet/install/</a></p>
</li>
<li><p><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="external">https://pjreddie.com/darknet/yolo/</a></p>
</li>
<li><p><a href="https://pjreddie.com/darknet/yolov1/" target="_blank" rel="external">https://pjreddie.com/darknet/yolov1/</a></p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Yolo-Darknet的安装和使用
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>ResNet论文翻译——中文版</title>
    <link href="noahsnail.com/2017/07/31/2017-7-31-ResNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/07/31/2017-7-31-ResNet论文翻译——中文版/</id>
    <published>2017-07-31T09:37:49.000Z</published>
    <updated>2017-08-01T01:32:47.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<h1 id="Deep-Residual-Learning-for-Image-Recognition"><a href="#Deep-Residual-Learning-for-Image-Recognition" class="headerlink" title="Deep Residual Learning for Image Recognition"></a>Deep Residual Learning for Image Recognition</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>更深的神经网络更难训练。</p>
<p>The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Deep convolutional neural networks [22, 21] have led to a series of breakthroughs for image classification [21, 49, 39]. Deep networks naturally integrate low/mid/high- level features [49] and classifiers in an end-to-end multi- layer fashion, and the “levels” of features can be enriched by the number of stacked layers (depth). Recent evidence [40, 43] reveals that network depth is of crucial importance, and the leading results [40, 43, 12, 16] on the challenging ImageNet dataset [35] all exploit “very deep” [40] models, with a depth of sixteen [40] to thirty [16]. Many other non- trivial visual recognition tasks [7, 11, 6, 32, 27] have also greatly benefited from very deep models.</p>
<p>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [14, 1, 8], which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initial- ization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start con- verging for stochastic gradient descent (SGD) with back- propagation [22].</p>
<p>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher train- ing error, as reported in [10, 41] and thoroughly verified by our experiments. Fig. 1 shows a typical example.</p>
<p>The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Y.Bengio,P.Simard,andP.Frasconi.Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.</p>
<p>[2] C. M. Bishop. Neural networks for pattern recognition. Oxford university press, 1995.</p>
<p>[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam, 2000.</p>
<p>[4] K.Chatfield,V.Lempitsky,A.Vedaldi,andA.Zisserman.Thedevil is in the details: an evaluation of recent feature encoding methods. In BMVC, 2011.</p>
<p>[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV, pages 303–338, 2010.</p>
<p>[6] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In<br>CVPR, 2014.</p>
<p>[8] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.</p>
<p>[9] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. arXiv:1302.4389, 2013.</p>
<p>[10] K.Heand J.Sun. Convolutional neural networks at constrained time cost. In CVPR, 2015.</p>
<p>[11] K.He, X.Zhang, S.Ren, and J.Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014.</p>
<p>[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In<br>ICCV, 2015.</p>
<p>[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.</p>
<p>[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma thesis, TU Munich, 1991.</p>
<p>[15] S.HochreiterandJ.Schmidhuber.Longshort-termmemory.Neural computation, 9(8):1735–1780, 1997.</p>
<p>[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.</p>
<p>[17] H.Jegou, M.Douze, and C.Schmid. Product quantization for nearest neighbor search. TPAMI, 33, 2011.</p>
<p>[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into compact codes.<br>TPAMI, 2012.</p>
<p>[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for<br>fast feature embedding. arXiv:1408.5093, 2014.</p>
<p>[20] A. Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.</p>
<p>[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification</p>
<p>with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to hand-<br>written zip code recognition. Neural computation, 1989.</p>
<p>[23] Y.LeCun,L.Bottou,G.B.Orr,and K.-R.Muller. Efficient back prop. In Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.</p>
<p>[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. arXiv:1409.5185, 2014.</p>
<p>[25] M.Lin,Q.Chen,andS.Yan.Networkinnetwork.arXiv:1312.4400,2013.</p>
<p>[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft COCO: Common objects in<br>context. In ECCV. 2014.</p>
<p>[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[28] G. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In NIPS, 2014.</p>
<p>[29] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.</p>
<p>[30] F.PerronninandC.Dance.Fisherkernelsonvisualvocabulariesfor image categorization. In CVPR, 2007.</p>
<p>[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons. In AISTATS, 2012.</p>
<p>[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[33] B. D. Ripley. Pattern recognition and neural networks. Cambridge university press, 1996.</p>
<p>[34] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.</p>
<p>[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014.</p>
<p>[36] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120, 2013.</p>
<p>[37] N.N.Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical report, 1998.</p>
<p>[38] N. N. Schraudolph. Centering neural network gradient factors. In Neural Networks: Tricks of the Trade, pages 207–226. Springer, 1998.</p>
<p>[39] P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[40] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[41] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv:1505.00387, 2015.</p>
<p>[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. 1507.06228, 2015.</p>
<p>[43] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.</p>
<p>[44] R. Szeliski. Fast surface interpolation using hierarchical basis functions. TPAMI, 1990.</p>
<p>[45] R. Szeliski. Locally adapted hierarchical basis preconditioning. In SIGGRAPH, 2006.</p>
<p>[46] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochastic gradient towards second-order methods–backpropagation learning with transformations in nonlinearities. In Neural Information Processing, 2013.</p>
<p>[47] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms, 2008.</p>
<p>[48] W. Venables and B. Ripley. Modern applied statistics with s-plus. 1999.</p>
<p>[49] M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolutional neural networks. In ECCV, 2014.</p>
]]></content>
    
    <summary type="html">
    
      ResNet论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>ResNet论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/07/31/2017-7-31-ResNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2017/07/31/2017-7-31-ResNet论文翻译——中英文对照/</id>
    <published>2017-07-31T09:37:23.000Z</published>
    <updated>2017-08-02T02:44:10.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<h1 id="Deep-Residual-Learning-for-Image-Recognition"><a href="#Deep-Residual-Learning-for-Image-Recognition" class="headerlink" title="Deep Residual Learning for Image Recognition"></a>Deep Residual Learning for Image Recognition</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>更深的神经网络更难训练。</p>
<p>The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Deep convolutional neural networks [22, 21] have led to a series of breakthroughs for image classification [21, 49, 39]. Deep networks naturally integrate low/mid/high- level features [49] and classifiers in an end-to-end multi- layer fashion, and the “levels” of features can be enriched by the number of stacked layers (depth). Recent evidence [40, 43] reveals that network depth is of crucial importance, and the leading results [40, 43, 12, 16] on the challenging ImageNet dataset [35] all exploit “very deep” [40] models, with a depth of sixteen [40] to thirty [16]. Many other non- trivial visual recognition tasks [7, 11, 6, 32, 27] have also greatly benefited from very deep models.</p>
<p>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [14, 1, 8], which hamper convergence from the beginning. This problem, however, has been largely addressed by normalized initial- ization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start con- verging for stochastic gradient descent (SGD) with back- propagation [22].</p>
<p>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher train- ing error, as reported in [10, 41] and thoroughly verified by our experiments. Fig. 1 shows a typical example.</p>
<p>The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Y.Bengio,P.Simard,andP.Frasconi.Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.</p>
<p>[2] C. M. Bishop. Neural networks for pattern recognition. Oxford university press, 1995.</p>
<p>[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam, 2000.</p>
<p>[4] K.Chatfield,V.Lempitsky,A.Vedaldi,andA.Zisserman.Thedevil is in the details: an evaluation of recent feature encoding methods. In BMVC, 2011.</p>
<p>[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV, pages 303–338, 2010.</p>
<p>[6] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In<br>CVPR, 2014.</p>
<p>[8] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.</p>
<p>[9] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. arXiv:1302.4389, 2013.</p>
<p>[10] K.Heand J.Sun. Convolutional neural networks at constrained time cost. In CVPR, 2015.</p>
<p>[11] K.He, X.Zhang, S.Ren, and J.Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014.</p>
<p>[12] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In<br>ICCV, 2015.</p>
<p>[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.</p>
<p>[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma thesis, TU Munich, 1991.</p>
<p>[15] S.HochreiterandJ.Schmidhuber.Longshort-termmemory.Neural computation, 9(8):1735–1780, 1997.</p>
<p>[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.</p>
<p>[17] H.Jegou, M.Douze, and C.Schmid. Product quantization for nearest neighbor search. TPAMI, 33, 2011.</p>
<p>[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid. Aggregating local image descriptors into compact codes.<br>TPAMI, 2012.</p>
<p>[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for<br>fast feature embedding. arXiv:1408.5093, 2014.</p>
<p>[20] A. Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.</p>
<p>[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification</p>
<p>with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to hand-<br>written zip code recognition. Neural computation, 1989.</p>
<p>[23] Y.LeCun,L.Bottou,G.B.Orr,and K.-R.Muller. Efficient back prop. In Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.</p>
<p>[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. arXiv:1409.5185, 2014.</p>
<p>[25] M.Lin,Q.Chen,andS.Yan.Networkinnetwork.arXiv:1312.4400,2013.</p>
<p>[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft COCO: Common objects in<br>context. In ECCV. 2014.</p>
<p>[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[28] G. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear regions of deep neural networks. In NIPS, 2014.</p>
<p>[29] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.</p>
<p>[30] F.PerronninandC.Dance.Fisherkernelsonvisualvocabulariesfor image categorization. In CVPR, 2007.</p>
<p>[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons. In AISTATS, 2012.</p>
<p>[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[33] B. D. Ripley. Pattern recognition and neural networks. Cambridge university press, 1996.</p>
<p>[34] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.</p>
<p>[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014.</p>
<p>[36] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120, 2013.</p>
<p>[37] N.N.Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical report, 1998.</p>
<p>[38] N. N. Schraudolph. Centering neural network gradient factors. In Neural Networks: Tricks of the Trade, pages 207–226. Springer, 1998.</p>
<p>[39] P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[40] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[41] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv:1505.00387, 2015.</p>
<p>[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. 1507.06228, 2015.</p>
<p>[43] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.</p>
<p>[44] R. Szeliski. Fast surface interpolation using hierarchical basis functions. TPAMI, 1990.</p>
<p>[45] R. Szeliski. Locally adapted hierarchical basis preconditioning. In SIGGRAPH, 2006.</p>
<p>[46] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochastic gradient towards second-order methods–backpropagation learning with transformations in nonlinearities. In Neural Information Processing, 2013.</p>
<p>[47] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms, 2008.</p>
<p>[48] W. Venables and B. Ripley. Modern applied statistics with s-plus. 1999.</p>
<p>[49] M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolutional neural networks. In ECCV, 2014.</p>
]]></content>
    
    <summary type="html">
    
      ResNet论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>GoogLeNet论文翻译——中文版</title>
    <link href="noahsnail.com/2017/07/21/2017-7-21-GoogleNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/07/21/2017-7-21-GoogleNet论文翻译——中文版/</id>
    <published>2017-07-21T09:50:07.000Z</published>
    <updated>2017-07-31T09:22:22.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/googlenet.png" alt="Deep Learning"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="Going-Deeper-with-Convolutions"><a href="#Going-Deeper-with-Convolutions" class="headerlink" title="Going Deeper with Convolutions"></a>Going Deeper with Convolutions</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们在ImageNet大规模视觉识别挑战赛2014（ILSVRC14）上提出了一种代号为Inception的深度卷积神经网络结构，并在分类和检测上取得了新的最好结果。这个架构的主要特点是提高了网络内部计算资源的利用率。通过精心的手工设计，我们在增加了网络深度和广度的同时保持了计算预算不变。为了优化质量，架构的设计以赫布理论和多尺度处理直觉为基础。我们在ILSVRC14提交中应用的一个特例被称为GoogLeNet，一个22层的深度网络，其质量在分类和检测的背景下进行了评估。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>过去三年中，由于深度学习和卷积网络的发展[10]，我们的目标分类和检测能力得到了显著提高。一个令人鼓舞的消息是，大部分的进步不仅仅是更强大硬件、更大数据集、更大模型的结果，而主要是新的想法、算法和网络结构改进的结果。例如，ILSVRC 2014竞赛中最靠前的输入除了用于检测目的的分类数据集之外，没有使用新的数据资源。我们在ILSVRC 2014中的GoogLeNet提交实际使用的参数只有两年前Krizhevsky等人[9]获胜结构参数的1/12，而结果明显更准确。在目标检测前沿，最大的收获不是来自于越来越大的深度网络的简单应用，而是来自于深度架构和经典计算机视觉的协同，像Girshick等人[6]的R-CNN算法那样。</p>
<p>另一个显著因素是随着移动和嵌入式设备的推动，我们的算法的效率很重要——尤其是它们的电力和内存使用。值得注意的是，正是包含了这个因素的考虑才得出了本文中呈现的深度架构设计，而不是单纯的为了提高准确率。对于大多数实验来说，模型被设计为在一次推断中保持15亿乘加的计算预算，所以最终它们不是单纯的学术好奇心，而是能在现实世界中应用，甚至是以合理的代价在大型数据集上使用。</p>
<p>在本文中，我们将关注一个高效的计算机视觉深度神经网络架构，代号为Inception，它的名字来自于Lin等人[12]网络论文中的Network与著名的“we need to go deeper”网络迷因[1]的结合。在我们的案例中，单词“deep”用在两个不同的含义中：首先，在某种意义上，我们以“Inception module”的形式引入了一种新层次的组织方式，在更直接的意义上增加了网络的深度。一般来说，可以把Inception模型看作论文[12]的逻辑顶点同时从Arora等人[2]的理论工作中受到了鼓舞和引导。这种架构的好处在ILSVRC 2014分类和检测挑战赛中通过实验得到了验证，它明显优于目前的最好水平。</p>
<h2 id="2-近期工作"><a href="#2-近期工作" class="headerlink" title="2. 近期工作"></a>2. 近期工作</h2><p>从LeNet-5 [10]开始，卷积神经网络（CNN）通常有一个标准结构——堆叠的卷积层（后面可以选择有对比归一化和最大池化）后面是一个或更多的全连接层。这个基本设计的变种在图像分类著作流行，并且目前为止在MNIST，CIFAR和更著名的ImageNet分类挑战赛中[9, 21]的已经取得了最佳结果。对于更大的数据集例如ImageNet来说，最近的趋势是增加层的数目[12]和层的大小[21, 14]，同时使用丢弃[7]来解决过拟合问题。</p>
<p>尽管担心最大池化层会引起准确空间信息的损失，但与[9]相同的卷积网络结构也已经成功的应用于定位[9, 14]，目标检测[6, 14, 18, 5]和行人姿态估计[19]。</p>
<p>受灵长类视觉皮层神经科学模型的启发，Serre等人[15]使用了一系列固定的不同大小的Gabor滤波器来处理多尺度。我们使用一个了类似的策略。然而，与[15]的固定的2层深度模型相反，Inception结构中所有的滤波器是学习到的。此外，Inception层重复了很多次，在GoogleNet模型中得到了一个22层的深度模型。</p>
<p>Network-in-Network是Lin等人[12]为了增加神经网络表现能力而提出的一种方法。在他们的模型中，网络中添加了额外的1 × 1卷积层，增加了网络的深度。我们的架构中大量的使用了这个方法。但是，在我们的设置中，1 × 1卷积有两个目的：最关键的是，它们主要是用来作为降维模块来移除卷积瓶颈，否则将会限制我们网络的大小。这不仅允许了深度的增加，而且允许我们网络的宽度增加但没有明显的性能损失。</p>
<p>最后，目前最好的目标检测是Girshick等人[6]的基于区域的卷积神经网络（R-CNN）方法。R-CNN将整个检测问题分解为两个子问题：利用低层次的信号例如颜色，纹理以跨类别的方式来产生目标位置候选区域，然后用CNN分类器来识别那些位置上的对象类别。这样一种两个阶段的方法利用了低层特征分割边界框的准确性，也利用了目前的CNN非常强大的分类能力。我们在我们的检测提交中采用了类似的方式，但探索增强这两个阶段，例如对于更高的目标边界框召回使用多盒[5]预测，并融合了更好的边界框候选区域分类方法。</p>
<h2 id="3-动机和高层思考"><a href="#3-动机和高层思考" class="headerlink" title="3. 动机和高层思考"></a>3. 动机和高层思考</h2><p>提高深度神经网络性能最直接的方式是增加它们的尺寸。这不仅包括增加深度——网络层次的数目——也包括它的宽度：每一层的单元数目。这是一种训练更高质量模型容易且安全的方法，尤其是在可获得大量标注的训练数据的情况下。但是这个简单方案有两个主要的缺点。更大的尺寸通常意味着更多的参数，这会使增大的网络更容易过拟合，尤其是在训练集的标注样本有限的情况下。这是一个主要的瓶颈，因为要获得强标注数据集费时费力且代价昂贵，经常需要专家评委在各种细粒度的视觉类别进行区分，例如图1中显示的ImageNet中的类别（甚至是1000类ILSVRC的子集）。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/g_figure%201.png" alt="Figure 1"></p>
<p>图1: ILSVRC 2014分类挑战赛的1000类中两个不同的类别。区分这些类别需要领域知识。</p>
<p>统一增加网络尺寸的另一个缺点是计算资源使用的显著增加。例如，在一个深度视觉网络中，如果两个卷积层相连，它们的滤波器数目的任何统一增加都会引起计算量平方式的增加。如果增加的能力使用时效率低下（例如，如果大多数权重结束时接近于0），那么会浪费大量的计算能力。由于计算预算总是有限的，计算资源的有效分布更偏向于尺寸无差别的增加，即使主要目标是增加性能的质量。</p>
<p>解决这两个问题的一个基本的方式就是引入稀疏性并将全连接层替换为稀疏的全连接层，甚至是卷积层。除了模仿生物系统之外，由于Arora等人[2]的开创性工作，这也具有更坚固的理论基础优势。他们的主要成果说明如果数据集的概率分布可以通过一个大型稀疏的深度神经网络表示，则最优的网络拓扑结构可以通过分析前一层激活的相关性统计和聚类高度相关的神经元来一层层的构建。虽然严格的数学证明需要在很强的条件下，但事实上这个声明与著名的赫布理论产生共鸣——神经元一起激发，一起连接——实践表明，基础概念甚至适用于不严格的条件下。</p>
<p>遗憾的是，当碰到在非均匀的稀疏数据结构上进行数值计算时，现在的计算架构效率非常低下。即使算法运算的数量减少100倍，查询和缓存丢失上的开销仍占主导地位：切换到稀疏矩阵可能是不可行的。随着稳定提升和高度调整的数值库的应用，差距仍在进一步扩大，数值库要求极度快速密集的矩阵乘法，利用底层的CPU或GPU硬件[16, 9]的微小细节。非均匀的稀疏模型也要求更多的复杂工程和计算基础结构。目前大多数面向视觉的机器学习系统通过采用卷积的优点来利用空域的稀疏性。然而，卷积被实现为对上一层块的密集连接的集合。为了打破对称性，提高学习水平，从论文[11]开始，ConvNets习惯上在特征维度使用随机的稀疏连接表，然而为了进一步优化并行计算，论文[9]中趋向于变回全连接。目前最新的计算机视觉架构有统一的结构。更多的滤波器和更大的批大小要求密集计算的有效使用。</p>
<p>这提出了下一个中间步骤是否有希望的问题：一个架构能利用滤波器水平的稀疏性，正如理论所建议的那样，但能通过利用密集矩阵计算来利用我们目前的硬件。稀疏矩阵乘法的大量文献（例如[3]）认为对于稀疏矩阵乘法，将稀疏矩阵聚类为相对密集的子矩阵会有更佳的性能。在不久的将来会利用类似的方法来进行非均匀深度学习架构的自动构建，这样的想法似乎并不牵强。</p>
<p>Inception架构开始是作为案例研究，用于评估一个复杂网络拓扑构建算法的假设输出，该算法试图近似[2]中所示的视觉网络的稀疏结构，并通过密集的、容易获得的组件来覆盖假设结果。尽管是一个非常投机的事情，但与基于[12]的参考网络相比，早期可以观测到适度的收益。随着一点点调整加宽差距，作为[6]和[5]的基础网络，Inception被证明在定位上下文和目标检测中尤其有用。有趣的是，虽然大多数最初的架构选择已被质疑并分离开进行全面测试，但结果证明它们是局部最优的。然而必须谨慎：尽管Inception架构在计算机上领域取得成功，但这是否可以归因于构建其架构的指导原则仍是有疑问的。确保这一点将需要更彻底的分析和验证。</p>
<h2 id="4-架构细节"><a href="#4-架构细节" class="headerlink" title="4. 架构细节"></a>4. 架构细节</h2><p>Inception架构的主要想法是考虑怎样近似卷积视觉网络的最优稀疏结构并用容易获得的密集组件进行覆盖。注意假设转换不变性，这意味着我们的网络将以卷积构建块为基础。我们所需要做的是找到最优的局部构造并在空间上重复它。Arora等人[2]提出了一个层次结构，其中应该分析最后一层的相关统计并将它们聚集成具有高相关性的单元组。这些聚类形成了下一层的单元并与前一层的单元连接。我们假设较早层的每个单元都对应输入层的某些区域，并且这些单元被分成滤波器组。在较低的层（接近输入的层）相关单元集中在局部区域。因此，如[12]所示，我们最终会有许多聚类集中在单个区域，它们可以通过下一层的1×1卷积层覆盖。然而也可以预期，将存在更小数目的在更大空间上扩展的聚类，其可以被更大块上的卷积覆盖，在越来越大的区域上块的数量将会下降。为了避免块校正的问题，目前Inception架构形式的滤波器的尺寸仅限于1×1、3×3、5×5，这个决定更多的是基于便易性而不是必要性。这也意味着提出的架构是所有这些层的组合，其输出滤波器组连接成单个输出向量形成了下一阶段的输入。另外，由于池化操作对于目前卷积网络的成功至关重要，因此建议在每个这样的阶段添加一个替代的并行池化路径应该也应该具有额外的有益效果（看图2(a)）。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/goolenet-Fig%202.png" alt="Figure 2"></p>
<p>由于这些“Inception模块”在彼此的顶部堆叠，其输出相关统计必然有变化：由于较高层会捕获较高的抽象特征，其空间集中度预计会减少。这表明随着转移到更高层，3×3和5×5卷积的比例应该会增加。</p>
<p>上述模块的一个大问题是在具有大量滤波器的卷积层之上，即使适量的5×5卷积也可能是非常昂贵的，至少在这种朴素形式中有这个问题。一旦池化单元添加到混合中，这个问题甚至会变得更明显：输出滤波器的数量等于前一阶段滤波器的数量。池化层输出和卷积层输出的合并会导致这一阶段到下一阶段输出数量不可避免的增加。虽然这种架构可能会覆盖最优稀疏结构，但它会非常低效，导致在几个阶段内计算量爆炸。</p>
<p>这导致了Inception架构的第二个想法：在计算要求会增加太多的地方，明智地减少维度。这是基于嵌入的成功：甚至低维嵌入可能包含大量关于较大图像块的信息。然而嵌入以密集、压缩形式表示信息并且压缩信息更难处理。这种表示应该在大多数地方保持稀疏（根据[2]中条件的要求】）并且仅在它们必须汇总时才压缩信号。也就是说，在昂贵的3×3和5×5卷积之前，1×1卷积用来计算降维。除了用来降维之外，它们也包括使用线性修正单元使其两用。最终的结果如图2(b)所示。</p>
<p>通常，Inception网络是一个由上述类型的模块互相堆叠组成的网络，偶尔会有步长为2的最大池化层将网络分辨率减半。出于技术原因（训练过程中内存效率），只在更高层开始使用Inception模块而在更低层仍保持传统的卷积形式似乎是有益的。这不是绝对必要的，只是反映了我们目前实现中的一些基础结构效率低下。</p>
<p>该架构的一个有用的方面是它允许显著增加每个阶段的单元数量，而不会在后面的阶段出现计算复杂度不受控制的爆炸。这是在尺寸较大的块进行昂贵的卷积之前通过普遍使用降维实现的。此外，设计遵循了实践直觉，即视觉信息应该在不同的尺度上处理然后聚合，为的是下一阶段可以从不同尺度同时抽象特征。</p>
<p>计算资源的改善使用允许增加每个阶段的宽度和阶段的数量，而不会陷入计算困境。可以利用Inception架构创建略差一些但计算成本更低的版本。我们发现所有可用的控制允许计算资源的受控平衡，导致网络比没有Inception结构的类似执行网络快3—10倍，但是在这一点上需要仔细的手动设计。</p>
<h2 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5. GoogLeNet"></a>5. GoogLeNet</h2><p>通过“GoogLeNet”这个名字，我们提到了在ILSVRC 2014竞赛的提交中使用的Inception架构的特例。我们也使用了一个稍微优质的更深更宽的Inception网络，但将其加入到组合中似乎只稍微提高了结果。我们忽略了该网络的细节，因为经验证据表明确切架构的参数影响相对较小。表1说明了竞赛中使用的最常见的Inception实例。这个网络（用不同的图像块采样方法训练的）使用了我们组合中7个模型中的6个。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet-Table%201.png" alt="Table 1"></p>
<p>所有的卷积都使用了修正线性激活，包括Inception模块内部的卷积。在我们的网络中感受野是在均值为0的RGB颜色空间中，大小是224×224。“#3×3 reduce”和“#5×5 reduce”表示在3×3和5×5卷积之前，降维层使用的1×1滤波器的数量。在pool proj列可以看到内置的最大池化之后，投影层中1×1滤波器的数量。所有的这些降维/投影层也都使用了线性修正激活。</p>
<p>网络的设计考虑了计算效率和实用性，因此推断可以单独的设备上运行，甚至包括那些计算资源有限的设备，尤其是低内存占用的设备。当只计算有参数的层时，网络有22层（如果我们也计算池化层是27层）。构建网络的全部层（独立构建块）的数目大约是100。确切的数量取决于机器学习基础设施对层的计算方式。分类器之前的平均池化是基于[12]的，尽管我们的实现有一个额外的线性层。线性层使我们的网络能很容易地适应其它的标签集，但它主要是为了方便使用，我们不期望它有重大的影响。我们发现从全连接层变为平均池化，提高了大约<code>top-1 %0.6</code>的准确率，然而即使在移除了全连接层之后，丢失的使用还是必不可少的。</p>
<p>给定深度相对较大的网络，有效传播梯度反向通过所有层的能力是一个问题。在这个任务上，更浅网络的强大性能表明网络中部层产生的特征应该是非常有识别力的。通过将辅助分类器添加到这些中间层，可以期望较低阶段分类器的判别力。这被认为是在提供正则化的同时克服梯度消失问题。这些分类器采用较小卷积网络的形式，放置在Inception (4a)和Inception (4b)模块的输出之上。在训练期间，它们的损失以折扣权重（辅助分类器损失的权重是0.3）加到网络的整个损失上。在推断时，这些辅助网络被丢弃。后面的控制实验表明辅助网络的影响相对较小（约0.5），只需要其中一个就能取得同样的效果。</p>
<p>包括辅助分类器在内的附加网络的具体结构如下：</p>
<ul>
<li>一个滤波器大小5×5，步长为3的平均池化层，导致(4a)阶段的输出为4×4×512，(4d)的输出为4×4×528。</li>
<li>具有128个滤波器的1×1卷积，用于降维和修正线性激活。</li>
<li>一个全连接层，具有1024个单元和修正线性激活。</li>
<li>丢弃70%输出的丢弃层。</li>
<li>使用带有softmax损失的线性层作为分类器（作为主分类器预测同样的1000类，但在推断时移除）。</li>
</ul>
<p>最终的网络模型图如图3所示。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/0001.jpg" alt="Figure 3"></p>
<p>图3：含有的所有结构的GoogLeNet网络。</p>
<h2 id="6-训练方法"><a href="#6-训练方法" class="headerlink" title="6. 训练方法"></a>6. 训练方法</h2><p>GoogLeNet网络使用DistBelief[4]分布式机器学习系统进行训练，该系统使用适量的模型和数据并行。尽管我们仅使用一个基于CPU的实现，但粗略的估计表明GoogLeNet网络可以用更少的高端GPU在一周之内训练到收敛，主要的限制是内存使用。我们的训练使用异步随机梯度下降，动量参数为0.9[17]，固定的学习率计划（每8次遍历下降学习率4%）。Polyak平均[13]在推断时用来创建最终的模型。</p>
<p>图像采样方法在过去几个月的竞赛中发生了重大变化，并且已收敛的模型在其他选项上进行了训练，有时还结合着超参数的改变，例如丢弃和学习率。因此，很难对训练这些网络的最有效的单一方式给出明确指导。让事情更复杂的是，受[8]的启发，一些模型主要是在相对较小的裁剪图像进行训练，其它模型主要是在相对较大的裁剪图像上进行训练。然而，一个经过验证的方案在竞赛后工作地很好，包括各种尺寸的图像块的采样，它的尺寸均匀分布在图像区域的8%——100%之间，方向角限制为$[\frac {3} {4}, \frac {4} {3}]$之间。另外，我们发现Andrew Howard[8]的光度扭曲对于克服训练数据成像条件的过拟合是有用的。</p>
<h2 id="7-ILSVRC-2014分类挑战赛设置和结果"><a href="#7-ILSVRC-2014分类挑战赛设置和结果" class="headerlink" title="7. ILSVRC 2014分类挑战赛设置和结果"></a>7. ILSVRC 2014分类挑战赛设置和结果</h2><p>ILSVRC 2014分类挑战赛包括将图像分类到ImageNet层级中1000个叶子结点类别的任务。训练图像大约有120万张，验证图像有5万张，测试图像有10万张。每一张图像与一个实际类别相关联，性能度量基于分类器预测的最高分。通常报告两个数字：top-1准确率，比较实际类别和第一个预测类别，top-5错误率，比较实际类别与前5个预测类别：如果图像实际类别在top-5中，则认为图像分类正确，不管它在top-5中的排名。挑战赛使用top-5错误率来进行排名。</p>
<p>我们参加竞赛时没有使用外部数据来训练。除了本文中前面提到的训练技术之外，我们在获得更高性能的测试中采用了一系列技巧，描述如下。</p>
<ol>
<li>我们独立训练了7个版本的相同的GoogLeNet模型（包括一个更广泛的版本），并用它们进行了整体预测。这些模型的训练具有相同的初始化（甚至具有相同的初始权重，由于监督）和学习率策略。它们仅在采样方法和随机输入图像顺序方面不同。</li>
<li>在测试中，我们采用比Krizhevsky等人[9]更积极的裁剪方法。具体来说，我们将图像归一化为四个尺度，其中较短维度（高度或宽度）分别为256，288，320和352，取这些归一化的图像的左，中，右方块（在肖像图片中，我们采用顶部，中心和底部方块）。对于每个方块，我们将采用4个角以及中心224×224裁剪图像以及方块尺寸归一化为224×224，以及它们的镜像版本。这导致每张图像会得到4×3×6×2 = 144的裁剪图像。前一年的输入中，Andrew Howard[8]采用了类似的方法，经过我们实证验证，其方法略差于我们提出的方案。我们注意到，在实际应用中，这种积极裁剪可能是不必要的，因为存在合理数量的裁剪图像后，更多裁剪图像的好处会变得很微小（正如我们后面展示的那样）。</li>
<li>softmax概率在多个裁剪图像上和所有单个分类器上进行平均，然后获得最终预测。在我们的实验中，我们分析了验证数据的替代方法，例如裁剪图像上的最大池化和分类器的平均，但是它们比简单平均的性能略逊。</li>
</ol>
<p>在本文的其余部分，我们分析了有助于最终提交整体性能的多个因素。</p>
<p>竞赛中我们的最终提交在验证集和测试集上得到了<code>top-5 6.67%</code>的错误率，在其它的参与者中排名第一。与2012年的SuperVision方法相比相对减少了56.5%，与前一年的最佳方法（Clarifai）相比相对减少了约40%，这两种方法都使用了外部数据训练分类器。表2显示了过去三年中一些表现最好的方法的统计。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet-Table%202.png" alt="Table 2"></p>
<p>我们也分析报告了多种测试选择的性能，当预测图像时通过改变表3中使用的模型数目和裁剪图像数目。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%203.png" alt="Table 3"></p>
<h2 id="8-ILSVRC-2014检测挑战赛设置和结果"><a href="#8-ILSVRC-2014检测挑战赛设置和结果" class="headerlink" title="8. ILSVRC 2014检测挑战赛设置和结果"></a>8. ILSVRC 2014检测挑战赛设置和结果</h2><p>ILSVRC检测任务是为了在200个可能的类别中生成图像中目标的边界框。如果检测到的对象匹配的它们实际类别并且它们的边界框重叠至少50%（使用Jaccard索引），则将检测到的对象记为正确。无关的检测记为假阳性且被惩罚。与分类任务相反，每张图像可能包含多个对象或没有对象，并且它们的尺度可能是变化的。报告的结果使用平均精度均值（mAP）。GoogLeNet检测采用的方法类似于R-CNN[6]，但用Inception模块作为区域分类器进行了增强。此外，为了更高的目标边界框召回率，通过选择搜索[20]方法和多箱[5]预测相结合改进了区域生成步骤。为了减少假阳性的数量，超分辨率的尺寸增加了2倍。这将选择搜索算法的区域生成减少了一半。我们总共补充了200个来自多盒结果的区域生成，大约60%的区域生成用于[6]，同时将覆盖率从92%提高到93%。减少区域生成的数量，增加覆盖率的整体影响是对于单个模型的情况平均精度均值增加了1%。最后，等分类单个区域时，我们使用了6个GoogLeNets的组合。这导致准确率从40%提高到43.9%。注意，与R-CNN相反，由于缺少时间我们没有使用边界框回归。</p>
<p>我们首先报告了最好检测结果，并显示了从第一版检测任务以来的进展。与2013年的结果相比，准确率几乎翻了一倍。所有表现最好的团队都使用了卷积网络。我们在表4中报告了官方的分数和每个队伍的常见策略：使用外部数据、集成模型或上下文模型。外部数据通常是ILSVRC12的分类数据，用来预训练模型，后面在检测数据集上进行改善。一些团队也提到使用定位数据。由于定位任务的边界框很大一部分不在检测数据集中，所以可以用该数据预训练一般的边界框回归器，这与分类预训练的方式相同。GoogLeNet输入没有使用定位数据进行预训练。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%204.png" alt="Table 4"></p>
<p>在表5中，我们仅比较了单个模型的结果。最好性能模型是Deep Insight的，令人惊讶的是3个模型的集合仅提高了0.3个点，而GoogLeNet在模型集成时明显获得了更好的结果。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%205.png" alt="Table 5"></p>
<h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9. 总结"></a>9. 总结</h2><p>我们的结果取得了坚实的证据，即通过易获得的密集构造块来近似期望的最优稀疏结果是改善计算机视觉神经网络的一种可行方法。相比于较浅且较窄的架构，这个方法的主要优势是在计算需求适度增加的情况下有显著的质量收益。</p>
<p>我们的目标检测工作虽然没有利用上下文，也没有执行边界框回归，但仍然具有竞争力，这进一步显示了Inception架构优势的证据。</p>
<p>对于分类和检测，预期通过更昂贵的类似深度和宽度的非Inception类型网络可以实现类似质量的结果。然而，我们的方法取得了可靠的证据，即转向更稀疏的结构一般来说是可行有用的想法。这表明未来的工作将在[2]的基础上以自动化方式创建更稀疏更精细的结构，以及将Inception架构的思考应用到其他领域。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Know your meme: We need to go deeper. <a href="http://knowyourmeme.com/memes/we-need-to-go-deeper" target="_blank" rel="external">http://knowyourmeme.com/memes/we-need-to-go-deeper</a>. Accessed: 2014-09-15.</p>
<p>[2] S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable bounds for learning some deep representations. CoRR, abs/1310.6343, 2013.</p>
<p>[3] U. V. C ̧atalyu ̈rek, C. Aykanat, and B. Uc ̧ar. On two-dimensional sparse matrix partitioning: Models, methods, and a recipe. SIAM J. Sci. Comput., 32(2):656–683, Feb. 2010.</p>
<p>[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, NIPS, pages 1232–1240. 2012.</p>
<p>[5] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014.</p>
<p>[6] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014. CVPR 2014. IEEE Conference on, 2014.</p>
<p>[7] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.</p>
<p>[8] A. G. Howard. Some improvements on deep convolutional neural network based image classification. CoRR, abs/1312.5402, 2013.</p>
<p>[9] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.</p>
<p>[10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comput., 1(4):541–551, Dec. 1989.</p>
<p>[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.</p>
<p>[12] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013.</p>
<p>[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Control Optim., 30(4):838–855, July 1992.</p>
<p>[14] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013.</p>
<p>[15] T. Serre, L. Wolf, S. M. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach. Intell., 29(3):411–426, 2007.</p>
<p>[16] F. Song and J. Dongarra. Scaling up matrix computations on shared-memory manycore systems with 1000 cpu cores. In Proceedings of the 28th ACM Interna- tional Conference on Supercomputing, ICS ’14, pages 333–342, New York, NY, USA, 2014. ACM.</p>
<p>[17] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. On the importance of initialization and momentum in deep learning. In ICML, volume 28 of JMLR Proceed- ings, pages 1139–1147. JMLR.org, 2013.</p>
<p>[18] C.Szegedy,A.Toshev,andD.Erhan.Deep neural networks for object detection. In C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, editors, NIPS, pages 2553–2561, 2013.</p>
<p>[19] A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. CoRR, abs/1312.4659, 2013.</p>
<p>[20] K. E. A. van de Sande, J. R. R. Uijlings, T. Gevers, and A. W. M. Smeulders. Segmentation as selective search for object recognition. In Proceedings of the 2011 International Conference on Computer Vision, ICCV ’11, pages 1879–1886, Washington, DC, USA, 2011. IEEE Computer Society.</p>
<p>[21] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, ECCV, volume 8689 of Lecture Notes in Computer Science, pages 818–833. Springer, 2014.</p>
]]></content>
    
    <summary type="html">
    
      GoogLeNet论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>GoogleNet论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/07/21/2017-7-21-GoogleNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2017/07/21/2017-7-21-GoogleNet论文翻译——中英文对照/</id>
    <published>2017-07-21T09:49:43.000Z</published>
    <updated>2017-07-31T09:18:41.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/googlenet.png" alt="Deep Learning"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="Going-Deeper-with-Convolutions"><a href="#Going-Deeper-with-Convolutions" class="headerlink" title="Going Deeper with Convolutions"></a>Going Deeper with Convolutions</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.</p>
<p>我们在ImageNet大规模视觉识别挑战赛2014（ILSVRC14）上提出了一种代号为Inception的深度卷积神经网络结构，并在分类和检测上取得了新的最好结果。这个架构的主要特点是提高了网络内部计算资源的利用率。通过精心的手工设计，我们在增加了网络深度和广度的同时保持了计算预算不变。为了优化质量，架构的设计以赫布理论和多尺度处理直觉为基础。我们在ILSVRC14提交中应用的一个特例被称为GoogLeNet，一个22层的深度网络，其质量在分类和检测的背景下进行了评估。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>In the last three years, our object classification and detection capabilities have dramatically improved due to advances in deep learning and convolutional networks [10]. One encouraging news is that most of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures. No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes. Our GoogLeNet submission to ILSVRC 2014 actually uses 12 times fewer parameters than the winning architecture of Krizhevsky et al [9] from two years ago, while being significantly more accurate. On the object detection front, the biggest gains have not come from naive application of bigger and bigger deep networks, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al [6].</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>过去三年中，由于深度学习和卷积网络的发展[10]，我们的目标分类和检测能力得到了显著提高。一个令人鼓舞的消息是，大部分的进步不仅仅是更强大硬件、更大数据集、更大模型的结果，而主要是新的想法、算法和网络结构改进的结果。例如，ILSVRC 2014竞赛中最靠前的输入除了用于检测目的的分类数据集之外，没有使用新的数据资源。我们在ILSVRC 2014中的GoogLeNet提交实际使用的参数只有两年前Krizhevsky等人[9]获胜结构参数的1/12，而结果明显更准确。在目标检测前沿，最大的收获不是来自于越来越大的深度网络的简单应用，而是来自于深度架构和经典计算机视觉的协同，像Girshick等人[6]的R-CNN算法那样。</p>
<p>Another notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms —— especially their power and memory use —— gains importance. It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that the they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.</p>
<p>另一个显著因素是随着移动和嵌入式设备的推动，我们的算法的效率很重要——尤其是它们的电力和内存使用。值得注意的是，正是包含了这个因素的考虑才得出了本文中呈现的深度架构设计，而不是单纯的为了提高准确率。对于大多数实验来说，模型被设计为在一次推断中保持15亿乘加的计算预算，所以最终它们不是单纯的学术好奇心，而是能在现实世界中应用，甚至是以合理的代价在大型数据集上使用。</p>
<p>In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al [12] in conjunction with the famous “we need to go deeper” internet meme [1]. In our case, the word “deep” is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the “Inception module” and also in the more direct sense of increased network depth. In general, one can view the Inception model as a logical culmination of [12] while taking inspiration and guidance from the theoretical work by Arora et al [2]. The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges, where it significantly outperforms the current state of the art.</p>
<p>在本文中，我们将关注一个高效的计算机视觉深度神经网络架构，代号为Inception，它的名字来自于Lin等人[12]网络论文中的Network与著名的“we need to go deeper”网络迷因[1]的结合。在我们的案例中，单词“deep”用在两个不同的含义中：首先，在某种意义上，我们以“Inception module”的形式引入了一种新层次的组织方式，在更直接的意义上增加了网络的深度。一般来说，可以把Inception模型看作论文[12]的逻辑顶点同时从Arora等人[2]的理论工作中受到了鼓舞和引导。这种架构的好处在ILSVRC 2014分类和检测挑战赛中通过实验得到了验证，它明显优于目前的最好水平。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>Starting with LeNet-5 [10], convolutional neural networks (CNN) have typically had a standard structure —— stacked convolutional layers (optionally followed by contrast normalization and max-pooling) are followed by one or more fully-connected layers. Variants of this basic design are prevalent in the image classification literature and have yielded the best results to-date on MNIST, CIFAR and most notably on the ImageNet classification challenge [9, 21]. For larger datasets such as Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14], while using dropout [7] to address the problem of overfitting.</p>
<h2 id="2-近期工作"><a href="#2-近期工作" class="headerlink" title="2. 近期工作"></a>2. 近期工作</h2><p>从LeNet-5 [10]开始，卷积神经网络（CNN）通常有一个标准结构——堆叠的卷积层（后面可以选择有对比归一化和最大池化）后面是一个或更多的全连接层。这个基本设计的变种在图像分类著作流行，并且目前为止在MNIST，CIFAR和更著名的ImageNet分类挑战赛中[9, 21]的已经取得了最佳结果。对于更大的数据集例如ImageNet来说，最近的趋势是增加层的数目[12]和层的大小[21, 14]，同时使用丢弃[7]来解决过拟合问题。</p>
<p>Despite concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network architecture as [9] has also been successfully employed for localization [9, 14], object detection [6, 14, 18, 5] and human pose estimation [19].</p>
<p>尽管担心最大池化层会引起准确空间信息的损失，但与[9]相同的卷积网络结构也已经成功的应用于定位[9, 14]，目标检测[6, 14, 18, 5]和行人姿态估计[19]。</p>
<p>Inspired by a neuroscience model of the primate visual cortex, Serre et al. [15] used a series of fixed Gabor filters of different sizes to handle multiple scales. We use a similar strategy here. However, contrary to the fixed 2-layer deep model of [15], all filters in the Inception architecture are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model.</p>
<p>受灵长类视觉皮层神经科学模型的启发，Serre等人[15]使用了一系列固定的不同大小的Gabor滤波器来处理多尺度。我们使用一个了类似的策略。然而，与[15]的固定的2层深度模型相反，Inception结构中所有的滤波器是学习到的。此外，Inception层重复了很多次，在GoogLeNet模型中得到了一个22层的深度模型。</p>
<p>Network-in-Network is an approach proposed by Lin et al. [12] in order to increase the representational power of neural networks. In their model, additional 1 × 1 convolutional layers are added to the network, increasing its depth. We use this approach heavily in our architecture. However, in our setting, 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks, that would otherwise limit the size of our networks. This allows for not just increasing the depth, but also the width of our networks without a significant performance penalty.</p>
<p>Network-in-Network是Lin等人[12]为了增加神经网络表现能力而提出的一种方法。在他们的模型中，网络中添加了额外的1 × 1卷积层，增加了网络的深度。我们的架构中大量的使用了这个方法。但是，在我们的设置中，1 × 1卷积有两个目的：最关键的是，它们主要是用来作为降维模块来移除卷积瓶颈，否则将会限制我们网络的大小。这不仅允许了深度的增加，而且允许我们网络的宽度增加但没有明显的性能损失。</p>
<p>Finally, the current state of the art for object detection is the Regions with Convolutional Neural Networks (R-CNN) method by Girshick et al. [6]. R-CNN decomposes the overall detection problem into two subproblems: utilizing low-level cues such as color and texture in order to generate object location proposals in a category-agnostic fashion and using CNN classifiers to identify object categories at those locations. Such a two stage approach leverages the accuracy of bounding box segmentation with low-level cues, as well as the highly powerful classification power of state-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have explored enhancements in both stages, such as multi-box [5] prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.</p>
<p>最后，目前最好的目标检测是Girshick等人[6]的基于区域的卷积神经网络（R-CNN）方法。R-CNN将整个检测问题分解为两个子问题：利用低层次的信号例如颜色，纹理以跨类别的方式来产生目标位置候选区域，然后用CNN分类器来识别那些位置上的对象类别。这样一种两个阶段的方法利用了低层特征分割边界框的准确性，也利用了目前的CNN非常强大的分类能力。我们在我们的检测提交中采用了类似的方式，但探索增强这两个阶段，例如对于更高的目标边界框召回使用多盒[5]预测，并融合了更好的边界框候选区域分类方法。</p>
<h2 id="3-Motivation-and-High-Level-Considerations"><a href="#3-Motivation-and-High-Level-Considerations" class="headerlink" title="3. Motivation and High Level Considerations"></a>3. Motivation and High Level Considerations</h2><p>The most straightforward way of improving the performance of deep neural networks is by increasing their size. This includes both increasing the depth —— the number of network levels —— as well as its width: the number of units at each level. This is an easy and safe way of training higher quality models, especially given the availability of a large amount of labeled training data. However, this simple solution comes with two major drawbacks.<br>Bigger size typically means a larger number of parameters, which makes the enlarged network more prone to overfitting, especially if the number of labeled examples in the training set is limited. This is a major bottleneck as strongly labeled datasets are laborious and expensive to obtain, often requiring expert human raters to distinguish between various fine-grained visual categories such as those in ImageNet (even in the 1000-class ILSVRC subset) as shown in Figure 1.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/g_figure%201.png" alt="Figure 1"></p>
<p>Figure 1: Two distinct classes from the 1000 classes of the ILSVRC 2014 classification challenge. Domain knowledge is required to distinguish between these classes.</p>
<h2 id="3-动机和高层思考"><a href="#3-动机和高层思考" class="headerlink" title="3. 动机和高层思考"></a>3. 动机和高层思考</h2><p>提高深度神经网络性能最直接的方式是增加它们的尺寸。这不仅包括增加深度——网络层次的数目——也包括它的宽度：每一层的单元数目。这是一种训练更高质量模型容易且安全的方法，尤其是在可获得大量标注的训练数据的情况下。但是这个简单方案有两个主要的缺点。更大的尺寸通常意味着更多的参数，这会使增大的网络更容易过拟合，尤其是在训练集的标注样本有限的情况下。这是一个主要的瓶颈，因为要获得强标注数据集费时费力且代价昂贵，经常需要专家评委在各种细粒度的视觉类别进行区分，例如图1中显示的ImageNet中的类别（甚至是1000类ILSVRC的子集）。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/g_figure%201.png" alt="Figure 1"></p>
<p>图1: ILSVRC 2014分类挑战赛的1000类中两个不同的类别。区分这些类别需要领域知识。</p>
<p>The other drawback of uniformly increased network size is the dramatically increased use of computational resources. For example, in a deep vision network, if two convolutional layers are chained, any uniform increase in the number of their filters results in a quadratic increase of computation. If the added capacity is used inefficiently (for example, if most weights end up to be close to zero), then much of the computation is wasted. As the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of size, even when the main objective is to increase the quality of performance.</p>
<p>均匀增加网络尺寸的另一个缺点是计算资源使用的显著增加。例如，在一个深度视觉网络中，如果两个卷积层相连，它们的滤波器数目的任何均匀增加都会引起计算量平方式的增加。如果增加的能力使用时效率低下（例如，如果大多数权重结束时接近于0），那么会浪费大量的计算能力。由于计算预算总是有限的，计算资源的有效分布更偏向于尺寸无差别的增加，即使主要目标是增加性能的质量。</p>
<p>A fundamental way of solving both of these issues would be to introduce sparsity and replace the fully connected layers by the sparse ones, even inside the convolutions. Besides mimicking biological systems, this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora et al. [2]. Their main result states that if the probability distribution of the dataset is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer after layer by analyzing the correlation statistics of the preceding layer activations and clustering neurons with highly correlated outputs. Although the strict mathematical proof requires very strong conditions, the fact that this statement resonates with the well known Hebbian principle —— neurons that fire together, wire together —— suggests that the underlying idea is applicable even under less strict conditions, in practice.</p>
<p>解决这两个问题的一个基本的方式就是引入稀疏性并将全连接层替换为稀疏的全连接层，甚至是卷积层。除了模仿生物系统之外，由于Arora等人[2]的开创性工作，这也具有更坚固的理论基础优势。他们的主要成果说明如果数据集的概率分布可以通过一个大型稀疏的深度神经网络表示，则最优的网络拓扑结构可以通过分析前一层激活的相关性统计和聚类高度相关的神经元来一层层的构建。虽然严格的数学证明需要在很强的条件下，但事实上这个声明与著名的赫布理论产生共鸣——神经元一起激发，一起连接——实践表明，基础概念甚至适用于不严格的条件下。</p>
<p>Unfortunately, today’s computing infrastructures are very inefficient when it comes to numerical calculation on non-uniform sparse data structures. Even if the number of arithmetic operations is reduced by 100×, the overhead of lookups and cache misses would dominate: switching to sparse matrices might not pay off. The gap is widened yet further by the use of steadily improving and highly tuned numerical libraries that allow for extremely fast dense matrix multiplication, exploiting the minute details of the underlying CPU or GPU hardware [16, 9]. Also, non-uniform sparse models require more sophisticated engineering and computing infrastructure. Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections to the patches in the earlier layer. ConvNets have traditionally used random and sparse connection tables in the feature dimensions since [11] in order to break the symmetry and improve learning, yet the trend changed back to full connections with [9] in order to further optimize parallel computation. Current state-of-the-art architectures for computer vision have uniform structure. The large number of filters and greater batch size allows for the efficient use of dense computation.</p>
<p>遗憾的是，当碰到在非均匀的稀疏数据结构上进行数值计算时，现在的计算架构效率非常低下。即使算法运算的数量减少100倍，查询和缓存丢失上的开销仍占主导地位：切换到稀疏矩阵可能是不可行的。随着稳定提升和高度调整的数值库的应用，差距仍在进一步扩大，数值库要求极度快速密集的矩阵乘法，利用底层的CPU或GPU硬件[16, 9]的微小细节。非均匀的稀疏模型也要求更多的复杂工程和计算基础结构。目前大多数面向视觉的机器学习系统通过采用卷积的优点来利用空域的稀疏性。然而，卷积被实现为对上一层块的密集连接的集合。为了打破对称性，提高学习水平，从论文[11]开始，ConvNets习惯上在特征维度使用随机的稀疏连接表，然而为了进一步优化并行计算，论文[9]中趋向于变回全连接。目前最新的计算机视觉架构有统一的结构。更多的滤波器和更大的批大小要求密集计算的有效使用。</p>
<p>This raises the question of whether there is any hope for a next, intermediate step: an architecture that makes use of filter-level sparsity, as suggested by the theory, but exploits our current hardware by utilizing computations on dense matrices. The vast literature on sparse matrix computations (e.g. [3]) suggests that clustering sparse matrices into relatively dense submatrices tends to give competitive performance for sparse matrix multiplication. It does not seem far-fetched to think that similar methods would be utilized for the automated construction of non-uniform deep-learning architectures in the near future.</p>
<p>这提出了下一个中间步骤是否有希望的问题：一个架构能利用滤波器水平的稀疏性，正如理论所认为的那样，但能通过利用密集矩阵计算来利用我们目前的硬件。稀疏矩阵乘法的大量文献（例如[3]）认为对于稀疏矩阵乘法，将稀疏矩阵聚类为相对密集的子矩阵会有更佳的性能。在不久的将来会利用类似的方法来进行非均匀深度学习架构的自动构建，这样的想法似乎并不牵强。</p>
<p>The Inception architecture started out as a case study for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by [2] for vision networks and covering the hypothesized outcome by dense, readily available components. Despite being a highly speculative undertaking, modest gains were observed early on when compared with reference networks based on [12]. With a bit of tuning the gap widened and Inception proved to be especially useful in the context of localization and object detection as the base network for [6] and [5]. Interestingly, while most of the original architectural choices have been questioned and tested thoroughly in separation, they turned out to be close to optimal locally. One must be cautious though: although the Inception architecture has become a success for computer vision, it is still questionable whether this can be attributed to the guiding principles that have lead to its construction. Making sure of this would require a much more thorough analysis and verification.</p>
<p>Inception架构开始是作为案例研究，用于评估一个复杂网络拓扑构建算法的假设输出，该算法试图近似[2]中所示的视觉网络的稀疏结构，并通过密集的、容易获得的组件来覆盖假设结果。尽管是一个非常投机的事情，但与基于[12]的参考网络相比，早期可以观测到适度的收益。随着一点点调整加宽差距，作为[6]和[5]的基础网络，Inception被证明在定位上下文和目标检测中尤其有用。有趣的是，虽然大多数最初的架构选择已被质疑并分离开进行全面测试，但结果证明它们是局部最优的。然而必须谨慎：尽管Inception架构在计算机上领域取得成功，但这是否可以归因于构建其架构的指导原则仍是有疑问的。确保这一点将需要更彻底的分析和验证。</p>
<h2 id="4-Architectural-Details"><a href="#4-Architectural-Details" class="headerlink" title="4. Architectural Details"></a>4. Architectural Details</h2><p>The main idea of the Inception architecture is to consider how an optimal local sparse structure of a convolutional vision network can be approximated and covered by readily available dense components. Note that assuming translation invariance means that our network will be built from convolutional building blocks. All we need is to find the optimal local construction and to repeat it spatially. Arora et al. [2] suggests a layer-by-layer construction where one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation. These clusters form the units of the next layer and are connected to the units in the previous layer. We assume that each unit from an earlier layer corresponds to some region of the input image and these units are grouped into filter banks. In the lower layers (the ones close to the input) correlated units would concentrate in local regions. Thus, we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of 1×1 convolutions in the next layer, as suggested in [12]. However, one can also expect that there will be a smaller number of more spatially spread out clusters that can be covered by convolutions over larger patches, and there will be a decreasing number of patches over larger and larger regions. In order to avoid patch-alignment issues, current incarnations of the Inception architecture are restricted to filter sizes 1×1, 3×3 and 5×5; this decision was based more on convenience rather than necessity. It also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage. Additionally, since pooling operations have been essential for the success of current convolutional networks, it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect, too (see Figure 2(a)).</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/goolenet-Fig%202.png" alt="Figure 2"></p>
<h2 id="4-架构细节"><a href="#4-架构细节" class="headerlink" title="4. 架构细节"></a>4. 架构细节</h2><p>Inception架构的主要想法是考虑怎样近似卷积视觉网络的最优稀疏结构并用容易获得的密集组件进行覆盖。注意假设转换不变性，这意味着我们的网络将以卷积构建块为基础。我们所需要做的是找到最优的局部构造并在空间上重复它。Arora等人[2]提出了一个层次结构，其中应该分析最后一层的相关统计并将它们聚集成具有高相关性的单元组。这些聚类形成了下一层的单元并与前一层的单元连接。我们假设较早层的每个单元都对应输入层的某些区域，并且这些单元被分成滤波器组。在较低的层（接近输入的层）相关单元集中在局部区域。因此，如[12]所示，我们最终会有许多聚类集中在单个区域，它们可以通过下一层的1×1卷积层覆盖。然而也可以预期，将存在更小数目的在更大空间上扩展的聚类，其可以被更大块上的卷积覆盖，在越来越大的区域上块的数量将会下降。为了避免块校正的问题，目前Inception架构形式的滤波器的尺寸仅限于1×1、3×3、5×5，这个决定更多的是基于便易性而不是必要性。这也意味着提出的架构是所有这些层的组合，其输出滤波器组连接成单个输出向量形成了下一阶段的输入。另外，由于池化操作对于目前卷积网络的成功至关重要，因此建议在每个这样的阶段添加一个替代的并行池化路径应该也应该具有额外的有益效果（看图2(a)）。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/goolenet-Fig%202.png" alt="Figure 2"></p>
<p>As these “Inception modules” are stacked on top of each other, their output correlation statistics are bound to vary: as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease. This suggests that the ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.</p>
<p>由于这些“Inception模块”在彼此的顶部堆叠，其输出相关统计必然有变化：由于较高层会捕获较高的抽象特征，其空间集中度预计会减少。这表明随着转移到更高层，3×3和5×5卷积的比例应该会增加。</p>
<p>One big problem with the above modules, at least in this naive form, is that even a modest number of 5×5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters. This problem becomes even more pronounced once pooling units are added to the mix: the number of output filters equals to the number of filters in the previous stage. The merging of output of the pooling layer with outputs of the convolutional layers would lead to an inevitable increase in the number of outputs from stage to stage. While this architecture might cover the optimal sparse structure, it would do it very inefficiently, leading to a computational blow up within a few stages.</p>
<p>上述模块的一个大问题是在具有大量滤波器的卷积层之上，即使适量的5×5卷积也可能是非常昂贵的，至少在这种朴素形式中有这个问题。一旦池化单元添加到混合中，这个问题甚至会变得更明显：输出滤波器的数量等于前一阶段滤波器的数量。池化层输出和卷积层输出的合并会导致这一阶段到下一阶段输出数量不可避免的增加。虽然这种架构可能会覆盖最优稀疏结构，但它会非常低效，导致在几个阶段内计算量爆炸。</p>
<p>This leads to the second idea of the Inception architecture: judiciously reducing dimension wherever the computational requirements would increase too much otherwise. This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch. However, embeddings represent information in a dense, compressed form and compressed information is harder to process. The representation should be kept sparse at most places (as required by the conditions of [2]) and compress the signals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation making them dual-purpose. The final result is depicted in Figure 2(b).</p>
<p>这导致了Inception架构的第二个想法：在计算要求会增加太多的地方，明智地减少维度。这是基于嵌入的成功：甚至低维嵌入可能包含大量关于较大图像块的信息。然而嵌入以密集、压缩形式表示信息并且压缩信息更难处理。这种表示应该在大多数地方保持稀疏（根据[2]中条件的要求】）并且仅在它们必须汇总时才压缩信号。也就是说，在昂贵的3×3和5×5卷积之前，1×1卷积用来计算降维。除了用来降维之外，它们也包括使用线性修正单元使其两用。最终的结果如图2(b)所示。</p>
<p>In general, an Inception network is a network consisting of modules of the above type stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid. For technical reasons (memory efficiency during training), it seemed beneficial to start using Inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion. This is not strictly necessary, simply reflecting some infrastructural inefficiencies in our current implementation.</p>
<p>通常，Inception网络是一个由上述类型的模块互相堆叠组成的网络，偶尔会有步长为2的最大池化层将网络分辨率减半。出于技术原因（训练过程中内存效率），只在更高层开始使用Inception模块而在更低层仍保持传统的卷积形式似乎是有益的。这不是绝对必要的，只是反映了我们目前实现中的一些基础结构效率低下。</p>
<p>A useful aspect of this architecture is that it allows for increasing the number of units at each stage significantly without an uncontrolled blow-up in computational complexity at later stages. This is achieved by the ubiquitous use of dimensionality reduction prior to expensive convolutions with larger patch sizes. Furthermore, the design follows the practical intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from the different scales simultaneously.</p>
<p>该架构的一个有用的方面是它允许显著增加每个阶段的单元数量，而不会在后面的阶段出现计算复杂度不受控制的爆炸。这是在尺寸较大的块进行昂贵的卷积之前通过普遍使用降维实现的。此外，设计遵循了实践直觉，即视觉信息应该在不同的尺度上处理然后聚合，为的是下一阶段可以从不同尺度同时抽象特征。</p>
<p>The improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties. One can utilize the Inception architecture to create slightly inferior, but computationally cheaper versions of it. We have found that all the available knobs and levers allow for a controlled balancing of computational resources resulting in networks that are 3—10× faster than similarly performing networks with non-Inception architecture, however this requires careful manual design at this point.</p>
<p>计算资源的改善使用允许增加每个阶段的宽度和阶段的数量，而不会陷入计算困境。可以利用Inception架构创建略差一些但计算成本更低的版本。我们发现所有可用的控制允许计算资源的受控平衡，导致网络比没有Inception结构的类似执行网络快3—10倍，但是在这一点上需要仔细的手动设计。</p>
<h2 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5. GoogLeNet"></a>5. GoogLeNet</h2><p>By the “GoogLeNet” name we refer to the particular incarnation of the Inception architecture used in our submission for the ILSVRC 2014 competition. We also used one deeper and wider Inception network with slightly superior quality, but adding it to the ensemble seemed to improve the results only marginally. We omit the details of that network, as empirical evidence suggests that the influence of the exact architectural parameters is relatively minor. Table 1 illustrates the most common instance of Inception used in the competition. This network (trained with different image patch sampling methods) was used for 6 out of the 7 models in our ensemble.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet-Table%201.png" alt="Table 1"></p>
<h2 id="5-GoogLeNet-1"><a href="#5-GoogLeNet-1" class="headerlink" title="5. GoogLeNet"></a>5. GoogLeNet</h2><p>通过“GoogLeNet”这个名字，我们提到了在ILSVRC 2014竞赛的提交中使用的Inception架构的特例。我们也使用了一个稍微优质的更深更宽的Inception网络，但将其加入到组合中似乎只稍微提高了结果。我们忽略了该网络的细节，因为经验证据表明确切架构的参数影响相对较小。表1说明了竞赛中使用的最常见的Inception实例。这个网络（用不同的图像块采样方法训练的）使用了我们组合中7个模型中的6个。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet-Table%201.png" alt="Table 1"></p>
<p>All the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224×224 in the RGB color space with zero mean. “#3×3 reduce” and “#5×5 reduce” stands for the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can see the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation as well.</p>
<p>所有的卷积都使用了修正线性激活，包括Inception模块内部的卷积。在我们的网络中感受野是在均值为0的RGB颜色空间中，大小是224×224。“#3×3 reduce”和“#5×5 reduce”表示在3×3和5×5卷积之前，降维层使用的1×1滤波器的数量。在pool proj列可以看到内置的最大池化之后，投影层中1×1滤波器的数量。所有的这些降维/投影层也都使用了线性修正激活。</p>
<p>The network was designed with computational efficiency and practicality in mind, so that inference can be run on individual devices including even those with limited computational resources, especially with low-memory footprint.The network is 22 layers deep when counting only layers with parameters (or 27 layers if we also count pooling). The overall number of layers (independent building blocks) used for the construction of the network is about 100. The exact number depends on how layers are counted by the machine learning infrastructure. The use of average pooling before the classifier is based on [12], although our implementation has an additional linear layer. The linear layer enables us to easily adapt our networks to other label sets, however it is used mostly for convenience and we do not expect it to have a major effect. We found that a move from fully connected layers to average pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained essential even after removing the fully connected layers.</p>
<p>网络的设计考虑了计算效率和实用性，因此推断可以单独的设备上运行，甚至包括那些计算资源有限的设备，尤其是低内存占用的设备。当只计算有参数的层时，网络有22层（如果我们也计算池化层是27层）。构建网络的全部层（独立构建块）的数目大约是100。确切的数量取决于机器学习基础设施对层的计算方式。分类器之前的平均池化是基于[12]的，尽管我们的实现有一个额外的线性层。线性层使我们的网络能很容易地适应其它的标签集，但它主要是为了方便使用，我们不期望它有重大的影响。我们发现从全连接层变为平均池化，提高了大约<code>top-1 %0.6</code>的准确率，然而即使在移除了全连接层之后，丢失的使用还是必不可少的。</p>
<p>Given relatively large depth of the network, the ability to propagate gradients back through all the layers in an effective manner was a concern. The strong performance of shallower networks on this task suggests that the features produced by the layers in the middle of the network should be very discriminative. By adding auxiliary classifiers connected to these intermediate layers, discrimination in the lower stages in the classifier was expected. This was thought to combat the vanishing gradient problem while providing regularization. These classifiers take the form of smaller convolutional networks put on top of the output of the Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are discarded. Later control experiments have shown that the effect of the auxiliary networks is relatively minor (around 0.5%) and that it required only one of them to achieve the same effect.</p>
<p>给定深度相对较大的网络，有效传播梯度反向通过所有层的能力是一个问题。在这个任务上，更浅网络的强大性能表明网络中部层产生的特征应该是非常有识别力的。通过将辅助分类器添加到这些中间层，可以期望较低阶段分类器的判别力。这被认为是在提供正则化的同时克服梯度消失问题。这些分类器采用较小卷积网络的形式，放置在Inception (4a)和Inception (4b)模块的输出之上。在训练期间，它们的损失以折扣权重（辅助分类器损失的权重是0.3）加到网络的整个损失上。在推断时，这些辅助网络被丢弃。后面的控制实验表明辅助网络的影响相对较小（约0.5），只需要其中一个就能取得同样的效果。</p>
<p>The exact structure of the extra network on the side, including the auxiliary classifier, is as follows:</p>
<ul>
<li>An average pooling layer with 5×5 filter size and stride 3, resulting in an 4×4×512 output for the (4a), and 4×4×528 for the (4d) stage.</li>
<li>A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.</li>
<li>A fully connected layer with 1024 units and rectified linear activation.</li>
<li>A dropout layer with 70% ratio of dropped outputs.</li>
<li>A linear layer with softmax loss as the classifier (predicting the same 1000 classes as the main classifier, but removed at inference time).</li>
</ul>
<p>A schematic view of the resulting network is depicted in Figure 3.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/0001.jpg" alt="Figure 3"></p>
<p>Figure 3: GoogLeNet network with all the bells and whistles.</p>
<p>包括辅助分类器在内的附加网络的具体结构如下：</p>
<ul>
<li>一个滤波器大小5×5，步长为3的平均池化层，导致(4a)阶段的输出为4×4×512，(4d)的输出为4×4×528。</li>
<li>具有128个滤波器的1×1卷积，用于降维和修正线性激活。</li>
<li>一个全连接层，具有1024个单元和修正线性激活。</li>
<li>丢弃70%输出的丢弃层。</li>
<li>使用带有softmax损失的线性层作为分类器（作为主分类器预测同样的1000类，但在推断时移除）。</li>
</ul>
<p>最终的网络模型图如图3所示。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/0001.jpg" alt="Figure 3"></p>
<p>图3：含有的所有结构的GoogLeNet网络。</p>
<h2 id="6-Training-Methodology"><a href="#6-Training-Methodology" class="headerlink" title="6. Training Methodology"></a>6. Training Methodology</h2><p>GoogLeNet networks were trained using the DistBelief [4] distributed machine learning system using modest amount of model and data-parallelism. Although we used a CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week, the main limitation being the memory usage. Our training used asynchronous stochastic gradient descent with 0.9 momentum [17], fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs). Polyak averaging [13] was used to create the final model used at inference time.</p>
<h2 id="6-训练方法"><a href="#6-训练方法" class="headerlink" title="6. 训练方法"></a>6. 训练方法</h2><p>GoogLeNet网络使用DistBelief[4]分布式机器学习系统进行训练，该系统使用适量的模型和数据并行。尽管我们仅使用一个基于CPU的实现，但粗略的估计表明GoogLeNet网络可以用更少的高端GPU在一周之内训练到收敛，主要的限制是内存使用。我们的训练使用异步随机梯度下降，动量参数为0.9[17]，固定的学习率计划（每8次遍历下降学习率4%）。Polyak平均[13]在推断时用来创建最终的模型。</p>
<p>Image sampling methods have changed substantially over the months leading to the competition, and already converged models were trained on with other options, sometimes in conjunction with changed hyperparameters, such as dropout and the learning rate. Therefore, it is hard to give a definitive guidance to the most effective single way to train these networks. To complicate matters further, some of the models were mainly trained on smaller relative crops, others on larger ones, inspired by [8]. Still, one prescription that was verified to work very well after the competition, includes sampling of various sized patches of the image whose size is distributed evenly between 8% and 100% of the image area with aspect ratio constrained to the interval $[\frac {3} {4}, \frac {4} {3}]$. Also, we found that the photometric distortions of Andrew Howard [8] were useful to combat overfitting to the imaging conditions of training data.</p>
<p>图像采样方法在过去几个月的竞赛中发生了重大变化，并且已收敛的模型在其他选项上进行了训练，有时还结合着超参数的改变，例如丢弃和学习率。因此，很难对训练这些网络的最有效的单一方式给出明确指导。让事情更复杂的是，受[8]的启发，一些模型主要是在相对较小的裁剪图像进行训练，其它模型主要是在相对较大的裁剪图像上进行训练。然而，一个经过验证的方案在竞赛后工作地很好，包括各种尺寸的图像块的采样，它的尺寸均匀分布在图像区域的8%——100%之间，方向角限制为$[\frac {3} {4}, \frac {4} {3}]$之间。另外，我们发现Andrew Howard[8]的光度扭曲对于克服训练数据成像条件的过拟合是有用的。</p>
<h2 id="7-ILSVRC-2014-Classification-Challenge-Setup-and-Results"><a href="#7-ILSVRC-2014-Classification-Challenge-Setup-and-Results" class="headerlink" title="7. ILSVRC 2014 Classification Challenge Setup and Results"></a>7. ILSVRC 2014 Classification Challenge Setup and Results</h2><p>The ILSVRC 2014 classification challenge involves the task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing. Each image is associated with one ground truth category, and performance is measured based on the highest scoring classifier predictions. Two numbers are usually reported: the top-1 accuracy rate, which compares the ground truth against the first predicted class, and the top-5 error rate, which compares the ground truth against the first 5 predicted classes: an image is deemed correctly classified if the ground truth is among the top-5, regardless of its rank in them. The challenge uses the top-5 error rate for ranking purposes.</p>
<h2 id="7-ILSVRC-2014分类挑战赛设置和结果"><a href="#7-ILSVRC-2014分类挑战赛设置和结果" class="headerlink" title="7. ILSVRC 2014分类挑战赛设置和结果"></a>7. ILSVRC 2014分类挑战赛设置和结果</h2><p>ILSVRC 2014分类挑战赛包括将图像分类到ImageNet层级中1000个叶子结点类别的任务。训练图像大约有120万张，验证图像有5万张，测试图像有10万张。每一张图像与一个实际类别相关联，性能度量基于分类器预测的最高分。通常报告两个数字：top-1准确率，比较实际类别和第一个预测类别，top-5错误率，比较实际类别与前5个预测类别：如果图像实际类别在top-5中，则认为图像分类正确，不管它在top-5中的排名。挑战赛使用top-5错误率来进行排名。</p>
<p>We participated in the challenge with no external data used for training. In addition to the training techniques aforementioned in this paper, we adopted a set of techniques during testing to obtain a higher performance, which we describe next.</p>
<ol>
<li>We independently trained 7 versions of the same GoogLeNet model (including one wider version), and performed ensemble prediction with them. These models were trained with the same initialization (even with the same initial weights, due to an oversight) and learning rate policies. They differed only in sampling methodologies and the randomized input image order.</li>
<li>During testing, we adopted a more aggressive cropping approach than that of Krizhevsky et al. [9]. Specifically, we resized the image to 4 scales where the shorter dimension (height or width) is 256, 288, 320 and 352 respectively, take the left, center and right square of these resized images (in the case of portrait images, we take the top, center and bottom squares). For each square, we then take the 4 corners and the center 224×224 crop as well as the square resized to 224×224, and their mirrored versions. This leads to 4×3×6×2 = 144 crops per image. A similar approach was used by Andrew Howard [8] in the previous year’s entry, which we empirically verified to perform slightly worse than the proposed scheme. We note that such aggressive cropping may not be necessary in real applications, as the benefit of more crops becomes marginal after a reasonable number of crops are present (as we will show later on).</li>
<li>The softmax probabilities are averaged over multiple crops and over all the individual classifiers to obtain the final prediction. In our experiments we analyzed alternative approaches on the validation data, such as max pooling over crops and averaging over classifiers, but they lead to inferior performance than the simple averaging.</li>
</ol>
<p>我们参加竞赛时没有使用外部数据来训练。除了本文中前面提到的训练技术之外，我们在获得更高性能的测试中采用了一系列技巧，描述如下。</p>
<ol>
<li>我们独立训练了7个版本的相同的GoogLeNet模型（包括一个更广泛的版本），并用它们进行了整体预测。这些模型的训练具有相同的初始化（甚至具有相同的初始权重，由于监督）和学习率策略。它们仅在采样方法和随机输入图像顺序方面不同。</li>
<li>在测试中，我们采用比Krizhevsky等人[9]更积极的裁剪方法。具体来说，我们将图像归一化为四个尺度，其中较短维度（高度或宽度）分别为256，288，320和352，取这些归一化的图像的左，中，右方块（在肖像图片中，我们采用顶部，中心和底部方块）。对于每个方块，我们将采用4个角以及中心224×224裁剪图像以及方块尺寸归一化为224×224，以及它们的镜像版本。这导致每张图像会得到4×3×6×2 = 144的裁剪图像。前一年的输入中，Andrew Howard[8]采用了类似的方法，经过我们实证验证，其方法略差于我们提出的方案。我们注意到，在实际应用中，这种积极裁剪可能是不必要的，因为存在合理数量的裁剪图像后，更多裁剪图像的好处会变得很微小（正如我们后面展示的那样）。</li>
<li>softmax概率在多个裁剪图像上和所有单个分类器上进行平均，然后获得最终预测。在我们的实验中，我们分析了验证数据的替代方法，例如裁剪图像上的最大池化和分类器的平均，但是它们比简单平均的性能略逊。</li>
</ol>
<p>In the remainder of this paper, we analyze the multiple factors that contribute to the overall performance of the final submission.</p>
<p>在本文的其余部分，我们分析了有助于最终提交整体性能的多个因素。</p>
<p>Our final submission to the challenge obtains a top-5 error of 6.67% on both the validation and testing data, ranking the first among other participants. This is a 56.5% relative reduction compared to the SuperVision approach in 2012, and about 40% relative reduction compared to the previous year’s best approach (Clarifai), both of which used external data for training the classifiers. Table 2 shows the statistics of some of the top-performing approaches over the past 3 years.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet-Table%202.png" alt="Table 2"></p>
<p>竞赛中我们的最终提交在验证集和测试集上得到了<code>top-5 6.67%</code>的错误率，在其它的参与者中排名第一。与2012年的SuperVision方法相比相对减少了56.5%，与前一年的最佳方法（Clarifai）相比相对减少了约40%，这两种方法都使用了外部数据训练分类器。表2显示了过去三年中一些表现最好的方法的统计。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet-Table%202.png" alt="Table 2"></p>
<p>We also analyze and report the performance of multiple testing choices, by varying the number of models and the number of crops used when predicting an image in Table 3. When we use one model, we chose the one with the lowest top-1 error rate on the validation data. All numbers are reported on the validation dataset in order to not overfit to the testing data statistics.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%203.png" alt="Table 3"></p>
<p>我们也分析报告了多种测试选择的性能，当预测图像时通过改变表3中使用的模型数目和裁剪图像数目。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%203.png" alt="Table 3"></p>
<h2 id="8-ILSVRC-2014-Detection-Challenge-Setup-and-Results"><a href="#8-ILSVRC-2014-Detection-Challenge-Setup-and-Results" class="headerlink" title="8. ILSVRC 2014 Detection Challenge Setup and Results"></a>8. ILSVRC 2014 Detection Challenge Setup and Results</h2><p>The ILSVRC detection task is to produce bounding boxes around objects in images among 200 possible classes. Detected objects count as correct if they match the class of the groundtruth and their bounding boxes overlap by at least 50% (using the Jaccard index). Extraneous detections count as false positives and are penalized. Contrary to the classification task, each image may contain many objects or none, and their scale may vary. Results are reported using the mean average precision (mAP). The approach taken by GoogLeNet for detection is similar to the R-CNN by [6], but is augmented with the Inception model as the region classifier. Additionally, the region proposal step is improved by combining the selective search [20] approach with multi-box [5] predictions for higher object bounding box recall. In order to reduce the number of false positives, the superpixel size was increased by 2×. This halves the proposals coming from the selective search algorithm. We added back 200 region proposals coming from multi-box [5] resulting, in total, in about 60% of the proposals used by [6], while increasing the coverage from 92% to 93%. The overall effect of cutting the number of proposals with increased coverage is a 1% improvement of the mean average precision for the single model case. Finally, we use an ensemble of 6 GoogLeNets when classifying each region. This leads to an increase in accuracy from 40% to 43.9%. Note that contrary to R-CNN, we did not use bounding box regression due to lack of time.</p>
<h2 id="8-ILSVRC-2014检测挑战赛设置和结果"><a href="#8-ILSVRC-2014检测挑战赛设置和结果" class="headerlink" title="8. ILSVRC 2014检测挑战赛设置和结果"></a>8. ILSVRC 2014检测挑战赛设置和结果</h2><p>ILSVRC检测任务是为了在200个可能的类别中生成图像中目标的边界框。如果检测到的对象匹配的它们实际类别并且它们的边界框重叠至少50%（使用Jaccard索引），则将检测到的对象记为正确。无关的检测记为假阳性且被惩罚。与分类任务相反，每张图像可能包含多个对象或没有对象，并且它们的尺度可能是变化的。报告的结果使用平均精度均值（mAP）。GoogLeNet检测采用的方法类似于R-CNN[6]，但用Inception模块作为区域分类器进行了增强。此外，为了更高的目标边界框召回率，通过选择搜索[20]方法和多箱[5]预测相结合改进了区域生成步骤。为了减少假阳性的数量，超分辨率的尺寸增加了2倍。这将选择搜索算法的区域生成减少了一半。我们总共补充了200个来自多盒结果的区域生成，大约60%的区域生成用于[6]，同时将覆盖率从92%提高到93%。减少区域生成的数量，增加覆盖率的整体影响是对于单个模型的情况平均精度均值增加了1%。最后，等分类单个区域时，我们使用了6个GoogLeNets的组合。这导致准确率从40%提高到43.9%。注意，与R-CNN相反，由于缺少时间我们没有使用边界框回归。</p>
<p>We first report the top detection results and show the progress since the first edition of the detection task. Compared to the 2013 result, the accuracy has almost doubled. The top performing teams all use convolutional networks. We report the official scores in Table 4 and common strategies for each team: the use of external data, ensemble models or contextual models. The external data is typically the ILSVRC12 classification data for pre-training a model that is later refined on the detection data. Some teams also mention the use of the localization data. Since a good portion of the localization task bounding boxes are not included in the detection dataset, one can pre-train a general bounding box regressor with this data the same way classification is used for pre-training. The GoogLeNet entry did not use the localization data for pretraining.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%204.png" alt="Table 4"></p>
<p>我们首先报告了最好检测结果，并显示了从第一版检测任务以来的进展。与2013年的结果相比，准确率几乎翻了一倍。所有表现最好的团队都使用了卷积网络。我们在表4中报告了官方的分数和每个队伍的常见策略：使用外部数据、集成模型或上下文模型。外部数据通常是ILSVRC12的分类数据，用来预训练模型，后面在检测数据集上进行改善。一些团队也提到使用定位数据。由于定位任务的边界框很大一部分不在检测数据集中，所以可以用该数据预训练一般的边界框回归器，这与分类预训练的方式相同。GoogLeNet输入没有使用定位数据进行预训练。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%204.png" alt="Table 4"></p>
<p>In Table 5, we compare results using a single model only. The top performing model is by Deep Insight and surprisingly only improves by 0.3 points with an ensemble of 3 models while the GoogLeNet obtains significantly stronger results with the ensemble.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%205.png" alt="Table 5"></p>
<p>在表5中，我们仅比较了单个模型的结果。最好性能模型是Deep Insight的，令人惊讶的是3个模型的集合仅提高了0.3个点，而GoogLeNet在模型集成时明显获得了更好的结果。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Googlenet%20Table%205.png" alt="Table 5"></p>
<h2 id="9-Conclusions"><a href="#9-Conclusions" class="headerlink" title="9. Conclusions"></a>9. Conclusions</h2><p>Our results yield a solid evidence that approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural networks for computer vision. The main advantage of this method is a significant quality gain at a modest increase of computational requirements compared to shallower and narrower architectures.</p>
<h2 id="9-总结"><a href="#9-总结" class="headerlink" title="9. 总结"></a>9. 总结</h2><p>我们的结果取得了可靠的证据，即通过易获得的密集构造块来近似期望的最优稀疏结果是改善计算机视觉神经网络的一种可行方法。相比于较浅且较窄的架构，这个方法的主要优势是在计算需求适度增加的情况下有显著的质量收益。</p>
<p>Our object detection work was competitive despite not utilizing context nor performing bounding box regression, suggesting yet further evidence of the strengths of the Inception architecture.</p>
<p>我们的目标检测工作虽然没有利用上下文，也没有执行边界框回归，但仍然具有竞争力，这进一步显示了Inception架构优势的证据。</p>
<p>For both classification and detection, it is expected that similar quality of result can be achieved by much more expensive non-Inception-type networks of similar depth and width. Still, our approach yields solid evidence that moving to sparser architectures is feasible and useful idea in general. This suggest future work towards creating sparser and more refined structures in automated ways on the basis of [2], as well as on applying the insights of the Inception architecture to other domains.</p>
<p>对于分类和检测，预期通过更昂贵的类似深度和宽度的非Inception类型网络可以实现类似质量的结果。 然而，我们的方法取得了可靠的证据，即转向更稀疏的结构一般来说是可行有用的想法。这表明未来的工作将在[2]的基础上以自动化方式创建更稀疏更精细的结构，以及将Inception架构的思考应用到其他领域。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Know your meme: We need to go deeper. <a href="http://knowyourmeme.com/memes/we-need-to-go-deeper" target="_blank" rel="external">http://knowyourmeme.com/memes/we-need-to-go-deeper</a>. Accessed: 2014-09-15.</p>
<p>[2] S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable bounds for learning some deep representations. CoRR, abs/1310.6343, 2013.</p>
<p>[3] U. V. C ̧atalyu ̈rek, C. Aykanat, and B. Uc ̧ar. On two-dimensional sparse matrix partitioning: Models, methods, and a recipe. SIAM J. Sci. Comput., 32(2):656–683, Feb. 2010.</p>
<p>[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, NIPS, pages 1232–1240. 2012.</p>
<p>[5] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014.</p>
<p>[6] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014. CVPR 2014. IEEE Conference on, 2014.</p>
<p>[7] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.</p>
<p>[8] A. G. Howard. Some improvements on deep convolutional neural network based image classification. CoRR, abs/1312.5402, 2013.</p>
<p>[9] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.</p>
<p>[10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comput., 1(4):541–551, Dec. 1989.</p>
<p>[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.</p>
<p>[12] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013.</p>
<p>[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Control Optim., 30(4):838–855, July 1992.</p>
<p>[14] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013.</p>
<p>[15] T. Serre, L. Wolf, S. M. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach. Intell., 29(3):411–426, 2007.</p>
<p>[16] F. Song and J. Dongarra. Scaling up matrix computations on shared-memory manycore systems with 1000 cpu cores. In Proceedings of the 28th ACM Interna- tional Conference on Supercomputing, ICS ’14, pages 333–342, New York, NY, USA, 2014. ACM.</p>
<p>[17] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. On the importance of initialization and momentum in deep learning. In ICML, volume 28 of JMLR Proceed- ings, pages 1139–1147. JMLR.org, 2013.</p>
<p>[18] C.Szegedy,A.Toshev,andD.Erhan.Deep neural networks for object detection. In C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, editors, NIPS, pages 2553–2561, 2013.</p>
<p>[19] A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. CoRR, abs/1312.4659, 2013.</p>
<p>[20] K. E. A. van de Sande, J. R. R. Uijlings, T. Gevers, and A. W. M. Smeulders. Segmentation as selective search for object recognition. In Proceedings of the 2011 International Conference on Computer Vision, ICCV ’11, pages 1879–1886, Washington, DC, USA, 2011. IEEE Computer Society.</p>
<p>[21] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, ECCV, volume 8689 of Lecture Notes in Computer Science, pages 818–833. Springer, 2014.</p>
]]></content>
    
    <summary type="html">
    
      GoogleNet论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet论文翻译——中文版</title>
    <link href="noahsnail.com/2017/07/18/2017-7-18-AlexNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/07/18/2017-7-18-AlexNet论文翻译——中文版/</id>
    <published>2017-07-18T10:00:15.000Z</published>
    <updated>2017-07-27T09:07:28.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="ImageNet-Classification-with-Deep-Convolutional-Neural-Networks"><a href="#ImageNet-Classification-with-Deep-Convolutional-Neural-Networks" class="headerlink" title="ImageNet Classification with Deep Convolutional Neural Networks"></a>ImageNet Classification with Deep Convolutional Neural Networks</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们训练了一个大型深度卷积神经网络来将<code>ImageNet LSVRC-2010</code>竞赛的120万高分辨率的图像分到1000不同的类别中。在测试数据上，我们得到了<code>top-1 37.5%, top-5 17.0%</code>的错误率，这个结果比目前的最好结果好很多。这个神经网络有6000万参数和650000个神经元，包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，最后是一个1000维的softmax。为了训练的更快，我们使用了非饱和神经元并对卷积操作进行了非常有效的GPU实现。为了减少全连接层的过拟合，我们采用了一个最近开发的名为<code>dropout</code>的正则化方法，结果证明是非常有效的。我们也使用这个模型的一个变种参加了<code>ILSVRC-2012</code>竞赛，赢得了冠军并且与第二名 <code>top-5 26.2%</code>的错误率相比，我们取得了<code>top-5 15.3%</code>的错误率。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>当前的目标识别方法基本上都使用了机器学习方法。为了提高目标识别的性能，我们可以收集更大的数据集，学习更强大的模型，使用更好的技术来防止过拟合。直到最近，标注图像的数据集都相对较小--在几万张图像的数量级上（例如，NORB[16]，Caltech-101/256 [8, 9]和CIFAR-10/100 [12]）。简单的识别任务在这样大小的数据集上可以被解决的相当好，尤其是如果通过标签保留变换进行数据增强的情况下。例如，目前在MNIST数字识别任务上（&lt;0.3%）的最好准确率已经接近了人类水平[4]。但真实环境中的对象表现出了相当大的可变性，因此为了学习识别它们，有必要使用更大的训练数据集。实际上，小图像数据集的缺点已经被广泛认识到（例如，Pinto et al. [21]），但收集上百万图像的标注数据仅在最近才变得的可能。新的更大的数据集包括LabelMe [23]，它包含了数十万张完全分割的图像，ImageNet [6]，它包含了22000个类别上的超过1500万张标注的高分辨率的图像。</p>
<p>为了从数百万张图像中学习几千个对象，我们需要一个有很强学习能力的模型。然而对象识别任务的巨大复杂性意味着这个问题不能被指定，即使通过像ImageNet这样的大数据集，因此我们的模型应该也有许多先验知识来补偿我们所没有的数据。卷积神经网络(CNNs)构成了一个这样的模型[16, 11, 13, 18, 15, 22, 26]。它们的能力可以通过改变它们的广度和深度来控制，它们也可以对图像的本质进行强大且通常正确的假设（也就是说，统计的稳定性和像素依赖的局部性）。因此，与具有层次大小相似的标准前馈神经网络，CNNs有更少的连接和参数，因此它们更容易训练，而它们理论上的最佳性能可能仅比标准前馈神经网络差一点。</p>
<p>尽管CNN具有引人注目的质量，尽管它们的局部架构相当有效，但将它们大规模的应用到到高分辨率图像中仍然是极其昂贵的。幸运的是，目前的GPU，搭配了高度优化的2D卷积实现，强大到足够促进有趣地大量CNN的训练，最近的数据集例如ImageNet包含足够的标注样本来训练这样的模型而没有严重的过拟合。</p>
<p>本文具体的贡献如下：我们在ILSVRC-2010和ILSVRC-2012[2]的ImageNet子集上训练了到目前为止最大的神经网络之一，并取得了迄今为止在这些数据集上报道过的最好结果。我们编写了高度优化的2D卷积GPU实现以及训练卷积神经网络内部的所有其它操作，我们把它公开了。我们的网络包含许多新的不寻常的特性，这些特性提高了神经网络的性能并减少了训练时间，详见第三节。即使使用了120万标注的训练样本，我们的网络尺寸仍然使过拟合成为一个明显的问题，因此我们使用了一些有效的技术来防止过拟合，详见第四节。我们最终的网络包含5个卷积层和3个全连接层，深度似乎是非常重要的：我们发现移除任何卷积层（每个卷积层包含的参数不超过模型参数的1%）都会导致更差的性能。</p>
<p>最后，网络尺寸主要受限于目前GPU的内存容量和我们能忍受的训练时间。我们的网络在两个GTX 580 3GB GPU上训练五六天。我们的所有实验表明我们的结果可以简单地通过等待更快的GPU和更大的可用数据集来提高。</p>
<h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2 数据集"></a>2 数据集</h2><p>ImageNet数据集有超过1500万的标注高分辨率图像，这些图像属于大约22000个类别。这些图像是从网上收集的，使用了Amazon’s Mechanical Turk的众包工具通过人工标注的。从2010年起，作为Pascal视觉对象挑战赛的一部分，每年都会举办ImageNet大规模视觉识别挑战赛（ILSVRC）。ILSVRC使用ImageNet的一个子集，1000个类别每个类别大约1000张图像。总计，大约120万训练图像，50000张验证图像和15万测试图像。</p>
<p>ILSVRC-2010是ILSVRC竞赛中唯一可以获得测试集标签的版本，因此我们大多数实验都是在这个版本上运行的。由于我们也使用我们的模型参加了ILSVRC-2012竞赛，因此在第六节我们也报告了模型在这个版本的数据集上的结果，这个版本的测试标签是不可获得的。在ImageNet上，按照惯例报告两个错误率：<code>top-1</code>和<code>top-5</code>，<code>top-5</code>错误率是指测试图像的正确标签不在模型认为的五个最可能的便签之中。</p>
<p>ImageNet包含各种分辨率的图像，而我们的系统要求不变的输入维度。因此，我们将图像进行下采样到固定的<code>256×256</code>分辨率。给定一个矩形图像，我们首先缩放图像短边长度为256，然后从结果图像中裁剪中心的<code>256×256</code>大小的图像块。除了在训练集上对像素减去平均活跃度外，我们不对图像做任何其它的预处理。因此我们在原始的RGB像素值（中心的）上训练我们的网络。</p>
<h2 id="3-架构"><a href="#3-架构" class="headerlink" title="3 架构"></a>3 架构</h2><p>我们的网络架构概括为图2。它包含八个学习层--5个卷积层和3个全连接层。下面，我们将描述我们网络结构中的一些新奇的不寻常的特性。3.1-3.4小节按照我们对它们评估的重要性进行排序，最重要的最有先。</p>
<h3 id="3-1-ReLU非线性"><a href="#3-1-ReLU非线性" class="headerlink" title="3.1 ReLU非线性"></a>3.1 ReLU非线性</h3><p>将神经元输出<code>f</code>建模为输入<code>x</code>的函数的标准方式是用<code>f(x) = tanh(x)</code>或<code>f(x) = (1 + e−x)−1</code>。考虑到梯度下降的训练时间，这些饱和的非线性比非饱和非线性<code>f(x) = max(0,x)</code>更慢。根据Nair和Hinton[20]的说法，我们将这种非线性神经元称为修正线性单元(ReLU)。采用ReLU的深度卷积神经网络训练时间比等价的<code>tanh</code>单元要快几倍。在图1中，对于一个特定的四层卷积网络，在CIFAR-10数据集上达到25%的训练误差所需要的迭代次数可以证实这一点。这幅图表明，如果我们采用传统的饱和神经元模型，我们将不能在如此大的神经网络上实验该工作。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%201.png" alt="Figure 1"></p>
<p>图1：使用ReLU的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍。为了使训练尽可能快，每个网络的学习率是单独选择的。没有采用任何类型的正则化。影响的大小随着网络结构的变化而变化，这一点已得到证实，但使用ReLU的网络都比等价的饱和神经元快几倍。</p>
<p>我们不是第一个考虑替代CNN中传统神经元模型的人。例如，Jarrett等人[11]声称非线性函数<code>f(x) = |tanh(x)|</code>与其对比度归一化一起，然后是局部均值池化，在Caltech-101数据集上工作的非常好。然而，在这个数据集上主要的关注点是防止过拟合，因此他们观测到的影响不同于我们使用ReLU拟合数据集时的加速能力。更快的学习对大型数据集上大型模型的性能有很大的影响。</p>
<h3 id="3-2-多GPU训练"><a href="#3-2-多GPU训练" class="headerlink" title="3.2 多GPU训练"></a>3.2 多GPU训练</h3><p>单个GTX580 GPU只有3G内存，这限制了可以在GTX580上进行训练的网络最大尺寸。事实证明120万图像用来进行网络训练是足够的，但网络太大因此不能在单个GPU上进行训练。因此我们将网络分布在两个GPU上。目前的GPU非常适合跨GPU并行，因为它们可以直接互相读写内存，而不需要通过主机内存。我们采用的并行方案基本上每个GPU放置一半的核（或神经元），还有一个额外的技巧：只在某些特定的层上进行GPU通信。这意味着，例如，第3层的核会将第2层的所有核映射作为输入。然而，第4层的核只将位于相同GPU上的第3层的核映射作为输入。连接模式的选择是一个交叉验证问题，但这可以让我们准确地调整通信数量，直到它的计算量在可接受的范围内。</p>
<p>除了我们的列不是独立的之外（看图2），最终的架构有点类似于Ciresan等人[5]采用的“columnar” CNN。与每个卷积层一半的核在单GPU上训练的网络相比，这个方案降分别低了我们的<code>top-1 1.7%</code>，<code>top-5 1.2%</code>的错误率。双GPU网络比单GPU网络稍微减少了训练时间。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Fig%202.png" alt="Figure 2"></p>
<p>图 2：我们CNN架构图解，明确描述了两个GPU之间的责任。在图的顶部，一个GPU运行在部分层上，而在图的底部，另一个GPU运行在部分层上。GPU只在特定的层进行通信。网络的输入是150,528维，网络剩下层的神经元数目分别是253,440–186,624–64,896–64,896–43,264–4096–4096–1000（8层）。</p>
<h3 id="3-3-局部响应归一化"><a href="#3-3-局部响应归一化" class="headerlink" title="3.3 局部响应归一化"></a>3.3 局部响应归一化</h3><p>ReLU具有让人满意的特性，它不需要通过输入归一化来防止饱和。如果至少一些训练样本对ReLU产生了正输入，那么那个神经元上将发生学习。然而，我们仍然发现接下来的局部响应归一化有助于泛化。$a_{x,y}^i$表示神经元激活，通过在$(x, y)$位置应用核$i$，然后应用ReLU非线性来计算，响应归一化激活$b^i_{x,y}$通过下式给定：</p>
<p>$$b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta$$</p>
<p>求和运算在n个“毗邻的”核映射的同一位置上执行，N是本层的卷积核数目。核映射的顺序当然是任意的，在训练开始前确定。响应归一化的顺序实现了一种侧抑制形式，灵感来自于真实神经元中发现的类型，为使用不同核进行神经元输出计算的较大活动创造了竞争。常量k，n，α，β是超参数，它们的值通过验证集确定；我们设k=2，n=5，α=0.0001，β=0.75。我们在特定的层使用的ReLU非线性之后应用了这种归一化（请看3.5小节）。</p>
<p>这个方案与Jarrett等人[11]的局部对比度归一化方案有一定的相似性，但我们更恰当的称其为“亮度归一化”，因此我们没有减去均值。响应归一化分别减少了<code>top-1 1.4%</code>，<code>top-5 1.2%</code>的错误率。我们也在CIFAR-10数据集上验证了这个方案的有效性：一个没有归一化的四层CNN取得了13%的错误率，而使用归一化取得了11%的错误率。</p>
<h3 id="3-4-重叠池化"><a href="#3-4-重叠池化" class="headerlink" title="3.4 重叠池化"></a>3.4 重叠池化</h3><p>CNN中的池化层归纳了同一核映射上相邻组神经元的输出。习惯上，相邻池化单元归纳的区域是不重叠的（例如[17, 11, 4]）。更确切的说，池化层可看作由池化单元网格组成，网格间距为$s$个像素，每个网格归纳池化单元中心位置$z × z$大小的邻居。如果设置$s = z$，我们会得到通常在CNN中采用的传统局部池化。如果设置$s &lt; z$，我们会得到重叠池化。这就是我们网络中使用的方法，设置$s = 2$，$z = 3$。这个方案分别降低了<code>top-1 0.4%</code>，<code>top-5 0.3%</code>的错误率，与非重叠方案$s = 2，z = 2$相比，输出的维度是相等的。我们在训练过程中通常观察采用重叠池化的模型，发现它更难过拟合。</p>
<h3 id="3-5-整体架构"><a href="#3-5-整体架构" class="headerlink" title="3.5 整体架构"></a>3.5 整体架构</h3><p>现在我们准备描述我们的CNN的整体架构。如图2所示，我们的网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签的分布。我们的网络最大化多项逻辑回归的目标，这等价于最大化预测分布下训练样本正确标签的对数概率的均值。</p>
<p>第2，4，5卷积层的核只与位于同一GPU上的前一层的核映射相连接（看图2）。第3卷积层的核与第2层的所有核映射相连。全连接层的神经元与前一层的所有神经元相连。第1，2卷积层之后是响应归一化层。3.4节描述的这种最大池化层在响应归一化层和第5卷积层之后。ReLU非线性应用在每个卷积层和全连接层的输出上。</p>
<p>第1卷积层使用96个核对224 × 224 × 3的输入图像进行滤波，核大小为11 × 11 × 3，步长是4个像素（核映射中相邻神经元感受野中心之间的距离）。第2卷积层使用用第1卷积层的输出（响应归一化和池化）作为输入，并使用256个核进行滤波，核大小为5 × 5 × 48。第3，4，5卷积层互相连接，中间没有接入池化层或归一化层。第3卷积层有384个核，核大小为3 × 3 × 256，与第2卷积层的输出（归一化的，池化的）相连。第4卷积层有384个核，核大小为3 × 3 × 192，第5卷积层有256个核，核大小为3 × 3 × 192。每个全连接层有4096个神经元。</p>
<h2 id="4-减少过拟合"><a href="#4-减少过拟合" class="headerlink" title="4 减少过拟合"></a>4 减少过拟合</h2><p>我们的神经网络架构有6000万参数。尽管ILSVRC的1000类使每个训练样本从图像到标签的映射上强加了10比特的约束，但这不足以学习这么多的参数而没有相当大的过拟合。下面，我们会描述我们用来克服过拟合的两种主要方式。</p>
<h3 id="4-1-数据增强"><a href="#4-1-数据增强" class="headerlink" title="4.1 数据增强"></a>4.1 数据增强</h3><p>图像数据上最简单常用的用来减少过拟合的方法是使用标签保留变换（例如[25, 4, 5]）来人工增大数据集。我们使用了两种独特的数据增强方式，这两种方式都可以从原始图像通过非常少的计算量产生变换的图像，因此变换图像不需要存储在硬盘上。在我们的实现中，变换图像通过CPU的Python代码生成，而此时GPU正在训练前一批图像。因此，实际上这些数据增强方案是计算免费的。</p>
<p>第一种数据增强方式包括产生图像变换和水平翻转。我们从256×256图像上通过随机提取224 × 224的图像块实现了这种方式，然后在这些提取的图像块上进行训练。这通过一个2048因子增大了我们的训练集，尽管最终的训练样本是高度相关的。没有这个方案，我们的网络会有大量的过拟合，这会迫使我们使用更小的网络。在测试时，网络会提取5个224 × 224的图像块（四个角上的图像块和中心的图像块）和它们的水平翻转（因此总共10个图像块）进行预测，然后对网络在10个图像块上的softmax层进行平均。</p>
<p>第二种数据增强方式包括改变训练图像的RGB通道的强度。具体地，我们在整个ImageNet训练集上对RGB像素值集合执行PCA。对于每幅训练图像，我们加上多倍找到的主成分，大小成正比的对应特征值乘以一个随机变量，随机变量通过均值为0，标准差为0.1的高斯分布得到。因此对于每幅RGB图像像素$I_xy = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$，我们加上下面的数量：</p>
<p>$$[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T$$</p>
<p>$p_i$，$\lambda_i$分别是RGB像素值3 × 3协方差矩阵的第$i$个特征向量和特征值，$\alpha_i$是前面提到的随机变量。对于某个训练图像的所有像素，每个$\alpha_i$只获取一次，直到图像进行下一次训练时才重新获取。这个方案近似抓住了自然图像的一个重要特性，即光照的颜色和强度发生变化时，目标身份是不变的。这个方案减少了<code>top 1</code>错误率1%以上</p>
<h3 id="4-2-失活-Dropout"><a href="#4-2-失活-Dropout" class="headerlink" title="4.2 失活(Dropout)"></a>4.2 失活(Dropout)</h3><p>将许多不同模型的预测结合起来是降低测试误差[1, 3]的一个非常成功的方法，但对于需要花费几天来训练的大型神经网络来说，这似乎太昂贵了。然而，有一个非常有效的模型结合版本，它只花费两倍的训练成本。这种最近引入的技术，叫做“dropout”[10]，它会以0.5的概率对每个隐层神经元的输出设为0。那些“失活的”的神经元不再进行前向传播并且不参与反向传播。因此每次输入时，神经网络会采样一个不同的架构，但所有架构共享权重。这个技术减少了复杂的神经元互适应，因为一个神经元不能依赖特定的其它神经元的存在。因此，神经元被强迫学习更鲁棒的特征，它在与许多不同的其它神经元的随机子集结合时是有用的。在测试时，我们使用所有的神经元但它们的输出乘以0.5，对指数级的许多失活网络的预测分布进行几何平均，这是一种合理的近似。</p>
<p>我们在图2中的前两个全连接层使用失活。如果没有失活，我们的网络表现出大量的过拟合。失活大致上使要求收敛的迭代次数翻了一倍。</p>
<h2 id="5-学习细节"><a href="#5-学习细节" class="headerlink" title="5 学习细节"></a>5 学习细节</h2><p>我们使用随机梯度下降来训练我们的模型，样本的batch size为128，动量为0.9，权重衰减为0.0005。我们发现少量的权重衰减对于模型的学习是重要的。换句话说，权重衰减不仅仅是一个正则项：它减少了模型的训练误差。权重$w$的更新规则是</p>
<p>$$v_{i+1} := 0.9 \bullet v_i - 0.0005 \bullet \varepsilon \bullet w_i - \varepsilon \bullet \langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$$</p>
<p>$i$是迭代索引，$v$是动量变量，$\varepsilon$是学习率，$\langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$是目标函数对$w$，在$w_i$上的第$i$批微分$D_i$的平均。</p>
<p>我们使用均值为0，标准差为0.01的高斯分布对每一层的权重进行初始化。我们在第2，4，5卷积层和全连接隐层将神经元偏置初始化为常量1。这个初始化通过为ReLU提供正输入加速了学习的早期阶段。我们在剩下的层将神经元偏置初始化为0。</p>
<p>我们对所有的层使用相等的学习率，这个是在整个训练过程中我们手动调整得到的。当验证误差在当前的学习率下停止提供时，我们遵循启发式的方法将学习率除以10。学习率初始化为0.01，在训练停止之前降低三次。我们在120万图像的训练数据集上训练神经网络大约90个循环，在两个NVIDIA GTX 580 3GB GPU上花费了五到六天。</p>
<h2 id="6-结果"><a href="#6-结果" class="headerlink" title="6 结果"></a>6 结果</h2><p>我们在ILSVRC-2010上的结果概括为表1。我们的神经网络取得了<code>top-1 37.5%</code>，<code>top-5 17.0%</code>的错误率。在ILSVRC-2010竞赛中最佳结果是<code>top-1 47.1%</code>，<code>top-5 28.2%</code>，使用的方法是对6个在不同特征上训练的稀疏编码模型生成的预测进行平均，从那时起已公布的最好结果是<code>top-1 45.7%</code>，<code>top-5 25.7%</code>，使用的方法是平均在Fisher向量（FV）上训练的两个分类器的预测结果，Fisher向量是通过两种密集采样特征计算得到的[24]。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%201.png" alt="表1"></p>
<p>表1：ILSVRC-2010测试集上的结果对比。斜体是其它人取得的最好结果。</p>
<p>我们也用我们的模型参加了ILSVRC-2012竞赛并在表2中报告了我们的结果。由于ILSVRC-2012的测试集标签不可以公开得到，我们不能报告我们尝试的所有模型的测试错误率。在这段的其余部分，我们会使用验证误差率和测试误差率互换，因为在我们的实验中它们的差别不会超过0.1%（看图2）。本文中描述的CNN取得了<code>top-5 18.2%</code>的错误率。五个类似的CNN预测的平均误差率为16.4%。为了对ImageNet 2011秋季发布的整个数据集（1500万图像，22000个类别）进行分类，我们在最后的池化层之后有一个额外的第6卷积层，训练了一个CNN，然后在它上面进行“fine-tuning”，在ILSVRC-2012取得了16.6%的错误率。对在ImageNet 2011秋季发布的整个数据集上预训练的两个CNN和前面提到的五个CNN的预测进行平均得到了15.3%的错误率。第二名的最好竞赛输入取得了26.2%的错误率，他的方法是对FV上训练的一些分类器的预测结果进行平均，FV在不同类型密集采样特征计算得到的。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%202.png" alt="表2"></p>
<p>表2：ILSVRC-2012验证集和测试集的误差对比。斜线部分是其它人取得的最好的结果。带星号的是“预训练的”对ImageNet 2011秋季数据集进行分类的模型。更多细节请看第六节。</p>
<p>最后，我们也报告了我们在ImageNet 2009秋季数据集上的误差率，ImageNet 2009秋季数据集有10,184个类，890万图像。在这个数据集上我们按照惯例用一半的图像来训练，一半的图像来测试。由于没有建立测试集，我们的数据集分割有必要不同于以前作者的数据集分割，但这对结果没有明显的影响。我们在这个数据集上的的top-1和top-5错误率是67.4%和40.9%，使用的是上面描述的在最后的池化层之后有一个额外的第6卷积层网络。这个数据集上公开可获得的最好结果是78.1%和60.9%[19]。</p>
<h3 id="6-1-定性评估"><a href="#6-1-定性评估" class="headerlink" title="6.1 定性评估"></a>6.1 定性评估</h3><p>图3显示了网络的两个数据连接层学习到的卷积核。网络学习到了大量的频率核、方向选择核，也学到了各种颜色点。注意两个GPU表现出的专业化，3.5小节中描述的受限连接的结果。GPU 1上的核主要是没有颜色的，而GPU 2上的核主要是针对颜色的。这种专业化在每次运行时都会发生，并且是与任何特别的随机权重初始化（以GPU的重新编号为模）无关的。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%203.png" alt="Figure 3"></p>
<p>图3：第一卷积层在224×224×3的输入图像上学习到的大小为11×11×3的96个卷积核。上面的48个核是在GPU 1上学习到的而下面的48个卷积核是在GPU 2上学习到的。更多细节请看6.1小节。</p>
<p>在图4的左边部分，我们通过在8张测试图像上计算它的top-5预测定性地评估了网络学习到的东西。注意即使是不在图像中心的目标也能被网络识别，例如左上角的小虫。大多数的top-5标签似乎是合理的。例如，对于美洲豹来说，只有其它类型的猫被认为是看似合理的标签。在某些案例（格栅，樱桃）中，网络在意的图片焦点真的很含糊。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%204.png" alt="Figure 4"></p>
<p>图4：（左）8张ILSVRC-2010测试图像和我们的模型认为最可能的5个标签。每张图像的下面是它的正确标签，正确标签的概率用红条表示（如果正确标签在top 5中）。（右）第一列是5张ILSVRC-2010测试图像。剩下的列展示了6张训练图像，这些图像在最后的隐藏层的特征向量与测试图像的特征向量有最小的欧氏距离。</p>
<p>探索网络可视化知识的另一种方式是思考最后的4096维隐藏层在图像上得到的特征激活。如果两幅图像生成的特征激活向量之间有较小的欧式距离，我们可以认为神经网络的更高层特征认为它们是相似的。图4表明根据这个度量标准，测试集的5张图像和训练集的6张图像中的每一张都是最相似的。注意在像素级别，检索到的训练图像与第一列的查询图像在L2上通常是不接近的。例如，检索的狗和大象似乎有很多姿态。我们在补充材料中对更多的测试图像呈现了这种结果。</p>
<p>通过两个4096维实值向量间的欧氏距离来计算相似性是效率低下的，但通过训练一个自动编码器将这些向量压缩为短二值编码可以使其变得高效。这应该会产生一种比将自动编码器应用到原始像素上[14]更好的图像检索方法，自动编码器应用到原始像素上的方法没有使用图像标签，因此会趋向于检索与要检索的图像具有相似边缘模式的图像，无论它们是否是语义上相似。</p>
<h2 id="7-探讨"><a href="#7-探讨" class="headerlink" title="7 探讨"></a>7 探讨</h2><p>我们的结果表明一个大型深度卷积神经网络在一个具有高度挑战性的数据集上使用纯有监督学习可以取得破纪录的结果。值得注意的是，如果移除一个卷积层，我们的网络性能会降低。例如，移除任何中间层都会引起网络损失大约2%的top-1性能。因此深度对于实现我们的结果非常重要。</p>
<p>为了简化我们的实验，我们没有使用任何无监督的预训练，尽管我们希望它会有所帮助，特别是在如果我们能获得足够的计算能力来显著增加网络的大小而标注的数据量没有对应增加的情况下。到目前为止，我们的结果已经提高了，因为我们的网络更大、训练时间更长，但为了匹配人类视觉系统的下颞线（视觉专业术语）我们仍然有许多数量级要达到。最后我们想在视频序列上使用非常大的深度卷积网络，视频序列的时序结构会提供非常有帮助的信息，这些信息在静态图像上是缺失的或远不那么明显。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] R.M.BellandY.Koren.Lessonsfromthenetflixprizechallenge.ACMSIGKDDExplorationsNewsletter, 9(2):75–79, 2007.</p>
<p>[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.imagenet.org/challenges. 2010.</p>
<p>[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.</p>
<p>[4] D. Cires ̧an, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. Arxiv preprint arXiv:1202.2745, 2012.</p>
<p>[5] D.C. Cires ̧an, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classification. Arxiv preprint arXiv:1102.0183, 2011.</p>
<p>[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.</p>
<p>[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL <a href="http://www.image-net.org/challenges/LSVRC/2012/" target="_blank" rel="external">http://www.image-net.org/challenges/LSVRC/2012/</a>.</p>
<p>[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59–70, 2007.</p>
<p>[9] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. URL <a href="http://authors.library.caltech.edu/7694" target="_blank" rel="external">http://authors.library.caltech.edu/7694</a>.</p>
<p>[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.</p>
<p>[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.</p>
<p>[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009.</p>
<p>[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.</p>
<p>[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In ESANN, 2011.</p>
<p>[15] Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, 1990.</p>
<p>[16] Y. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.</p>
<p>[17] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256. IEEE, 2010.</p>
<p>[18] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.</p>
<p>[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer Vision, Florence, Italy, October 2012.</p>
<p>[20] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 2010.</p>
<p>[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computational biology, 4(1):e27, 2008.</p>
<p>[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,2009.</p>
<p>[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for image annotation. International journal of computer vision, 77(1):157–173, 2008.</p>
<p>[24] J.SánchezandF.Perronnin.High-dimensionalsignaturecompressionforlarge-scaleimageclassification. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,2011.</p>
<p>[25] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition, volume 2, pages 958–962, 2003.</p>
<p>[26] S.C.Turaga,J.F.Murray,V.Jain,F.Roth,M.Helmstaedter,K.Briggman,W.Denk,andH.S.Seung.Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22(2):511–538, 2010.</p>
]]></content>
    
    <summary type="html">
    
      AlexNet论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux快捷键总结</title>
    <link href="noahsnail.com/2017/07/18/2017-7-18-Linux%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%80%BB%E7%BB%93/"/>
    <id>noahsnail.com/2017/07/18/2017-7-18-Linux快捷键总结/</id>
    <published>2017-07-18T06:11:02.000Z</published>
    <updated>2017-07-18T08:44:50.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/002.jpg" alt="image"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文的Linux快捷键总结主要是作者使用Linux过程中常用的。</p>
<ul>
<li>清屏，等价于clear命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + l</div></pre></td></tr></table></figure>
<ul>
<li>切换到命令行开始</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + a</div></pre></td></tr></table></figure>
<ul>
<li>切换到命令行末尾</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + e</div></pre></td></tr></table></figure>
<ul>
<li>剪切光标至行首内容，可以用来清除输入的命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + w</div></pre></td></tr></table></figure>
<ul>
<li>粘贴</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + y</div></pre></td></tr></table></figure>
<ul>
<li>终止当前运行的程序</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + c</div></pre></td></tr></table></figure>
<ul>
<li>退出当前shell</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + d</div></pre></td></tr></table></figure>
<ul>
<li>查找历史命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + r</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Linux快捷键总结
    
    </summary>
    
      <category term="Linux" scheme="noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Caffe关闭日志输出</title>
    <link href="noahsnail.com/2017/07/18/2017-7-18-Caffe%E5%85%B3%E9%97%ADlog%E8%BE%93%E5%87%BA/"/>
    <id>noahsnail.com/2017/07/18/2017-7-18-Caffe关闭log输出/</id>
    <published>2017-07-18T05:34:58.000Z</published>
    <updated>2017-07-18T05:49:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>在训练Caffe模型后，部署Caffe服务时我们通常会使用pycaffe来加载模型并处理图像，但是pycaffe加载模型时通常会输出加载模型的日志，影响我们查看自己的日志，因此需要移除Caffe加载模型时的日志。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">os.environ[&apos;GLOG_minloglevel&apos;] = &apos;2&apos;</div><div class="line">import caffe</div></pre></td></tr></table></figure>
<p>Caffe使用的日志是GLOG，其日志级别如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">0 - debug</div><div class="line">1 - info (still a LOT of outputs)</div><div class="line">2 - warnings</div><div class="line">3 - errors</div></pre></td></tr></table></figure>
<p>注意：由于在导入Caffe时Caffe会加载GLOG，因此<code>os.environ[&#39;GLOG_minloglevel&#39;] = &#39;2&#39;</code>需要在<code>import caffe</code>之前。</p>
<p>参考资料</p>
<ol>
<li><a href="https://stackoverflow.com/questions/29788075/setting-glog-minloglevel-1-to-prevent-output-in-shell-from-caffe" target="_blank" rel="external">https://stackoverflow.com/questions/29788075/setting-glog-minloglevel-1-to-prevent-output-in-shell-from-caffe</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Caffe关闭日志输出
    
    </summary>
    
      <category term="Caffe" scheme="noahsnail.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="noahsnail.com/tags/Caffe/"/>
    
  </entry>
  
  <entry>
    <title>Caffe源码调试</title>
    <link href="noahsnail.com/2017/07/17/2017-7-17-Caffe%E4%BB%A3%E7%A0%81%E8%B0%83%E8%AF%95/"/>
    <id>noahsnail.com/2017/07/17/2017-7-17-Caffe代码调试/</id>
    <published>2017-07-17T03:46:44.000Z</published>
    <updated>2017-07-18T08:50:40.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/NASA_Mars_Rover.jpg" alt="Mars Exploration Rovers"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>这篇文件主要介绍如何使用Linux的gdb调试Caffe的源码，源码调试主要是为了阅读并更好的了解Caffe源码。</p>
<h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2><ol>
<li>首先要在编译Caffe源码时打开debug模式，即将<code>Makefile.config</code>中的<code>DEBUG := 1</code>打开。</li>
<li>下载mnist数据集，主要是在mnist数据集上进行调试，执行<code>bash data/mnist/get_mnist.sh</code>。</li>
<li>转换mnist数据集为LMDB，<code>bash examples/mnist/create_mnist.sh</code>。</li>
<li>修改<code>examples/mnist/lenet_solver.prototxt</code>，将GPU改为CPU。</li>
</ol>
<h2 id="2-调试"><a href="#2-调试" class="headerlink" title="2. 调试"></a>2. 调试</h2><h3 id="1-激活GDB"><a href="#1-激活GDB" class="headerlink" title="1. 激活GDB"></a>1. 激活GDB</h3><p>使用GDB启动调试，执行<code>gdb --args build/tools/caffe train --solver examples/mnist/lenet_solver.prototxt</code>，<code>--args</code>表示我们调试时需要输入的参数，调试的命令为<code>build/tools/caffe</code>，caffe命令的参数为<code>--solver examples/mnist/lenet_solver.prototxt</code>。</p>
<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ gdb --args build/tools/caffe train --solver examples/mnist/lenet_solver.prototxt</div><div class="line">GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-94.el7</div><div class="line">Copyright (C) 2013 Free Software Foundation, Inc.</div><div class="line">License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;</div><div class="line">This is free software: you are free to change and redistribute it.</div><div class="line">There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;</div><div class="line">and &quot;show warranty&quot; for details.</div><div class="line">This GDB was configured as &quot;x86_64-redhat-linux-gnu&quot;.</div><div class="line">For bug reporting instructions, please see:</div><div class="line">&lt;http://www.gnu.org/software/gdb/bugs/&gt;...</div><div class="line">Reading symbols from /home/irteam/line-brain/deploy/caffe/.build_debug/tools/caffe.bin...done.</div></pre></td></tr></table></figure>
<h3 id="2-设置断点"><a href="#2-设置断点" class="headerlink" title="2. 设置断点"></a>2. 设置断点</h3><p>执行<code>b src/caffe/layers/base_conv_layer.cpp:117</code>，<code>b</code>表示插入断点（breakpoint），断点的位置是<code>base_conv_layer.cpp</code>文件中的<code>117</code>行。插入断点的命令形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">b path/to/code.cpp:#line</div></pre></td></tr></table></figure>
<p>118行相关代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">117</span> channels_ = bottom[<span class="number">0</span>]-&gt;shape(channel_axis_);</div><div class="line"><span class="number">118</span> num_output_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().num_output();</div><div class="line"><span class="number">119</span> CHECK_GT(num_output_, <span class="number">0</span>);</div></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">(gdb) b src/caffe/layers/base_conv_layer.cpp:117</div><div class="line">No source file named src/caffe/layers/base_conv_layer.cpp.</div><div class="line">Make breakpoint pending on future shared library load? (y or [n]) y</div><div class="line"></div><div class="line">Breakpoint 1 (src/caffe/layers/base_conv_layer.cpp:117) pending.</div></pre></td></tr></table></figure>
<h3 id="3-运行程序"><a href="#3-运行程序" class="headerlink" title="3. 运行程序"></a>3. 运行程序</h3><p>运行程序的命令是<code>r</code>。</p>
<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div></pre></td><td class="code"><pre><div class="line">Starting program: /*/caffe/build/tools/caffe train --solver examples/mnist/lenet_solver.prototxt</div><div class="line">[Thread debugging using libthread_db enabled]</div><div class="line">Using host libthread_db library &quot;/lib64/libthread_db.so.1&quot;.</div><div class="line">I0718 15:19:19.671941 29986 caffe.cpp:211] Use CPU.</div><div class="line">[New Thread 0x7fffd81c7700 (LWP 29991)]</div><div class="line">[New Thread 0x7fffd79c6700 (LWP 29992)]</div><div class="line">I0718 15:19:20.437239 29986 solver.cpp:44] Initializing solver from parameters:</div><div class="line">test_iter: 100</div><div class="line">test_interval: 500</div><div class="line">base_lr: 0.01</div><div class="line">display: 100</div><div class="line">max_iter: 10000</div><div class="line">lr_policy: &quot;inv&quot;</div><div class="line">gamma: 0.0001</div><div class="line">power: 0.75</div><div class="line">momentum: 0.9</div><div class="line">weight_decay: 0.0005</div><div class="line">snapshot: 5000</div><div class="line">snapshot_prefix: &quot;examples/mnist/lenet&quot;</div><div class="line">solver_mode: CPU</div><div class="line">net: &quot;examples/mnist/lenet_train_test.prototxt&quot;</div><div class="line">train_state &#123;</div><div class="line">  level: 0</div><div class="line">  stage: &quot;&quot;</div><div class="line">&#125;</div><div class="line">I0718 15:19:20.437687 29986 solver.cpp:87] Creating training net from net file: examples/mnist/lenet_train_test.prototxt</div><div class="line">I0718 15:19:20.438357 29986 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist</div><div class="line">I0718 15:19:20.438398 29986 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy</div><div class="line">I0718 15:19:20.438499 29986 net.cpp:51] Initializing net from parameters:</div><div class="line">name: &quot;LeNet&quot;</div><div class="line">state &#123;</div><div class="line">  phase: TRAIN</div><div class="line">  level: 0</div><div class="line">  stage: &quot;&quot;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;mnist&quot;</div><div class="line">  type: &quot;Data&quot;</div><div class="line">  top: &quot;data&quot;</div><div class="line">  top: &quot;label&quot;</div><div class="line">  include &#123;</div><div class="line">    phase: TRAIN</div><div class="line">  &#125;</div><div class="line">  transform_param &#123;</div><div class="line">    scale: 0.00390625</div><div class="line">  &#125;</div><div class="line">  data_param &#123;</div><div class="line">    source: &quot;examples/mnist/mnist_train_lmdb&quot;</div><div class="line">    batch_size: 64</div><div class="line">    backend: LMDB</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;conv1&quot;</div><div class="line">  type: &quot;Convolution&quot;</div><div class="line">  bottom: &quot;data&quot;</div><div class="line">  top: &quot;conv1&quot;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 2</div><div class="line">  &#125;</div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: 20</div><div class="line">    kernel_size: 5</div><div class="line">    stride: 1</div><div class="line">    weight_filler &#123;</div><div class="line">      type: &quot;xavier&quot;</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: &quot;constant&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;pool1&quot;</div><div class="line">  type: &quot;Pooling&quot;</div><div class="line">  bottom: &quot;conv1&quot;</div><div class="line">  top: &quot;pool1&quot;</div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: 2</div><div class="line">    stride: 2</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;conv2&quot;</div><div class="line">  type: &quot;Convolution&quot;</div><div class="line">  bottom: &quot;pool1&quot;</div><div class="line">  top: &quot;conv2&quot;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 2</div><div class="line">  &#125;</div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: 50</div><div class="line">    kernel_size: 5</div><div class="line">    stride: 1</div><div class="line">    weight_filler &#123;</div><div class="line">      type: &quot;xavier&quot;</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: &quot;constant&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;pool2&quot;</div><div class="line">  type: &quot;Pooling&quot;</div><div class="line">  bottom: &quot;conv2&quot;</div><div class="line">  top: &quot;pool2&quot;</div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: 2</div><div class="line">    stride: 2</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;ip1&quot;</div><div class="line">  type: &quot;InnerProduct&quot;</div><div class="line">  bottom: &quot;pool2&quot;</div><div class="line">  top: &quot;ip1&quot;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 2</div><div class="line">  &#125;</div><div class="line">  inner_product_param &#123;</div><div class="line">    num_output: 500</div><div class="line">    weight_filler &#123;</div><div class="line">      type: &quot;xavier&quot;</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: &quot;constant&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;relu1&quot;</div><div class="line">  type: &quot;ReLU&quot;</div><div class="line">  bottom: &quot;ip1&quot;</div><div class="line">  top: &quot;ip1&quot;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;ip2&quot;</div><div class="line">  type: &quot;InnerProduct&quot;</div><div class="line">  bottom: &quot;ip1&quot;</div><div class="line">  top: &quot;ip2&quot;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 2</div><div class="line">  &#125;</div><div class="line">  inner_product_param &#123;</div><div class="line">    num_output: 10</div><div class="line">    weight_filler &#123;</div><div class="line">      type: &quot;xavier&quot;</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: &quot;constant&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;loss&quot;</div><div class="line">  type: &quot;SoftmaxWithLoss&quot;</div><div class="line">  bottom: &quot;ip2&quot;</div><div class="line">  bottom: &quot;label&quot;</div><div class="line">  top: &quot;loss&quot;</div><div class="line">&#125;</div><div class="line">I0718 15:19:20.439380 29986 layer_factory.hpp:77] Creating layer mnist</div><div class="line">I0718 15:19:20.439625 29986 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb</div><div class="line">I0718 15:19:20.439702 29986 net.cpp:84] Creating Layer mnist</div><div class="line">I0718 15:19:20.439735 29986 net.cpp:380] mnist -&gt; data</div><div class="line">I0718 15:19:20.439853 29986 net.cpp:380] mnist -&gt; label</div><div class="line">I0718 15:19:20.444980 29986 data_layer.cpp:45] output data size: 64,1,28,28</div><div class="line">I0718 15:19:20.445436 29986 base_data_layer.cpp:72] Initializing prefetch</div><div class="line">[New Thread 0x7fffd603d700 (LWP 29993)]</div><div class="line">I0718 15:19:20.448151 29986 base_data_layer.cpp:75] Prefetch initialized.</div><div class="line">I0718 15:19:20.448186 29986 net.cpp:122] Setting up mnist</div><div class="line">I0718 15:19:20.448216 29986 net.cpp:129] Top shape: 64 1 28 28 (50176)</div><div class="line">I0718 15:19:20.448235 29986 net.cpp:129] Top shape: 64 (64)</div><div class="line">I0718 15:19:20.448245 29986 net.cpp:137] Memory required for data: 200960</div><div class="line">I0718 15:19:20.448264 29986 layer_factory.hpp:77] Creating layer conv1</div><div class="line">I0718 15:19:20.448324 29986 net.cpp:84] Creating Layer conv1</div><div class="line">I0718 15:19:20.448345 29986 net.cpp:406] conv1 &lt;- data</div><div class="line">I0718 15:19:20.448393 29986 net.cpp:380] conv1 -&gt; conv1</div><div class="line"></div><div class="line">Breakpoint 1, caffe::BaseConvolutionLayer&lt;float&gt;::LayerSetUp (this=0x91edd70,</div><div class="line">    bottom=std::vector of length 1, capacity 1 = &#123;...&#125;, top=std::vector of length 1, capacity 1 = &#123;...&#125;)</div><div class="line">    at src/caffe/layers/base_conv_layer.cpp:117</div><div class="line">117       channels_ = bottom[0]-&gt;shape(channel_axis_);</div><div class="line">Missing separate debuginfos, use: debuginfo-install OpenEXR-libs-1.7.1-7.el7.x86_64 atk-2.14.0-1.el7.x86_64 atlas-3.10.1-10.el7.x86_64 boost-filesystem-1.53.0-26.el7.x86_64 boost-python-1.53.0-26.el7.x86_64 boost-system-1.53.0-26.el7.x86_64 boost-thread-1.53.0-26.el7.x86_64 cairo-1.14.2-1.el7.x86_64 expat-2.1.0-10.el7_3.x86_64 fontconfig-2.10.95-10.el7.x86_64 freetype-2.4.11-12.el7.x86_64 gdk-pixbuf2-2.31.6-3.el7.x86_64 gflags-2.1.1-6.el7.x86_64 glib2-2.46.2-4.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 glog-0.3.3-8.el7.x86_64 graphite2-1.3.6-1.el7_2.x86_64 gstreamer-0.10.36-7.el7.x86_64 gstreamer-plugins-base-0.10.36-10.el7.x86_64 gtk2-2.24.28-8.el7.x86_64 harfbuzz-0.9.36-1.el7.x86_64 hdf5-1.8.12-8.el7.x86_64 ilmbase-1.0.3-7.el7.x86_64 jasper-libs-1.900.1-29.el7.x86_64 jbigkit-libs-2.0-11.el7.x86_64 leveldb-1.12.0-11.el7.x86_64 libX11-1.6.3-3.el7.x86_64 libXau-1.0.8-2.1.el7.x86_64 libXcomposite-0.4.4-4.1.el7.x86_64 libXcursor-1.1.14-2.1.el7.x86_64 libXdamage-1.1.4-4.1.el7.x86_64 libXext-1.3.3-3.el7.x86_64 libXfixes-5.0.1-2.1.el7.x86_64 libXi-1.7.4-2.el7.x86_64 libXinerama-1.1.3-2.1.el7.x86_64 libXrandr-1.4.2-2.el7.x86_64 libXrender-0.9.8-2.1.el7.x86_64 libffi-3.0.13-18.el7.x86_64 libgcc-4.8.5-11.el7.x86_64 libgfortran-4.8.5-11.el7.x86_64 libjpeg-turbo-1.2.90-5.el7.x86_64 libpng-1.5.13-7.el7_2.x86_64 libquadmath-4.8.5-11.el7.x86_64 libselinux-2.5-6.el7.x86_64 libstdc++-4.8.5-11.el7.x86_64 libtiff-4.0.3-27.el7_3.x86_64 libv4l-0.9.5-4.el7.x86_64 libxcb-1.11-4.el7.x86_64 libxml2-2.9.1-6.el7_2.3.x86_64 lmdb-libs-0.9.18-1.el7.x86_64 opencv-2.4.5-3.el7.x86_64 opencv-core-2.4.5-3.el7.x86_64 orc-0.4.22-5.el7.x86_64 pango-1.36.8-2.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 pixman-0.34.0-1.el7.x86_64 protobuf-2.5.0-8.el7.x86_64 python-libs-2.7.5-48.el7.x86_64 snappy-1.1.0-3.el7.x86_64 xz-libs-5.2.2-1.el7.x86_64 zlib-1.2.7-17.el7.x86_64</div></pre></td></tr></table></figure>
<p><code>Breakpoint 1</code>之前是正常的程序日志输出，程序在断点处暂停。</p>
<p>查看变量命令为<code>p var</code>，命令与结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">(gdb) p channels_</div><div class="line">$1 = 0</div><div class="line"></div><div class="line">(gdb) p channel_axis_</div><div class="line">$2 = 1</div></pre></td></tr></table></figure>
<p>此时，<code>channels_</code>值为<code>0</code>。下一行命令为<code>n</code>，执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) n</div><div class="line">118       num_output_ = this-&gt;layer_param_.convolution_param().num_output();</div></pre></td></tr></table></figure>
<p>此时查看<code>channels_</code>值为<code>1</code>，mnist数据是灰度图像，<code>channels_</code>为<code>1</code>没问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) p channels_</div><div class="line">$3 = 1</div></pre></td></tr></table></figure>
<p>命令<code>c</code>是继续执行直到下一个断点。</p>
<p>如果需要调试GPU程序，可以使用<code>cuda-gdb</code>，文档地址为：<a href="http://docs.nvidia.com/cuda/cuda-gdb/index.html#axzz4nAAR7ujZ" target="_blank" rel="external">http://docs.nvidia.com/cuda/cuda-gdb/index.html#axzz4nAAR7ujZ</a>。</p>
<p>参考资料</p>
<ol>
<li><a href="http://zhaok.xyz/blog/post/debug-caffe/" target="_blank" rel="external">http://zhaok.xyz/blog/post/debug-caffe/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Caffe源码调试
    
    </summary>
    
      <category term="Caffe" scheme="noahsnail.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="noahsnail.com/tags/Caffe/"/>
    
  </entry>
  
  <entry>
    <title>vim使用总结</title>
    <link href="noahsnail.com/2017/07/13/2017-7-13-vim%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"/>
    <id>noahsnail.com/2017/07/13/2017-7-13-vim使用总结/</id>
    <published>2017-07-13T02:36:29.000Z</published>
    <updated>2017-07-18T06:17:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是记录使用vim过程中的一些用法，本文中介绍的命令都在vim中使用验证过。</p>
<h2 id="1-删除一行或多行"><a href="#1-删除一行或多行" class="headerlink" title="1. 删除一行或多行"></a>1. 删除一行或多行</h2><ul>
<li>删除一行，命令格式：[:行号d]</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 删除第10行</div><div class="line"></div><div class="line">:10d</div></pre></td></tr></table></figure>
<ul>
<li>删除多行，命令格式：[:起始行号,结束行号d]</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 删除103-104行</div><div class="line"></div><div class="line">:103,104d</div></pre></td></tr></table></figure>
<h2 id="2-显示-不显示行号"><a href="#2-显示-不显示行号" class="headerlink" title="2. 显示/不显示行号"></a>2. 显示/不显示行号</h2><ul>
<li>显示行号</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">:set nu</div></pre></td></tr></table></figure>
<ul>
<li>不显示行号</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">:set nonu</div></pre></td></tr></table></figure>
<h2 id="3-字符串替换"><a href="#3-字符串替换" class="headerlink" title="3. 字符串替换"></a>3. 字符串替换</h2><ul>
<li>命令格式：[:%s/原始字符串/要替换的字符串]</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># test替换为global，默认情况下全部替换</div><div class="line"></div><div class="line">:%s/test/global</div></pre></td></tr></table></figure>
<h2 id="4-跳到指定行"><a href="#4-跳到指定行" class="headerlink" title="4. 跳到指定行"></a>4. 跳到指定行</h2><ul>
<li>命令格式：[:line-number]</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 跳到118行</div><div class="line">:118</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      vim使用总结
    
    </summary>
    
      <category term="开发工具" scheme="noahsnail.com/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="vim" scheme="noahsnail.com/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>Caffe源码解析(一)——caffe.proto</title>
    <link href="noahsnail.com/2017/07/11/2017-7-12-Caffe%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(%E4%B8%80)%E2%80%94%E2%80%94caffe.proto/"/>
    <id>noahsnail.com/2017/07/11/2017-7-12-Caffe源码解析(一)——caffe.proto/</id>
    <published>2017-07-11T10:01:32.000Z</published>
    <updated>2017-07-13T07:14:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>caffe.proto是caffe数据结构定义的主要文件，本文主要是在caffe.proto代码的基础上加上了部分中文注释，其中的内容与caffe的prototxt文件中的结构相对应。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div><div class="line">488</div><div class="line">489</div><div class="line">490</div><div class="line">491</div><div class="line">492</div><div class="line">493</div><div class="line">494</div><div class="line">495</div><div class="line">496</div><div class="line">497</div><div class="line">498</div><div class="line">499</div><div class="line">500</div><div class="line">501</div><div class="line">502</div><div class="line">503</div><div class="line">504</div><div class="line">505</div><div class="line">506</div><div class="line">507</div><div class="line">508</div><div class="line">509</div><div class="line">510</div><div class="line">511</div><div class="line">512</div><div class="line">513</div><div class="line">514</div><div class="line">515</div><div class="line">516</div><div class="line">517</div><div class="line">518</div><div class="line">519</div><div class="line">520</div><div class="line">521</div><div class="line">522</div><div class="line">523</div><div class="line">524</div><div class="line">525</div><div class="line">526</div><div class="line">527</div><div class="line">528</div><div class="line">529</div><div class="line">530</div><div class="line">531</div><div class="line">532</div><div class="line">533</div><div class="line">534</div><div class="line">535</div><div class="line">536</div><div class="line">537</div><div class="line">538</div><div class="line">539</div><div class="line">540</div><div class="line">541</div><div class="line">542</div><div class="line">543</div><div class="line">544</div><div class="line">545</div><div class="line">546</div><div class="line">547</div><div class="line">548</div><div class="line">549</div><div class="line">550</div><div class="line">551</div><div class="line">552</div><div class="line">553</div><div class="line">554</div><div class="line">555</div><div class="line">556</div><div class="line">557</div><div class="line">558</div><div class="line">559</div><div class="line">560</div><div class="line">561</div><div class="line">562</div><div class="line">563</div><div class="line">564</div><div class="line">565</div><div class="line">566</div><div class="line">567</div><div class="line">568</div><div class="line">569</div><div class="line">570</div><div class="line">571</div><div class="line">572</div><div class="line">573</div><div class="line">574</div><div class="line">575</div><div class="line">576</div><div class="line">577</div><div class="line">578</div><div class="line">579</div><div class="line">580</div><div class="line">581</div><div class="line">582</div><div class="line">583</div><div class="line">584</div><div class="line">585</div><div class="line">586</div><div class="line">587</div><div class="line">588</div><div class="line">589</div><div class="line">590</div><div class="line">591</div><div class="line">592</div><div class="line">593</div><div class="line">594</div><div class="line">595</div><div class="line">596</div><div class="line">597</div><div class="line">598</div><div class="line">599</div><div class="line">600</div><div class="line">601</div><div class="line">602</div><div class="line">603</div><div class="line">604</div><div class="line">605</div><div class="line">606</div><div class="line">607</div><div class="line">608</div><div class="line">609</div><div class="line">610</div><div class="line">611</div><div class="line">612</div><div class="line">613</div><div class="line">614</div><div class="line">615</div><div class="line">616</div><div class="line">617</div><div class="line">618</div><div class="line">619</div><div class="line">620</div><div class="line">621</div><div class="line">622</div><div class="line">623</div><div class="line">624</div><div class="line">625</div><div class="line">626</div><div class="line">627</div><div class="line">628</div><div class="line">629</div><div class="line">630</div><div class="line">631</div><div class="line">632</div><div class="line">633</div><div class="line">634</div><div class="line">635</div><div class="line">636</div><div class="line">637</div><div class="line">638</div><div class="line">639</div><div class="line">640</div><div class="line">641</div><div class="line">642</div><div class="line">643</div><div class="line">644</div><div class="line">645</div><div class="line">646</div><div class="line">647</div><div class="line">648</div><div class="line">649</div><div class="line">650</div><div class="line">651</div><div class="line">652</div><div class="line">653</div><div class="line">654</div><div class="line">655</div><div class="line">656</div><div class="line">657</div><div class="line">658</div><div class="line">659</div><div class="line">660</div><div class="line">661</div><div class="line">662</div><div class="line">663</div><div class="line">664</div><div class="line">665</div><div class="line">666</div><div class="line">667</div><div class="line">668</div><div class="line">669</div><div class="line">670</div><div class="line">671</div><div class="line">672</div><div class="line">673</div><div class="line">674</div><div class="line">675</div><div class="line">676</div><div class="line">677</div><div class="line">678</div><div class="line">679</div><div class="line">680</div><div class="line">681</div><div class="line">682</div><div class="line">683</div><div class="line">684</div><div class="line">685</div><div class="line">686</div><div class="line">687</div><div class="line">688</div><div class="line">689</div><div class="line">690</div><div class="line">691</div><div class="line">692</div><div class="line">693</div><div class="line">694</div><div class="line">695</div><div class="line">696</div><div class="line">697</div><div class="line">698</div><div class="line">699</div><div class="line">700</div><div class="line">701</div><div class="line">702</div><div class="line">703</div><div class="line">704</div><div class="line">705</div><div class="line">706</div><div class="line">707</div><div class="line">708</div><div class="line">709</div><div class="line">710</div><div class="line">711</div><div class="line">712</div><div class="line">713</div><div class="line">714</div><div class="line">715</div><div class="line">716</div><div class="line">717</div><div class="line">718</div><div class="line">719</div><div class="line">720</div><div class="line">721</div><div class="line">722</div><div class="line">723</div><div class="line">724</div><div class="line">725</div><div class="line">726</div><div class="line">727</div><div class="line">728</div><div class="line">729</div><div class="line">730</div><div class="line">731</div><div class="line">732</div><div class="line">733</div><div class="line">734</div><div class="line">735</div><div class="line">736</div><div class="line">737</div><div class="line">738</div><div class="line">739</div><div class="line">740</div><div class="line">741</div><div class="line">742</div><div class="line">743</div><div class="line">744</div><div class="line">745</div><div class="line">746</div><div class="line">747</div><div class="line">748</div><div class="line">749</div><div class="line">750</div><div class="line">751</div><div class="line">752</div><div class="line">753</div><div class="line">754</div><div class="line">755</div><div class="line">756</div><div class="line">757</div><div class="line">758</div><div class="line">759</div><div class="line">760</div><div class="line">761</div><div class="line">762</div><div class="line">763</div><div class="line">764</div><div class="line">765</div><div class="line">766</div><div class="line">767</div><div class="line">768</div><div class="line">769</div><div class="line">770</div><div class="line">771</div><div class="line">772</div><div class="line">773</div><div class="line">774</div><div class="line">775</div><div class="line">776</div><div class="line">777</div><div class="line">778</div><div class="line">779</div><div class="line">780</div><div class="line">781</div><div class="line">782</div><div class="line">783</div><div class="line">784</div><div class="line">785</div><div class="line">786</div><div class="line">787</div><div class="line">788</div><div class="line">789</div><div class="line">790</div><div class="line">791</div><div class="line">792</div><div class="line">793</div><div class="line">794</div><div class="line">795</div><div class="line">796</div><div class="line">797</div><div class="line">798</div><div class="line">799</div><div class="line">800</div><div class="line">801</div><div class="line">802</div><div class="line">803</div><div class="line">804</div><div class="line">805</div><div class="line">806</div><div class="line">807</div><div class="line">808</div><div class="line">809</div><div class="line">810</div><div class="line">811</div><div class="line">812</div><div class="line">813</div><div class="line">814</div><div class="line">815</div><div class="line">816</div><div class="line">817</div><div class="line">818</div><div class="line">819</div><div class="line">820</div><div class="line">821</div><div class="line">822</div><div class="line">823</div><div class="line">824</div><div class="line">825</div><div class="line">826</div><div class="line">827</div><div class="line">828</div><div class="line">829</div><div class="line">830</div><div class="line">831</div><div class="line">832</div><div class="line">833</div><div class="line">834</div><div class="line">835</div><div class="line">836</div><div class="line">837</div><div class="line">838</div><div class="line">839</div><div class="line">840</div><div class="line">841</div><div class="line">842</div><div class="line">843</div><div class="line">844</div><div class="line">845</div><div class="line">846</div><div class="line">847</div><div class="line">848</div><div class="line">849</div><div class="line">850</div><div class="line">851</div><div class="line">852</div><div class="line">853</div><div class="line">854</div><div class="line">855</div><div class="line">856</div><div class="line">857</div><div class="line">858</div><div class="line">859</div><div class="line">860</div><div class="line">861</div><div class="line">862</div><div class="line">863</div><div class="line">864</div><div class="line">865</div><div class="line">866</div><div class="line">867</div><div class="line">868</div><div class="line">869</div><div class="line">870</div><div class="line">871</div><div class="line">872</div><div class="line">873</div><div class="line">874</div><div class="line">875</div><div class="line">876</div><div class="line">877</div><div class="line">878</div><div class="line">879</div><div class="line">880</div><div class="line">881</div><div class="line">882</div><div class="line">883</div><div class="line">884</div><div class="line">885</div><div class="line">886</div><div class="line">887</div><div class="line">888</div><div class="line">889</div><div class="line">890</div><div class="line">891</div><div class="line">892</div><div class="line">893</div><div class="line">894</div><div class="line">895</div><div class="line">896</div><div class="line">897</div><div class="line">898</div><div class="line">899</div><div class="line">900</div><div class="line">901</div><div class="line">902</div><div class="line">903</div><div class="line">904</div><div class="line">905</div><div class="line">906</div><div class="line">907</div><div class="line">908</div><div class="line">909</div><div class="line">910</div><div class="line">911</div><div class="line">912</div><div class="line">913</div><div class="line">914</div><div class="line">915</div><div class="line">916</div><div class="line">917</div><div class="line">918</div><div class="line">919</div><div class="line">920</div><div class="line">921</div><div class="line">922</div><div class="line">923</div><div class="line">924</div><div class="line">925</div><div class="line">926</div><div class="line">927</div><div class="line">928</div><div class="line">929</div><div class="line">930</div><div class="line">931</div><div class="line">932</div><div class="line">933</div><div class="line">934</div><div class="line">935</div><div class="line">936</div><div class="line">937</div><div class="line">938</div><div class="line">939</div><div class="line">940</div><div class="line">941</div><div class="line">942</div><div class="line">943</div><div class="line">944</div><div class="line">945</div><div class="line">946</div><div class="line">947</div><div class="line">948</div><div class="line">949</div><div class="line">950</div><div class="line">951</div><div class="line">952</div><div class="line">953</div><div class="line">954</div><div class="line">955</div><div class="line">956</div><div class="line">957</div><div class="line">958</div><div class="line">959</div><div class="line">960</div><div class="line">961</div><div class="line">962</div><div class="line">963</div><div class="line">964</div><div class="line">965</div><div class="line">966</div><div class="line">967</div><div class="line">968</div><div class="line">969</div><div class="line">970</div><div class="line">971</div><div class="line">972</div><div class="line">973</div><div class="line">974</div><div class="line">975</div><div class="line">976</div><div class="line">977</div><div class="line">978</div><div class="line">979</div><div class="line">980</div><div class="line">981</div><div class="line">982</div><div class="line">983</div><div class="line">984</div><div class="line">985</div><div class="line">986</div><div class="line">987</div><div class="line">988</div><div class="line">989</div><div class="line">990</div><div class="line">991</div><div class="line">992</div><div class="line">993</div><div class="line">994</div><div class="line">995</div><div class="line">996</div><div class="line">997</div><div class="line">998</div><div class="line">999</div><div class="line">1000</div><div class="line">1001</div><div class="line">1002</div><div class="line">1003</div><div class="line">1004</div><div class="line">1005</div><div class="line">1006</div><div class="line">1007</div><div class="line">1008</div><div class="line">1009</div><div class="line">1010</div><div class="line">1011</div><div class="line">1012</div><div class="line">1013</div><div class="line">1014</div><div class="line">1015</div><div class="line">1016</div><div class="line">1017</div><div class="line">1018</div><div class="line">1019</div><div class="line">1020</div><div class="line">1021</div><div class="line">1022</div><div class="line">1023</div><div class="line">1024</div><div class="line">1025</div><div class="line">1026</div><div class="line">1027</div><div class="line">1028</div><div class="line">1029</div><div class="line">1030</div><div class="line">1031</div><div class="line">1032</div><div class="line">1033</div><div class="line">1034</div><div class="line">1035</div><div class="line">1036</div><div class="line">1037</div><div class="line">1038</div><div class="line">1039</div><div class="line">1040</div><div class="line">1041</div><div class="line">1042</div><div class="line">1043</div><div class="line">1044</div><div class="line">1045</div><div class="line">1046</div><div class="line">1047</div><div class="line">1048</div><div class="line">1049</div><div class="line">1050</div><div class="line">1051</div><div class="line">1052</div><div class="line">1053</div><div class="line">1054</div><div class="line">1055</div><div class="line">1056</div><div class="line">1057</div><div class="line">1058</div><div class="line">1059</div><div class="line">1060</div><div class="line">1061</div><div class="line">1062</div><div class="line">1063</div><div class="line">1064</div><div class="line">1065</div><div class="line">1066</div><div class="line">1067</div><div class="line">1068</div><div class="line">1069</div><div class="line">1070</div><div class="line">1071</div><div class="line">1072</div><div class="line">1073</div><div class="line">1074</div><div class="line">1075</div><div class="line">1076</div><div class="line">1077</div><div class="line">1078</div><div class="line">1079</div><div class="line">1080</div><div class="line">1081</div><div class="line">1082</div><div class="line">1083</div><div class="line">1084</div><div class="line">1085</div><div class="line">1086</div><div class="line">1087</div><div class="line">1088</div><div class="line">1089</div><div class="line">1090</div><div class="line">1091</div><div class="line">1092</div><div class="line">1093</div><div class="line">1094</div><div class="line">1095</div><div class="line">1096</div><div class="line">1097</div><div class="line">1098</div><div class="line">1099</div><div class="line">1100</div><div class="line">1101</div><div class="line">1102</div><div class="line">1103</div><div class="line">1104</div><div class="line">1105</div><div class="line">1106</div><div class="line">1107</div><div class="line">1108</div><div class="line">1109</div><div class="line">1110</div><div class="line">1111</div><div class="line">1112</div><div class="line">1113</div><div class="line">1114</div><div class="line">1115</div><div class="line">1116</div><div class="line">1117</div><div class="line">1118</div><div class="line">1119</div><div class="line">1120</div><div class="line">1121</div><div class="line">1122</div><div class="line">1123</div><div class="line">1124</div><div class="line">1125</div><div class="line">1126</div><div class="line">1127</div><div class="line">1128</div><div class="line">1129</div><div class="line">1130</div><div class="line">1131</div><div class="line">1132</div><div class="line">1133</div><div class="line">1134</div><div class="line">1135</div><div class="line">1136</div><div class="line">1137</div><div class="line">1138</div><div class="line">1139</div><div class="line">1140</div><div class="line">1141</div><div class="line">1142</div><div class="line">1143</div><div class="line">1144</div><div class="line">1145</div><div class="line">1146</div><div class="line">1147</div><div class="line">1148</div><div class="line">1149</div><div class="line">1150</div><div class="line">1151</div><div class="line">1152</div><div class="line">1153</div><div class="line">1154</div><div class="line">1155</div><div class="line">1156</div><div class="line">1157</div><div class="line">1158</div><div class="line">1159</div><div class="line">1160</div><div class="line">1161</div><div class="line">1162</div><div class="line">1163</div><div class="line">1164</div><div class="line">1165</div><div class="line">1166</div><div class="line">1167</div><div class="line">1168</div><div class="line">1169</div><div class="line">1170</div><div class="line">1171</div><div class="line">1172</div><div class="line">1173</div><div class="line">1174</div><div class="line">1175</div><div class="line">1176</div><div class="line">1177</div><div class="line">1178</div><div class="line">1179</div><div class="line">1180</div><div class="line">1181</div><div class="line">1182</div><div class="line">1183</div><div class="line">1184</div><div class="line">1185</div><div class="line">1186</div><div class="line">1187</div><div class="line">1188</div><div class="line">1189</div><div class="line">1190</div><div class="line">1191</div><div class="line">1192</div><div class="line">1193</div><div class="line">1194</div><div class="line">1195</div><div class="line">1196</div><div class="line">1197</div><div class="line">1198</div><div class="line">1199</div><div class="line">1200</div><div class="line">1201</div><div class="line">1202</div><div class="line">1203</div><div class="line">1204</div><div class="line">1205</div><div class="line">1206</div><div class="line">1207</div><div class="line">1208</div><div class="line">1209</div><div class="line">1210</div><div class="line">1211</div><div class="line">1212</div><div class="line">1213</div><div class="line">1214</div><div class="line">1215</div><div class="line">1216</div><div class="line">1217</div><div class="line">1218</div><div class="line">1219</div><div class="line">1220</div><div class="line">1221</div><div class="line">1222</div><div class="line">1223</div><div class="line">1224</div><div class="line">1225</div><div class="line">1226</div><div class="line">1227</div><div class="line">1228</div><div class="line">1229</div><div class="line">1230</div><div class="line">1231</div><div class="line">1232</div><div class="line">1233</div><div class="line">1234</div><div class="line">1235</div><div class="line">1236</div><div class="line">1237</div><div class="line">1238</div><div class="line">1239</div><div class="line">1240</div><div class="line">1241</div><div class="line">1242</div><div class="line">1243</div><div class="line">1244</div><div class="line">1245</div><div class="line">1246</div><div class="line">1247</div><div class="line">1248</div><div class="line">1249</div><div class="line">1250</div><div class="line">1251</div><div class="line">1252</div><div class="line">1253</div><div class="line">1254</div><div class="line">1255</div><div class="line">1256</div><div class="line">1257</div><div class="line">1258</div><div class="line">1259</div><div class="line">1260</div><div class="line">1261</div><div class="line">1262</div><div class="line">1263</div><div class="line">1264</div><div class="line">1265</div><div class="line">1266</div><div class="line">1267</div><div class="line">1268</div><div class="line">1269</div><div class="line">1270</div><div class="line">1271</div><div class="line">1272</div><div class="line">1273</div><div class="line">1274</div><div class="line">1275</div><div class="line">1276</div><div class="line">1277</div><div class="line">1278</div><div class="line">1279</div><div class="line">1280</div><div class="line">1281</div><div class="line">1282</div><div class="line">1283</div><div class="line">1284</div><div class="line">1285</div><div class="line">1286</div><div class="line">1287</div><div class="line">1288</div><div class="line">1289</div><div class="line">1290</div><div class="line">1291</div><div class="line">1292</div><div class="line">1293</div><div class="line">1294</div><div class="line">1295</div><div class="line">1296</div><div class="line">1297</div><div class="line">1298</div><div class="line">1299</div><div class="line">1300</div><div class="line">1301</div><div class="line">1302</div><div class="line">1303</div><div class="line">1304</div><div class="line">1305</div><div class="line">1306</div><div class="line">1307</div><div class="line">1308</div><div class="line">1309</div><div class="line">1310</div><div class="line">1311</div><div class="line">1312</div><div class="line">1313</div><div class="line">1314</div><div class="line">1315</div><div class="line">1316</div><div class="line">1317</div><div class="line">1318</div><div class="line">1319</div><div class="line">1320</div><div class="line">1321</div><div class="line">1322</div><div class="line">1323</div><div class="line">1324</div><div class="line">1325</div><div class="line">1326</div><div class="line">1327</div><div class="line">1328</div><div class="line">1329</div><div class="line">1330</div><div class="line">1331</div><div class="line">1332</div><div class="line">1333</div><div class="line">1334</div><div class="line">1335</div><div class="line">1336</div><div class="line">1337</div><div class="line">1338</div><div class="line">1339</div><div class="line">1340</div><div class="line">1341</div><div class="line">1342</div><div class="line">1343</div><div class="line">1344</div><div class="line">1345</div><div class="line">1346</div><div class="line">1347</div><div class="line">1348</div><div class="line">1349</div><div class="line">1350</div><div class="line">1351</div><div class="line">1352</div><div class="line">1353</div><div class="line">1354</div><div class="line">1355</div><div class="line">1356</div><div class="line">1357</div><div class="line">1358</div><div class="line">1359</div><div class="line">1360</div><div class="line">1361</div><div class="line">1362</div><div class="line">1363</div><div class="line">1364</div><div class="line">1365</div><div class="line">1366</div><div class="line">1367</div><div class="line">1368</div><div class="line">1369</div><div class="line">1370</div><div class="line">1371</div><div class="line">1372</div><div class="line">1373</div><div class="line">1374</div><div class="line">1375</div><div class="line">1376</div><div class="line">1377</div><div class="line">1378</div><div class="line">1379</div><div class="line">1380</div><div class="line">1381</div><div class="line">1382</div><div class="line">1383</div><div class="line">1384</div><div class="line">1385</div><div class="line">1386</div><div class="line">1387</div><div class="line">1388</div><div class="line">1389</div><div class="line">1390</div><div class="line">1391</div><div class="line">1392</div><div class="line">1393</div><div class="line">1394</div><div class="line">1395</div><div class="line">1396</div><div class="line">1397</div><div class="line">1398</div><div class="line">1399</div><div class="line">1400</div><div class="line">1401</div><div class="line">1402</div><div class="line">1403</div><div class="line">1404</div><div class="line">1405</div><div class="line">1406</div><div class="line">1407</div><div class="line">1408</div><div class="line">1409</div><div class="line">1410</div><div class="line">1411</div><div class="line">1412</div><div class="line">1413</div><div class="line">1414</div><div class="line">1415</div><div class="line">1416</div><div class="line">1417</div><div class="line">1418</div><div class="line">1419</div><div class="line">1420</div><div class="line">1421</div><div class="line">1422</div><div class="line">1423</div><div class="line">1424</div><div class="line">1425</div><div class="line">1426</div><div class="line">1427</div><div class="line">1428</div><div class="line">1429</div><div class="line">1430</div><div class="line">1431</div><div class="line">1432</div><div class="line">1433</div><div class="line">1434</div><div class="line">1435</div><div class="line">1436</div><div class="line">1437</div><div class="line">1438</div><div class="line">1439</div><div class="line">1440</div><div class="line">1441</div><div class="line">1442</div><div class="line">1443</div><div class="line">1444</div><div class="line">1445</div><div class="line">1446</div><div class="line">1447</div><div class="line">1448</div><div class="line">1449</div><div class="line">1450</div><div class="line">1451</div><div class="line">1452</div><div class="line">1453</div><div class="line">1454</div><div class="line">1455</div><div class="line">1456</div><div class="line">1457</div><div class="line">1458</div><div class="line">1459</div><div class="line">1460</div><div class="line">1461</div><div class="line">1462</div><div class="line">1463</div><div class="line">1464</div><div class="line">1465</div><div class="line">1466</div><div class="line">1467</div><div class="line">1468</div><div class="line">1469</div><div class="line">1470</div><div class="line">1471</div><div class="line">1472</div><div class="line">1473</div><div class="line">1474</div><div class="line">1475</div><div class="line">1476</div><div class="line">1477</div><div class="line">1478</div><div class="line">1479</div><div class="line">1480</div><div class="line">1481</div><div class="line">1482</div><div class="line">1483</div><div class="line">1484</div><div class="line">1485</div><div class="line">1486</div><div class="line">1487</div><div class="line">1488</div><div class="line">1489</div><div class="line">1490</div><div class="line">1491</div><div class="line">1492</div><div class="line">1493</div><div class="line">1494</div><div class="line">1495</div><div class="line">1496</div><div class="line">1497</div><div class="line">1498</div><div class="line">1499</div><div class="line">1500</div><div class="line">1501</div><div class="line">1502</div><div class="line">1503</div><div class="line">1504</div><div class="line">1505</div><div class="line">1506</div><div class="line">1507</div><div class="line">1508</div><div class="line">1509</div><div class="line">1510</div><div class="line">1511</div><div class="line">1512</div><div class="line">1513</div><div class="line">1514</div><div class="line">1515</div><div class="line">1516</div><div class="line">1517</div><div class="line">1518</div><div class="line">1519</div><div class="line">1520</div><div class="line">1521</div><div class="line">1522</div><div class="line">1523</div><div class="line">1524</div><div class="line">1525</div><div class="line">1526</div><div class="line">1527</div><div class="line">1528</div><div class="line">1529</div><div class="line">1530</div><div class="line">1531</div><div class="line">1532</div><div class="line">1533</div><div class="line">1534</div><div class="line">1535</div><div class="line">1536</div><div class="line">1537</div><div class="line">1538</div><div class="line">1539</div><div class="line">1540</div><div class="line">1541</div><div class="line">1542</div><div class="line">1543</div><div class="line">1544</div><div class="line">1545</div><div class="line">1546</div><div class="line">1547</div><div class="line">1548</div><div class="line">1549</div><div class="line">1550</div><div class="line">1551</div><div class="line">1552</div><div class="line">1553</div><div class="line">1554</div><div class="line">1555</div><div class="line">1556</div><div class="line">1557</div><div class="line">1558</div><div class="line">1559</div><div class="line">1560</div><div class="line">1561</div><div class="line">1562</div><div class="line">1563</div><div class="line">1564</div><div class="line">1565</div><div class="line">1566</div><div class="line">1567</div><div class="line">1568</div><div class="line">1569</div><div class="line">1570</div><div class="line">1571</div><div class="line">1572</div><div class="line">1573</div><div class="line">1574</div><div class="line">1575</div><div class="line">1576</div><div class="line">1577</div><div class="line">1578</div><div class="line">1579</div><div class="line">1580</div><div class="line">1581</div><div class="line">1582</div><div class="line">1583</div><div class="line">1584</div><div class="line">1585</div><div class="line">1586</div><div class="line">1587</div><div class="line">1588</div><div class="line">1589</div><div class="line">1590</div><div class="line">1591</div><div class="line">1592</div><div class="line">1593</div><div class="line">1594</div><div class="line">1595</div><div class="line">1596</div><div class="line">1597</div><div class="line">1598</div><div class="line">1599</div><div class="line">1600</div><div class="line">1601</div><div class="line">1602</div><div class="line">1603</div><div class="line">1604</div><div class="line">1605</div><div class="line">1606</div><div class="line">1607</div><div class="line">1608</div><div class="line">1609</div><div class="line">1610</div><div class="line">1611</div><div class="line">1612</div><div class="line">1613</div><div class="line">1614</div><div class="line">1615</div><div class="line">1616</div><div class="line">1617</div><div class="line">1618</div><div class="line">1619</div><div class="line">1620</div><div class="line">1621</div><div class="line">1622</div><div class="line">1623</div><div class="line">1624</div><div class="line">1625</div><div class="line">1626</div><div class="line">1627</div><div class="line">1628</div><div class="line">1629</div><div class="line">1630</div><div class="line">1631</div><div class="line">1632</div><div class="line">1633</div><div class="line">1634</div><div class="line">1635</div><div class="line">1636</div><div class="line">1637</div><div class="line">1638</div><div class="line">1639</div><div class="line">1640</div><div class="line">1641</div><div class="line">1642</div><div class="line">1643</div><div class="line">1644</div><div class="line">1645</div><div class="line">1646</div><div class="line">1647</div><div class="line">1648</div><div class="line">1649</div><div class="line">1650</div><div class="line">1651</div><div class="line">1652</div><div class="line">1653</div><div class="line">1654</div><div class="line">1655</div><div class="line">1656</div><div class="line">1657</div><div class="line">1658</div><div class="line">1659</div><div class="line">1660</div><div class="line">1661</div><div class="line">1662</div><div class="line">1663</div><div class="line">1664</div><div class="line">1665</div><div class="line">1666</div><div class="line">1667</div><div class="line">1668</div><div class="line">1669</div><div class="line">1670</div><div class="line">1671</div><div class="line">1672</div><div class="line">1673</div><div class="line">1674</div><div class="line">1675</div><div class="line">1676</div><div class="line">1677</div><div class="line">1678</div><div class="line">1679</div><div class="line">1680</div><div class="line">1681</div><div class="line">1682</div><div class="line">1683</div><div class="line">1684</div><div class="line">1685</div><div class="line">1686</div><div class="line">1687</div><div class="line">1688</div><div class="line">1689</div><div class="line">1690</div><div class="line">1691</div><div class="line">1692</div><div class="line">1693</div><div class="line">1694</div><div class="line">1695</div><div class="line">1696</div><div class="line">1697</div><div class="line">1698</div><div class="line">1699</div><div class="line">1700</div><div class="line">1701</div><div class="line">1702</div><div class="line">1703</div><div class="line">1704</div><div class="line">1705</div><div class="line">1706</div><div class="line">1707</div><div class="line">1708</div><div class="line">1709</div><div class="line">1710</div><div class="line">1711</div><div class="line">1712</div><div class="line">1713</div><div class="line">1714</div><div class="line">1715</div><div class="line">1716</div><div class="line">1717</div><div class="line">1718</div><div class="line">1719</div><div class="line">1720</div><div class="line">1721</div><div class="line">1722</div><div class="line">1723</div><div class="line">1724</div><div class="line">1725</div><div class="line">1726</div><div class="line">1727</div><div class="line">1728</div><div class="line">1729</div><div class="line">1730</div><div class="line">1731</div><div class="line">1732</div><div class="line">1733</div><div class="line">1734</div><div class="line">1735</div><div class="line">1736</div><div class="line">1737</div><div class="line">1738</div><div class="line">1739</div><div class="line">1740</div><div class="line">1741</div><div class="line">1742</div><div class="line">1743</div><div class="line">1744</div><div class="line">1745</div><div class="line">1746</div><div class="line">1747</div><div class="line">1748</div><div class="line">1749</div><div class="line">1750</div><div class="line">1751</div><div class="line">1752</div></pre></td><td class="code"><pre><div class="line">// syntax用来指定protobuf的版本</div><div class="line">syntax = &quot;proto2&quot;;</div><div class="line"></div><div class="line">// package可以看作C++中的namespace，与Caffe C++代码中的namespace caffe对应</div><div class="line">// package用来避免名称冲突</div><div class="line">package caffe;</div><div class="line"></div><div class="line"></div><div class="line">// 在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。</div><div class="line">// 注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。</div><div class="line">// required：一个格式良好的消息一定要含有一个这种字段，表示该值是必须要设置的。</div><div class="line">// optional：消息格式中该字段可以有0个或1个值（不超过1个）。</div><div class="line">// repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留，表示该值可以重复，相当于Java中的List。</div><div class="line"></div><div class="line"></div><div class="line">// Specifies the shape (dimensions) of a Blob.</div><div class="line">// 指定Blob的shape，4-D shape</div><div class="line">message BlobShape &#123;</div><div class="line">  //数据块形状定义为Num * Channel * Height * Wight, 原因在于caffe基于容器的多维嵌套来实现高维数据的封装, 即vector。 </div><div class="line">  repeated int64 dim = 1 [packed = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Blob数据块，包括Blob shape，数据和微分</div><div class="line">message BlobProto &#123;</div><div class="line">  // Blob的shape, 即numpy中的shape</div><div class="line">  optional BlobShape shape = 7;</div><div class="line">  // Blob的数据部分</div><div class="line">  repeated float data = 5 [packed = true];</div><div class="line">  // Blob的微分部分</div><div class="line">  repeated float diff = 6 [packed = true];</div><div class="line">  // Blob中的数据部分(double类型)</div><div class="line">  repeated double double_data = 8 [packed = true];</div><div class="line">  // Blob的微分部分(double类型)</div><div class="line">  repeated double double_diff = 9 [packed = true];</div><div class="line"></div><div class="line">  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.</div><div class="line">  // Blob的4个维度，已被Blob shape代替</div><div class="line">  // Blob中数据的个数(例如卷积核的个数)</div><div class="line">  optional int32 num = 1 [default = 0];</div><div class="line">  // Blob中数据的通道数</div><div class="line">  optional int32 channels = 2 [default = 0];</div><div class="line">  // Blob中数据的高度</div><div class="line">  optional int32 height = 3 [default = 0];</div><div class="line">  // Blob中数据的宽度</div><div class="line">  optional int32 width = 4 [default = 0];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// The BlobProtoVector is simply a way to pass multiple blobproto instances</div><div class="line">// around.</div><div class="line">// BlobProtoVector, 用来保存多个BlobProb对象的Vector</div><div class="line">message BlobProtoVector &#123;</div><div class="line">  repeated BlobProto blobs = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//图像数据, channel-图像通道数, height-高度, width-宽度, data-图像像素数据, label-图像标签, float_data-图像浮点型数据(0-1之间), encoded-图像编码方式</div><div class="line">message Datum &#123;</div><div class="line">  // 图像的通道数</div><div class="line">  optional int32 channels = 1;</div><div class="line">  // 图像的高度</div><div class="line">  optional int32 height = 2;</div><div class="line">  // 图像的宽度</div><div class="line">  optional int32 width = 3;</div><div class="line">  // the actual image data, in bytes</div><div class="line">  // 实际的图像数据，以字节形式(uint8)表示</div><div class="line">  optional bytes data = 4;</div><div class="line">  // 图像对应的标签，必须为整形</div><div class="line">  optional int32 label = 5;</div><div class="line">  // Optionally, the datum could also hold float data.</div><div class="line">  // 可选表示，图像数据表示为float数据，即0-255归一化到0-1之间</div><div class="line">  repeated float float_data = 6;</div><div class="line">  // If true data contains an encoded image that need to be decoded</div><div class="line">  // encoded为true表示图像采用压缩表示，需要解码</div><div class="line">  optional bool encoded = 7 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Filler参数, filler主要对网络权重进行初始化</div><div class="line">// Filler类型分为常量初始化（constant）、高斯分布初始化（gaussian）、positive_unitball初始化、均匀分布初始化（uniform）、xavier初始化、msra初始化、双线性初始化（bilinear）</div><div class="line">message FillerParameter &#123;</div><div class="line">  // The filler type.</div><div class="line">  // Filler的类型</div><div class="line">  optional string type = 1 [default = &apos;constant&apos;];</div><div class="line">  // 常量初始化的值</div><div class="line">  optional float value = 2 [default = 0]; // the value in constant filler</div><div class="line">  // 均匀分布初始化中的最小值</div><div class="line">  optional float min = 3 [default = 0]; // the min value in uniform filler</div><div class="line">  // 均匀分布初始化中的最大值</div><div class="line">  optional float max = 4 [default = 1]; // the max value in uniform filler</div><div class="line">  // 高斯分布初始化中的均值</div><div class="line">  optional float mean = 5 [default = 0]; // the mean value in Gaussian filler</div><div class="line">  // 高斯分布初始化中的标准差</div><div class="line">  optional float std = 6 [default = 1]; // the std value in Gaussian filler</div><div class="line">  // The expected number of non-zero output weights for a given input in</div><div class="line">  // Gaussian filler -- the default -1 means don&apos;t perform sparsification.</div><div class="line">  // 在高斯分布初始化中给定输入及权重，期望输出非0值，默认值-1表示不进行稀疏化</div><div class="line">  optional int32 sparse = 7 [default = -1];</div><div class="line">  // Normalize the filler variance by fan_in, fan_out, or their average.</div><div class="line">  // Applies to &apos;xavier&apos; and &apos;msra&apos; fillers.</div><div class="line">  // 通过fan_in, fan_out或average来归一化filler方差，主要应用到&apos;xavier&apos;和&apos;msra&apos; filler中</div><div class="line">  enum VarianceNorm &#123;</div><div class="line">    FAN_IN = 0;</div><div class="line">    FAN_OUT = 1;</div><div class="line">    AVERAGE = 2;</div><div class="line">  &#125;</div><div class="line">  // 定义filler方差归一化，默认为FAN_IN</div><div class="line">  optional VarianceNorm variance_norm = 8 [default = FAN_IN];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//神经网络参数</div><div class="line">message NetParameter &#123;</div><div class="line">  // 神经网络名字</div><div class="line">  optional string name = 1; // consider giving the network a name</div><div class="line"></div><div class="line">  // DEPRECATED. See InputParameter. The input blobs to the network.</div><div class="line">  // 已废弃。网络的输入部分，具体请看InputParameter。</div><div class="line">  repeated string input = 3;</div><div class="line"></div><div class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</div><div class="line">  // 已废弃。输入blob的shape，具体请看InputParameter。</div><div class="line">  repeated BlobShape input_shape = 8;</div><div class="line"></div><div class="line">  // 4D input dimensions -- deprecated.  Use &quot;input_shape&quot; instead.</div><div class="line">  // If specified, for each input blob there should be four</div><div class="line">  // values specifying the num, channels, height and width of the input blob.</div><div class="line">  // Thus, there should be a total of (4 * #input) numbers.</div><div class="line">  // 已废弃。用input_shape代替。</div><div class="line">  repeated int32 input_dim = 4;</div><div class="line"></div><div class="line">  // Whether the network will force every layer to carry out backward operation.</div><div class="line">  // If set False, then whether to carry out backward is determined</div><div class="line">  // automatically according to the net structure and learning rates.</div><div class="line">  // 网络中是否每一层都执行反向传播的标志，如果设为false，反向传播会根据网络结构和学习率自动进行。</div><div class="line">  optional bool force_backward = 5 [default = false];</div><div class="line"></div><div class="line">  // The current &quot;state&quot; of the network, including the phase, level, and stage.</div><div class="line">  // Some layers may be included/excluded depending on this state and the states</div><div class="line">  // specified in the layers&apos; include and exclude fields.</div><div class="line">  // 网络的当前状态，包括phase, level和stage，(phase应该是对应prototxt文件中的TRAIN,TEST)</div><div class="line">  // 某些层是否included/excluded依赖于层中include，exclue字段指定的state。</div><div class="line">  optional NetState state = 6;</div><div class="line"></div><div class="line">  // Print debugging information about results while running Net::Forward,</div><div class="line">  // Net::Backward, and Net::Update.</div><div class="line">  // 在执行Net::Forward,Net::Backward, Net::Update时是否打印调试信息。</div><div class="line">  optional bool debug_info = 7 [default = false];</div><div class="line"></div><div class="line">  // The layers that make up the net.  Each of their configurations, including</div><div class="line">  // connectivity and behavior, is specified as a LayerParameter.</div><div class="line">  // 构成网络的layer，每一个layer的配置，包括连接性和行为都在LayerParameter中指定。</div><div class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</div><div class="line"></div><div class="line">  // DEPRECATED: use &apos;layer&apos; instead.</div><div class="line">  // 已废弃，用layer代替。</div><div class="line">  repeated V1LayerParameter layers = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// NOTE</div><div class="line">// Update the next available ID when you add a new SolverParameter field.</div><div class="line">// 注意：当你添加一个新的SolverParameter字段时，要更新下一个可获取的ID</div><div class="line">// SolverParameter next available ID: 41 (last added: type)</div><div class="line">// Solver参数</div><div class="line">message SolverParameter &#123;</div><div class="line">  //////////////////////////////////////////////////////////////////////////////</div><div class="line">  // Specifying the train and test networks</div><div class="line">  //</div><div class="line">  // Exactly one train net must be specified using one of the following fields:</div><div class="line">  //     train_net_param, train_net, net_param, net</div><div class="line">  // One or more test nets may be specified using any of the following fields:</div><div class="line">  //     test_net_param, test_net, net_param, net</div><div class="line">  // If more than one test net field is specified (e.g., both net and</div><div class="line">  // test_net are specified), they will be evaluated in the field order given</div><div class="line">  // above: (1) test_net_param, (2) test_net, (3) net_param/net.</div><div class="line">  // A test_iter must be specified for each test_net.</div><div class="line">  // A test_level and/or a test_stage may also be specified for each test_net.</div><div class="line">  //////////////////////////////////////////////////////////////////////////////</div><div class="line"></div><div class="line">  // Proto filename for the train net, possibly combined with one or more test nets.</div><div class="line">  // 训练网络的prototxt文件名，可能结合一个或多个测试网络</div><div class="line">  optional string net = 24;</div><div class="line">  // Inline train net param, possibly combined with one or more test nets.</div><div class="line">  // 内联训练网络参数，可能结合一个或多个测试网络</div><div class="line">  optional NetParameter net_param = 25;</div><div class="line"></div><div class="line">  // 训练网络的proto文件名</div><div class="line">  optional string train_net = 1; // Proto filename for the train net.</div><div class="line">  // 测试网络的proto文件名</div><div class="line">  repeated string test_net = 2; // Proto filenames for the test nets.</div><div class="line">  // 内联训练网络参数</div><div class="line">  optional NetParameter train_net_param = 21; // Inline train net params.</div><div class="line">  // 内联测试网络参数</div><div class="line">  repeated NetParameter test_net_param = 22; // Inline test net params.</div><div class="line"></div><div class="line">  // The states for the train/test nets. Must be unspecified or specified once per net.</div><div class="line">  // By default, all states will have solver = true;</div><div class="line">  // train_state will have phase = TRAIN,</div><div class="line">  // and all test_state&apos;s will have phase = TEST.</div><div class="line">  // Other defaults are set according to the NetState defaults.</div><div class="line">  // train/test网络的状态，必须不指定或每个网络指定一次</div><div class="line">  // 默认情况下，所有的状态都有solver = true，train_state的phase = TRAIN，其它默认情况根据NetState默认值设定。</div><div class="line">  </div><div class="line">  // train网络的状态，必须不指定或每个网络指定一次</div><div class="line">  optional NetState train_state = 26;</div><div class="line">  // test网络的状态，必须不指定或每个网络指定一次</div><div class="line">  repeated NetState test_state = 27;</div><div class="line"></div><div class="line">  // The number of iterations for each test net.</div><div class="line">  // 每个测试网络的迭代次数，即测试数据的迭代次数，测试数据总数=测试迭代次数*测试数据的batch_size。</div><div class="line">  repeated int32 test_iter = 3;</div><div class="line"></div><div class="line">  // The number of iterations between two testing phases.</div><div class="line">  // 两次测试间隔的迭代次数，即训练数据迭代多少次进行一次测试。</div><div class="line">  optional int32 test_interval = 4 [default = 0];</div><div class="line">  // 测试数据的loss，默认情况下不计算</div><div class="line">  optional bool test_compute_loss = 19 [default = false];</div><div class="line">  // If true, run an initial test pass before the first iteration,</div><div class="line">  // ensuring memory availability and printing the starting value of the loss.</div><div class="line">  // 如果为true，在第一次迭代之前进行一次初始测试，从而确保内存可用性并输出初始损失值。</div><div class="line">  optional bool test_initialization = 32 [default = true];</div><div class="line">  // 基本学习率</div><div class="line">  optional float base_lr = 5; // The base learning rate</div><div class="line">  // the number of iterations between displaying info. If display = 0, no info will be displayed.</div><div class="line">  // 执行多少次迭代显示一次信息，如果display = 0，不输出信息。</div><div class="line">  optional int32 display = 6;</div><div class="line">  // Display the loss averaged over the last average_loss iterations</div><div class="line">  // 输出的平均损失是之前多少次迭代的平均损失。</div><div class="line">  optional int32 average_loss = 33 [default = 1];</div><div class="line">  // 训练的最大迭代次数</div><div class="line">  optional int32 max_iter = 7; // the maximum number of iterations</div><div class="line">  // accumulate gradients over `iter_size` x `batch_size` instances</div><div class="line">  // 累积`iter_size` x `batch_size`个实例的梯度</div><div class="line">  optional int32 iter_size = 36 [default = 1];</div><div class="line"></div><div class="line">  // The learning rate decay policy. The currently implemented learning rate</div><div class="line">  // policies are as follows:</div><div class="line">  //    - fixed: always return base_lr.</div><div class="line">  //    - step: return base_lr * gamma ^ (floor(iter / step))</div><div class="line">  //    - exp: return base_lr * gamma ^ iter</div><div class="line">  //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)</div><div class="line">  //    - multistep: similar to step but it allows non uniform steps defined by</div><div class="line">  //      stepvalue</div><div class="line">  //    - poly: the effective learning rate follows a polynomial decay, to be</div><div class="line">  //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)</div><div class="line">  //    - sigmoid: the effective learning rate follows a sigmod decay</div><div class="line">  //      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</div><div class="line">  //</div><div class="line">  // where base_lr, max_iter, gamma, step, stepvalue and power are defined</div><div class="line">  // in the solver parameter protocol buffer, and iter is the current iteration.</div><div class="line"></div><div class="line">  // 学习率的变化策略</div><div class="line">  optional string lr_policy = 8;</div><div class="line">  // 学习率的计算参数</div><div class="line">  optional float gamma = 9; // The parameter to compute the learning rate.</div><div class="line">  // 学习率的计算参数</div><div class="line">  optional float power = 10; // The parameter to compute the learning rate.</div><div class="line">  // 动量参数</div><div class="line">  optional float momentum = 11; // The momentum value.</div><div class="line">  // 权重衰减，权重衰减主要影响神经网络的正则项，具体可参考Caffe文档</div><div class="line">  optional float weight_decay = 12; // The weight decay.</div><div class="line">  // regularization types supported: L1 and L2, controlled by weight_decay</div><div class="line">  // 正则化类型支持L1和L2，受weight_decay控制。</div><div class="line">  optional string regularization_type = 29 [default = &quot;L2&quot;];</div><div class="line">  // the stepsize for learning rate policy &quot;step&quot;</div><div class="line">  // 学习率方案为step时的参数</div><div class="line">  optional int32 stepsize = 13;</div><div class="line">  // the stepsize for learning rate policy &quot;multistep&quot;</div><div class="line">  // 学习率方案为multistep时的参数</div><div class="line">  repeated int32 stepvalue = 34;</div><div class="line"></div><div class="line">  // Set clip_gradients to &gt;= 0 to clip parameter gradients to that L2 norm,</div><div class="line">  // whenever their actual L2 norm is larger.</div><div class="line">  // 设置clip_gradients &gt;= 0可以削减L2范数的梯度，当真实L2范数的梯度大于clip_gradients，将L2范数的梯度设为clip_gradients</div><div class="line">  optional float clip_gradients = 35 [default = -1];</div><div class="line">  // snapshot的间隔，即迭代多少次保存一次snapshot</div><div class="line">  optional int32 snapshot = 14 [default = 0]; // The snapshot interval</div><div class="line">  // snapshot的前缀</div><div class="line">  optional string snapshot_prefix = 15; // The prefix for the snapshot.</div><div class="line">  // whether to snapshot diff in the results or not. Snapshotting diff will help</div><div class="line">  // debugging but the final protocol buffer size will be much larger.</div><div class="line">  // 是否在结果中保存snapshot的差分，snapshot diff有助于调试，但snapshot的文件会更大。</div><div class="line">  optional bool snapshot_diff = 16 [default = false];</div><div class="line">  // snapshot的保存格式（hdf5,binaryproto）。</div><div class="line">  enum SnapshotFormat &#123;</div><div class="line">    HDF5 = 0;</div><div class="line">    BINARYPROTO = 1;</div><div class="line">  &#125;</div><div class="line">  // snapshot默认保存为BINARYPROTO。</div><div class="line">  optional SnapshotFormat snapshot_format = 37 [default = BINARYPROTO];</div><div class="line">  // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default.</div><div class="line">  // 求解神经网络的方式，0 CPU, 1 GPU。默认使用GPU</div><div class="line">  enum SolverMode &#123;</div><div class="line">    CPU = 0;</div><div class="line">    GPU = 1;</div><div class="line">  &#125;</div><div class="line">  // 求解神经网络的模式，0 CPU, 1 GPU。默认使用GPU</div><div class="line">  optional SolverMode solver_mode = 17 [default = GPU];</div><div class="line">  // the device_id will that be used in GPU mode. Use device_id = 0 in default.</div><div class="line">  // device_id是GPU模式下GPU的ID。</div><div class="line">  optional int32 device_id = 18 [default = 0];</div><div class="line">  // If non-negative, the seed with which the Solver will initialize the Caffe</div><div class="line">  // random number generator -- useful for reproducible results. Otherwise,</div><div class="line">  // (and by default) initialize using a seed derived from the system clock.</div><div class="line">  // 如果是非负值，seed用来初始化Caffe的随机数生成器，对于再见结果是很有用的，默认情况下，seed的是从系统时钟获取。</div><div class="line">  optional int64 random_seed = 20 [default = -1];</div><div class="line"></div><div class="line">  // type of the solver</div><div class="line">  // 神经网络求解的类型, 默认为SGD</div><div class="line">  optional string type = 40 [default = &quot;SGD&quot;];</div><div class="line"></div><div class="line">  // numerical stability for RMSProp, AdaGrad and AdaDelta and Adam</div><div class="line">  // RMSProp, AdaGrad, AdaDelta, Adam求解类型的参数</div><div class="line">  optional float delta = 31 [default = 1e-8];</div><div class="line">  // parameters for the Adam solver</div><div class="line">  // Adam求解类型的参数</div><div class="line">  optional float momentum2 = 39 [default = 0.999];</div><div class="line"></div><div class="line">  // RMSProp decay value</div><div class="line">  // MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)</div><div class="line">  // RMSProp类型的衰减值</div><div class="line">  optional float rms_decay = 38 [default = 0.99];</div><div class="line"></div><div class="line">  // If true, print information about the state of the net that may help with</div><div class="line">  // debugging learning problems.</div><div class="line">  // 如果设为true，会输出网络的状态信息，有助于调试</div><div class="line">  optional bool debug_info = 23 [default = false];</div><div class="line"></div><div class="line">  // If false, don&apos;t save a snapshot after training finishes.</div><div class="line">  // 如果设为false，不保存训练结束的snapshot。</div><div class="line">  optional bool snapshot_after_train = 28 [default = true];</div><div class="line"></div><div class="line">  // DEPRECATED: old solver enum types, use string instead</div><div class="line">  // 已废弃，使用string代替</div><div class="line">  enum SolverType &#123;</div><div class="line">    SGD = 0;</div><div class="line">    NESTEROV = 1;</div><div class="line">    ADAGRAD = 2;</div><div class="line">    RMSPROP = 3;</div><div class="line">    ADADELTA = 4;</div><div class="line">    ADAM = 5;</div><div class="line">  &#125;</div><div class="line">  // DEPRECATED: use type instead of solver_type</div><div class="line">  // 已废弃：使用type代替</div><div class="line">  optional SolverType solver_type = 30 [default = SGD];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// A message that stores the solver snapshots</div><div class="line">// 保存solver snapshots</div><div class="line">message SolverState &#123;</div><div class="line">  // 当前的迭代次数</div><div class="line">  optional int32 iter = 1; // The current iteration</div><div class="line">  // 保存学习到的网络</div><div class="line">  optional string learned_net = 2; // The file that stores the learned net.</div><div class="line">  // sgd的求解历史</div><div class="line">  repeated BlobProto history = 3; // The history for sgd solvers</div><div class="line">  // 学习的当前step</div><div class="line">  optional int32 current_step = 4 [default = 0]; // The current step for learning rate</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 定义phase</div><div class="line">enum Phase &#123;</div><div class="line">   TRAIN = 0;</div><div class="line">   TEST = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 网络状态</div><div class="line">message NetState &#123;</div><div class="line">  // 属于哪个phase</div><div class="line">  optional Phase phase = 1 [default = TEST];</div><div class="line">  optional int32 level = 2 [default = 0];</div><div class="line">  repeated string stage = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 网络状态分类</div><div class="line">message NetStateRule &#123;</div><div class="line">  // Set phase to require the NetState have a particular phase (TRAIN or TEST)</div><div class="line">  // to meet this rule.</div><div class="line">  // 设置phase</div><div class="line">  optional Phase phase = 1;</div><div class="line"></div><div class="line">  // Set the minimum and/or maximum levels in which the layer should be used.</div><div class="line">  // Leave undefined to meet the rule regardless of level.</div><div class="line">  // 设置layer的level</div><div class="line">  optional int32 min_level = 2;</div><div class="line">  optional int32 max_level = 3;</div><div class="line"></div><div class="line">  // Customizable sets of stages to include or exclude.</div><div class="line">  // The net must have ALL of the specified stages and NONE of the specified</div><div class="line">  // &quot;not_stage&quot;s to meet the rule.</div><div class="line">  // (Use multiple NetStateRules to specify conjunctions of stages.)</div><div class="line">  // 可定制的stage集合</div><div class="line">  repeated string stage = 4;</div><div class="line">  repeated string not_stage = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Specifies training parameters (multipliers on global learning constants,</div><div class="line">// and the name and other settings used for weight sharing).</div><div class="line">// 指定训练参数及名称以及权重共享的其它设置</div><div class="line">message ParamSpec &#123;</div><div class="line">  // The names of the parameter blobs -- useful for sharing parameters among</div><div class="line">  // layers, but never required otherwise.  To share a parameter between two</div><div class="line">  // layers, give it a (non-empty) name.</div><div class="line">  // 两个layer之间进行参数共享的blob名字</div><div class="line">  optional string name = 1;</div><div class="line"></div><div class="line">  // Whether to require shared weights to have the same shape, or just the same</div><div class="line">  // count -- defaults to STRICT if unspecified.</div><div class="line">  // 参数共享时是否需要具有相同的shape，默认情况下需要有相同的shape</div><div class="line">  optional DimCheckMode share_mode = 2;</div><div class="line">  // 参数共享时的维度检查</div><div class="line">  enum DimCheckMode &#123;</div><div class="line">    // STRICT (default) requires that num, channels, height, width each match.</div><div class="line">    STRICT = 0;</div><div class="line">    // PERMISSIVE requires only the count (num*channels*height*width) to match.</div><div class="line">    PERMISSIVE = 1;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // The multiplier on the global learning rate for this parameter.</div><div class="line">  // 学习率参数, learning rate = base_lr * lr_mult</div><div class="line">  optional float lr_mult = 3 [default = 1.0];</div><div class="line"></div><div class="line">  // The multiplier on the global weight decay for this parameter.</div><div class="line">  // 权重衰减参数, weight = weight_decay * decay_mult</div><div class="line">  optional float decay_mult = 4 [default = 1.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// NOTE</div><div class="line">// Update the next available ID when you add a new LayerParameter field.</div><div class="line">// LayerParameter next available layer-specific ID: 147 (last added: recurrent_param)</div><div class="line">// 注意：当你添加一个新的LayerParameter字段时，要更新下一个可获取的ID</div><div class="line">message LayerParameter &#123;</div><div class="line">  // layer名称</div><div class="line">  optional string name = 1; // the layer name</div><div class="line">  // layer类型</div><div class="line">  optional string type = 2; // the layer type</div><div class="line">  // layer的输入</div><div class="line">  repeated string bottom = 3; // the name of each bottom blob</div><div class="line">  // layer的输出</div><div class="line">  repeated string top = 4; // the name of each top blob</div><div class="line"></div><div class="line">  // The train / test phase for computation.</div><div class="line">  // layer用在train/test phase</div><div class="line">  optional Phase phase = 10;</div><div class="line"></div><div class="line">  // The amount of weight to assign each top blob in the objective.</div><div class="line">  // Each layer assigns a default value, usually of either 0 or 1,</div><div class="line">  // to each top blob.</div><div class="line">  // layer对最终的loss损失值的贡献率</div><div class="line">  repeated float loss_weight = 5;</div><div class="line"></div><div class="line">  // Specifies training parameters (multipliers on global learning constants,</div><div class="line">  // and the name and other settings used for weight sharing).</div><div class="line">  // 指定训练参数</div><div class="line">  repeated ParamSpec param = 6;</div><div class="line"></div><div class="line">  // The blobs containing the numeric parameters of the layer.</div><div class="line">  // layer的blobs</div><div class="line">  repeated BlobProto blobs = 7;</div><div class="line"></div><div class="line">  // Specifies whether to backpropagate to each bottom. If unspecified,</div><div class="line">  // Caffe will automatically infer whether each input needs backpropagation</div><div class="line">  // to compute parameter gradients. If set to true for some inputs,</div><div class="line">  // backpropagation to those inputs is forced; if set false for some inputs,</div><div class="line">  // backpropagation to those inputs is skipped.</div><div class="line">  //</div><div class="line">  // The size must be either 0 or equal to the number of bottoms.</div><div class="line">  // 指定反向传播是否传播到每一个bottom，如果不指定，caffe会自动检查推断是否每一个输入都需要反向传播来计算梯度。如果一些输入设为true,</div><div class="line">  // 则这些layer强制进行反向传播，如果设为false，这些layer将跳过反向传播。</div><div class="line">  repeated bool propagate_down = 11;</div><div class="line"></div><div class="line">  // Rules controlling whether and when a layer is included in the network,</div><div class="line">  // based on the current NetState.  You may specify a non-zero number of rules</div><div class="line">  // to include OR exclude, but not both.  If no include or exclude rules are</div><div class="line">  // specified, the layer is always included.  If the current NetState meets</div><div class="line">  // ANY (i.e., one or more) of the specified rules, the layer is</div><div class="line">  // included/excluded.</div><div class="line">  // 控制layer included/excluded</div><div class="line">  repeated NetStateRule include = 8;</div><div class="line">  repeated NetStateRule exclude = 9;</div><div class="line"></div><div class="line">  // Parameters for data pre-processing.</div><div class="line">  // 数据预处理参数</div><div class="line">  optional TransformationParameter transform_param = 100;</div><div class="line"></div><div class="line">  // Parameters shared by loss layers.</div><div class="line">  // loss layer的参数共享</div><div class="line">  optional LossParameter loss_param = 101;</div><div class="line"></div><div class="line">  // Layer type-specific parameters.</div><div class="line">  //</div><div class="line">  // Note: certain layers may have more than one computational engine</div><div class="line">  // for their implementation. These layers include an Engine type and</div><div class="line">  // engine parameter for selecting the implementation.</div><div class="line">  // The default for the engine is set by the ENGINE switch at compile-time.</div><div class="line"></div><div class="line">  // 特定layer的参数</div><div class="line">  optional AccuracyParameter accuracy_param = 102;</div><div class="line">  optional ArgMaxParameter argmax_param = 103;</div><div class="line">  optional BatchNormParameter batch_norm_param = 139;</div><div class="line">  optional BiasParameter bias_param = 141;</div><div class="line">  optional ConcatParameter concat_param = 104;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = 105;</div><div class="line">  optional ConvolutionParameter convolution_param = 106;</div><div class="line">  optional CropParameter crop_param = 144;</div><div class="line">  optional DataParameter data_param = 107;</div><div class="line">  optional DropoutParameter dropout_param = 108;</div><div class="line">  optional DummyDataParameter dummy_data_param = 109;</div><div class="line">  optional EltwiseParameter eltwise_param = 110;</div><div class="line">  optional ELUParameter elu_param = 140;</div><div class="line">  optional EmbedParameter embed_param = 137;</div><div class="line">  optional ExpParameter exp_param = 111;</div><div class="line">  optional FlattenParameter flatten_param = 135;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = 112;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 113;</div><div class="line">  optional HingeLossParameter hinge_loss_param = 114;</div><div class="line">  optional ImageDataParameter image_data_param = 115;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = 116;</div><div class="line">  optional InnerProductParameter inner_product_param = 117;</div><div class="line">  optional InputParameter input_param = 143;</div><div class="line">  optional LogParameter log_param = 134;</div><div class="line">  optional LRNParameter lrn_param = 118;</div><div class="line">  optional MemoryDataParameter memory_data_param = 119;</div><div class="line">  optional MVNParameter mvn_param = 120;</div><div class="line">  optional ParameterParameter parameter_param = 145;</div><div class="line">  optional PoolingParameter pooling_param = 121;</div><div class="line">  optional PowerParameter power_param = 122;</div><div class="line">  optional PReLUParameter prelu_param = 131;</div><div class="line">  optional PythonParameter python_param = 130;</div><div class="line">  optional RecurrentParameter recurrent_param = 146;</div><div class="line">  optional ReductionParameter reduction_param = 136;</div><div class="line">  optional ReLUParameter relu_param = 123;</div><div class="line">  optional ReshapeParameter reshape_param = 133;</div><div class="line">  optional ScaleParameter scale_param = 142;</div><div class="line">  optional SigmoidParameter sigmoid_param = 124;</div><div class="line">  optional SoftmaxParameter softmax_param = 125;</div><div class="line">  optional SPPParameter spp_param = 132;</div><div class="line">  optional SliceParameter slice_param = 126;</div><div class="line">  optional TanHParameter tanh_param = 127;</div><div class="line">  optional ThresholdParameter threshold_param = 128;</div><div class="line">  optional TileParameter tile_param = 138;</div><div class="line">  optional WindowDataParameter window_data_param = 129;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used to apply transformation to the data layer&apos;s data</div><div class="line">// 用来进行数据层（图像）变换的参数</div><div class="line">message TransformationParameter &#123;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  // 像素归一化，归一化之前会减去均值</div><div class="line">  optional float scale = 1 [default = 1];</div><div class="line">  // Specify if we want to randomly mirror data.</div><div class="line">  // 图像进行随机mirror操作</div><div class="line">  optional bool mirror = 2 [default = false];</div><div class="line">  // Specify if we would like to randomly crop an image.</div><div class="line">  // 图像随机crop操作</div><div class="line">  optional uint32 crop_size = 3 [default = 0];</div><div class="line">  // mean_file and mean_value cannot be specified at the same time</div><div class="line">  // 图像的均值文件</div><div class="line">  optional string mean_file = 4;</div><div class="line">  // if specified can be repeated once (would subtract it from all the channels)</div><div class="line">  // or can be repeated the same number of times as channels</div><div class="line">  // (would subtract them from the corresponding channel)</div><div class="line">  // 图像的均值，手动指定，通常是三个</div><div class="line">  repeated float mean_value = 5;</div><div class="line">  // Force the decoded image to have 3 color channels.</div><div class="line">  // 强制图像必须有三个颜色通道</div><div class="line">  optional bool force_color = 6 [default = false];</div><div class="line">  // Force the decoded image to have 1 color channels.</div><div class="line">  // 强制图像为灰度图像</div><div class="line">  optional bool force_gray = 7 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters shared by loss layers</div><div class="line">// loss层参数</div><div class="line">message LossParameter &#123;</div><div class="line">  // If specified, ignore instances with the given label.</div><div class="line">  // 如果指定，则label等于ignore_label的样本将不参与Loss计算，并且反向传播时梯度直接置0。</div><div class="line">  optional int32 ignore_label = 1;</div><div class="line">  // How to normalize the loss for loss layers that aggregate across batches,</div><div class="line">  // spatial dimensions, or other dimensions.  Currently only implemented in</div><div class="line">  // SoftmaxWithLoss and SigmoidCrossEntropyLoss layers.</div><div class="line">  // 指定loss归一化的方式</div><div class="line">  enum NormalizationMode &#123;</div><div class="line">    // Divide by the number of examples in the batch times spatial dimensions.</div><div class="line">    // Outputs that receive the ignore label will NOT be ignored in computing</div><div class="line">    // the normalization factor.</div><div class="line">    // 所有样本都参与计算，包括ignore label</div><div class="line">    FULL = 0;</div><div class="line">    // Divide by the total number of output locations that do not take the</div><div class="line">    // ignore_label.  If ignore_label is not set, this behaves like FULL.</div><div class="line">    // 所有样本都参与计算，不包括ignore label</div><div class="line">    VALID = 1;</div><div class="line">    // Divide by the batch size.</div><div class="line">    // 除以给定的batch size。</div><div class="line">    BATCH_SIZE = 2;</div><div class="line">    // Do not normalize the loss.</div><div class="line">    // 不归一化loss</div><div class="line">    NONE = 3;</div><div class="line">  &#125;</div><div class="line">  // For historical reasons, the default normalization for</div><div class="line">  // SigmoidCrossEntropyLoss is BATCH_SIZE and *not* VALID.</div><div class="line">  // loss归一化方式</div><div class="line">  optional NormalizationMode normalization = 3 [default = VALID];</div><div class="line">  // Deprecated.  Ignored if normalization is specified.  If normalization</div><div class="line">  // is not specified, then setting this to false will be equivalent to</div><div class="line">  // normalization = BATCH_SIZE to be consistent with previous behavior.</div><div class="line">  // 已废弃。Loss会除以参与计算的样本总数；否则Loss等于直接求和</div><div class="line">  optional bool normalize = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Messages that store parameters used by individual layer types follow, in</div><div class="line">// alphabetical order.</div><div class="line">// accuracy层参数</div><div class="line">message AccuracyParameter &#123;</div><div class="line">  // When computing accuracy, count as correct by comparing the true label to</div><div class="line">  // the top k scoring classes.  By default, only compare to the top scoring</div><div class="line">  // class (i.e. argmax).</div><div class="line">  // 计算前top-k的准确率，默认计算top-1准确率</div><div class="line">  optional uint32 top_k = 1 [default = 1];</div><div class="line"></div><div class="line">  // The &quot;label&quot; axis of the prediction blob, whose argmax corresponds to the</div><div class="line">  // predicted label -- may be negative to index from the end (e.g., -1 for the</div><div class="line">  // last axis).  For example, if axis == 1 and the predictions are</div><div class="line">  // (N x C x H x W), the label blob is expected to contain N*H*W ground truth</div><div class="line">  // labels with integer values in &#123;0, 1, ..., C-1&#125;.</div><div class="line">  // 指定在哪个维度上计算label</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line"></div><div class="line">  // If specified, ignore instances with the given label.</div><div class="line">  // 如果指定，则忽略给定标签的实例</div><div class="line">  optional int32 ignore_label = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// 标签最大化参数，标签最大化即确定概率最大的label</div><div class="line">message ArgMaxParameter &#123;</div><div class="line">  // If true produce pairs (argmax, maxval)</div><div class="line">  // 如果为真，则生成(argmax, maxval)</div><div class="line">  optional bool out_max_val = 1 [default = false];</div><div class="line">  // 类别的top-k</div><div class="line">  optional uint32 top_k = 2 [default = 1];</div><div class="line">  // The axis along which to maximise -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).</div><div class="line">  // By default ArgMaxLayer maximizes over the flattened trailing dimensions</div><div class="line">  // for each index of the first / num dimension.</div><div class="line">  // 根据axis进行标签最大化</div><div class="line">  optional int32 axis = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 参数拼接，在deconv的prototxt文件中见过</div><div class="line">message ConcatParameter &#123;</div><div class="line">  // The axis along which to concatenate -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).  Other axes must have the</div><div class="line">  // same dimension for all the bottom blobs.</div><div class="line">  // By default, ConcatLayer concatenates blobs along the &quot;channels&quot; axis (1).</div><div class="line">  // 参数拼接时的维度，按axis进行拼接</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line"></div><div class="line">  // DEPRECATED: alias for &quot;axis&quot; -- does not support negative indexing.</div><div class="line">  // 已废弃。与axis一样。</div><div class="line">  optional uint32 concat_dim = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// batch norm层的相关参数, batch norm layer通常配与scale layer一起使用，具体用法可参考Resnet结构</div><div class="line">message BatchNormParameter &#123;</div><div class="line">  // If false, accumulate global mean/variance values via a moving average. If</div><div class="line">  // true, use those accumulated values instead of computing mean/variance</div><div class="line">  // across the batch.</div><div class="line">  // 如果设为false，累计全部的mean/variance，如果为true，使用累计值代替batch上mean/variance的计算</div><div class="line">  // true是使用了caffe内部的均值和方差，false是使用了每个Batch里的数据的均值和方差</div><div class="line">  optional bool use_global_stats = 1;</div><div class="line">  // How much does the moving average decay each iteration?</div><div class="line">  // 每次迭代平均值衰减比例</div><div class="line">  optional float moving_average_fraction = 2 [default = .999];</div><div class="line">  // Small value to add to the variance estimate so that we don&apos;t divide by</div><div class="line">  // zero.</div><div class="line">  // variance估计时为了使除数不为0，需要加上eps</div><div class="line">  optional float eps = 3 [default = 1e-5];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// bias层参数，没找到实际的应用例子</div><div class="line">message BiasParameter &#123;</div><div class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</div><div class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  //</div><div class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</div><div class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</div><div class="line">  // following shapes (for the given value of axis):</div><div class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</div><div class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</div><div class="line">  //    (axis == 2 == -2)                   40;       40x60</div><div class="line">  //    (axis == 3 == -1)                                60</div><div class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</div><div class="line">  // &quot;axis&quot;) -- a scalar bias.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // (num_axes is ignored unless just one bottom is given and the bias is</div><div class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</div><div class="line">  // number of axes by the second bottom.)</div><div class="line">  // The number of axes of the input (bottom[0]) covered by the bias</div><div class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</div><div class="line">  // Set num_axes := 0, to add a zero-axis Blob: a scalar.</div><div class="line">  optional int32 num_axes = 2 [default = 1];</div><div class="line"></div><div class="line">  // (filler is ignored unless just one bottom is given and the bias is</div><div class="line">  // a learned parameter of the layer.)</div><div class="line">  // The initialization for the learned bias parameter.</div><div class="line">  // Default is the zero (0) initialization, resulting in the BiasLayer</div><div class="line">  // initially performing the identity operation.</div><div class="line">  optional FillerParameter filler = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 对比损失层，siamese network中使用了对比损失</div><div class="line">message ContrastiveLossParameter &#123;</div><div class="line">  // margin for dissimilar pair</div><div class="line">  // 不相似的样本对的距离保持在margin以上</div><div class="line">  optional float margin = 1 [default = 1.0];</div><div class="line">  // The first implementation of this cost did not exactly match the cost of</div><div class="line">  // Hadsell et al 2006 -- using (margin - d^2) instead of (margin - d)^2.</div><div class="line">  // legacy_version = false (the default) uses (margin - d)^2 as proposed in the</div><div class="line">  // Hadsell paper. New models should probably use this version.</div><div class="line">  // legacy_version = true uses (margin - d^2). This is kept to support /</div><div class="line">  // reproduce existing models and results</div><div class="line">  // 第一版对比损失没有完全按论文写，如果为false，则按照论文原来的公式计算</div><div class="line">  optional bool legacy_version = 2 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 卷积层参数</div><div class="line">message ConvolutionParameter &#123;</div><div class="line">  // 输出数据的个数</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // 是否有偏置项</div><div class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</div><div class="line"></div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in all spatial dimensions, or once per spatial dimension.</div><div class="line">  // 卷积padding的大小</div><div class="line">  repeated uint32 pad = 3; // The padding size; defaults to 0</div><div class="line">  // 卷积核的大小</div><div class="line">  repeated uint32 kernel_size = 4; // The kernel size</div><div class="line">  // 卷积的步长</div><div class="line">  repeated uint32 stride = 6; // The stride; defaults to 1</div><div class="line">  // Factor used to dilate the kernel, (implicitly) zero-filling the resulting</div><div class="line">  // holes. (Kernel dilation is sometimes referred to by its use in the</div><div class="line">  // algorithme à trous from Holschneider et al. 1987.)</div><div class="line">  // 卷积膨胀，在卷积的时候可以skip一定长度的像素</div><div class="line">  repeated uint32 dilation = 18; // The dilation; defaults to 1</div><div class="line"></div><div class="line">  // For 2D convolution only, the *_h and *_w versions may also be used to</div><div class="line">  // specify both spatial dimensions.</div><div class="line">  // padding, kernel, stride的宽度和高度</div><div class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height (2D only)</div><div class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width (2D only)</div><div class="line">  optional uint32 kernel_h = 11; // The kernel height (2D only)</div><div class="line">  optional uint32 kernel_w = 12; // The kernel width (2D only)</div><div class="line">  optional uint32 stride_h = 13; // The stride height (2D only)</div><div class="line">  optional uint32 stride_w = 14; // The stride width (2D only)</div><div class="line"></div><div class="line">  // 来自于AlexNet论文</div><div class="line">  optional uint32 group = 5 [default = 1]; // The group size for group conv</div><div class="line"></div><div class="line">  // 权重初始化</div><div class="line">  optional FillerParameter weight_filler = 7; // The filler for the weight</div><div class="line">  // 偏置初始化</div><div class="line">  optional FillerParameter bias_filler = 8; // The filler for the bias</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 卷积的方式的选择，default是正常的卷积，caffe是矩阵乘法的卷积，cudnn是cuda库流并行式的卷积</div><div class="line">  optional Engine engine = 15 [default = DEFAULT];</div><div class="line"></div><div class="line">  // The axis to interpret as &quot;channels&quot; when performing convolution.</div><div class="line">  // Preceding dimensions are treated as independent inputs;</div><div class="line">  // succeeding dimensions are treated as &quot;spatial&quot;.</div><div class="line">  // With (N, C, H, W) inputs, and axis == 1 (the default), we perform</div><div class="line">  // N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</div><div class="line">  // groups g&gt;1) filters across the spatial axes (H, W) of the input.</div><div class="line">  // With (N, C, D, H, W) inputs, and axis == 1, we perform</div><div class="line">  // N independent 3D convolutions, sliding (C/g)-channels</div><div class="line">  // filters across the spatial axes (D, H, W) of the input.</div><div class="line">  // 通道channel所在的维度</div><div class="line">  optional int32 axis = 16 [default = 1];</div><div class="line"></div><div class="line">  // Whether to force use of the general ND convolution, even if a specific</div><div class="line">  // implementation for blobs of the appropriate number of spatial dimensions</div><div class="line">  // is available. (Currently, there is only a 2D-specific convolution</div><div class="line">  // implementation; for input blobs with num_axes != 2, this option is</div><div class="line">  // ignored and the ND implementation will be used.)</div><div class="line">  // 如果输入数据维度等于2，则执行通用的ND卷积，否则正常执行卷积</div><div class="line">  optional bool force_nd_im2col = 17 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 图像裁剪参数</div><div class="line">message CropParameter &#123;</div><div class="line">  // To crop, elements of the first bottom are selected to fit the dimensions</div><div class="line">  // of the second, reference bottom. The crop is configured by</div><div class="line">  // - the crop `axis` to pick the dimensions for cropping</div><div class="line">  // - the crop `offset` to set the shift for all/each dimension</div><div class="line">  // to align the cropped bottom with the reference bottom.</div><div class="line">  // All dimensions up to but excluding `axis` are preserved, while</div><div class="line">  // the dimensions including and trailing `axis` are cropped.</div><div class="line">  // If only one `offset` is set, then all dimensions are offset by this amount.</div><div class="line">  // Otherwise, the number of offsets must equal the number of cropped axes to</div><div class="line">  // shift the crop in each dimension accordingly.</div><div class="line">  // Note: standard dimensions are N,C,H,W so the default is a spatial crop,</div><div class="line">  // and `axis` may be negative to index from the end (e.g., -1 for the last</div><div class="line">  // axis).</div><div class="line">  // axis是在哪个维度上进行裁剪，会裁剪轴2及之后的所有轴</div><div class="line">  optional int32 axis = 1 [default = 2];</div><div class="line">  // offset设置是每个维度进行裁剪时的偏移量</div><div class="line">  repeated uint32 offset = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 数据层参数</div><div class="line">message DataParameter &#123;</div><div class="line">  enum DB &#123;</div><div class="line">    LEVELDB = 0;</div><div class="line">    LMDB = 1;</div><div class="line">  &#125;</div><div class="line">  // Specify the data source.</div><div class="line">  // 设定数据源路径</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 指定一次处理的图片数量</div><div class="line">  optional uint32 batch_size = 4;</div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  // DEPRECATED. Each solver accesses a different subset of the database.</div><div class="line">  // rand_skip跳过指定的数据点，避免异步的sgd从同一个数据点开始</div><div class="line">  optional uint32 rand_skip = 7 [default = 0];</div><div class="line">  // 使用的数据库类型，LMDB or LEVELDB</div><div class="line">  optional DB backend = 8 [default = LEVELDB];</div><div class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</div><div class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</div><div class="line">  // mean subtraction is always carried out before scaling.</div><div class="line">  // 已废弃。图像归一化，在TransformationParameter中。</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 已废弃。均值文件，在TransformationParameter中。</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</div><div class="line">  // crop an image.</div><div class="line">  // 已废弃。图像裁剪，在TransformationParameter中。</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</div><div class="line">  // data.</div><div class="line">  // 已废弃。图像翻转，在TransformationParameter中。</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // Force the encoded image to have 3 color channels</div><div class="line">  // 强制图像数据有三个颜色通道</div><div class="line">  optional bool force_encoded_color = 9 [default = false];</div><div class="line">  // Prefetch queue (Number of batches to prefetch to host memory, increase if</div><div class="line">  // data access bandwidth varies).</div><div class="line">  // 预先拉取batch的数目</div><div class="line">  optional uint32 prefetch = 10 [default = 4];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// dropout层参数</div><div class="line">message DropoutParameter &#123;</div><div class="line">  // 为了避免过拟合，参数随机失活的比例</div><div class="line">  optional float dropout_ratio = 1 [default = 0.5]; // dropout ratio</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// DummyDataLayer fills any number of arbitrarily shaped blobs with random</div><div class="line">// (or constant) data generated by &quot;Fillers&quot; (see &quot;message FillerParameter&quot;).</div><div class="line">// DummyData层的参数</div><div class="line">message DummyDataParameter &#123;</div><div class="line">  // This layer produces N &gt;= 1 top blobs.  DummyDataParameter must specify 1 or N</div><div class="line">  // shape fields, and 0, 1 or N data_fillers.</div><div class="line">  // If 0 data_fillers are specified, ConstantFiller with a value of 0 is used.</div><div class="line">  // If 1 data_filler is specified, it is applied to all top blobs.  If N are</div><div class="line">  // specified, the ith is applied to the ith top blob.</div><div class="line">  // blob数据的生成方式</div><div class="line">  repeated FillerParameter data_filler = 1;</div><div class="line">  // 数据的维度</div><div class="line">  repeated BlobShape shape = 6;</div><div class="line"></div><div class="line">  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.</div><div class="line">  // 已废弃。使用shape代替。</div><div class="line">  repeated uint32 num = 2;</div><div class="line">  repeated uint32 channels = 3;</div><div class="line">  repeated uint32 height = 4;</div><div class="line">  repeated uint32 width = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">//Eltwise层的参数</div><div class="line">message EltwiseParameter &#123;</div><div class="line">  // 操作的类型</div><div class="line">  enum EltwiseOp &#123;</div><div class="line">    PROD = 0;</div><div class="line">    SUM = 1;</div><div class="line">    MAX = 2;</div><div class="line">  &#125;</div><div class="line">  // 数据操作分三种：点乘，相加，取最大值</div><div class="line">  optional EltwiseOp operation = 1 [default = SUM]; // element-wise operation</div><div class="line">  // SUM操作时各个blob对应的系数</div><div class="line">  repeated float coeff = 2; // blob-wise coefficient for SUM operation</div><div class="line"></div><div class="line">  // Whether to use an asymptotically slower (for &gt;2 inputs) but stabler method</div><div class="line">  // of computing the gradient for the PROD operation. (No effect for SUM op.)</div><div class="line">  // 在进行PROD操作，即乘法时是否使用异步操作来计算梯度，更慢但更稳定。</div><div class="line">  optional bool stable_prod_grad = 3 [default = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ELULayer</div><div class="line">// ELU层的参数，具体看论文</div><div class="line">message ELUParameter &#123;</div><div class="line">  // Described in:</div><div class="line">  // Clevert, D.-A., Unterthiner, T., &amp; Hochreiter, S. (2015). Fast and Accurate</div><div class="line">  // Deep Network Learning by Exponential Linear Units (ELUs). arXiv</div><div class="line">  optional float alpha = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by EmbedLayer</div><div class="line">// Embed层的参数，主要用于LSTM等翻译网络</div><div class="line">message EmbedParameter &#123;</div><div class="line">  // Embed层的输出</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // The input is given as integers to be interpreted as one-hot</div><div class="line">  // vector indices with dimension num_input.  Hence num_input should be</div><div class="line">  // 1 greater than the maximum possible input value.</div><div class="line">  // Embed层的输入</div><div class="line">  optional uint32 input_dim = 2;</div><div class="line">  // 是否使用偏置项</div><div class="line">  optional bool bias_term = 3 [default = true]; // Whether to use a bias term</div><div class="line">  // 权重生成</div><div class="line">  optional FillerParameter weight_filler = 4; // The filler for the weight</div><div class="line">  // 偏置生成</div><div class="line">  optional FillerParameter bias_filler = 5; // The filler for the bias</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ExpLayer</div><div class="line">// Exp层的参数，即指数层参数</div><div class="line">message ExpParameter &#123;</div><div class="line">  // ExpLayer computes outputs y = base ^ (shift + scale * x), for base &gt; 0.</div><div class="line">  // Or if base is set to the default (-1), base is set to e,</div><div class="line">  // so y = exp(shift + scale * x).</div><div class="line">  // 指数层的计算是y = base ^ (shift + scale * x)，下面分别是公式中的三个参数</div><div class="line">  optional float base = 1 [default = -1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Message that stores parameters used by FlattenLayer</div><div class="line">// Flatten层的参数，主要是按某个轴展开（平铺），mnist demo的mnist_autoencode就使用了Flatten层</div><div class="line">message FlattenParameter &#123;</div><div class="line">  // The first axis to flatten: all preceding axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</div><div class="line">  // 从哪一层开始展开</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // The last axis to flatten: all following axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., the default -1 for the last</div><div class="line">  // axis).</div><div class="line">  // 展开到哪一层结束</div><div class="line">  optional int32 end_axis = 2 [default = -1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by HDF5DataLayer</div><div class="line">// HDF5数据层的参数</div><div class="line">message HDF5DataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // HDF5层输入数据的数据源</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 2;</div><div class="line"></div><div class="line">  // Specify whether to shuffle the data.</div><div class="line">  // If shuffle == true, the ordering of the HDF5 files is shuffled,</div><div class="line">  // and the ordering of data within any given HDF5 file is shuffled,</div><div class="line">  // but data between different files are not interleaved; all of a file&apos;s</div><div class="line">  // data are output (in a random order) before moving onto another file.</div><div class="line">  // 是否对HDF5的输入数据进行shuffle</div><div class="line">  optional bool shuffle = 3 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// HDF5输出层参数</div><div class="line">message HDF5OutputParameter &#123;</div><div class="line">  // 输出的HDF5文件的文件名</div><div class="line">  optional string file_name = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// HingeLoss层参数</div><div class="line">message HingeLossParameter &#123;</div><div class="line">  enum Norm &#123;</div><div class="line">    L1 = 1;</div><div class="line">    L2 = 2;</div><div class="line">  &#125;</div><div class="line">  // Specify the Norm to use L1 or L2</div><div class="line">  // 指定HingeLoss的类型</div><div class="line">  optional Norm norm = 1 [default = L1];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// ImageData层参数，网络中直接输入原图</div><div class="line">message ImageDataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // 描述图像路径及标签的文件</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch size</div><div class="line">  optional uint32 batch_size = 4 [default = 1];</div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  // rand_skip跳过指定的数据点，避免异步的sgd从同一个数据点开始，与Data层中是一样的</div><div class="line">  optional uint32 rand_skip = 7 [default = 0];</div><div class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</div><div class="line">  // 是否对图像顺序进行shuffle</div><div class="line">  optional bool shuffle = 8 [default = false];</div><div class="line">  // It will also resize images if new_height or new_width are not zero.</div><div class="line">  // 图像resize的高度</div><div class="line">  optional uint32 new_height = 9 [default = 0];</div><div class="line">  // 图像resize的宽度</div><div class="line">  optional uint32 new_width = 10 [default = 0];</div><div class="line">  // Specify if the images are color or gray</div><div class="line">  // 指定图像彩色图像还是灰度图像，默认彩色</div><div class="line">  optional bool is_color = 11 [default = true];</div><div class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</div><div class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</div><div class="line">  // mean subtraction is always carried out before scaling.</div><div class="line">  // 已废弃。参考TransformationParameter中的scale</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 指定均值文件</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</div><div class="line">  // crop an image.</div><div class="line">  // 已废弃。参考TransformationParameter中的crop_size</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</div><div class="line">  // data.</div><div class="line">  // 已废弃，参考TransformationParameter的mirror。</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // 不太清楚root_folder具体是什么</div><div class="line">  optional string root_folder = 12 [default = &quot;&quot;];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 信息增益损失层参数</div><div class="line">message InfogainLossParameter &#123;</div><div class="line">  // Specify the infogain matrix source.</div><div class="line">  // 指定存储信息增益矩阵的源文件</div><div class="line">  optional string source = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// InnerProduct层的参数</div><div class="line">message InnerProductParameter &#123;</div><div class="line">  // InnerProduct层的输出</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // 是否有偏置项</div><div class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</div><div class="line">  // 权重初始化，随机生成</div><div class="line">  optional FillerParameter weight_filler = 3; // The filler for the weight</div><div class="line">  // 偏置初始化，随机生成</div><div class="line">  optional FillerParameter bias_filler = 4; // The filler for the bias</div><div class="line"></div><div class="line">  // The first axis to be lumped into a single inner product computation;</div><div class="line">  // all preceding axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</div><div class="line">  // 从某一维度开始进行内积计算，前面的维度保留</div><div class="line">  optional int32 axis = 5 [default = 1];</div><div class="line">  // Specify whether to transpose the weight matrix or not.</div><div class="line">  // If transpose == true, any operations will be performed on the transpose</div><div class="line">  // of the weight matrix. The weight matrix itself is not going to be transposed</div><div class="line">  // but rather the transfer flag of operations will be toggled accordingly.</div><div class="line">  // 是否对权重矩阵进行转置</div><div class="line">  optional bool transpose = 6 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Input参数，caffe网络部署时会用到</div><div class="line">message InputParameter &#123;</div><div class="line">  // This layer produces N &gt;= 1 top blob(s) to be assigned manually.</div><div class="line">  // Define N shapes to set a shape for each top.</div><div class="line">  // Define 1 shape to set the same shape for every top.</div><div class="line">  // Define no shape to defer to reshaping manually.</div><div class="line">  // 输入数据的shape</div><div class="line">  repeated BlobShape shape = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by LogLayer</div><div class="line">// Log层参数，对数据进行Log运算</div><div class="line">message LogParameter &#123;</div><div class="line">  // LogLayer computes outputs y = log_base(shift + scale * x), for base &gt; 0.</div><div class="line">  // Or if base is set to the default (-1), base is set to e,</div><div class="line">  // so y = ln(shift + scale * x) = log_e(shift + scale * x)</div><div class="line">  // Log层计算公式为y = log_base(shift + scale * x)，下面分别是公式中的三个参数</div><div class="line">  optional float base = 1 [default = -1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by LRNLayer</div><div class="line">// LRN层的参数，局部归一化，AlexNet中的LRN</div><div class="line">message LRNParameter &#123;</div><div class="line">  // 如果是跨通道LRN，则表示求和的通道数；如果是在通道内LRN，则表示求和的正方形区域长度。</div><div class="line">  optional uint32 local_size = 1 [default = 5];</div><div class="line">  // 归一化公式中的参数</div><div class="line">  optional float alpha = 2 [default = 1.];</div><div class="line">  optional float beta = 3 [default = 0.75];</div><div class="line">  enum NormRegion &#123;</div><div class="line">    ACROSS_CHANNELS = 0;</div><div class="line">    WITHIN_CHANNEL = 1;</div><div class="line">  &#125;</div><div class="line">  // 归一化的区域，分为通道内和跨通道两种</div><div class="line">  optional NormRegion norm_region = 4 [default = ACROSS_CHANNELS];</div><div class="line">  optional float k = 5 [default = 1.];</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 与前面的engine是一样的</div><div class="line">  optional Engine engine = 6 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 内存数据层参数</div><div class="line">message MemoryDataParameter &#123;</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 1;</div><div class="line">  // 图像通道数</div><div class="line">  optional uint32 channels = 2;</div><div class="line">  // 图像高度</div><div class="line">  optional uint32 height = 3;</div><div class="line">  // 图像宽度</div><div class="line">  optional uint32 width = 4;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// mean-variance normalization层参数</div><div class="line">message MVNParameter &#123;</div><div class="line">  // This parameter can be set to false to normalize mean only</div><div class="line">  // 是否对方差进行归一化</div><div class="line">  optional bool normalize_variance = 1 [default = true];</div><div class="line"></div><div class="line">  // This parameter can be set to true to perform DNN-like MVN</div><div class="line">  // 是否进行跨通道的MVN</div><div class="line">  optional bool across_channels = 2 [default = false];</div><div class="line"></div><div class="line">  // Epsilon for not dividing by zero while normalizing variance</div><div class="line">  // 避免除数为0，与前面的一样</div><div class="line">  optional float eps = 3 [default = 1e-9];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 参数层参数</div><div class="line">message ParameterParameter &#123;</div><div class="line">  // 用户自己定义的shape</div><div class="line">  optional BlobShape shape = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 池化层参数</div><div class="line">message PoolingParameter &#123;</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  // 池化的方式</div><div class="line">  optional PoolMethod pool = 1 [default = MAX]; // The pooling method</div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in height and width or as Y, X pairs.</div><div class="line">  // padding的大小</div><div class="line">  optional uint32 pad = 4 [default = 0]; // The padding size (equal in Y, X)</div><div class="line">  // padding的高度</div><div class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height</div><div class="line">  // padding的宽度</div><div class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width</div><div class="line">  // 池化的核大小</div><div class="line">  optional uint32 kernel_size = 2; // The kernel size (square)</div><div class="line">  // 核高度</div><div class="line">  optional uint32 kernel_h = 5; // The kernel height</div><div class="line">  // 核宽度</div><div class="line">  optional uint32 kernel_w = 6; // The kernel width</div><div class="line">  // 池化的步长</div><div class="line">  optional uint32 stride = 3 [default = 1]; // The stride (equal in Y, X)</div><div class="line">  // 步长的高度</div><div class="line">  optional uint32 stride_h = 7; // The stride height</div><div class="line">  // 步长的宽度</div><div class="line">  optional uint32 stride_w = 8; // The stride width</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行池化操作的类型，与前面的一样</div><div class="line">  optional Engine engine = 11 [default = DEFAULT];</div><div class="line">  // If global_pooling then it will pool over the size of the bottom by doing</div><div class="line">  // kernel_h = bottom-&gt;height and kernel_w = bottom-&gt;width</div><div class="line">  // global_pooling是对多个通道进行pooling，例如从三通道pooling为单通道</div><div class="line">  optional bool global_pooling = 12 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Power层参数</div><div class="line">message PowerParameter &#123;</div><div class="line">  // PowerLayer computes outputs y = (shift + scale * x) ^ power.</div><div class="line">  // Power的计算公式为y = (shift + scale * x) ^ power，下面是公式中的参数</div><div class="line">  optional float power = 1 [default = 1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// python layer参数，在faster rcnn中有应用</div><div class="line">message PythonParameter &#123;</div><div class="line">  // python模块名称</div><div class="line">  optional string module = 1;</div><div class="line">  // python模块中层的名字，即类名</div><div class="line">  optional string layer = 2;</div><div class="line">  // This value is set to the attribute `param_str` of the `PythonLayer` object</div><div class="line">  // in Python before calling the `setup()` method. This could be a number,</div><div class="line">  // string, dictionary in Python dict format, JSON, etc. You may parse this</div><div class="line">  // string in `setup` method and use it in `forward` and `backward`.</div><div class="line">  // 可以用来设置参数，key-value形式，可以参考faster rcnn中模型的train.prototxt</div><div class="line">  optional string param_str = 3 [default = &apos;&apos;];</div><div class="line">  // Whether this PythonLayer is shared among worker solvers during data parallelism.</div><div class="line">  // If true, each worker solver sequentially run forward from this layer.</div><div class="line">  // This value should be set true if you are using it as a data layer.</div><div class="line">  // 是否需要在并行时共享layer</div><div class="line">  optional bool share_in_parallel = 4 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Message that stores parameters used by RecurrentLayer</div><div class="line">// Recurrent层参数</div><div class="line">message RecurrentParameter &#123;</div><div class="line">  // The dimension of the output (and usually hidden state) representation --</div><div class="line">  // must be explicitly set to non-zero.</div><div class="line">  // Recurrent层的输出——必须非零</div><div class="line">  optional uint32 num_output = 1 [default = 0];</div><div class="line">  // 权重初始化，随机生成初始化</div><div class="line">  optional FillerParameter weight_filler = 2; // The filler for the weight</div><div class="line">  // 偏置初始化，随机生成</div><div class="line">  optional FillerParameter bias_filler = 3; // The filler for the bias</div><div class="line"></div><div class="line">  // Whether to enable displaying debug_info in the unrolled recurrent net.</div><div class="line">  // 是否输出调试信息</div><div class="line">  optional bool debug_info = 4 [default = false];</div><div class="line"></div><div class="line">  // Whether to add as additional inputs (bottoms) the initial hidden state</div><div class="line">  // blobs, and add as additional outputs (tops) the final timestep hidden state</div><div class="line">  // blobs.  The number of additional bottom/top blobs required depends on the</div><div class="line">  // recurrent architecture -- e.g., 1 for RNNs, 2 for LSTMs.</div><div class="line">  // 是否添加额外的输入</div><div class="line">  optional bool expose_hidden = 5 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ReductionLayer</div><div class="line">// Reduction层参数</div><div class="line">message ReductionParameter &#123;</div><div class="line">  enum ReductionOp &#123;</div><div class="line">    SUM = 1;</div><div class="line">    ASUM = 2;</div><div class="line">    SUMSQ = 3;</div><div class="line">    MEAN = 4;</div><div class="line">  &#125;</div><div class="line">  // 通过reduction操作来将数据减少到一维，可以通过上面的四种方式</div><div class="line">  optional ReductionOp operation = 1 [default = SUM]; // reduction operation</div><div class="line"></div><div class="line">  // The first axis to reduce to a scalar -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).</div><div class="line">  // (Currently, only reduction along ALL &quot;tail&quot; axes is supported; reduction</div><div class="line">  // of axis M through N, where N &lt; num_axes - 1, is unsupported.)</div><div class="line">  // Suppose we have an n-axis bottom Blob with shape:</div><div class="line">  //     (d0, d1, d2, ..., d(m-1), dm, d(m+1), ..., d(n-1)).</div><div class="line">  // If axis == m, the output Blob will have shape</div><div class="line">  //     (d0, d1, d2, ..., d(m-1)),</div><div class="line">  // and the ReductionOp operation is performed (d0 * d1 * d2 * ... * d(m-1))</div><div class="line">  // times, each including (dm * d(m+1) * ... * d(n-1)) individual data.</div><div class="line">  // If axis == 0 (the default), the output Blob always has the empty shape</div><div class="line">  // (count 1), performing reduction across the entire input --</div><div class="line">  // often useful for creating new loss functions.</div><div class="line">  // 在哪个轴上执行reduction操作</div><div class="line">  optional int32 axis = 2 [default = 0];</div><div class="line">  // 输出系数</div><div class="line">  optional float coeff = 3 [default = 1.0]; // coefficient for output</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ReLULayer</div><div class="line">// ReLU层参数</div><div class="line">message ReLUParameter &#123;</div><div class="line">  // Allow non-zero slope for negative inputs to speed up optimization</div><div class="line">  // Described in:</div><div class="line">  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities</div><div class="line">  // improve neural network acoustic models. In ICML Workshop on Deep Learning</div><div class="line">  // for Audio, Speech, and Language Processing.</div><div class="line">  // ReLUU操作的阈值</div><div class="line">  optional float negative_slope = 1 [default = 0];</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行ReLU操作的类型，与前面的一样</div><div class="line">  optional Engine engine = 2 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Reshape层参数，与numpy中的Reshape作用是一样的</div><div class="line">message ReshapeParameter &#123;</div><div class="line">  // Specify the output dimensions. If some of the dimensions are set to 0,</div><div class="line">  // the corresponding dimension from the bottom layer is used (unchanged).</div><div class="line">  // Exactly one dimension may be set to -1, in which case its value is</div><div class="line">  // inferred from the count of the bottom blob and the remaining dimensions.</div><div class="line">  // For example, suppose we want to reshape a 2D blob &quot;input&quot; with shape 2 x 8:</div><div class="line">  //</div><div class="line">  //   layer &#123;</div><div class="line">  //     type: &quot;Reshape&quot; bottom: &quot;input&quot; top: &quot;output&quot;</div><div class="line">  //     reshape_param &#123; ... &#125;</div><div class="line">  //   &#125;</div><div class="line">  //</div><div class="line">  // If &quot;input&quot; is 2D with shape 2 x 8, then the following reshape_param</div><div class="line">  // specifications are all equivalent, producing a 3D blob &quot;output&quot; with shape</div><div class="line">  // 2 x 2 x 4:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  2  dim: 2  dim:  4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim:  4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim: -1 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim:-1  dim:  4 &#125; &#125;</div><div class="line">  // reshape之后输出的维度</div><div class="line">  optional BlobShape shape = 1;</div><div class="line"></div><div class="line">  // axis and num_axes control the portion of the bottom blob&apos;s shape that are</div><div class="line">  // replaced by (included in) the reshape. By default (axis == 0 and</div><div class="line">  // num_axes == -1), the entire bottom blob shape is included in the reshape,</div><div class="line">  // and hence the shape field must specify the entire output shape.</div><div class="line">  //</div><div class="line">  // axis may be non-zero to retain some portion of the beginning of the input</div><div class="line">  // shape (and may be negative to index from the end; e.g., -1 to begin the</div><div class="line">  // reshape after the last axis, including nothing in the reshape,</div><div class="line">  // -2 to include only the last axis, etc.).</div><div class="line">  //</div><div class="line">  // For example, suppose &quot;input&quot; is a 2D blob with shape 2 x 8.</div><div class="line">  // Then the following ReshapeLayer specifications are all equivalent,</div><div class="line">  // producing a blob &quot;output&quot; with shape 2 x 2 x 4:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 2  dim: 4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis:  1 &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis: -3 &#125;</div><div class="line">  //</div><div class="line">  // num_axes specifies the extent of the reshape.</div><div class="line">  // If num_axes &gt;= 0 (and axis &gt;= 0), the reshape will be performed only on</div><div class="line">  // input axes in the range [axis, axis+num_axes].</div><div class="line">  // num_axes may also be -1, the default, to include all remaining axes</div><div class="line">  // (starting from axis).</div><div class="line">  //</div><div class="line">  // For example, suppose &quot;input&quot; is a 2D blob with shape 2 x 8.</div><div class="line">  // Then the following ReshapeLayer specifications are equivalent,</div><div class="line">  // producing a blob &quot;output&quot; with shape 1 x 2 x 8.</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  dim:  8 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  &#125;  num_axes: 1 &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  &#125;  num_axes: 0 &#125;</div><div class="line">  //</div><div class="line">  // On the other hand, these would produce output blob shape 2 x 1 x 8:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 1  dim: 8  &#125;  &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 1 &#125;  axis: 1  num_axes: 0 &#125;</div><div class="line"></div><div class="line">  optional int32 axis = 2 [default = 0];</div><div class="line">  optional int32 num_axes = 3 [default = -1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Scale层参数，与batch norm layer配合使用，可参考Resnet结构</div><div class="line">message ScaleParameter &#123;</div><div class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</div><div class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  //</div><div class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</div><div class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</div><div class="line">  // following shapes (for the given value of axis):</div><div class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</div><div class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</div><div class="line">  //    (axis == 2 == -2)                   40;       40x60</div><div class="line">  //    (axis == 3 == -1)                                60</div><div class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</div><div class="line">  // &quot;axis&quot;) -- a scalar multiplier.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // (num_axes is ignored unless just one bottom is given and the scale is</div><div class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</div><div class="line">  // number of axes by the second bottom.)</div><div class="line">  // The number of axes of the input (bottom[0]) covered by the scale</div><div class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</div><div class="line">  // Set num_axes := 0, to multiply with a zero-axis Blob: a scalar.</div><div class="line">  optional int32 num_axes = 2 [default = 1];</div><div class="line"></div><div class="line">  // (filler is ignored unless just one bottom is given and the scale is</div><div class="line">  // a learned parameter of the layer.)</div><div class="line">  // The initialization for the learned scale parameter.</div><div class="line">  // Default is the unit (1) initialization, resulting in the ScaleLayer</div><div class="line">  // initially performing the identity operation.</div><div class="line">  optional FillerParameter filler = 3;</div><div class="line"></div><div class="line">  // Whether to also learn a bias (equivalent to a ScaleLayer+BiasLayer, but</div><div class="line">  // may be more efficient).  Initialized with bias_filler (defaults to 0).</div><div class="line">  // 是否使用偏置项</div><div class="line">  optional bool bias_term = 4 [default = false];</div><div class="line">  // 偏置项初始化</div><div class="line">  optional FillerParameter bias_filler = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Sigmoid层参数</div><div class="line">message SigmoidParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 使用哪种sigmoid实现</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Slice层参数</div><div class="line">message SliceParameter &#123;</div><div class="line">  // The axis along which to slice -- may be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  // By default, SliceLayer concatenates blobs along the &quot;channels&quot; axis (1).</div><div class="line">  // 在哪个维度上进行拆分</div><div class="line">  optional int32 axis = 3 [default = 1];</div><div class="line">  // 指定拆分点</div><div class="line">  repeated uint32 slice_point = 2;</div><div class="line"></div><div class="line">  // DEPRECATED: alias for &quot;axis&quot; -- does not support negative indexing.</div><div class="line">  // 已废弃。</div><div class="line">  optional uint32 slice_dim = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer</div><div class="line">// Softmax层参数</div><div class="line">message SoftmaxParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 使用哪种softmax实现</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line"></div><div class="line">  // The axis along which to perform the softmax -- may be negative to index</div><div class="line">  // from the end (e.g., -1 for the last axis).</div><div class="line">  // Any other axes will be evaluated as independent softmaxes.</div><div class="line">  // 在哪个维度上进行softmax</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// TanH层参数</div><div class="line">message TanHParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行tanh激活函数的类型</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by TileLayer</div><div class="line">// Tile层参数，扩大某一维度</div><div class="line">message TileParameter &#123;</div><div class="line">  // The index of the axis to tile.</div><div class="line">  // 扩大哪个维度</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // The number of copies (tiles) of the blob to output.</div><div class="line">  // 创建多少个副本</div><div class="line">  optional int32 tiles = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ThresholdLayer</div><div class="line">// Threshold层参数，主要用来测试输入是否超过阈值</div><div class="line">message ThresholdParameter &#123;</div><div class="line">  // 设置阈值</div><div class="line">  optional float threshold = 1 [default = 0]; // Strictly positive values</div><div class="line">&#125;</div><div class="line"></div><div class="line">// WindowData层参数</div><div class="line">message WindowDataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // 指定数据源</div><div class="line">  optional string source = 1;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  // 是否归一化</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 图像均值文件</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 4;</div><div class="line">  // Specify if we would like to randomly crop an image.</div><div class="line">  // 是否随机crop</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // Specify if we want to randomly mirror data.</div><div class="line">  // 是否随机mirror</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // Foreground (object) overlap threshold</div><div class="line">  // 前景重叠阈值</div><div class="line">  optional float fg_threshold = 7 [default = 0.5];</div><div class="line">  // Background (non-object) overlap threshold</div><div class="line">  // 背景重叠阈值</div><div class="line">  optional float bg_threshold = 8 [default = 0.5];</div><div class="line">  // Fraction of batch that should be foreground objects</div><div class="line">  // 前景比例</div><div class="line">  optional float fg_fraction = 9 [default = 0.25];</div><div class="line">  // Amount of contextual padding to add around a window</div><div class="line">  // (used only by the window_data_layer)</div><div class="line">  // 是否padding</div><div class="line">  optional uint32 context_pad = 10 [default = 0];</div><div class="line">  // Mode for cropping out a detection window</div><div class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</div><div class="line">  // square: the tightest square around the window is cropped</div><div class="line">  // crop的方式</div><div class="line">  optional string crop_mode = 11 [default = &quot;warp&quot;];</div><div class="line">  // cache_images: will load all images in memory for faster access</div><div class="line">  // 是否缓存图像，即将图像都转入内存</div><div class="line">  optional bool cache_images = 12 [default = false];</div><div class="line">  // append root_folder to locate images</div><div class="line">  // 图像文件的根目录</div><div class="line">  optional string root_folder = 13 [default = &quot;&quot;];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// SPP层参数，SPP是spatial pyramid pooling，空间金字塔池化，具体可参考何凯明论文Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</div><div class="line">message SPPParameter &#123;</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  // 空间金字塔高度</div><div class="line">  optional uint32 pyramid_height = 1;</div><div class="line">  // 池化方法</div><div class="line">  optional PoolMethod pool = 2 [default = MAX]; // The pooling method</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行SPP的方式</div><div class="line">  optional Engine engine = 6 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DEPRECATED: use LayerParameter.</div><div class="line">// 已废弃，使用LayerParameter。</div><div class="line">message V1LayerParameter &#123;</div><div class="line">  repeated string bottom = 2;</div><div class="line">  repeated string top = 3;</div><div class="line">  optional string name = 4;</div><div class="line">  repeated NetStateRule include = 32;</div><div class="line">  repeated NetStateRule exclude = 33;</div><div class="line">  enum LayerType &#123;</div><div class="line">    NONE = 0;</div><div class="line">    ABSVAL = 35;</div><div class="line">    ACCURACY = 1;</div><div class="line">    ARGMAX = 30;</div><div class="line">    BNLL = 2;</div><div class="line">    CONCAT = 3;</div><div class="line">    CONTRASTIVE_LOSS = 37;</div><div class="line">    CONVOLUTION = 4;</div><div class="line">    DATA = 5;</div><div class="line">    DECONVOLUTION = 39;</div><div class="line">    DROPOUT = 6;</div><div class="line">    DUMMY_DATA = 32;</div><div class="line">    EUCLIDEAN_LOSS = 7;</div><div class="line">    ELTWISE = 25;</div><div class="line">    EXP = 38;</div><div class="line">    FLATTEN = 8;</div><div class="line">    HDF5_DATA = 9;</div><div class="line">    HDF5_OUTPUT = 10;</div><div class="line">    HINGE_LOSS = 28;</div><div class="line">    IM2COL = 11;</div><div class="line">    IMAGE_DATA = 12;</div><div class="line">    INFOGAIN_LOSS = 13;</div><div class="line">    INNER_PRODUCT = 14;</div><div class="line">    LRN = 15;</div><div class="line">    MEMORY_DATA = 29;</div><div class="line">    MULTINOMIAL_LOGISTIC_LOSS = 16;</div><div class="line">    MVN = 34;</div><div class="line">    POOLING = 17;</div><div class="line">    POWER = 26;</div><div class="line">    RELU = 18;</div><div class="line">    SIGMOID = 19;</div><div class="line">    SIGMOID_CROSS_ENTROPY_LOSS = 27;</div><div class="line">    SILENCE = 36;</div><div class="line">    SOFTMAX = 20;</div><div class="line">    SOFTMAX_LOSS = 21;</div><div class="line">    SPLIT = 22;</div><div class="line">    SLICE = 33;</div><div class="line">    TANH = 23;</div><div class="line">    WINDOW_DATA = 24;</div><div class="line">    THRESHOLD = 31;</div><div class="line">  &#125;</div><div class="line">  optional LayerType type = 5;</div><div class="line">  repeated BlobProto blobs = 6;</div><div class="line">  repeated string param = 1001;</div><div class="line">  repeated DimCheckMode blob_share_mode = 1002;</div><div class="line">  enum DimCheckMode &#123;</div><div class="line">    STRICT = 0;</div><div class="line">    PERMISSIVE = 1;</div><div class="line">  &#125;</div><div class="line">  repeated float blobs_lr = 7;</div><div class="line">  repeated float weight_decay = 8;</div><div class="line">  repeated float loss_weight = 35;</div><div class="line">  optional AccuracyParameter accuracy_param = 27;</div><div class="line">  optional ArgMaxParameter argmax_param = 23;</div><div class="line">  optional ConcatParameter concat_param = 9;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = 40;</div><div class="line">  optional ConvolutionParameter convolution_param = 10;</div><div class="line">  optional DataParameter data_param = 11;</div><div class="line">  optional DropoutParameter dropout_param = 12;</div><div class="line">  optional DummyDataParameter dummy_data_param = 26;</div><div class="line">  optional EltwiseParameter eltwise_param = 24;</div><div class="line">  optional ExpParameter exp_param = 41;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = 13;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 14;</div><div class="line">  optional HingeLossParameter hinge_loss_param = 29;</div><div class="line">  optional ImageDataParameter image_data_param = 15;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = 16;</div><div class="line">  optional InnerProductParameter inner_product_param = 17;</div><div class="line">  optional LRNParameter lrn_param = 18;</div><div class="line">  optional MemoryDataParameter memory_data_param = 22;</div><div class="line">  optional MVNParameter mvn_param = 34;</div><div class="line">  optional PoolingParameter pooling_param = 19;</div><div class="line">  optional PowerParameter power_param = 21;</div><div class="line">  optional ReLUParameter relu_param = 30;</div><div class="line">  optional SigmoidParameter sigmoid_param = 38;</div><div class="line">  optional SoftmaxParameter softmax_param = 39;</div><div class="line">  optional SliceParameter slice_param = 31;</div><div class="line">  optional TanHParameter tanh_param = 37;</div><div class="line">  optional ThresholdParameter threshold_param = 25;</div><div class="line">  optional WindowDataParameter window_data_param = 20;</div><div class="line">  optional TransformationParameter transform_param = 36;</div><div class="line">  optional LossParameter loss_param = 42;</div><div class="line">  optional V0LayerParameter layer = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DEPRECATED: V0LayerParameter is the old way of specifying layer parameters</div><div class="line">// in Caffe.  We keep this message type around for legacy support.</div><div class="line">// 已废弃。</div><div class="line">message V0LayerParameter &#123;</div><div class="line">  optional string name = 1; // the layer name</div><div class="line">  optional string type = 2; // the string to specify the layer type</div><div class="line"></div><div class="line">  // Parameters to specify layers with inner products.</div><div class="line">  optional uint32 num_output = 3; // The number of outputs for the layer</div><div class="line">  optional bool biasterm = 4 [default = true]; // whether to have bias terms</div><div class="line">  optional FillerParameter weight_filler = 5; // The filler for the weight</div><div class="line">  optional FillerParameter bias_filler = 6; // The filler for the bias</div><div class="line"></div><div class="line">  optional uint32 pad = 7 [default = 0]; // The padding size</div><div class="line">  optional uint32 kernelsize = 8; // The kernel size</div><div class="line">  optional uint32 group = 9 [default = 1]; // The group size for group conv</div><div class="line">  optional uint32 stride = 10 [default = 1]; // The stride</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  optional PoolMethod pool = 11 [default = MAX]; // The pooling method</div><div class="line">  optional float dropout_ratio = 12 [default = 0.5]; // dropout ratio</div><div class="line"></div><div class="line">  optional uint32 local_size = 13 [default = 5]; // for local response norm</div><div class="line">  optional float alpha = 14 [default = 1.]; // for local response norm</div><div class="line">  optional float beta = 15 [default = 0.75]; // for local response norm</div><div class="line">  optional float k = 22 [default = 1.];</div><div class="line"></div><div class="line">  // For data layers, specify the data source</div><div class="line">  optional string source = 16;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  optional float scale = 17 [default = 1];</div><div class="line">  optional string meanfile = 18;</div><div class="line">  // For data layers, specify the batch size.</div><div class="line">  optional uint32 batchsize = 19;</div><div class="line">  // For data layers, specify if we would like to randomly crop an image.</div><div class="line">  optional uint32 cropsize = 20 [default = 0];</div><div class="line">  // For data layers, specify if we want to randomly mirror data.</div><div class="line">  optional bool mirror = 21 [default = false];</div><div class="line"></div><div class="line">  // The blobs containing the numeric parameters of the layer</div><div class="line">  repeated BlobProto blobs = 50;</div><div class="line">  // The ratio that is multiplied on the global learning rate. If you want to</div><div class="line">  // set the learning ratio for one blob, you need to set it for all blobs.</div><div class="line">  repeated float blobs_lr = 51;</div><div class="line">  // The weight decay that is multiplied on the global weight decay.</div><div class="line">  repeated float weight_decay = 52;</div><div class="line"></div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  optional uint32 rand_skip = 53 [default = 0];</div><div class="line"></div><div class="line">  // Fields related to detection (det_*)</div><div class="line">  // foreground (object) overlap threshold</div><div class="line">  optional float det_fg_threshold = 54 [default = 0.5];</div><div class="line">  // background (non-object) overlap threshold</div><div class="line">  optional float det_bg_threshold = 55 [default = 0.5];</div><div class="line">  // Fraction of batch that should be foreground objects</div><div class="line">  optional float det_fg_fraction = 56 [default = 0.25];</div><div class="line"></div><div class="line">  // optional bool OBSOLETE_can_clobber = 57 [default = true];</div><div class="line"></div><div class="line">  // Amount of contextual padding to add around a window</div><div class="line">  // (used only by the window_data_layer)</div><div class="line">  optional uint32 det_context_pad = 58 [default = 0];</div><div class="line"></div><div class="line">  // Mode for cropping out a detection window</div><div class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</div><div class="line">  // square: the tightest square around the window is cropped</div><div class="line">  optional string det_crop_mode = 59 [default = &quot;warp&quot;];</div><div class="line"></div><div class="line">  // For ReshapeLayer, one needs to specify the new dimensions.</div><div class="line">  optional int32 new_num = 60 [default = 0];</div><div class="line">  optional int32 new_channels = 61 [default = 0];</div><div class="line">  optional int32 new_height = 62 [default = 0];</div><div class="line">  optional int32 new_width = 63 [default = 0];</div><div class="line"></div><div class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</div><div class="line">  // It will also resize images if new_height or new_width are not zero.</div><div class="line">  optional bool shuffle_images = 64 [default = false];</div><div class="line"></div><div class="line">  // For ConcatLayer, one needs to specify the dimension for concatenation, and</div><div class="line">  // the other dimensions must be the same for all the bottom blobs.</div><div class="line">  // By default it will concatenate blobs along the channels dimension.</div><div class="line">  optional uint32 concat_dim = 65 [default = 1];</div><div class="line"></div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 1001;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// PReLU层参数，ReLU的进化版本</div><div class="line">message PReLUParameter &#123;</div><div class="line">  // Parametric ReLU described in K. He et al, Delving Deep into Rectifiers:</div><div class="line">  // Surpassing Human-Level Performance on ImageNet Classification, 2015.</div><div class="line"></div><div class="line">  // Initial value of a_i. Default is a_i=0.25 for all i.</div><div class="line">  // 参数初始化</div><div class="line">  optional FillerParameter filler = 1;</div><div class="line">  // Whether or not slope parameters are shared across channels.</div><div class="line">  // 是否在各通道共享参数</div><div class="line">  optional bool channel_shared = 2 [default = false];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      Caffe源码解析(一)——caffe.proto
    
    </summary>
    
      <category term="Caffe" scheme="noahsnail.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="noahsnail.com/tags/Caffe/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/07/04/2017-7-4-AlexNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    <id>noahsnail.com/2017/07/04/2017-7-4-AlexNet论文翻译/</id>
    <published>2017-07-04T10:04:42.000Z</published>
    <updated>2017-07-27T09:07:14.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="ImageNet-Classification-with-Deep-Convolutional-Neural-Networks"><a href="#ImageNet-Classification-with-Deep-Convolutional-Neural-Networks" class="headerlink" title="ImageNet Classification with Deep Convolutional Neural Networks"></a>ImageNet Classification with Deep Convolutional Neural Networks</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们训练了一个大型深度卷积神经网络来将<code>ImageNet LSVRC-2010</code>竞赛的120万高分辨率的图像分到1000不同的类别中。在测试数据上，我们得到了<code>top-1 37.5%, top-5 17.0%</code>的错误率，这个结果比目前的最好结果好很多。这个神经网络有6000万参数和650000个神经元，包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，最后是一个1000维的softmax。为了训练的更快，我们使用了非饱和神经元并对卷积操作进行了非常有效的GPU实现。为了减少全连接层的过拟合，我们采用了一个最近开发的名为<code>dropout</code>的正则化方法，结果证明是非常有效的。我们也使用这个模型的一个变种参加了<code>ILSVRC-2012</code>竞赛，赢得了冠军并且与第二名 <code>top-5 26.2%</code>的错误率相比，我们取得了<code>top-5 15.3%</code>的错误率。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively small -- on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the current best error rate on the MNIST digit-recognition task (&lt;0.3%) approaches human performance [4]. But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>当前的目标识别方法基本上都使用了机器学习方法。为了提高目标识别的性能，我们可以收集更大的数据集，学习更强大的模型，使用更好的技术来防止过拟合。直到最近，标注图像的数据集都相对较小--在几万张图像的数量级上（例如，NORB[16]，Caltech-101/256 [8, 9]和CIFAR-10/100 [12]）。简单的识别任务在这样大小的数据集上可以被解决的相当好，尤其是如果通过标签保留变换进行数据增强的情况下。例如，目前在MNIST数字识别任务上（&lt;0.3%）的最好准确率已经接近了人类水平[4]。但真实环境中的对象表现出了相当大的可变性，因此为了学习识别它们，有必要使用更大的训练数据集。实际上，小图像数据集的缺点已经被广泛认识到（例如，Pinto et al. [21]），但收集上百万图像的标注数据仅在最近才变得的可能。新的更大的数据集包括LabelMe [23]，它包含了数十万张完全分割的图像，ImageNet[6]，它包含了22000个类别上的超过1500万张标注的高分辨率的图像。</p>
<p>To learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.</p>
<p>为了从数百万张图像中学习几千个对象，我们需要一个有很强学习能力的模型。然而对象识别任务的巨大复杂性意味着这个问题不能被指定，即使通过像ImageNet这样的大数据集，因此我们的模型应该也有许多先验知识来补偿我们所没有的数据。卷积神经网络(CNNs)构成了一个这样的模型[16, 11, 13, 18, 15, 22, 26]。它们的能力可以通过改变它们的广度和深度来控制，它们也可以对图像的本质进行强大且通常正确的假设（也就是说，统计的稳定性和像素依赖的局部性）。因此，与具有层次大小相似的标准前馈神经网络，CNNs有更少的连接和参数，因此它们更容易训练，而它们理论上的最佳性能可能仅比标准前馈神经网络差一点。</p>
<p>Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting.</p>
<p>尽管CNN具有引人注目的质量，尽管它们的局部架构相当有效，但将它们大规模的应用到到高分辨率图像中仍然是极其昂贵的。幸运的是，目前的GPU，搭配了高度优化的2D卷积实现，强大到足够促进有趣地大量CNN的训练，最近的数据集例如ImageNet包含足够的标注样本来训练这样的模型而没有严重的过拟合。</p>
<p>The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets. We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly. Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3. The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4. Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance.</p>
<p>本文具体的贡献如下：我们在ILSVRC-2010和ILSVRC-2012[2]的ImageNet子集上训练了到目前为止最大的神经网络之一，并取得了迄今为止在这些数据集上报道过的最好结果。我们编写了高度优化的2D卷积GPU实现以及训练卷积神经网络内部的所有其它操作，我们把它公开了。我们的网络包含许多新的不寻常的特性，这些特性提高了神经网络的性能并减少了训练时间，详见第三节。即使使用了120万标注的训练样本，我们的网络尺寸仍然使过拟合成为一个明显的问题，因此我们使用了一些有效的技术来防止过拟合，详见第四节。我们最终的网络包含5个卷积层和3个全连接层，深度似乎是非常重要的：我们发现移除任何卷积层（每个卷积层包含的参数不超过模型参数的1%）都会导致更差的性能。</p>
<p>In the end, the network’s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate. Our network takes between five and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.</p>
<p>最后，网络尺寸主要受限于目前GPU的内存容量和我们能忍受的训练时间。我们的网络在两个GTX 580 3GB GPU上训练五六天。我们的所有实验表明我们的结果可以简单地通过等待更快的GPU和更大的可用数据集来提高。</p>
<h2 id="2-The-Dataset"><a href="#2-The-Dataset" class="headerlink" title="2 The Dataset"></a>2 The Dataset</h2><p>ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.</p>
<h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2 数据集"></a>2 数据集</h2><p>ImageNet数据集有超过1500万的标注高分辨率图像，这些图像属于大约22000个类别。这些图像是从网上收集的，使用了Amazon’s Mechanical Turk的众包工具通过人工标注的。从2010年起，作为Pascal视觉对象挑战赛的一部分，每年都会举办ImageNet大规模视觉识别挑战赛（ILSVRC）。ILSVRC使用ImageNet的一个子集，1000个类别每个类别大约1000张图像。总计，大约120万训练图像，50000张验证图像和15万测试图像。</p>
<p>ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.</p>
<p>ILSVRC-2010是ILSVRC竞赛中唯一可以获得测试集标签的版本，因此我们大多数实验都是在这个版本上运行的。由于我们也使用我们的模型参加了ILSVRC-2012竞赛，因此在第六节我们也报告了模型在这个版本的数据集上的结果，这个版本的测试标签是不可获得的。在ImageNet上，按照惯例报告两个错误率：<code>top-1</code>和<code>top-5</code>，<code>top-5</code>错误率是指测试图像的正确标签不在模型认为的五个最可能的便签之中。</p>
<p>ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality. Therefore, we down-sampled the images to a fixed resolution of 256 × 256. Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256×256 patch from the resulting image. We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.</p>
<p>ImageNet包含各种分辨率的图像，而我们的系统要求不变的输入维度。因此，我们将图像进行下采样到固定的<code>256×256</code>分辨率。给定一个矩形图像，我们首先缩放图像短边长度为256，然后从结果图像中裁剪中心的<code>256×256</code>大小的图像块。除了在训练集上对像素减去平均活跃度外，我们不对图像做任何其它的预处理。因此我们在原始的RGB像素值（中心的）上训练我们的网络。</p>
<h2 id="3-The-Architecture"><a href="#3-The-Architecture" class="headerlink" title="3 The Architecture"></a>3 The Architecture</h2><p>The architecture of our network is summarized in Figure 2. It contains eight learned layers — five convolutional and three fully-connected. Below, we describe some of the novel or unusual features of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of their importance, with the most important first.</p>
<h2 id="3-架构"><a href="#3-架构" class="headerlink" title="3 架构"></a>3 架构</h2><p>我们的网络架构概括为图2。它包含八个学习层--5个卷积层和3个全连接层。下面，我们将描述我们网络结构中的一些新奇的不寻常的特性。3.1-3.4小节按照我们对它们评估的重要性进行排序，最重要的最有先。</p>
<h3 id="3-1-ReLU-Nonlinearity"><a href="#3-1-ReLU-Nonlinearity" class="headerlink" title="3.1 ReLU Nonlinearity"></a>3.1 ReLU Nonlinearity</h3><p>The standard way to model a neuron’s output <code>f</code> as a function of its input <code>x</code> is with <code>f(x) = tanh(x)</code> or <code>f(x) = (1 + e−x)−1</code>. In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity <code>f(x) = max(0,x)</code>. Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units. This is demonstrated in Figure 1, which shows the number of iterations required to reach 25% training error on the CIFAR-10 dataset for a particular four-layer convolutional network. This plot shows that we would not have been able to experiment with such large neural networks for this work if we had used traditional saturating neuron models.</p>
<h3 id="3-1-ReLU非线性"><a href="#3-1-ReLU非线性" class="headerlink" title="3.1 ReLU非线性"></a>3.1 ReLU非线性</h3><p>将神经元输出<code>f</code>建模为输入<code>x</code>的函数的标准方式是用<code>f(x) = tanh(x)</code>或<code>f(x) = (1 + e−x)−1</code>。考虑到梯度下降的训练时间，这些饱和的非线性比非饱和非线性<code>f(x) = max(0,x)</code>更慢。根据Nair和Hinton[20]的说法，我们将这种非线性神经元称为修正线性单元(ReLU)。采用ReLU的深度卷积神经网络训练时间比等价的<code>tanh</code>单元要快几倍。在图1中，对于一个特定的四层卷积网络，在CIFAR-10数据集上达到25%的训练误差所需要的迭代次数可以证实这一点。这幅图表明，如果我们采用传统的饱和神经元模型，我们将不能在如此大的神经网络上实验该工作。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%201.png" alt="Figure 1"></p>
<p>Figure 1: A four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). The learning rates for each network were chosen independently to make training as fast as possible. No regularization of any kind was employed. The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons.</p>
<p>图1：使用ReLU的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍。为了使训练尽可能快，每个网络的学习率是单独选择的。没有采用任何类型的正则化。影响的大小随着网络结构的变化而变化，这一点已得到证实，但使用ReLU的网络都比等价的饱和神经元快几倍。</p>
<p>We are not the first to consider alternatives to traditional neuron models in CNNs. For example, Jarrett et al. [11] claim that the nonlinearity <code>f(x) = |tanh(x)|</code> works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset. However, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs. Faster learning has a great influence on the performance of large models trained on large datasets.</p>
<p>我们不是第一个考虑替代CNN中传统神经元模型的人。例如，Jarrett等人[11]声称非线性函数<code>f(x) = |tanh(x)|</code>与其对比度归一化一起，然后是局部均值池化，在Caltech-101数据集上工作的非常好。然而，在这个数据集上主要的关注点是防止过拟合，因此他们观测到的影响不同于我们使用ReLU拟合数据集时的加速能力。更快的学习对大型数据集上大型模型的性能有很大的影响。</p>
<h3 id="3-2-Training-on-Multiple-GPUs"><a href="#3-2-Training-on-Multiple-GPUs" class="headerlink" title="3.2 Training on Multiple GPUs"></a>3.2 Training on Multiple GPUs</h3><p>A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another’s memory directly, without going through host machine memory. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers. This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation.</p>
<h3 id="3-2-多GPU训练"><a href="#3-2-多GPU训练" class="headerlink" title="3.2 多GPU训练"></a>3.2 多GPU训练</h3><p>单个GTX580 GPU只有3G内存，这限制了可以在GTX580上进行训练的网络最大尺寸。事实证明120万图像用来进行网络训练是足够的，但网络太大因此不能在单个GPU上进行训练。因此我们将网络分布在两个GPU上。目前的GPU非常适合跨GPU并行，因为它们可以直接互相读写内存，而不需要通过主机内存。我们采用的并行方案基本上每个GPU放置一半的核（或神经元），还有一个额外的技巧：只在某些特定的层上进行GPU通信。这意味着，例如，第3层的核会将第2层的所有核映射作为输入。然而，第4层的核只将位于相同GPU上的第3层的核映射作为输入。连接模式的选择是一个交叉验证问题，但这可以让我们准确地调整通信数量，直到它的计算量在可接受的范围内。</p>
<p>The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Ciresan et al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as<br>many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU net.</p>
<p>除了我们的列不是独立的之外（看图2），最终的架构有点类似于Ciresan等人[5]采用的“columnar” CNN。与每个卷积层一半的核在单GPU上训练的网络相比，这个方案降分别低了我们的<code>top-1 1.7%</code>，<code>top-5 1.2%</code>的错误率。双GPU网络比单GPU网络稍微减少了训练时间。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Fig%202.png" alt="Figure 2"></p>
<p>Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and the number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264– 4096–4096–1000.</p>
<p>图 2：我们CNN架构图解，明确描述了两个GPU之间的责任。在图的顶部，一个GPU运行在部分层上，而在图的底部，另一个GPU运行在部分层上。GPU只在特定的层进行通信。网络的输入是150,528维，网络剩下层的神经元数目分别是253,440–186,624–64,896–64,896–43,264–4096–4096–1000（8层）。</p>
<h3 id="3-3-Local-Response-Normalization"><a href="#3-3-Local-Response-Normalization" class="headerlink" title="3.3 Local Response Normalization"></a>3.3 Local Response Normalization</h3><p>ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization. Denoting by $a_{x,y}^i$ the activity of a neuron computed by applying kernel $i$ at position $(x, y)$ and then applying the ReLU nonlinearity, the response-normalized activity $b^i_{x,y}$ is given by the expression</p>
<p>$$b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta$$</p>
<p>where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined before training begins. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. The constants k, n, α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = 0.0001, and β = 0.75. We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).</p>
<h3 id="3-3-局部响应归一化"><a href="#3-3-局部响应归一化" class="headerlink" title="3.3 局部响应归一化"></a>3.3 局部响应归一化</h3><p>ReLU具有让人满意的特性，它不需要通过输入归一化来防止饱和。如果至少一些训练样本对ReLU产生了正输入，那么那个神经元上将发生学习。然而，我们仍然发现接下来的局部响应归一化有助于泛化。$a_{x,y}^i$表示神经元激活，通过在$(x, y)$位置应用核$i$，然后应用ReLU非线性来计算，响应归一化激活$b^i_{x,y}$通过下式给定：</p>
<p>$$b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta$$</p>
<p>求和运算在n个“毗邻的”核映射的同一位置上执行，N是本层的卷积核数目。核映射的顺序当然是任意的，在训练开始前确定。响应归一化的顺序实现了一种侧抑制形式，灵感来自于真实神经元中发现的类型，为使用不同核进行神经元输出计算的较大活动创造了竞争。常量k，n，α，β是超参数，它们的值通过验证集确定；我们设k=2，n=5，α=0.0001，β=0.75。我们在特定的层使用的ReLU非线性之后应用了这种归一化（请看3.5小节）。</p>
<p>This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11], but ours would be more correctly termed “brightness normalization”, since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization.</p>
<p>这个方案与Jarrett等人[11]的局部对比度归一化方案有一定的相似性，但我们更恰当的称其为“亮度归一化”，因此我们没有减去均值。响应归一化分别减少了<code>top-1 1.4%</code>，<code>top-5 1.2%</code>的错误率。我们也在CIFAR-10数据集上验证了这个方案的有效性：一个没有归一化的四层CNN取得了13%的错误率，而使用归一化取得了11%的错误率。</p>
<h3 id="3-4-Overlapping-Pooling"><a href="#3-4-Overlapping-Pooling" class="headerlink" title="3.4 Overlapping Pooling"></a>3.4 Overlapping Pooling</h3><p>Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced $s$ pixels apart, each summarizing a neighborhood of size $z × z$ centered at the location of the pooling unit. If we set $s = z$, we obtain traditional local pooling as commonly employed in CNNs. If we set $s &lt; z$, we obtain overlapping pooling. This is what we use throughout our network, with $s = 2$ and $z = 3$. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme $s = 2, z = 2$, which produces output of equivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.</p>
<h3 id="3-4-重叠池化"><a href="#3-4-重叠池化" class="headerlink" title="3.4 重叠池化"></a>3.4 重叠池化</h3><p>CNN中的池化层归纳了同一核映射上相邻组神经元的输出。习惯上，相邻池化单元归纳的区域是不重叠的（例如[17, 11, 4]）。更确切的说，池化层可看作由池化单元网格组成，网格间距为$s$个像素，每个网格归纳池化单元中心位置$z × z$大小的邻居。如果设置$s = z$，我们会得到通常在CNN中采用的传统局部池化。如果设置$s &lt; z$，我们会得到重叠池化。这就是我们网络中使用的方法，设置$s = 2$，$z = 3$。这个方案分别降低了<code>top-1 0.4%</code>，<code>top-5 0.3%</code>的错误率，与非重叠方案$s = 2，z = 2$相比，输出的维度是相等的。我们在训练过程中通常观察采用重叠池化的模型，发现它更难过拟合。</p>
<h3 id="3-5-Overall-Architecture"><a href="#3-5-Overall-Architecture" class="headerlink" title="3.5 Overall Architecture"></a>3.5 Overall Architecture</h3><p>Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fully-connected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.</p>
<h3 id="3-5-整体架构"><a href="#3-5-整体架构" class="headerlink" title="3.5 整体架构"></a>3.5 整体架构</h3><p>现在我们准备描述我们的CNN的整体架构。如图2所示，我们的网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签的分布。我们的网络最大化多项逻辑回归的目标，这等价于最大化预测分布下训练样本正确标签的对数概率的均值。</p>
<p>The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-connected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.</p>
<p>第2，4，5卷积层的核只与位于同一GPU上的前一层的核映射相连接（看图2）。第3卷积层的核与第2层的所有核映射相连。全连接层的神经元与前一层的所有神经元相连。第1，2卷积层之后是响应归一化层。3.4节描述的这种最大池化层在响应归一化层和第5卷积层之后。ReLU非线性应用在每个卷积层和全连接层的输出上。</p>
<p>The first convolutional layer filters the 224 × 224 × 3 input image with 96 kernels of size 11 × 11 × 3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each.</p>
<p>第1卷积层使用96个核对224 × 224 × 3的输入图像进行滤波，核大小为11 × 11 × 3，步长是4个像素（核映射中相邻神经元感受野中心之间的距离）。第2卷积层使用用第1卷积层的输出（响应归一化和池化）作为输入，并使用256个核进行滤波，核大小为5 × 5 × 48。第3，4，5卷积层互相连接，中间没有接入池化层或归一化层。第3卷积层有384个核，核大小为3 × 3 × 256，与第2卷积层的输出（归一化的，池化的）相连。第4卷积层有384个核，核大小为3 × 3 × 192，第5卷积层有256个核，核大小为3 × 3 × 192。每个全连接层有4096个神经元。</p>
<h2 id="4-Reducing-Overfitting"><a href="#4-Reducing-Overfitting" class="headerlink" title="4 Reducing Overfitting"></a>4 Reducing Overfitting</h2><p>Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting.</p>
<h2 id="4-减少过拟合"><a href="#4-减少过拟合" class="headerlink" title="4 减少过拟合"></a>4 减少过拟合</h2><p>我们的神经网络架构有6000万参数。尽管ILSVRC的1000类使每个训练样本从图像到标签的映射上强加了10比特的约束，但这不足以学习这么多的参数而没有相当大的过拟合。下面，我们会描述我们用来克服过拟合的两种主要方式。</p>
<h3 id="4-1-Data-Augmentation"><a href="#4-1-Data-Augmentation" class="headerlink" title="4.1 Data Augmentation"></a>4.1 Data Augmentation</h3><p>The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free.</p>
<h3 id="4-1-数据增强"><a href="#4-1-数据增强" class="headerlink" title="4.1 数据增强"></a>4.1 数据增强</h3><p>图像数据上最简单常用的用来减少过拟合的方法是使用标签保留变换（例如[25, 4, 5]）来人工增大数据集。我们使用了两种独特的数据增强方式，这两种方式都可以从原始图像通过非常少的计算量产生变换的图像，因此变换图像不需要存储在硬盘上。在我们的实现中，变换图像通过CPU的Python代码生成，而此时GPU正在训练前一批图像。因此，实际上这些数据增强方案是计算免费的。</p>
<p>The first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patches. This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches.</p>
<p>第一种数据增强方式包括产生图像变换和水平翻转。我们从256×256图像上通过随机提取224 × 224的图像块实现了这种方式，然后在这些提取的图像块上进行训练。这通过一个2048因子增大了我们的训练集，尽管最终的训练样本是高度相关的。没有这个方案，我们的网络会有大量的过拟合，这会迫使我们使用更小的网络。在测试时，网络会提取5个224 × 224的图像块（四个角上的图像块和中心的图像块）和它们的水平翻转（因此总共10个图像块）进行预测，然后对网络在10个图像块上的softmax层进行平均。</p>
<p>The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set. To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel $I_xy = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$ we add the following quantity: </p>
<p>$$[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T$$</p>
<p>where $p_i$ and $\lambda_i$ are $i$th eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and $\alpha_i$ is the aforementioned random variable. Each $\alpha_i$ is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn. This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. This scheme reduces the top-1 error rate by over 1%.</p>
<p>第二种数据增强方式包括改变训练图像的RGB通道的强度。具体地，我们在整个ImageNet训练集上对RGB像素值集合执行PCA。对于每幅训练图像，我们加上多倍找到的主成分，大小成正比的对应特征值乘以一个随机变量，随机变量通过均值为0，标准差为0.1的高斯分布得到。因此对于每幅RGB图像像素$I_xy = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$，我们加上下面的数量：</p>
<p>$$[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T$$</p>
<p>$p_i$，$\lambda_i$分别是RGB像素值3 × 3协方差矩阵的第$i$个特征向量和特征值，$\alpha_i$是前面提到的随机变量。对于某个训练图像的所有像素，每个$\alpha_i$只获取一次，直到图像进行下一次训练时才重新获取。这个方案近似抓住了自然图像的一个重要特性，即光照的颜色和强度发生变化时，目标身份是不变的。这个方案减少了<code>top 1</code>错误率1%以上。</p>
<h3 id="4-2-Dropout"><a href="#4-2-Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h3><p>Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back-propagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.</p>
<h3 id="4-2-失活-Dropout"><a href="#4-2-失活-Dropout" class="headerlink" title="4.2 失活(Dropout)"></a>4.2 失活(Dropout)</h3><p>将许多不同模型的预测结合起来是降低测试误差[1, 3]的一个非常成功的方法，但对于需要花费几天来训练的大型神经网络来说，这似乎太昂贵了。然而，有一个非常有效的模型结合版本，它只花费两倍的训练成本。这种最近引入的技术，叫做“dropout”[10]，它会以0.5的概率对每个隐层神经元的输出设为0。那些“失活的”的神经元不再进行前向传播并且不参与反向传播。因此每次输入时，神经网络会采样一个不同的架构，但所有架构共享权重。这个技术减少了复杂的神经元互适应，因为一个神经元不能依赖特定的其它神经元的存在。因此，神经元被强迫学习更鲁棒的特征，它在与许多不同的其它神经元的随机子集结合时是有用的。在测试时，我们使用所有的神经元但它们的输出乘以0.5，对指数级的许多失活网络的预测分布进行几何平均，这是一种合理的近似。</p>
<p>We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.</p>
<p>我们在图2中的前两个全连接层使用失活。如果没有失活，我们的网络表现出大量的过拟合。失活大致上使要求收敛的迭代次数翻了一倍。</p>
<h2 id="5-Details-of-learning"><a href="#5-Details-of-learning" class="headerlink" title="5 Details of learning"></a>5 Details of learning</h2><p>We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s training error. The update rule for weight $w$ was</p>
<p>$$v_{i+1} := 0.9 \bullet v_i - 0.0005 \bullet \varepsilon \bullet w_i - \varepsilon \bullet \langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$$</p>
<p>where $i$ is the iteration index, $v$ is the momentum variable, $\varepsilon$ is the learning rate, and $\langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$ is the average over the $i$th batch $D_i$ of the derivative of the objective with respect to $w$, evaluated at $w_i$.</p>
<h2 id="5-学习细节"><a href="#5-学习细节" class="headerlink" title="5 学习细节"></a>5 学习细节</h2><p>我们使用随机梯度下降来训练我们的模型，样本的batch size为128，动量为0.9，权重衰减为0.0005。我们发现少量的权重衰减对于模型的学习是重要的。换句话说，权重衰减不仅仅是一个正则项：它减少了模型的训练误差。权重$w$的更新规则是</p>
<p>$$v_{i+1} := 0.9 \bullet v_i - 0.0005 \bullet \varepsilon \bullet w_i - \varepsilon \bullet \langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$$</p>
<p>$i$是迭代索引，$v$是动量变量，$\varepsilon$是学习率，$\langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$是目标函数对$w$，在$w_i$上的第$i$批微分$D_i$的平均。</p>
<p>We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0.</p>
<p>我们使用均值为0，标准差为0.01的高斯分布对每一层的权重进行初始化。我们在第2，4，5卷积层和全连接隐层将神经元偏置初始化为常量1。这个初始化通过为ReLU提供正输入加速了学习的早期阶段。我们在剩下的层将神经元偏置初始化为0。</p>
<p>We used an equal learning rate for all layers, which we adjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination. We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.</p>
<p>我们对所有的层使用相等的学习率，这个是在整个训练过程中我们手动调整得到的。当验证误差在当前的学习率下停止提供时，我们遵循启发式的方法将学习率除以10。学习率初始化为0.01，在训练停止之前降低三次。我们在120万图像的训练数据集上训练神经网络大约90个循环，在两个NVIDIA GTX 580 3GB GPU上花费了五到六天。</p>
<h2 id="6-Results"><a href="#6-Results" class="headerlink" title="6 Results"></a>6 Results</h2><p>Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%. The best performance achieved during the ILSVRC-2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%201.png" alt="Table 1"></p>
<p>Table 1: Comparison of results on ILSVRC-2010 test set.In italics are best results achieved by others.</p>
<h2 id="6-结果"><a href="#6-结果" class="headerlink" title="6 结果"></a>6 结果</h2><p>我们在ILSVRC-2010上的结果概括为表1。我们的神经网络取得了<code>top-1 37.5%</code>，<code>top-5 17.0%</code>的错误率。在ILSVRC-2010竞赛中最佳结果是<code>top-1 47.1%</code>，<code>top-5 28.2%</code>，使用的方法是对6个在不同特征上训练的稀疏编码模型生成的预测进行平均，从那时起已公布的最好结果是<code>top-1 45.7%</code>，<code>top-5 25.7%</code>，使用的方法是平均在Fisher向量（FV）上训练的两个分类器的预测结果，Fisher向量是通过两种密集采样特征计算得到的[24]。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%201.png" alt="表1"></p>
<p>表1：ILSVRC-2010测试集上的结果对比。斜体是其它人取得的最好结果。</p>
<p>We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7].</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%202.png" alt="Table 2"></p>
<p>Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk were “pre-trained” to classify the entire ImageNet 2011 Fall release. See Section 6 for details. </p>
<p>我们也用我们的模型参加了ILSVRC-2012竞赛并在表2中报告了我们的结果。由于ILSVRC-2012的测试集标签不可以公开得到，我们不能报告我们尝试的所有模型的测试错误率。在这段的其余部分，我们会使用验证误差率和测试误差率互换，因为在我们的实验中它们的差别不会超过0.1%（看图2）。本文中描述的CNN取得了<code>top-5 18.2%</code>的错误率。五个类似的CNN预测的平均误差率为16.4%。为了对ImageNet 2011秋季发布的整个数据集（1500万图像，22000个类别）进行分类，我们在最后的池化层之后有一个额外的第6卷积层，训练了一个CNN，然后在它上面进行“fine-tuning”，在ILSVRC-2012取得了16.6%的错误率。对在ImageNet 2011秋季发布的整个数据集上预训练的两个CNN和前面提到的五个CNN的预测进行平均得到了15.3%的错误率。第二名的最好竞赛输入取得了26.2%的错误率，他的方法是对FV上训练的一些分类器的预测结果进行平均，FV在不同类型密集采样特征计算得到的。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%202.png" alt="表2"></p>
<p>表2：ILSVRC-2012验证集和测试集的误差对比。斜线部分是其它人取得的最好的结果。带星号的是“预训练的”对ImageNet 2011秋季数据集进行分类的模型。更多细节请看第六节。</p>
<p>Finally, we also report our error rates on the Fall 2009 version of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the literature of using half of the images for training and half for testing. Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. Our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].</p>
<p>最后，我们也报告了我们在ImageNet 2009秋季数据集上的误差率，ImageNet 2009秋季数据集有10,184个类，890万图像。在这个数据集上我们按照惯例用一半的图像来训练，一半的图像来测试。由于没有建立测试集，我们的数据集分割有必要不同于以前作者的数据集分割，但这对结果没有明显的影响。我们在这个数据集上的的top-1和top-5错误率是67.4%和40.9%，使用的是上面描述的在最后的池化层之后有一个额外的第6卷积层网络。这个数据集上公开可获得的最好结果是78.1%和60.9%[19]。</p>
<h3 id="6-1-Qualitative-Evaluations"><a href="#6-1-Qualitative-Evaluations" class="headerlink" title="6.1 Qualitative Evaluations"></a>6.1 Qualitative Evaluations</h3><p>Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The network has learned a variety of frequency and orientation-selective kernels, as well as various colored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs).</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%203.png" alt="Figure 3"></p>
<p>Figure 3: 96 convolutional kernels of size 11×11×3 learned by the first convolutional layer on the 224×224×3 input images. The top 48 kernels were learned on GPU 1 while the bottom 48 kernels were learned on GPU 2. See Section 6.1 for details.</p>
<h3 id="6-1-定性评估"><a href="#6-1-定性评估" class="headerlink" title="6.1 定性评估"></a>6.1 定性评估</h3><p>图3显示了网络的两个数据连接层学习到的卷积核。网络学习到了大量的频率核、方向选择核，也学到了各种颜色点。注意两个GPU表现出的专业化，3.5小节中描述的受限连接的结果。GPU 1上的核主要是没有颜色的，而GPU 2上的核主要是针对颜色的。这种专业化在每次运行时都会发生，并且是与任何特别的随机权重初始化（以GPU的重新编号为模）无关的。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%203.png" alt="Figure 3"></p>
<p>图3：第一卷积层在224×224×3的输入图像上学习到的大小为11×11×3的96个卷积核。上面的48个核是在GPU 1上学习到的而下面的48个卷积核是在GPU 2上学习到的。更多细节请看6.1小节。</p>
<p>In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its top-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the top-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example, only other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry) there is genuine ambiguity about the intended focus of the photograph.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%204.png" alt="Figure 4"></p>
<p>Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.</p>
<p>在图4的左边部分，我们通过在8张测试图像上计算它的top-5预测定性地评估了网络学习到的东西。注意即使是不在图像中心的目标也能被网络识别，例如左上角的小虫。大多数的top-5标签似乎是合理的。例如，对于美洲豹来说，只有其它类型的猫被认为是看似合理的标签。在某些案例（格栅，樱桃）中，网络在意的图片焦点真的很含糊。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%204.png" alt="Figure 4"></p>
<p>图4：（左）8张ILSVRC-2010测试图像和我们的模型认为最可能的5个标签。每张图像的下面是它的正确标签，正确标签的概率用红条表示（如果正确标签在top 5中）。（右）第一列是5张ILSVRC-2010测试图像。剩下的列展示了6张训练图像，这些图像在最后的隐藏层的特征向量与测试图像的特征向量有最小的欧氏距离。</p>
<p>Another way to probe the network’s visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar. Figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure. Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the results for many more test images in the supplementary material.</p>
<p>探索网络可视化知识的另一种方式是思考最后的4096维隐藏层在图像上得到的特征激活。如果两幅图像生成的特征激活向量之间有较小的欧式距离，我们可以认为神经网络的更高层特征认为它们是相似的。图4表明根据这个度量标准，测试集的5张图像和训练集的6张图像中的每一张都是最相似的。注意在像素级别，检索到的训练图像与第一列的查询图像在L2上通常是不接近的。例如，检索的狗和大象似乎有很多姿态。我们在补充材料中对更多的测试图像呈现了这种结果。</p>
<p>Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vectors is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes. This should produce a much better image retrieval method than applying auto-encoders to the raw pixels [14], which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar.</p>
<p>通过两个4096维实值向量间的欧氏距离来计算相似性是效率低下的，但通过训练一个自动编码器将这些向量压缩为短二值编码可以使其变得高效。这应该会产生一种比将自动编码器应用到原始像素上[14]更好的图像检索方法，自动编码器应用到原始像素上的方法没有使用图像标签，因此会趋向于检索与要检索的图像具有相似边缘模式的图像，无论它们是否是语义上相似。</p>
<h2 id="7-Discussion"><a href="#7-Discussion" class="headerlink" title="7 Discussion"></a>7 Discussion</h2><p>Our results show that a large, deep convolutional neural network is capable of achieving record-breaking results on a highly challenging dataset using purely supervised learning. It is notable that our network’s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results.</p>
<h2 id="7-探讨"><a href="#7-探讨" class="headerlink" title="7 探讨"></a>7 探讨</h2><p>我们的结果表明一个大型深度卷积神经网络在一个具有高度挑战性的数据集上使用纯有监督学习可以取得破纪录的结果。值得注意的是，如果移除一个卷积层，我们的网络性能会降低。例如，移除任何中间层都会引起网络损失大约2%的top-1性能。因此深度对于实现我们的结果非常重要。</p>
<p>To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data. Thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system. Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.</p>
<p>为了简化我们的实验，我们没有使用任何无监督的预训练，尽管我们希望它会有所帮助，特别是在如果我们能获得足够的计算能力来显著增加网络的大小而标注的数据量没有对应增加的情况下。到目前为止，我们的结果已经提高了，因为我们的网络更大、训练时间更长，但为了匹配人类视觉系统的下颞线（视觉专业术语）我们仍然有许多数量级要达到。最后我们想在视频序列上使用非常大的深度卷积网络，视频序列的时序结构会提供非常有帮助的信息，这些信息在静态图像上是缺失的或远不那么明显。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] R.M.BellandY.Koren.Lessonsfromthenetflixprizechallenge.ACMSIGKDDExplorationsNewsletter, 9(2):75–79, 2007.</p>
<p>[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.imagenet.org/challenges. 2010.</p>
<p>[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.</p>
<p>[4] D. Cires ̧an, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. Arxiv preprint arXiv:1202.2745, 2012.</p>
<p>[5] D.C. Cires ̧an, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classification. Arxiv preprint arXiv:1102.0183, 2011.</p>
<p>[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.</p>
<p>[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL <a href="http://www.image-net.org/challenges/LSVRC/2012/" target="_blank" rel="external">http://www.image-net.org/challenges/LSVRC/2012/</a>.</p>
<p>[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59–70, 2007.</p>
<p>[9] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. URL <a href="http://authors.library.caltech.edu/7694" target="_blank" rel="external">http://authors.library.caltech.edu/7694</a>.</p>
<p>[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.</p>
<p>[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.</p>
<p>[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009.</p>
<p>[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.</p>
<p>[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In ESANN, 2011.</p>
<p>[15] Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, 1990.</p>
<p>[16] Y. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.</p>
<p>[17] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256. IEEE, 2010.</p>
<p>[18] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.</p>
<p>[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer Vision, Florence, Italy, October 2012.</p>
<p>[20] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 2010.</p>
<p>[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computational biology, 4(1):e27, 2008.</p>
<p>[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,2009.</p>
<p>[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for image annotation. International journal of computer vision, 77(1):157–173, 2008.</p>
<p>[24] J.SánchezandF.Perronnin.High-dimensionalsignaturecompressionforlarge-scaleimageclassification. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,2011.</p>
<p>[25] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition, volume 2, pages 958–962, 2003.</p>
<p>[26] S.C.Turaga,J.F.Murray,V.Jain,F.Roth,M.Helmstaedter,K.Briggman,W.Denk,andH.S.Seung.Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22(2):511–538, 2010.</p>
]]></content>
    
    <summary type="html">
    
      AlexNet论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>LeNet在caffe中的实现分析</title>
    <link href="noahsnail.com/2017/07/04/2017-7-4-LeNet%E5%88%86%E6%9E%90%E5%8F%8A%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>noahsnail.com/2017/07/04/2017-7-4-LeNet分析及可视化/</id>
    <published>2017-07-04T09:29:07.000Z</published>
    <updated>2017-07-04T09:35:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是对Caffe中mnist数据集上训练的LeNet模型进行结构分析和可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> caffe</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义LeNet模型信息</span></div><div class="line">deploy = <span class="string">'lenet.prototxt'</span></div><div class="line">model = <span class="string">'lenet_iter_10000.caffemodel'</span></div><div class="line"></div><div class="line"><span class="comment"># 加载模型</span></div><div class="line">net = caffe.Net(deploy, model, caffe.TEST)</div><div class="line"></div><div class="line"><span class="comment"># 计算均值</span></div><div class="line"><span class="comment"># blob = caffe.proto.caffe_pb2.BlobProto()</span></div><div class="line"><span class="comment"># bin_mean = open(mean_file, 'rb' ).read()</span></div><div class="line"><span class="comment"># blob.ParseFromString(bin_mean)</span></div><div class="line"><span class="comment"># arr = np.array(caffe.io.blobproto_to_array(blob))</span></div><div class="line"><span class="comment"># npy_mean = arr[0]</span></div><div class="line"><span class="comment"># mu = npy_mean.mean(1).mean(1)</span></div><div class="line"></div><div class="line"><span class="comment"># init transformer</span></div><div class="line">transformer = caffe.io.Transformer(&#123;<span class="string">'data'</span>: net.blobs[<span class="string">'data'</span>].data.shape&#125;)</div><div class="line">transformer.set_transpose(<span class="string">'data'</span>, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</div><div class="line"><span class="comment"># transformer.set_mean('data', mu)</span></div><div class="line">transformer.set_raw_scale(<span class="string">'data'</span>, <span class="number">255</span>)</div><div class="line"><span class="comment"># transformer.set_channel_swap('data', (2, 1, 0))</span></div><div class="line"></div><div class="line"><span class="comment"># get certain layer feature</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(pimg, lay_name)</span>:</span></div><div class="line">    <span class="keyword">global</span> transformer</div><div class="line">    <span class="keyword">global</span> net</div><div class="line">    image = caffe.io.load_image(pimg, color = <span class="keyword">False</span>)</div><div class="line">    image</div><div class="line">    transformed_image = transformer.preprocess(<span class="string">'data'</span>, image)</div><div class="line">    net.blobs[<span class="string">'data'</span>].data[...] = transformed_image</div><div class="line">    output = net.forward()</div><div class="line">    result = output[lay_name]</div><div class="line">    <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Test</span></div><div class="line">result = init(<span class="string">'test.jpg'</span>, <span class="string">'prob'</span>)</div><div class="line"><span class="keyword">print</span> result.shape</div><div class="line"><span class="keyword">print</span> result</div></pre></td></tr></table></figure>
<pre><code>(1, 10)
[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
</code></pre><p><strong>LeNet网络的所有layer以及layer的输出数据</strong><br>data: 输入图片数据大小为28*28<br>conv1: 20个卷积核，卷积之后feature map大小24*24<br>pool1: pooling后feature map大小变为12*12, 共20层<br>conv2: 50个卷积核, 卷积之后feature map大小为8*8<br>pool2: pooling后feature map大小变为4*4, 共50层<br>ip1: 全连接层一, 500个结点<br>ip2: 全连接层二, 10个结点<br>prob: 对ip2进行softmax</p>
<p>备注: conv1之后得到20个feature map, conv2有50个卷积核, 每个卷积核在20个feature map卷积之后, 20个卷积之后的feature map对应位置上的点的数据累加之后取激活函数(ReLU)得到该卷积核的对应的feature map, 因此conv2执行之后的feature map个数为50, 而不是50*20.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all layer name and blob shape</span></div><div class="line"><span class="comment"># blob shape is (batch_size, channel_dim, height, width).</span></div><div class="line"><span class="keyword">for</span> layer_name, blob <span class="keyword">in</span> net.blobs.iteritems():</div><div class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(blob.data.shape)</div></pre></td></tr></table></figure>
<pre><code>data  (1, 1, 28, 28)
conv1 (1, 20, 24, 24)
pool1 (1, 20, 12, 12)
conv2 (1, 50, 8, 8)
pool2 (1, 50, 4, 4)
ip1 (1, 500)
ip2 (1, 10)
prob  (1, 10)
</code></pre><p><strong>LeNet网络的权重(weights + biases)</strong><br>conv1: 20个卷积核, weights大小为5*5, 20个biases<br>conv2: 50个卷积核, weights大小为5*5, 50个biases<br>ip1: conv2之后得到50个4*4大小的feature map, 排列起来大小为800, 与ip1的500个结点进行全连接, weights个数为500*800, biases个数为500<br>ip2: ip1的500个结点与ip2的10个结点进行全连接, weights个数为500*10, biases个数为10</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all layer name and parameters shape</span></div><div class="line"><span class="comment"># param[0] is weights, param[1] is biases</span></div><div class="line"><span class="comment"># weights shape is (output_channels, input_channels, filter_height, filter_width)</span></div><div class="line"><span class="comment"># biases shape is (output_channels,)</span></div><div class="line"><span class="keyword">for</span> layer_name, param <span class="keyword">in</span> net.params.iteritems():</div><div class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(param[<span class="number">0</span>].data.shape) + <span class="string">'\t'</span> + str(param[<span class="number">1</span>].data.shape)</div></pre></td></tr></table></figure>
<pre><code>conv1 (20, 1, 5, 5) (20,)
conv2 (50, 20, 5, 5)  (50,)
ip1 (500, 800)  (500,)
ip2 (10, 500) (10,)
</code></pre><p><strong>numpy pad</strong><br>padding分为四部分<br>第一部分: (0, n ** 2 - data.shape[0]), 补充方阵的缺少的部分, 0表示前面不补, 后面补n ** 2 - data.shape[0]列<br>第二部分: (0, 1)表示每个filter的前面不补, 后面补1列, filter补了一行<br>第三部分: (0, 1)表示每个filter的前面不补, 后面补1列, filter补了一列<br>第四部分: (0, 0)剩下的不补充数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># param(weights) visualization</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualization</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="comment"># normalize data for display</span></div><div class="line">    data = (data - data.min()) / (data.max() - data.min())</div><div class="line">    </div><div class="line">    <span class="comment"># force the number of filters to be square</span></div><div class="line">    n = int(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</div><div class="line">    </div><div class="line">    <span class="comment"># add some space between filters</span></div><div class="line">    padding = (((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)) + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>)) </div><div class="line">    data = np.pad(data, padding, mode = <span class="string">'constant'</span>, constant_values = <span class="number">1</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># tile the filters into an image</span></div><div class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + tuple(range(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</div><div class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</div><div class="line">    plt.imshow(data, cmap=<span class="string">'gray'</span>)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># feature map visualization</span></div><div class="line">feature_map = net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>]</div><div class="line">visualization(feature_map)</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/output_8_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># filter visualization</span></div><div class="line">filters = net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data</div><div class="line">visualization(filters.reshape(<span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>))</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/output_9_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      LeNet在caffe中的实现分析
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>keras的基本用法(五)——图像predict</title>
    <link href="noahsnail.com/2017/06/12/2017-6-12-keras%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%94)%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8Fpredict/"/>
    <id>noahsnail.com/2017/06/12/2017-6-12-keras的基本用法(五)——图像predict/</id>
    <published>2017-06-12T07:48:52.000Z</published>
    <updated>2017-06-12T07:57:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要介绍Keras的一些基本用法，主要是根据已有模型预测图像的类别，以ResNet50为例。</p>
<ul>
<li>Demo</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">from keras.layers import Dense</div><div class="line">from keras.models import Model</div><div class="line">from keras.preprocessing import image</div><div class="line">from keras.applications.resnet50 import ResNet50</div><div class="line"></div><div class="line"># 使用ResNet的结构，不包括最后一层</div><div class="line">base_model = ResNet50(include_top = False, pooling = &apos;avg&apos;)</div><div class="line"></div><div class="line"># 定义网络结构最后一层</div><div class="line">predictions = Dense(3, activation=&apos;softmax&apos;)(base_model.output)</div><div class="line"></div><div class="line"># 定义模型</div><div class="line">model = Model(inputs=base_model.input, outputs=predictions)</div><div class="line"></div><div class="line"># 加载训练好的模型</div><div class="line">model.load_weights(&apos;./weights.h5&apos;)</div><div class="line"></div><div class="line">image_path = &apos;./lena.jpg&apos;</div><div class="line"></div><div class="line"># 加载图像</div><div class="line">img = image.load_img(image_path, target_size=(224, 224))</div><div class="line"></div><div class="line"># 图像预处理</div><div class="line">x = image.img_to_array(img)</div><div class="line">x = np.expand_dims(x, axis=0)</div><div class="line">x = preprocess_input(x)</div><div class="line"></div><div class="line"># 对图像进行分类</div><div class="line">preds = model.predict(x)</div><div class="line"></div><div class="line"># 输出预测概率</div><div class="line">print &apos;Predicted:&apos;, preds</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      keras的基本用法(五)——图像predict
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>keras的基本用法(四)——Fine Tuning神经网络</title>
    <link href="noahsnail.com/2017/06/07/2017-6-7-keras%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%9B%9B)%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9CFine%20Tuning/"/>
    <id>noahsnail.com/2017/06/07/2017-6-7-keras的基本用法(四)——网络Fine Tuning/</id>
    <published>2017-06-07T01:53:09.000Z</published>
    <updated>2017-06-12T08:23:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要介绍Keras的一些基本用法，主要涉及已有网络的fine tuning，以ResNet50为例。</p>
<ul>
<li>Demo</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">from keras.models import Model</div><div class="line">from keras.layers import Dense</div><div class="line">from keras.applications.resnet50 import ResNet50</div><div class="line">from keras.preprocessing.image import ImageDataGenerator</div><div class="line"></div><div class="line"># 训练的batch_size</div><div class="line">batch_size = 16</div><div class="line"># 训练的epoch</div><div class="line">epochs = 100</div><div class="line"></div><div class="line"># 图像Generator，用来构建输入数据</div><div class="line">train_datagen = ImageDataGenerator(</div><div class="line">        width_shift_range=0.1,</div><div class="line">        height_shift_range=0.1,</div><div class="line">        zoom_range=0.2,</div><div class="line">        horizontal_flip=True)</div><div class="line"></div><div class="line"># 从文件中读取数据，目录结构应为train下面是各个类别的子目录，每个子目录中为对应类别的图像</div><div class="line">train_generator = train_datagen.flow_from_directory(&apos;./train&apos;, target_size = (224, 224), batch_size = batch_size)</div><div class="line"></div><div class="line"># 输出类别信息</div><div class="line">print train_generator.class_indices</div><div class="line"></div><div class="line"># 生成测试数据</div><div class="line">test_datagen = ImageDataGenerator()</div><div class="line">validation_generator = test_datagen.flow_from_directory(&apos;./validation&apos;, target_size = (224, 224), batch_size = batch_size)</div><div class="line"></div><div class="line"># 使用ResNet的结构，不包括最后一层，且加载ImageNet的预训练参数</div><div class="line">base_model = ResNet50(weights = &apos;imagenet&apos;, include_top = False, pooling = &apos;avg&apos;)</div><div class="line"></div><div class="line"># 构建网络的最后一层，3是自己的数据的类别</div><div class="line">predictions = Dense(3, activation=&apos;softmax&apos;)(base_model.output)</div><div class="line"></div><div class="line"># 定义整个模型</div><div class="line">model = Model(inputs=base_model.input, outputs=predictions)</div><div class="line"></div><div class="line"># 编译模型，loss为交叉熵损失</div><div class="line">model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;categorical_crossentropy&apos;)</div><div class="line"></div><div class="line"># 训练模型</div><div class="line">model.fit_generator(train_generator,steps_per_epoch = batch_size, epochs = epochs, validation_data = validation_generator, validation_steps = batch_size)</div><div class="line"></div><div class="line"># 保存训练得到的模型</div><div class="line">model.save_weights(&apos;weights.h5&apos;)</div></pre></td></tr></table></figure>
<ul>
<li>部分结果</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">&#123;&apos;Type_3&apos;: 2, &apos;Type_2&apos;: 1, &apos;Type_1&apos;: 0&#125;</div><div class="line">Found 761 images belonging to 3 classes.</div><div class="line">Epoch 1/40</div><div class="line"> 1/16 [&gt;.............................] - ETA: 119s - loss: 1.33922017-06-07 10:18:48.246289: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2521 get requests, put_count=2161 evicted_count=1000 eviction_rate=0.462749 and unsatisfied allocation rate=0.579135</div><div class="line">2017-06-07 10:18:48.246348: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110</div><div class="line">16/16 [==============================] - 120s - loss: 2.3753 - val_loss: 10.8293</div><div class="line">Epoch 2/40</div><div class="line"> 1/16 [&gt;.............................] - ETA: 5s - loss: 1.00542017-06-07 10:20:40.464589: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2270 get requests, put_count=2642 evicted_count=1000 eviction_rate=0.378501 and unsatisfied allocation rate=0.286784</div><div class="line">2017-06-07 10:20:40.464643: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281</div><div class="line">16/16 [==============================] - 83s - loss: 1.7988 - val_loss: 11.5219</div><div class="line">Epoch 3/40</div><div class="line">16/16 [==============================] - 81s - loss: 1.6640 - val_loss: 11.0043</div><div class="line">Epoch 4/40</div><div class="line"> 3/16 [====&gt;.........................] - ETA: 4s - loss: 1.87452017-06-07 10:23:26.725923: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 11057 get requests, put_count=11071 evicted_count=1000 eviction_rate=0.0903261 and unsatisfied allocation rate=0.0945103</div><div class="line">2017-06-07 10:23:26.725986: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720</div><div class="line">16/16 [==============================] - 83s - loss: 1.7237 - val_loss: 11.7738</div><div class="line">Epoch 5/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.6304 - val_loss: 10.6538</div><div class="line">Epoch 6/40</div><div class="line">16/16 [==============================] - 80s - loss: 1.2182 - val_loss: 4.5027</div><div class="line">Epoch 7/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.3179 - val_loss: 11.5891</div><div class="line">Epoch 8/40</div><div class="line">16/16 [==============================] - 82s - loss: 1.1806 - val_loss: 10.5800</div><div class="line">Epoch 9/40</div><div class="line">16/16 [==============================] - 81s - loss: 1.1935 - val_loss: 11.1477</div><div class="line">Epoch 10/40</div><div class="line">16/16 [==============================] - 80s - loss: 1.1727 - val_loss: 7.0913</div><div class="line">Epoch 11/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.2058 - val_loss: 6.4474</div><div class="line">Epoch 12/40</div><div class="line">16/16 [==============================] - 82s - loss: 1.2702 - val_loss: 7.7678</div><div class="line">Epoch 13/40</div><div class="line">16/16 [==============================] - 84s - loss: 1.2060 - val_loss: 7.9961</div><div class="line">Epoch 14/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.0768 - val_loss: 11.2121</div><div class="line">Epoch 15/40</div><div class="line">16/16 [==============================] - 80s - loss: 1.1401 - val_loss: 13.2052</div><div class="line">Epoch 16/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.1961 - val_loss: 13.0330</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      keras的基本用法(四)——Fine Tuning神经网络
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>pandas总结(一)——Series的使用</title>
    <link href="noahsnail.com/2017/06/06/2017-6-6-pandas%E6%80%BB%E7%BB%93(%E4%B8%80)%E2%80%94%E2%80%94Series%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>noahsnail.com/2017/06/06/2017-6-6-pandas总结(一)——Series的使用/</id>
    <published>2017-06-06T11:33:34.000Z</published>
    <updated>2017-06-06T11:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># pandas是一个用来进行数据分析的基于numpy的库</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Series是一个一维的数据结构</span></div><div class="line"></div><div class="line"><span class="comment"># 用list构建Series</span></div><div class="line">series1 = pd.Series([<span class="number">3</span>, <span class="number">5</span>, <span class="string">'test'</span>, <span class="number">-5</span>, <span class="number">0.3</span>])</div><div class="line"><span class="keyword">print</span> series1</div></pre></td></tr></table></figure>
<pre><code>0       3
1       5
2    test
3      -5
4     0.3
dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 用list, index构建Series</span></div><div class="line">series2 = pd.Series([<span class="number">3</span>, <span class="number">5</span>, <span class="string">'test'</span>, <span class="number">-5</span>, <span class="number">0.3</span>], index = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>])</div><div class="line"><span class="keyword">print</span> series2</div></pre></td></tr></table></figure>
<pre><code>A       3
B       5
C    test
D      -5
E     0.3
dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 通过dict构建Series</span></div><div class="line">companies = &#123;<span class="string">'Baidu'</span>: <span class="number">400</span>, <span class="string">'Alibaba'</span>: <span class="number">500</span>, <span class="string">'Tecent'</span>: <span class="number">600</span>, <span class="string">'Jingdong'</span>: <span class="number">300</span>&#125;</div><div class="line">series3 = pd.Series(companies)</div><div class="line"><span class="keyword">print</span> series3</div></pre></td></tr></table></figure>
<pre><code>Alibaba     500
Baidu       400
Jingdong    300
Tecent      600
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Series数据选择</span></div><div class="line"></div><div class="line"><span class="comment"># 通过index选择数据</span></div><div class="line"><span class="keyword">print</span> series3[<span class="string">'Baidu'</span>]</div><div class="line"></div><div class="line"><span class="comment"># 选择多个数据</span></div><div class="line"><span class="keyword">print</span> series3[[<span class="string">'Baidu'</span>, <span class="string">'Tecent'</span>]]</div></pre></td></tr></table></figure>
<pre><code>400
Baidu     400
Tecent    600
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 根据条件选择数据</span></div><div class="line"><span class="keyword">print</span> series3[series3 &lt; <span class="number">500</span>]</div></pre></td></tr></table></figure>
<pre><code>Baidu       400
Jingdong    300
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># 条件选择原理</span></div><div class="line"><span class="keyword">print</span> series3 &lt; <span class="number">500</span></div><div class="line">temp = series3 &lt; <span class="number">500</span></div><div class="line"><span class="keyword">print</span> series3[temp]</div></pre></td></tr></table></figure>
<pre><code>Alibaba     False
Baidu        True
Jingdong     True
Tecent      False
dtype: bool
Baidu       400
Jingdong    300
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Series元素赋值</span></div><div class="line"><span class="keyword">print</span> <span class="string">'old value: '</span>, series3[<span class="string">'Baidu'</span>]</div><div class="line">series3[<span class="string">'Baidu'</span>] = <span class="number">450</span></div><div class="line"><span class="keyword">print</span> <span class="string">'new value: '</span>, series3[<span class="string">'Baidu'</span>]</div></pre></td></tr></table></figure>
<pre><code>old value:  400
new value:  450
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 根据条件赋值</span></div><div class="line"><span class="keyword">print</span> <span class="string">'old series: '</span></div><div class="line"><span class="keyword">print</span> series3</div><div class="line">series3[series3 &lt; <span class="number">500</span>] = <span class="number">500</span></div><div class="line"><span class="keyword">print</span> <span class="string">'new series: '</span></div><div class="line"><span class="keyword">print</span> series3</div></pre></td></tr></table></figure>
<pre><code>old series: 
Alibaba     500
Baidu       400
Jingdong    300
Tecent      600
dtype: int64
new series: 
Alibaba     500
Baidu       500
Jingdong    500
Tecent      600
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Series数学运算</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Division: '</span></div><div class="line"><span class="keyword">print</span> series3 / <span class="number">2</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Square: '</span></div><div class="line"><span class="keyword">print</span> series3 ** <span class="number">2</span></div><div class="line"><span class="keyword">print</span> np.square(series3)</div></pre></td></tr></table></figure>
<pre><code>Division: 
Alibaba     250.0
Baidu       250.0
Jingdong    250.0
Tecent      300.0
dtype: float64
Square: 
Alibaba     250000
Baidu       250000
Jingdong    250000
Tecent      360000
dtype: int64
Alibaba     250000
Baidu       250000
Jingdong    250000
Tecent      360000
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义新的Series, 公司人数</span></div><div class="line">people = &#123;<span class="string">'Baidu'</span>: <span class="number">50000</span>, <span class="string">'Alibaba'</span>: <span class="number">45000</span>, <span class="string">'Tecent'</span>: <span class="number">60000</span>, <span class="string">'Jingdong'</span>: <span class="number">80000</span>, <span class="string">'Netease'</span>: <span class="number">30000</span>&#125;</div><div class="line">series4 = pd.Series(people)</div><div class="line"><span class="keyword">print</span> series4</div></pre></td></tr></table></figure>
<pre><code>Alibaba     45000
Baidu       50000
Jingdong    80000
Netease     30000
Tecent      60000
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Series相加, series3没有Netease, 因此结果为NaN</span></div><div class="line"><span class="keyword">print</span> series3 + series4</div></pre></td></tr></table></figure>
<pre><code>Alibaba     45500.0
Baidu       50500.0
Jingdong    80500.0
Netease         NaN
Tecent      60600.0
dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 判断数据是否数据缺失</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Netease'</span> <span class="keyword">in</span> series3</div><div class="line"><span class="keyword">print</span> <span class="string">'Baidu'</span> <span class="keyword">in</span> series3</div></pre></td></tr></table></figure>
<pre><code>False
True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 找出数据为null或非null的元素</span></div><div class="line">result = series3 + series4</div><div class="line"><span class="keyword">print</span> result.notnull()</div><div class="line"><span class="keyword">print</span> result.isnull()</div><div class="line"></div><div class="line"><span class="keyword">print</span> result[result.isnull()]</div><div class="line"><span class="keyword">print</span> result[result.isnull() != <span class="keyword">True</span>]</div></pre></td></tr></table></figure>
<pre><code>Alibaba      True
Baidu        True
Jingdong     True
Netease     False
Tecent       True
dtype: bool
Alibaba     False
Baidu       False
Jingdong    False
Netease      True
Tecent      False
dtype: bool
Netease   NaN
dtype: float64
Alibaba     45500.0
Baidu       50500.0
Jingdong    80500.0
Tecent      60600.0
dtype: float64
</code></pre>]]></content>
    
    <summary type="html">
    
      pandas总结(一)——Series的使用
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Unix、Posix和标准UniX规范</title>
    <link href="noahsnail.com/2017/06/03/2017-6-3-Unix%E3%80%81Posix%E5%92%8C%E6%A0%87%E5%87%86UniX%E8%A7%84%E8%8C%83/"/>
    <id>noahsnail.com/2017/06/03/2017-6-3-Unix、Posix和标准UniX规范/</id>
    <published>2017-06-03T10:06:44.000Z</published>
    <updated>2017-06-03T10:14:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>20世纪60年代是大型、复杂操作系统盛行的年代，比如IBM的OS/360和Honeywell的Multics系统。OS/360是历史上最成功的软件项目之一，而 Multics虽然持续存在了多年，却从来没有被广泛应用过。贝尔实验室曾经是Multics项目的最初参与者，但是因为考虑到该项目的复杂性和缺乏进展而于1969年退出。鉴于Mutics项目不愉快的经历，一群贝尔实验室的研究人员Ken Thompson、 Dennis Ritchie、 Doug Mcllroy和 Joe Ossanna，从1969年开始在DEC PDP-7计算机上完全用机器语言编写了一个简单得多的操作系统。这个新系统中的很多思想，比如层次文件系统、作为用户级进程的 shell概念，都是来自于 Multics，只不过在一个更小、更简单的程序包里实现。1970年，Brian Kernighan给新系统命名为“Unix”，这也是一个双关语，暗指“Multics”的复杂性。1973年用C重新编写其内核，1974年，Unix开始正式对外发布。</p>
<p>贝尔实验室以慷慨的条件向学校提供源代码，所以Unix在大专院校里获得了很多支持并得以持续发展。最有影响的工作发生在20世纪70年代晚期到80年代早期，在美国加州大学伯克利分校，研究人员在一系列发布版本中增加了虚拟内存和Internet协议，称为Unix4.xBSD(Berkeley Software Distribution)。与此同时，贝尔实验室也在发布自己的版本，称为System V Unix。其他厂商的版本，比如Sun Microsystems的Solaris系统,则是从这些原始的BSD和System V版本中衍生而来。</p>
<p>20世纪80年代中期，Unix厂商试图通过加入新的、往往不兼容的特性来使它们的程序与众不同，麻烦也就随之而来了。为了阻止这种趋势，IEEE(电气和电子工程师协会)开始努力标准化Unix的开发，后来由 Richard Stallman命名为“Posix”。结果就得到了一系列的标准，称作Posix标准。这套标准涵盖了很多方面，比如Unix系统调用的C语言接口、shell程序和工具、线程及网络编程。最近，一个被称为“标准Unix规范”的独立标准化工作已经与Posix一起创建了统一的Unix系统标准。这些标准化工作的结果是Unix版本之间的差异已经基本消失。</p>
<p>参考资料：</p>
<ol>
<li>深度理解计算机系统（P11）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Unix、Posix和标准UniX规范
    
    </summary>
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/categories/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>GNU项目</title>
    <link href="noahsnail.com/2017/06/03/2017-6-3-GNU%E9%A1%B9%E7%9B%AE/"/>
    <id>noahsnail.com/2017/06/03/2017-6-3-GNU项目/</id>
    <published>2017-06-03T09:57:06.000Z</published>
    <updated>2017-06-03T10:03:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>GCC是GNU(GNU是GNU’s Not Unix的缩写)项目开发出来的众多有用工具之。GNU项目是1984年由Richard Stallman发起的一个免税的慈善项目。该项目的目标非常宏大，就是开发出一个完整的类Unix的系统，其源代码能够不受限制地被修改和传播。GNU项目已经开发出了一个包含Unix操作系统的所有主要部件的环境，但内核除外，内核是由 Linux项目独立发展而来的。GNU环境包括 EMACS编辑器、GCC编译器、GDB调试器、汇编器、链接器、处理二进制文件的工具以及其他一些部件。GCC编译器已经发展到支持许多不同的语言，能够为许多不同的机器生成代码。支持的语言包括C、C++、 Fortran、Java、Pascal、面向对象C语言(Objective-C)和Ada。</p>
<p>GNU项目取得了非凡的成绩，但是却常常被忽略。现代开放源码运动(通常和Linux联系在一起)的思想起源是GNU项目中自由软件(free software)的概念。(此处的free为自由言论(free speech)中的“自由”之意，而非免费啤酒(free beer)中的“免费”之意。) 而且，Linux如此受欢迎在很大程度上还要归功于GNU工具，它们给Linux内核提供了环境。</p>
<p>参考资料：</p>
<ol>
<li>深度理解计算机系统（P4）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      GNU项目
    
    </summary>
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/categories/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>C语言的起源</title>
    <link href="noahsnail.com/2017/06/03/2017-6-3-C%E8%AF%AD%E8%A8%80%E7%9A%84%E8%B5%B7%E6%BA%90/"/>
    <id>noahsnail.com/2017/06/03/2017-6-3-C语言的起源/</id>
    <published>2017-06-03T09:43:06.000Z</published>
    <updated>2017-06-03T09:54:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>C语言是贝尔实验室的Dennis Ritchie于1969年~1973年间创建的。美国国家标准学会（American national standards institute，ANSI）在1989年颁布了ANSI C的标准，后来语言的标准化成了国际标准化组织（International Standards Organization，ISO)的责任。这些标准定义了C语言和一系列函数库,即所谓的<strong>C标准库</strong>。Kernighan和 Ritchie在他们的经典著作中描述了ANSI C，这本著作被人们满怀感情地称为“K8R”。用Ritchie的话来说，C语言是“古怪的、有缺陷的，但同时也是一个巨大的成功”。为什么会成功呢?</p>
<ul>
<li><p>C语言与Unⅸ操作系统关系密切。<br>C从一开始就是作为一种用于Unix系统的程府语言开发出来的。大部分Unix内核(操作系统的核心部分)，以及所有支撑工具和函数库都是用C语言编写的。20世纪70年代后期到80年代初期，Unix风行于高等院校，许多人开始接触C语言并喜欢上它。因为Unix几乎全部是用C编写的，它可以很方便地移植到新的机器上，这种特点为C和Unix赢得了更为广泛的支持。</p>
</li>
<li><p>C语言小而简单。<br>C语言的设计是由一个人而非一个协会掌控的，因此这是一个简洁明了、没有什么冗赘的设计。K&amp;R这本书用大量的例子和练习描述了完整的C语言及其标准库，而全书不过261页。C语言的简单使它相对而言易于学习，也易于移植到不同的计算机上。</p>
</li>
<li><p>C语言是为实践目的设计的。C语言是设计用来实现Unix操作系统的。后来其他人发现能够用这门语言无障碍地编写他们想要的程序。</p>
</li>
</ul>
<p>C语言是系统级编程的首选，同时它也非常适用于应用级程序的编写。然而，它也并非适用于所有的程序员和所有的情况。C语言的指针是造成程序员困惑和程序错误的一个常见原因。同时，C语言还缺乏对非常有用的抽象的显式支持，例如类、对象和异常，像C++和Java这样针对应用级程序的新程序语言解决了这些问题。</p>
<p>参考资料：</p>
<ol>
<li>深度理解计算机系统（P2，3）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      C语言的起源
    
    </summary>
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/categories/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Caffe神经网络结构汇总</title>
    <link href="noahsnail.com/2017/06/01/2017-6-1-Caffe%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%80%BB%E7%BB%93/"/>
    <id>noahsnail.com/2017/06/01/2017-6-1-Caffe网络结构总结/</id>
    <published>2017-06-01T07:16:35.000Z</published>
    <updated>2017-06-01T07:56:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>自2012年Alexnet赢得了ImageNet竞赛以来，深度学习（神经网络）得到了飞速发展，产生了许多的神经网络结构，本文主要总结Caffe中使用的神经网络(分类的神经网络)，本文的神经网络作者都使用Caffe训练过，并在Kaggle的Intel癌症预测比赛中进行了测试与使用（top 8%）。</p>
<h2 id="1-Alexnet"><a href="#1-Alexnet" class="headerlink" title="1. Alexnet"></a>1. Alexnet</h2><p>Alexnet，2012年ImageNet竞赛冠军，深度学习的里程碑。</p>
<p>网络结构地址：<a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet" target="_blank" rel="external">https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet</a></p>
<p>预训练模型地址：<a href="http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel" target="_blank" rel="external">http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel</a></p>
<h2 id="2-Squeezenet"><a href="#2-Squeezenet" class="headerlink" title="2. Squeezenet"></a>2. Squeezenet</h2><p>Squeezenet设计目标不是为了提高识别的准确率，而是希望简化网络复杂度。squeezenet的模型结构确实很小，没压缩的情况下才5M左右，而且识别的精度还可以。</p>
<p>网络结构地址：<a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="external">https://github.com/DeepScale/SqueezeNet</a></p>
<p>预训练模型地址：<a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="external">https://github.com/DeepScale/SqueezeNet</a></p>
<h2 id="3-VGG系列"><a href="#3-VGG系列" class="headerlink" title="3. VGG系列"></a>3. VGG系列</h2><p>VGG和GoogLenet是2014年imagenet竞赛的双雄，VGG主要分为VGG16和VGG19。其网络结构与预训练模型的地址如下：</p>
<p>VGG16的网络结构：<a href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md" target="_blank" rel="external">https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md</a></p>
<p>VGG16的预训练模型：<a href="http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel" target="_blank" rel="external"> http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel</a></p>
<p>VGG19的网络结构：<a href="https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md" target="_blank" rel="external">https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md</a></p>
<p>VGG19的预训练模型：<a href="http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel" target="_blank" rel="external">http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel</a></p>
<p>备注：上面的网络结构需要进行细微调整才能在Caffe中直接训练，主要是网络结构中的Type类型。</p>
<h2 id="4-Resnet系列"><a href="#4-Resnet系列" class="headerlink" title="4. Resnet系列"></a>4. Resnet系列</h2><p>Resnet网络，2015年ImageNet竞赛冠军，网络结构主要分为Resnet-50、Resnet-101、Resnet-152三种，当然也有一些其它的结构，例如Resnet-18，Resnet-14。</p>
<p>Github地址：<a href="https://github.com/KaimingHe/deep-residual-networks" target="_blank" rel="external">https://github.com/KaimingHe/deep-residual-networks</a></p>
<p>Resnet-50、Resnet-101、Resnet-152的网络结构及预训练模型的下载地址：<a href="https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&amp;id=4006CBB8476FF777%2117887&amp;cid=4006CBB8476FF777" target="_blank" rel="external">https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&amp;id=4006CBB8476FF777%2117887&amp;cid=4006CBB8476FF777</a></p>
<h2 id="5-Inception系列"><a href="#5-Inception系列" class="headerlink" title="5. Inception系列"></a>5. Inception系列</h2><p>Inception系列是Google发明的一系列神经网络结构。</p>
<p>Inception-v1：</p>
<p>Inception-v1，即大名鼎鼎的GoogLenet，2014年ImageNet竞赛冠军。</p>
<p>网络结构地址：<a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet" target="_blank" rel="external">https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet</a></p>
<p>预训练模型地址：<a href="http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel" target="_blank" rel="external">http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel</a></p>
<p>Inception-v2：</p>
<p>即Inception V1 + Batch Normalization。</p>
<p>网络结构地址：<a href="https://github.com/pertusa/InceptionBN-21K-for-Caffe" target="_blank" rel="external">https://github.com/pertusa/InceptionBN-21K-for-Caffe</a></p>
<p>预训练模型地址：<a href="http://www.dlsi.ua.es/~pertusa/deep/Inception21k.caffemodel" target="_blank" rel="external">http://www.dlsi.ua.es/~pertusa/deep/Inception21k.caffemodel</a></p>
<p>Inception-v3：</p>
<p>网络结构地址：<a href="https://pan.baidu.com/s/1boC0HEf#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1boC0HEf#list/path=%2F</a></p>
<p>预训练模型地址：<a href="https://pan.baidu.com/s/1boC0HEf#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1boC0HEf#list/path=%2F</a></p>
<p>Inception-v4：</p>
<p>网络结构地址：<a href="https://pan.baidu.com/s/1c6D150#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1c6D150#list/path=%2F</a></p>
<p>预训练模型地址：<a href="https://pan.baidu.com/s/1c6D150#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1c6D150#list/path=%2F</a></p>
<p>Inception-resnet-v2：</p>
<p>网络结构地址：<a href="https://pan.baidu.com/s/1jHPJCX4#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1jHPJCX4#list/path=%2F</a></p>
<p>预训练模型地址：<a href="https://pan.baidu.com/s/1jHPJCX4#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1jHPJCX4#list/path=%2F</a></p>
]]></content>
    
    <summary type="html">
    
      Caffe神经网络结构汇总
    
    </summary>
    
      <category term="Caffe" scheme="noahsnail.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="noahsnail.com/tags/Caffe/"/>
    
  </entry>
  
</feed>
