<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="noahsnail.com/"/>
  <updated>2017-10-12T11:06:02.000Z</updated>
  <id>noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Inception-V3论文翻译——中文版</title>
    <link href="noahsnail.com/2017/10/09/2017-10-9-Inception-V3%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/10/09/2017-10-9-Inception-V3论文翻译——中文版/</id>
    <published>2017-10-09T08:25:03.000Z</published>
    <updated>2017-10-12T11:06:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="Rethinking-the-Inception-Architecture-for-Computer-Vision"><a href="#Rethinking-the-Inception-Architecture-for-Computer-Vision" class="headerlink" title="Rethinking the Inception Architecture for Computer Vision"></a>Rethinking the Inception Architecture for Computer Vision</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>对许多任务而言，卷积网络是目前最新的计算机视觉解决方案的核心。从2014年开始，深度卷积网络开始变成主流，在各种基准数据集上都取得了实质性成果。对于大多数任务而言，虽然增加的模型大小和计算成本都趋向于转化为直接的质量收益（只要提供足够的标注数据去训练），但计算效率和低参数计数仍是各种应用场景的限制因素，例如移动视觉和大数据场景。目前，我们正在探索增大网络的方法，目标是通过适当的分解卷积和积极的正则化来尽可能地有效利用增加的计算。我们在ILSVRC 2012分类挑战赛的验证集上评估了我们的方法，结果证明我们的方法超过了目前最先进的方法并取得了实质性收益：对于单一框架评估错误率为：<code>21.2% top-1</code>和<code>5.6% top-5</code>，使用的网络计算代价为每次推断需要进行50亿次乘加运算并使用不到2500万的参数。通过四个模型组合和多次评估，我们报告了<code>3.5% top-5</code>和<code>17.3% top-1</code>的错误率。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>从2012年Krizhevsky等人[9]赢得了ImageNet竞赛[16]起，他们的网络“AlexNet”已经成功了应用到了许多计算机视觉任务中，例如目标检测[5]，分割[12]，行人姿势评估[22]，视频分类[8]，目标跟踪[23]和超分辨率[3]。</p>
<p>这些成功推动了一个新研究领域，这个领域主要专注于寻找更高效运行的卷积神经网络。从2014年开始，通过利用更深更宽的网络，网络架构的质量得到了明显改善。VGGNet[18]和GoogLeNet[20]在2014 ILSVRC [16]分类挑战上取得了类似的高性能。一个有趣的发现是在分类性能上的收益趋向于转换成各种应用领域上的显著质量收益。这意味着深度卷积架构上的架构改进可以用来改善大多数越来越多地依赖于高质量、可学习视觉特征的其它计算机视觉任务的性能。网络质量的改善也导致了卷积网络在新领域的应用，在AlexNet特征不能与手工精心设计的解决方案竞争的情况下，例如，检测时的候选区域生成[4]。</p>
<p>尽管VGGNet[18]具有架构简洁的强有力特性，但它的成本很高：评估网络需要大量的计算。另一方面，GoogLeNet[20]的Inception架构也被设计为在内存和计算预算严格限制的情况下也能表现良好。例如，GoogleNet只使用了500万参数，与其前身AlexNet相比减少了12倍，AlexNet使用了6000万参数。此外，VGGNet使用了比AlexNet大约多3倍的参数。</p>
<p>Inception的计算成本也远低于VGGNet或其更高性能的后继者[6]。这使得可以在大数据场景中[17]，[13]，在大量数据需要以合理成本处理的情况下或在内存或计算能力固有地受限情况下，利用Inception网络变得可行，例如在移动视觉设定中。通过应用针对内存使用的专门解决方案[2]，[15]或通过计算技巧优化某些操作的执行[10]，可以减轻部分这些问题。但是这些方法增加了额外的复杂性。此外，这些方法也可以应用于优化Inception架构，再次扩大效率差距。</p>
<p>然而，Inception架构的复杂性使得更难以对网络进行更改。如果单纯地放大架构，大部分的计算收益可能会立即丢失。此外，[20]并没有提供关于导致GoogLeNet架构的各种设计决策的贡献因素的明确描述。这使得它更难以在适应新用例的同时保持其效率。例如，如果认为有必要增加一些Inception模型的能力，将滤波器组大小的数量加倍的简单变换将导致计算成本和参数数量增加4倍。这在许多实际情况下可能会被证明是禁止或不合理的，尤其是在相关收益适中的情况下。在本文中，我们从描述一些一般原则和优化思想开始，对于以有效的方式扩展卷积网络来说，这被证实是有用的。虽然我们的原则不局限于Inception类型的网络，但是在这种情况下，它们更容易观察，因为Inception类型构建块的通用结构足够灵活，可以自然地合并这些约束。这通过大量使用降维和Inception模块的并行结构来实现，这允许减轻结构变化对邻近组件的影响。但是，对于这样做需要谨慎，因为应该遵守一些指导原则来保持模型的高质量。</p>
]]></content>
    
    <summary type="html">
    
      Inception-V3论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Inception-V3论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/10/09/2017-10-9-Inception-V3%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2017/10/09/2017-10-9-Inception-V3论文翻译——中英文对照/</id>
    <published>2017-10-09T08:24:27.000Z</published>
    <updated>2017-10-12T10:26:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="Rethinking-the-Inception-Architecture-for-Computer-Vision"><a href="#Rethinking-the-Inception-Architecture-for-Computer-Vision" class="headerlink" title="Rethinking the Inception Architecture for Computer Vision"></a>Rethinking the Inception Architecture for Computer Vision</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error and 17.3% top-1 error.</p>
<p>对许多任务而言，卷积网络是目前最新的计算机视觉解决方案的核心。从2014年开始，深度卷积网络开始变成主流，在各种基准数据集上都取得了实质性成果。对于大多数任务而言，虽然增加的模型大小和计算成本都趋向于转化为直接的质量收益（只要提供足够的标注数据去训练），但计算效率和低参数计数仍是各种应用场景的限制因素，例如移动视觉和大数据场景。目前，我们正在探索增大网络的方法，目标是通过适当的分解卷积和积极的正则化来尽可能地有效利用增加的计算。我们在ILSVRC 2012分类挑战赛的验证集上评估了我们的方法，结果证明我们的方法超过了目前最先进的方法并取得了实质性收益：对于单一框架评估错误率为：<code>21.2% top-1</code>和<code>5.6% top-5</code>，使用的网络计算代价为每次推断需要进行50亿次乘加运算并使用不到2500万的参数。通过四个模型组合和多次评估，我们报告了<code>3.5% top-5</code>和<code>17.3% top-1</code>的错误率。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Since the 2012 ImageNet competition [16] winning entry by Krizhevsky et al [9], their network “AlexNet” has been successfully applied to a larger variety of computer vision tasks, for example to object-detection [5], segmentation [12], human pose estimation [22], video classification [8], object tracking [23], and superresolution [3].</p>
<p>从2012年Krizhevsky等人[9]赢得了ImageNet竞赛[16]起，他们的网络“AlexNet”已经成功了应用到了许多计算机视觉任务中，例如目标检测[5]，分割[12]，行人姿势评估[22]，视频分类[8]，目标跟踪[23]和超分辨率[3]。</p>
<p>These successes spurred a new line of research that focused on finding higher performing convolutional neural networks. Starting in 2014, the quality of network architectures significantly improved by utilizing deeper and wider networks. VGGNet [18] and GoogLeNet [20] yielded similarly high performance in the 2014 ILSVRC [16] classification challenge. One interesting observation was that gains in the classification performance tend to transfer to significant quality gains in a wide variety of application domains. This means that architectural improvements in deep convolutional architecture can be utilized for improving performance for most other computer vision tasks that are increasingly reliant on high quality, learned visual features. Also, improvements in the network quality resulted in new application domains for convolutional networks in cases where AlexNet features could not compete with hand engineered, crafted solutions, e.g. proposal generation in detection[4].</p>
<p>这些成功推动了一个新研究领域，这个领域主要专注于寻找更高效运行的卷积神经网络。从2014年开始，通过利用更深更宽的网络，网络架构的质量得到了明显改善。VGGNet[18]和GoogLeNet[20]在2014 ILSVRC [16]分类挑战上取得了类似的高性能。一个有趣的发现是在分类性能上的收益趋向于转换成各种应用领域上的显著质量收益。这意味着深度卷积架构上的架构改进可以用来改善大多数越来越多地依赖于高质量、可学习视觉特征的其它计算机视觉任务的性能。网络质量的改善也导致了卷积网络在新领域的应用，在AlexNet特征不能与手工精心设计的解决方案竞争的情况下，例如，检测时的候选区域生成[4]。</p>
<p>Although VGGNet [18] has the compelling feature of architectural simplicity, this comes at a high cost: evaluating the network requires a lot of computation. On the other hand, the Inception architecture of GoogLeNet [20] was also designed to perform well even under strict constraints on memory and computational budget. For example, GoogleNet employed only 5 million parameters, which represented a 12× reduction with respect to its predecessor AlexNet, which used 60 million parameters. Furthermore, VGGNet employed about 3x more parameters than AlexNet.</p>
<p>尽管VGGNet[18]具有架构简洁的强有力特性，但它的成本很高：评估网络需要大量的计算。另一方面，GoogLeNet[20]的Inception架构也被设计为在内存和计算预算严格限制的情况下也能表现良好。例如，GoogleNet只使用了500万参数，与其前身AlexNet相比减少了12倍，AlexNet使用了6000万参数。此外，VGGNet使用了比AlexNet大约多3倍的参数。</p>
<p>The computational cost of Inception is also much lower than VGGNet or its higher performing successors [6]. This has made it feasible to utilize Inception networks in big-data scenarios[17], [13], where huge amount of data needed to be processed at reasonable cost or scenarios where memory or computational capacity is inherently limited, for example in mobile vision settings. It is certainly possible to mitigate parts of these issues by applying specialized solutions to target memory use [2], [15] or by optimizing the execution of certain operations via computational tricks [10]. However, these methods add extra complexity. Furthermore, these methods could be applied to optimize the Inception architecture as well, widening the efficiency gap again.</p>
<p>Inception的计算成本也远低于VGGNet或其更高性能的后继者[6]。这使得可以在大数据场景中[17]，[13]，在大量数据需要以合理成本处理的情况下或在内存或计算能力固有地受限情况下，利用Inception网络变得可行，例如在移动视觉设定中。通过应用针对内存使用的专门解决方案[2]，[15]或通过计算技巧优化某些操作的执行[10]，可以减轻部分这些问题。但是这些方法增加了额外的复杂性。此外，这些方法也可以应用于优化Inception架构，再次扩大效率差距。</p>
<p>Still, the complexity of the Inception architecture makes it more difficult to make changes to the network. If the architecture is scaled up naively, large parts of the computational gains can be immediately lost. Also, [20] does not provide a clear description about the contributing factors that lead to the various design decisions of the GoogLeNet architecture. This makes it much harder to adapt it to new use-cases while maintaining its efficiency. For example, if it is deemed necessary to increase the capacity of some Inception-style model, the simple transformation of just doubling the number of all filter bank sizes will lead to a 4x increase in both computational cost and number of parameters. This might prove prohibitive or unreasonable in a lot of practical scenarios, especially if the associated gains are modest. In this paper, we start with describing a few general principles and optimization ideas that that proved to be useful for scaling up convolution networks in efficient ways. Although our principles are not limited to Inception-type networks, they are easier to observe in that context as the generic structure of the Inception style building blocks is flexible enough to incorporate those constraints naturally. This is enabled by the generous use of dimensional reduction and parallel structures of the Inception modules which allows for mitigating the impact of structural changes on nearby components. Still, one needs to be cautious about doing so, as some guiding principles should be observed to maintain high quality of the models.</p>
<p>然而，Inception架构的复杂性使得更难以对网络进行更改。如果单纯地放大架构，大部分的计算收益可能会立即丢失。此外，[20]并没有提供关于导致GoogLeNet架构的各种设计决策的贡献因素的明确描述。这使得它更难以在适应新用例的同时保持其效率。例如，如果认为有必要增加一些Inception模型的能力，将滤波器组大小的数量加倍的简单变换将导致计算成本和参数数量增加4倍。这在许多实际情况下可能会被证明是禁止或不合理的，尤其是在相关收益适中的情况下。在本文中，我们从描述一些一般原则和优化思想开始，对于以有效的方式扩展卷积网络来说，这被证实是有用的。虽然我们的原则不局限于Inception类型的网络，但是在这种情况下，它们更容易观察，因为Inception类型构建块的通用结构足够灵活，可以自然地合并这些约束。这通过大量使用降维和Inception模块的并行结构来实现，这允许减轻结构变化对邻近组件的影响。但是，对于这样做需要谨慎，因为应该遵守一些指导原则来保持模型的高质量。</p>
<h2 id="2-General-Design-Principles"><a href="#2-General-Design-Principles" class="headerlink" title="2. General Design Principles"></a>2. General Design Principles</h2><p>Here we will describe a few design principles based on large-scale experimentation with various architectural choices with convolutional networks. At this point, the utility of the principles below are speculative and additional future experimental evidence will be necessary to assess their accuracy and domain of validity. Still, grave deviations from these principles tended to result in deterioration in the quality of the networks and fixing situations where those deviations were detected resulted in improved architectures in general.</p>
<ol>
<li><p>Avoid representational bottlenecks, especially early in the network. Feed-forward networks can be represented by an acyclic graph from the input layer(s) to the classifier or regressor. This defines a clear direction for the information flow. For any cut separating the inputs from the outputs, one can access the amount of information passing though the cut. One should avoid bottlenecks with extreme compression. In general the representation size should gently decrease from the inputs to the outputs before reaching the final representation used for the task at hand. Theoretically, information content can not be assessed merely by the dimensionality of the representation as it discards important factors like correlation structure; the dimensionality merely provides a rough estimate of information content.</p>
</li>
<li><p>Higher dimensional representations are easier to process locally within a network. Increasing the activations per tile in a convolutional network allows for more disentangled features. The resulting networks will train faster.</p>
</li>
<li><p>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. For example, before performing a more spread out (e.g. 3 × 3) convolution, one can reduce the dimension of the input representation before the spatial aggregation without expecting serious adverse effects. We hypothesize that the reason for that is the strong correlation between adjacent unit results in much less loss of information during dimension reduction, if the outputs are used in a spatial aggregation context. Given that these signals should be easily compressible, the dimension reduction even promotes faster learning.</p>
</li>
<li><p>Balance the width and depth of the network. Optimal performance of the network can be reached by balancing the number of filters per stage and the depth of the network. Increasing both the width and the depth of the network can contribute to higher quality networks. However, the optimal improvement for a constant amount of computation can be reached if both are increased in parallel. The computational budget should therefore be distributed in a balanced way between the depth and width of the network.</p>
</li>
</ol>
<p>Although these principles might make sense, it is not straightforward to use them to improve the quality of net- works out of box. The idea is to use them judiciously in ambiguous situations only.</p>
<h2 id="3-Factorizing-Convolutions-with-Large-Filter-Size"><a href="#3-Factorizing-Convolutions-with-Large-Filter-Size" class="headerlink" title="3. Factorizing Convolutions with Large Filter Size"></a>3. Factorizing Convolutions with Large Filter Size</h2><p>Much of the original gains of the GoogLeNet network [20] arise from a very generous use of dimension reduction. This can be viewed as a special case of factorizing convolutions in a computationally efficient manner. Consider for example the case of a 1 × 1 convolutional layer followed by a 3 × 3 convolutional layer. In a vision network, it is expected that the outputs of near-by activations are highly correlated. Therefore, we can expect that their activations can be reduced before aggregation and that this should result in similarly expressive local representations.</p>
<p>Here we explore other ways of factorizing convolutions in various settings, especially in order to increase the computational efficiency of the solution. Since Inception networks are fully convolutional, each weight corresponds to one multiplication per activation. Therefore, any reduction in computational cost results in reduced number of parameters. This means that with suitable factorization, we can end up with more disentangled parameters and therefore with faster training. Also, we can use the computational and memory savings to increase the filter-bank sizes of our network while maintaining our ability to train each model replica on a single computer.</p>
<h3 id="3-1-Factorization-into-smaller-convolutions"><a href="#3-1-Factorization-into-smaller-convolutions" class="headerlink" title="3.1. Factorization into smaller convolutions"></a>3.1. Factorization into smaller convolutions</h3><p>Convolutions with larger spatial filters (e.g. 5 × 5 or 7 × 7) tend to be disproportionally expensive in terms of computation. For example, a 5 × 5 convolution with n fil- ters over a grid with m filters is 25/9 = 2.78 times more computationally expensive than a 3 × 3 convolution with the same number of filters. Of course, a 5 × 5 filter can capture dependencies between signals between activations of units further away in the earlier layers, so a reduction of the geometric size of the filters comes at a large cost of expressiveness. However, we can ask whether a 5 × 5 convolution could be replaced by a multi-layer network with less parameters with the same input size and output depth. If we zoom into the computation graph of the 5 × 5 convolution, we see that each output looks like a small fully-connected network sliding over 5 × 5 tiles over its input (see Figure 1). Since we are constructing a vision network, it seems natural to exploit translation invariance again and replace the fully connected component by a two layer convolutional architecture: the first layer is a 3 × 3 convolution, the second is a fully connected layer on top of the 3 × 3 output grid of the first layer (see Figure 1). Sliding this small network over the input activation grid boils down to replacing the 5 × 5 convolution with two layers of 3 × 3 convolution (compare Figure 4 with 5).</p>
<p>This setup clearly reduces the parameter count by shar- ing the weights between adjacent tiles. To analyze the ex-pected computational cost savings, we will make a few sim-plifying assumptions that apply for the typical situations:We can assume that n = αm, that is that we want to change the number of activations/unit by a constant alpha factor. Since the 5 × 5 convolution is aggregating, α is typically slightly larger than one (around 1.5 in the case<br>of GoogLeNet). Having a two layer replacement for the 5 × 5 layer, it seems reasonable to reach this expansion in two steps: increasing the number of filters by √ α in both steps. In order to simplify our estimate by choosing α = 1 (no expansion), If we would naivly slide a network without reusing the computation between neighboring grid tiles, we would increase the computational cost. sliding this network can be represented by two 3 × 3 convolutional layers which reuses the activations between adjacent tiles. This way, we end up with a net 9+9 × reduction of computation, resulting 25 in a relative gain of 28% by this factorization. The exact same saving holds for the parameter count as each parame- ter is used exactly once in the computation of the activation of each unit. Still, this setup raises two general questions: Does this replacement result in any loss of expressiveness? If our main goal is to factorize the linear part of the compu- tation, would it not suggest to keep linear activations in the first layer? We have ran several control experiments (for ex- ample see figure 2) and using linear activation was always inferior to using rectified linear units in all stages of the fac- torization. We attribute this gain to the enhanced space of variations that the network can learn especially if we batch- normalize [7] the output activations. One can see similar effects when using linear activations for the dimension reduction components.</p>
<h3 id="3-2-Spatial-Factorization-into-Asymmetric-Convo-lutions"><a href="#3-2-Spatial-Factorization-into-Asymmetric-Convo-lutions" class="headerlink" title="3.2. Spatial Factorization into Asymmetric Convo- lutions"></a>3.2. Spatial Factorization into Asymmetric Convo- lutions</h3><p>The above results suggest that convolutions with filters larger 3 × 3 a might not be generally useful as they can always be reduced into a sequence of 3 × 3 convolutional layers. Still we can ask the question whether one should factorize them into smaller, for example 2 × 2 convolutions. However, it turns out that one can do even better than 2 × 2 by using asymmetric convolutions, e.g. n × 1. For example using a 3 × 1 convolution followed by a 1 × 3 convolution is equivalent to sliding a two layer network with the same receptive field as in a 3 × 3 convolution (see figure 3). Still the two-layer solution is 33% cheaper for the same number of output filters, if the number of input and output filters is equal. By comparison, factorizing a 3 × 3 convolution into a two 2 × 2 convolution represents only a 11% saving of computation.</p>
<p>In theory, we could go even further and argue that one can replace any n × n convolution by a 1 × n convolution followed by a n × 1 convolution and the computational cost saving increases dramatically as n grows (see figure 6). In practice, we have found that employing this factorization does not work well on early layers, but it gives very good re- sults on medium grid-sizes (On m × m feature maps, where m ranges between 12 and 20). On that level, very good re- sults can be achieved by using 1 × 7 convolutions followed by 7 × 1 convolutions.</p>
<h2 id="4-Utility-of-Auxiliary-Classifiers"><a href="#4-Utility-of-Auxiliary-Classifiers" class="headerlink" title="4. Utility of Auxiliary Classifiers"></a>4. Utility of Auxiliary Classifiers</h2><p>[20] has introduced the notion of auxiliary classifiers to improve the convergence of very deep networks. The origi- nal motivation was to push useful gradients to the lower lay- ers to make them immediately useful and improve the con- vergence during training by combating the vanishing gra- dient problem in very deep networks. Also Lee et al[11] argues that auxiliary classifiers promote more stable learn- ing and better convergence. Interestingly, we found that auxiliary classifiers did not result in improved convergence early in the training: the training progression of network with and without side head looks virtually identical before both models reach high accuracy. Near the end of training, the network with the auxiliary branches starts to overtake the accuracy of the network without any auxiliary branch and reaches a slightly higher plateau.</p>
<p>Also [20] used two side-heads at different stages in the network. The removal of the lower auxiliary branch did not have any adverse effect on the final quality of the network. Together with the earlier observation in the previous para-graph, this means that original the hypothesis of [20] that these branches help evolving the low-level features is most likely misplaced. Instead, we argue that the auxiliary clas- sifiers act as regularizer. This is supported by the fact that the main classifier of the network performs better if the side branch is batch-normalized [7] or has a dropout layer. This also gives a weak supporting evidence for the conjecture that batch normalization acts as a regularizer.</p>
<h2 id="5-Efficient-Grid-Size-Reduction"><a href="#5-Efficient-Grid-Size-Reduction" class="headerlink" title="5. Efficient Grid Size Reduction"></a>5. Efficient Grid Size Reduction</h2><p>Traditionally, convolutional networks used some pooling operation to decrease the grid size of the feature maps. In order to avoid a representational bottleneck, before apply-ing maximum or average pooling the activation dimension of the network filters is expanded. For example, starting a d×d grid with k filters, if we would like to arrive at a d × d 22 grid with 2k filters, we first need to compute a stride-1 con- volution with 2k filters and then apply an additional pooling step. This means that the overall computational cost is dom- inated by the expensive convolution on the larger grid using 2d2k2 operations. One possibility would be to switch to poolingwithconvolutionandthereforeresultingin2(d)2k2  reducing the computational cost by a quarter. However, this<br>creates a representational bottlenecks as the overall dimen-<br>sionality of the representation drops to ( d )2 k resulting in 2<br>less expressive networks (see Figure 9). Instead of doing so, we suggest another variant the reduces the computational cost even further while removing the representational bot- tleneck. (see Figure 10). We can use two parallel stride 2 blocks: P and C. P is a pooling layer (either average or maximum pooling) the activation, both of them are stride 2 the filter banks of which are concatenated as in figure 10.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane ́, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vie ́gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.</p>
<p>[2] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen. Compressing neural networks with the hashing trick. In Proceedings of The 32nd International Conference on Machine Learning, 2015.</p>
<p>[3] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In Computer Vision–ECCV 2014, pages 184–199. Springer, 2014.</p>
<p>[4] D.Erhan,C.Szegedy,A.Toshev,andD.Anguelov.Scalable object detection using deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 2155–2162. IEEE, 2014.</p>
<p>[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.</p>
<p>[6] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852, 2015.</p>
<p>[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning, pages 448–456, 2015.</p>
<p>[8] A.Karpathy,G.Toderici,S.Shetty,T.Leung,R.Sukthankar, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 1725–1732. IEEE, 2014.</p>
<p>[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012.</p>
<p>[10] A. Lavin. Fast algorithms for convolutional neural networks. arXiv preprint arXiv:1509.09308, 2015.</p>
<p>[11] C.-Y.Lee,S.Xie,P.Gallagher,Z.Zhang,andZ.Tu.Deeply-supervised nets. arXiv preprint arXiv:1409.5185, 2014. </p>
<p>[12] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431–3440, 2015.</p>
<p>[13] Y. Movshovitz-Attias, Q. Yu, M. C. Stumpe, V. Shet, S. Arnoud, and L. Yatziv. Ontological supervision for fine grained classification of street view storefronts. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1693–1702, 2015.</p>
<p>[14] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063, 2012.</p>
<p>[15] D. C. Psichogios and L. H. Ungar. Svd-net: an algorithm that automatically selects network structure. IEEE transactions on neural networks/a publication of the IEEE Neural Networks Council, 5(3):513–515, 1993.</p>
<p>[16] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. 2014.</p>
<p>[17] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified embedding for face recognition and clustering. arXiv preprint arXiv:1503.03832, 2015.</p>
<p>[18] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.</p>
<p>[19] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th<br>International Conference on Machine Learning (ICML-13), volume 28, pages 1139–1147. JMLR Workshop and Conference Proceedings, May 2013.</p>
<p>[20] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9, 2015.</p>
<p>[21] T. Tieleman and G. Hinton. Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012. Accessed: 2015-11-05.</p>
<p>[22] A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 1653–1660. IEEE, 2014.</p>
<p>[23] N. Wang and D.-Y. Yeung. Learning a deep compact image representation for visual tracking. In Advances in Neural Information Processing Systems, pages 809–817, 2013.</p>
]]></content>
    
    <summary type="html">
    
      Inception-V3论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Structuring Machine Learning Projects学习笔记(一)</title>
    <link href="noahsnail.com/2017/09/24/2017-9-24-Structuring%20Machine%20Learning%20Projects%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
    <id>noahsnail.com/2017/09/24/2017-9-24-Structuring Machine Learning Projects学习笔记(一)/</id>
    <published>2017-09-24T05:39:05.000Z</published>
    <updated>2017-09-25T13:01:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-Introduction-to-ML-Strategy"><a href="#1-Introduction-to-ML-Strategy" class="headerlink" title="1. Introduction to ML Strategy"></a>1. Introduction to ML Strategy</h2><h4 id="1-1-Why-ML-Strategy"><a href="#1-1-Why-ML-Strategy" class="headerlink" title="1.1 Why ML Strategy?"></a>1.1 Why ML Strategy?</h4><p>Teach you ways of analyzing a machine learning problem that will point you in the direction of the most promising things to try.</p>
<h4 id="1-2-Orthogonalization"><a href="#1-2-Orthogonalization" class="headerlink" title="1.2 Orthogonalization"></a>1.2 Orthogonalization</h4><p>Chain of assumptions in ML</p>
<ul>
<li>Fit training set well on cost function</li>
<li>Fit dev set well on cost function</li>
<li>Fit test set well on cost function</li>
<li>Performs well in real world</li>
</ul>
<p>Orthogonalization or orthogonality is a system design property that assures that modifying an instruction or a component of an algorithm will not create or propagate side effects to other components of the system. It becomes easier to verify the algorithms independently from one another, it reduces testing and development time.</p>
<p>When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal.</p>
<ol>
<li>Fit training set well in cost function<br>- If it doesn’t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.</li>
<li>Fit development set well on cost function<br>- If it doesn’t fit well, regularization or using bigger training set might help.</li>
<li>Fit test set well on cost function<br>- If it doesn’t fit well, the use of a bigger development set might help</li>
<li>Performs well in real world<br>- If it doesn’t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing.</li>
</ol>
<h2 id="2-Setting-up-your-goal"><a href="#2-Setting-up-your-goal" class="headerlink" title="2. Setting up your goal"></a>2. Setting up your goal</h2><h4 id="2-1-Single-number-evaluation-metric"><a href="#2-1-Single-number-evaluation-metric" class="headerlink" title="2.1 Single number evaluation metric"></a>2.1 Single number evaluation metric</h4><p>Set up a single real number evaluation metric for your problem.</p>
<p>Dev set + single number evaluation metric.</p>
<h4 id="2-2-Satisficing-and-Optimiziong-metric"><a href="#2-2-Satisficing-and-Optimiziong-metric" class="headerlink" title="2.2 Satisficing and Optimiziong metric"></a>2.2 Satisficing and Optimiziong metric</h4><p>multiple metrics</p>
<h4 id="2-3-Train-dev-test-distribution"><a href="#2-3-Train-dev-test-distribution" class="headerlink" title="2.3 Train/dev/test distribution"></a>2.3 Train/dev/test distribution</h4><h4 id="2-4-Size-of-the-dev-and-test-sets"><a href="#2-4-Size-of-the-dev-and-test-sets" class="headerlink" title="2.4 Size of the dev and test sets"></a>2.4 Size of the dev and test sets</h4><h4 id="2-5-When-to-change-dev-test-sets-and-metrics"><a href="#2-5-When-to-change-dev-test-sets-and-metrics" class="headerlink" title="2.5 When to change dev/test sets and metrics"></a>2.5 When to change dev/test sets and metrics</h4>]]></content>
    
    <summary type="html">
    
      Structuring Machine Learning Projects学习笔记(一)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(十)——卷积神经网络</title>
    <link href="noahsnail.com/2017/09/22/2017-9-22-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%8D%81)%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>noahsnail.com/2017/09/22/2017-9-22-PyTorch基本用法(十)——卷积神经网络/</id>
    <published>2017-09-22T12:24:41.000Z</published>
    <updated>2017-09-22T12:26:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torchvision</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 超参数定义</span></div><div class="line">EPOCH = <span class="number">1</span></div><div class="line">LR = <span class="number">0.01</span></div><div class="line">BATCH_SIZE = <span class="number">64</span></div><div class="line"></div><div class="line"><span class="comment"># 下载MNIST数据集</span></div><div class="line">train_data = torchvision.datasets.MNIST(</div><div class="line">    root = <span class="string">'./mnist/'</span>,</div><div class="line">    <span class="comment"># 是否是训练数据</span></div><div class="line">    train = <span class="keyword">True</span>,</div><div class="line">    <span class="comment"># 数据变换(0, 255) -&gt; (0, 1)</span></div><div class="line">    transform = torchvision.transforms.ToTensor(),</div><div class="line">    <span class="comment"># 是否下载MNIST数据</span></div><div class="line">    download = <span class="keyword">True</span></div><div class="line">)</div><div class="line"></div><div class="line">test_data = torchvision.datasets.MNIST(</div><div class="line">    root = <span class="string">'./mnist/'</span>,</div><div class="line">    <span class="comment"># 是否是训练数据</span></div><div class="line">    train = <span class="keyword">False</span>,</div><div class="line">    <span class="comment"># 数据变换(0, 255) -&gt; (0, 1)</span></div><div class="line">    transform = torchvision.transforms.ToTensor(),</div><div class="line">    <span class="comment"># 是否下载MNIST数据</span></div><div class="line">    download = <span class="keyword">True</span></div><div class="line">)</div><div class="line"></div><div class="line"><span class="keyword">print</span> train_data.train_data.size()</div><div class="line"><span class="keyword">print</span> train_data.train_labels.size()</div><div class="line"><span class="keyword">print</span> test_data.test_data.size()</div><div class="line"><span class="keyword">print</span> test_data.test_labels.size()</div></pre></td></tr></table></figure>
<pre><code>torch.Size([60000, 28, 28])
torch.Size([60000])
torch.Size([10000, 28, 28])
torch.Size([10000])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 查看图像</span></div><div class="line">plt.imshow(train_data.train_data[<span class="number">0</span>].numpy(), cmap = <span class="string">'gray'</span>)</div><div class="line">plt.title(<span class="string">'%i'</span> % train_data.train_labels[<span class="number">0</span>])</div><div class="line">plt.show()</div><div class="line"></div><div class="line">plt.imshow(test_data.test_data[<span class="number">0</span>].numpy(), cmap = <span class="string">'gray'</span>)</div><div class="line">plt.title(<span class="string">'%i'</span> % test_data.test_labels[<span class="number">0</span>])</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/cnn_1_0.png" alt="png"></p>
<p><img src="http://ocs628urt.bkt.clouddn.com/cnn_1_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 数据加载</span></div><div class="line">train_loader = Data.DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = <span class="keyword">True</span>, num_workers = <span class="number">2</span>)</div><div class="line">test_loader = Data.DataLoader(dataset = test_data, batch_size = BATCH_SIZE, shuffle = <span class="keyword">False</span>, num_workers = <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义卷积神经网络</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(CNN, self).__init__()</div><div class="line">        self.conv1 = nn.Sequential(</div><div class="line">            nn.Conv2d(</div><div class="line">                in_channels = <span class="number">1</span>,</div><div class="line">                out_channels = <span class="number">16</span>,</div><div class="line">                kernel_size = <span class="number">5</span>,</div><div class="line">                stride = <span class="number">1</span>,</div><div class="line">                padding = <span class="number">2</span></div><div class="line">            ),</div><div class="line">            nn.ReLU(),</div><div class="line">            nn.MaxPool2d(kernel_size = <span class="number">2</span>)</div><div class="line">        )</div><div class="line">        <span class="comment"># conv1输出为(16, 14, 14)</span></div><div class="line">        self.conv2 = nn.Sequential(</div><div class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</div><div class="line">            nn.ReLU(),</div><div class="line">            nn.MaxPool2d(<span class="number">2</span>)</div><div class="line">        )</div><div class="line">        <span class="comment"># conv2输出为(32, 7, 7)</span></div><div class="line">        self.output = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.conv1(x)</div><div class="line">        x = self.conv2(x)</div><div class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        prediction = self.output(x)</div><div class="line">        <span class="keyword">return</span> prediction</div><div class="line"></div><div class="line">cnn = CNN()</div><div class="line"><span class="keyword">print</span> cnn</div></pre></td></tr></table></figure>
<pre><code>CNN (
  (conv1): Sequential (
    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU ()
    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  )
  (conv2): Sequential (
    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU ()
    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  )
  (output): Linear (1568 -&gt; 10)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义优化器</span></div><div class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr = LR, betas= (<span class="number">0.9</span>, <span class="number">0.999</span>))</div><div class="line"></div><div class="line"><span class="comment"># 定义损失函数</span></div><div class="line">loss_func = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(EPOCH):</div><div class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(train_loader):</div><div class="line">        x_var = Variable(x)</div><div class="line">        y_var = Variable(y)</div><div class="line">        prediction = cnn(x_var)</div><div class="line">        loss = loss_func(prediction, y_var)</div><div class="line">        optimizer.zero_grad()</div><div class="line">        loss.backward()</div><div class="line">        optimizer.step()</div><div class="line">        </div><div class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            correct = <span class="number">0.0</span></div><div class="line">            <span class="keyword">for</span> step_test, (test_x, test_y) <span class="keyword">in</span> enumerate(test_loader):</div><div class="line">                test_x = Variable(test_x)</div><div class="line">                test_output = cnn(test_x)</div><div class="line">                pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.squeeze()</div><div class="line">                correct += sum(pred_y == test_y)</div><div class="line">            accuracy = correct / test_data.test_data.size(<span class="number">0</span>)</div><div class="line">            <span class="keyword">print</span> <span class="string">'Epoch: '</span>, epoch, <span class="string">'| train loss: %.4f'</span> % loss.data[<span class="number">0</span>], <span class="string">'| accuracy: '</span>, accuracy</div></pre></td></tr></table></figure>
<pre><code>Epoch:  0 | train loss: 2.2787 | accuracy:  0.0982
Epoch:  0 | train loss: 0.0788 | accuracy:  0.9592
Epoch:  0 | train loss: 0.0587 | accuracy:  0.9626
Epoch:  0 | train loss: 0.0188 | accuracy:  0.9745
Epoch:  0 | train loss: 0.0707 | accuracy:  0.9759
Epoch:  0 | train loss: 0.0564 | accuracy:  0.9775
Epoch:  0 | train loss: 0.0489 | accuracy:  0.9779
Epoch:  0 | train loss: 0.0925 | accuracy:  0.9791
Epoch:  0 | train loss: 0.0566 | accuracy:  0.9834
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(十)——卷积神经网络
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improving Deep Neural Networks学习笔记(三)</title>
    <link href="noahsnail.com/2017/09/22/2017-9-22-Improving%20Deep%20Neural%20Networks%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%89)/"/>
    <id>noahsnail.com/2017/09/22/2017-9-22-Improving Deep Neural Networks学习笔记(三)/</id>
    <published>2017-09-21T23:59:41.000Z</published>
    <updated>2017-09-25T08:13:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="5-Hyperparameter-tuning"><a href="#5-Hyperparameter-tuning" class="headerlink" title="5. Hyperparameter tuning"></a>5. Hyperparameter tuning</h2><h4 id="5-1-Tuning-process"><a href="#5-1-Tuning-process" class="headerlink" title="5.1 Tuning process"></a>5.1 Tuning process</h4><p>Hyperparameters:</p>
<p>$\alpha$, $\beta$, $\beta_1,\beta_2, \epsilon$, layers, hidden units, learning rate decay, mini-batch size.</p>
<p>The learning rate is the most important hyperparameter to tune. $\beta$, mini-batch size and hidden units is second in importance to tune.</p>
<p>Try random values: Don’t use a grid. Corarse to fine.</p>
<h4 id="5-2-Using-an-appropriate-scale-to-pick-hyperparameters"><a href="#5-2-Using-an-appropriate-scale-to-pick-hyperparameters" class="headerlink" title="5.2 Using an appropriate scale to pick hyperparameters"></a>5.2 Using an appropriate scale to pick hyperparameters</h4><p>Appropriate scale to hyperparameters:</p>
<p>$\alpha = [0.0001, 1]$, r = -4 * np.random.rand(), $\alpha = 10^r$.</p>
<p>If $\alpha = [10^a, 10^b]$, random pick from [a, b] uniformly, and set $\alpha = 10^r$.</p>
<p>Hyperparameters for exponentially weighted average</p>
<p>$\beta = [0.9, 0.999]$, don’t random pick from $[0.9, 0.999]$. Use $1-\beta = [0.001, 0.1]$, use similar method lik $\alpha$.</p>
<p>Why don’t use linear pick? Because when $\beta$ is close one, even if a little change, it will have a huge impact on algorithm.</p>
<h4 id="5-3-Hyperparameters-tuning-in-practice-Pandas-vs-Caviar"><a href="#5-3-Hyperparameters-tuning-in-practice-Pandas-vs-Caviar" class="headerlink" title="5.3 Hyperparameters tuning in practice: Pandas vs Caviar"></a>5.3 Hyperparameters tuning in practice: Pandas vs Caviar</h4><ul>
<li><p>Re-test hyperparamters occasionally</p>
</li>
<li><p>Babysitting one model(Pandas)</p>
</li>
<li><p>Training many models in parallel(Caviar)</p>
</li>
</ul>
<h2 id="6-Batch-Normalization"><a href="#6-Batch-Normalization" class="headerlink" title="6. Batch Normalization"></a>6. Batch Normalization</h2><h4 id="6-1-Normalizing-activations-in-a-network"><a href="#6-1-Normalizing-activations-in-a-network" class="headerlink" title="6.1 Normalizing activations in a network"></a>6.1 Normalizing activations in a network</h4><p>In logistic regression, normalizing inputs to speed up learning.</p>
<ol>
<li>compute means$\mu = \frac {1} {m} \sum_{i=1}^n x^{(i)}$</li>
<li>subtract off the means from training set $x = x - \mu$\</li>
<li>compute the variances $\sigma ^2 = \frac {1} {m} \sum_{i=1}^n {x^{(i)}}^2$</li>
<li>normalize training set $X = \frac {X} {\sigma ^2}$</li>
</ol>
<p>Similarly, in order to speed up training neural network, we can normalize intermediate values in layers（<code>z</code> in hidden layer）, it is called Batch Normalization or Batch Norm.</p>
<p>Implementing Batch Norm</p>
<ol>
<li>Given some intermediate value in neural network, $z^{(1)}, z^{(2)},…,z^{(m)}$</li>
<li>compute means $\mu = \frac {1} {m} \sum_{i=1} z^{(i)}$</li>
<li>compute the variances $\sigma ^2 = \frac {1} {m} \sum_{i=1} (z^{(i)} - \mu)^2$</li>
<li>normalize $z$, $z^{(i)} = \frac {z^{(i)} - \mu} {\sqrt {(\sigma ^2 + \epsilon)}}$</li>
<li>compute $\hat z$, $\hat z = \gamma z^{(i)} + \beta$.</li>
</ol>
<p>Now we have normalized Z to have mean zero and standard unit variance. But maybe it makes sense for hidden units to have a different distribution. So we use $\hat z$ instead of $z$, $\gamma$ and $\beta$ are learnable parameters of your model.</p>
<h4 id="6-2-Fitting-Batch-Norm-into-a-neural-network"><a href="#6-2-Fitting-Batch-Norm-into-a-neural-network" class="headerlink" title="6.2 Fitting Batch Norm into a neural network"></a>6.2 Fitting Batch Norm into a neural network</h4><p>Add Batch Norm to a network</p>
<p>$X \rightarrow Z^{[1]} \rightarrow {\hat Z^{[1]}} \rightarrow {a^{[1]}} \rightarrow Z^{[2]} \rightarrow {\hat Z^{[2]}} \rightarrow {a^{[2]}}…$</p>
<p>Parameters:<br>$W^{[1]}, b^{[1]}$, $W^{[2]}, b^{[2]}…$<br>$\gamma^{[1]}, \beta^{[1]}$, $\gamma^{[2]}, \beta^{[2]}…$</p>
<p>If you use Batch Norm, you need to computing means and subtracting means, so $b^{[i]}$ is useless, so we can set $b^{[i]} = 0$ permanently.</p>
<h4 id="6-3-Why-does-Batch-Norm-work"><a href="#6-3-Why-does-Batch-Norm-work" class="headerlink" title="6.3 Why does Batch Norm work?"></a>6.3 Why does Batch Norm work?</h4><p>Covariate Shift: You have learned a function from $x \rightarrow y$, it works well. If the distribution of $x$ changes, you need to learn a new function to make it work well.</p>
<p>Hidden unit values change all the time, and so it’s suffering from the problem of covariate.</p>
<p>Batch Norm as regularization</p>
<ul>
<li>Each mini-batch is scaled by the mean/variance computed on just that mini-batch.</li>
<li>This adds some noise to the values $z^{[l]}$ within that mini-batch. So similar to dropout, it adds some noise to each hidden layer’s activations.</li>
<li>This has a slight regularization effect.</li>
</ul>
<h4 id="6-4-Batch-Norm-at-test-time"><a href="#6-4-Batch-Norm-at-test-time" class="headerlink" title="6.4 Batch Norm at test time"></a>6.4 Batch Norm at test time</h4><p>In order to apply neural network at test time, come up with some seperate estimate of mu and sigma squared.</p>
<h2 id="7-Multi-class-classification"><a href="#7-Multi-class-classification" class="headerlink" title="7. Multi-class classification"></a>7. Multi-class classification</h2><h4 id="7-1-Softmax-regression"><a href="#7-1-Softmax-regression" class="headerlink" title="7.1 Softmax regression"></a>7.1 Softmax regression</h4><h4 id="7-2-Training-a-softmax-classifier"><a href="#7-2-Training-a-softmax-classifier" class="headerlink" title="7.2 Training a softmax classifier"></a>7.2 Training a softmax classifier</h4><p>Hard max. </p>
<p>Loss function.</p>
<p>Gradient descent with softmax.</p>
<h2 id="8-Programming-Frameworks"><a href="#8-Programming-Frameworks" class="headerlink" title="8. Programming Frameworks"></a>8. Programming Frameworks</h2><h4 id="8-1-Deep-Learning-frameworks"><a href="#8-1-Deep-Learning-frameworks" class="headerlink" title="8.1 Deep Learning frameworks"></a>8.1 Deep Learning frameworks</h4><ul>
<li>Caffe/Caffe2</li>
<li>TensorFlow</li>
<li>Torch</li>
<li>Theano</li>
<li>mxnet</li>
<li>PaddlePaddle</li>
<li>Keras</li>
<li>CNTK</li>
</ul>
<p>Choosing deep learning frameworks</p>
<ul>
<li>Ease of programming (development and deployment)</li>
<li>Running speed</li>
<li>Truly open (open source with good governance)</li>
</ul>
<h4 id="8-2-TensorFlow"><a href="#8-2-TensorFlow" class="headerlink" title="8.2 TensorFlow"></a>8.2 TensorFlow</h4><p>…</p>
]]></content>
    
    <summary type="html">
    
      Improving Deep Neural Networks学习笔记(三)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(九)——优化器</title>
    <link href="noahsnail.com/2017/09/21/2017-9-21-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B9%9D)%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    <id>noahsnail.com/2017/09/21/2017-9-21-PyTorch基本用法(九)——优化器/</id>
    <published>2017-09-21T11:32:17.000Z</published>
    <updated>2017-09-21T11:34:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义超参数</span></div><div class="line">LR = <span class="number">0.01</span></div><div class="line">BATCH_SIZE = <span class="number">32</span></div><div class="line">EPOCH = <span class="number">10</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1000</span>), dim = <span class="number">1</span>)</div><div class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.1</span>  * torch.normal(torch.zeros(x.size()))</div><div class="line"></div><div class="line"><span class="comment"># 绘制数据图像</span></div><div class="line">plt.scatter(x.numpy(), y.numpy())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/opt_0_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义数据库</span></div><div class="line">dataset = Data.TensorDataset(data_tensor = x, target_tensor = y)</div><div class="line"></div><div class="line"><span class="comment"># 定义数据加载器</span></div><div class="line">loader = Data.DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = <span class="keyword">True</span>, num_workers = <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义pytorch网络</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, n_hidden, n_output)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.hidden = torch.nn.Linear(n_features, n_hidden)</div><div class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.relu(self.hidden(x))</div><div class="line">        y = self.predict(x)</div><div class="line">        <span class="keyword">return</span> y</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义不同的优化器网络</span></div><div class="line">net_SGD = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line">net_Momentum = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line">net_RMSprop = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line">net_Adam = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 选择不同的优化方法</span></div><div class="line">opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr = LR)</div><div class="line">opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr = LR, momentum = <span class="number">0.9</span>)</div><div class="line">opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr = LR, alpha = <span class="number">0.9</span>)</div><div class="line">opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr = LR, betas= (<span class="number">0.9</span>, <span class="number">0.99</span>))</div><div class="line"></div><div class="line">nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]</div><div class="line">optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</div><div class="line"></div><div class="line"><span class="comment"># 选择损失函数</span></div><div class="line">loss_func = torch.nn.MSELoss()</div><div class="line"></div><div class="line"><span class="comment"># 不同方法的loss</span></div><div class="line">loss_SGD = []</div><div class="line">loss_Momentum = []</div><div class="line">loss_RMSprop =[]</div><div class="line">loss_Adam = []</div><div class="line"></div><div class="line"><span class="comment"># 保存所有loss</span></div><div class="line">losses = [loss_SGD, loss_Momentum, loss_RMSprop, loss_Adam]</div><div class="line"></div><div class="line"><span class="comment"># 执行训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(EPOCH):</div><div class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> enumerate(loader):</div><div class="line">        var_x = Variable(batch_x)</div><div class="line">        var_y = Variable(batch_y)</div><div class="line">        <span class="keyword">for</span> net, optimizer, loss_history <span class="keyword">in</span> zip(nets, optimizers, losses):</div><div class="line">            <span class="comment"># 对x进行预测</span></div><div class="line">            prediction = net(var_x)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = loss_func(prediction, var_y)</div><div class="line">            <span class="comment"># 每次迭代清空上一次的梯度</span></div><div class="line">            optimizer.zero_grad()</div><div class="line">            <span class="comment"># 反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新梯度</span></div><div class="line">            optimizer.step()</div><div class="line">            <span class="comment"># 保存loss记录</span></div><div class="line">            loss_history.append(loss.data[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 画图</span></div><div class="line">labels = [<span class="string">'SGD'</span>, <span class="string">'Momentum'</span>, <span class="string">'RMSprop'</span>, <span class="string">'Adam'</span>]</div><div class="line"><span class="keyword">for</span> i, loss_history <span class="keyword">in</span> enumerate(losses):</div><div class="line">    plt.plot(loss_history, label = labels[i])</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line">plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">plt.ylabel(<span class="string">'Loss'</span>)</div><div class="line">plt.ylim((<span class="number">0</span>, <span class="number">0.2</span>))</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/opt_3_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(九)——优化器
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(八)——批训练</title>
    <link href="noahsnail.com/2017/09/20/2017-9-20-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AB)%E2%80%94%E2%80%94%E6%89%B9%E8%AE%AD%E7%BB%83/"/>
    <id>noahsnail.com/2017/09/20/2017-9-20-PyTorch基本用法(八)——批训练/</id>
    <published>2017-09-20T13:30:52.000Z</published>
    <updated>2017-09-20T13:31:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 定义batch size</span></div><div class="line">BATCH_SIZE = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 定义数据</span></div><div class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>)</div><div class="line">y = torch.linspace(<span class="number">10</span>, <span class="number">1</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> x.numpy()</div><div class="line"><span class="keyword">print</span> y.numpy()</div></pre></td></tr></table></figure>
<pre><code>[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]
[ 10.   9.   8.   7.   6.   5.   4.   3.   2.   1.]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义数据库</span></div><div class="line">dataset = Data.TensorDataset(data_tensor = x, target_tensor = y)</div><div class="line"></div><div class="line"><span class="comment"># 定义数据加载器</span></div><div class="line">loader = Data.DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = <span class="keyword">True</span>, num_workers = <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 训练过程</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(<span class="number">5</span>):</div><div class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> enumerate(loader):</div><div class="line">        <span class="comment"># 训练过程</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'Epoch: '</span>, epoch, <span class="string">'| Step: '</span>, step, <span class="string">'| batch x: '</span>, batch_x.numpy(), <span class="string">'| betch y: '</span>, batch_y.numpy()</div></pre></td></tr></table></figure>
<pre><code>Epoch:  0 | Step:  0 | batch x:  [ 7.  4.  8.  5.  2.] | betch y:  [ 4.  7.  3.  6.  9.]
Epoch:  0 | Step:  1 | batch x:  [ 10.   6.   3.   1.   9.] | betch y:  [  1.   5.   8.  10.   2.]
Epoch:  1 | Step:  0 | batch x:  [  6.   7.  10.   1.   3.] | betch y:  [  5.   4.   1.  10.   8.]
Epoch:  1 | Step:  1 | batch x:  [ 9.  4.  5.  8.  2.] | betch y:  [ 2.  7.  6.  3.  9.]
Epoch:  2 | Step:  0 | batch x:  [ 5.  4.  7.  3.  8.] | betch y:  [ 6.  7.  4.  8.  3.]
Epoch:  2 | Step:  1 | batch x:  [  6.   9.   2.  10.   1.] | betch y:  [  5.   2.   9.   1.  10.]
Epoch:  3 | Step:  0 | batch x:  [  9.   1.   5.   3.  10.] | betch y:  [  2.  10.   6.   8.   1.]
Epoch:  3 | Step:  1 | batch x:  [ 8.  6.  4.  2.  7.] | betch y:  [ 3.  5.  7.  9.  4.]
Epoch:  4 | Step:  0 | batch x:  [ 10.   5.   9.   7.   3.] | betch y:  [ 1.  6.  2.  4.  8.]
Epoch:  4 | Step:  1 | batch x:  [ 6.  8.  2.  4.  1.] | betch y:  [  5.   3.   9.   7.  10.]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(八)——批训练
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(七)——模型的保存与加载</title>
    <link href="noahsnail.com/2017/09/20/2017-9-20-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%83)%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/"/>
    <id>noahsnail.com/2017/09/20/2017-9-20-PyTorch基本用法(七)——模型的保存与加载/</id>
    <published>2017-09-20T11:42:59.000Z</published>
    <updated>2017-09-20T12:24:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim = <span class="number">1</span>)</div><div class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())</div><div class="line"></div><div class="line"><span class="comment"># 变为Variable</span></div><div class="line">x, y = Variable(x), Variable(y)</div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line">net = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</div><div class="line">)</div><div class="line"></div><div class="line"><span class="keyword">print</span> net</div></pre></td></tr></table></figure>
<pre><code>Sequential (
  (0): Linear (1 -&gt; 10)
  (1): ReLU ()
  (2): Linear (10 -&gt; 1)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 选择优化方法</span></div><div class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># 选择损失函数</span></div><div class="line">loss_func = torch.nn.MSELoss()</div><div class="line"></div><div class="line"><span class="comment"># 训练网络</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</div><div class="line">    <span class="comment"># 对x进行预测</span></div><div class="line">    prediction = net(x)</div><div class="line">    <span class="comment"># 计算损失</span></div><div class="line">    loss = loss_func(prediction, y)</div><div class="line">    <span class="comment"># 每次迭代清空上一次的梯度</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    <span class="comment"># 反向传播</span></div><div class="line">    loss.backward()</div><div class="line">    <span class="comment"># 更新梯度</span></div><div class="line">    optimizer.step()</div><div class="line"></div><div class="line">plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.4f'</span> % loss.data[<span class="number">0</span>], fontdict=&#123;<span class="string">'size'</span>: <span class="number">10</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/model_save_1_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 保存训练的模型</span></div><div class="line"></div><div class="line"><span class="comment"># 保存整个网络和参数</span></div><div class="line">torch.save(net, <span class="string">'net.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 重新加载模型</span></div><div class="line">net = torch.load(<span class="string">'net.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 用新加载的模型进行预测</span></div><div class="line">prediction = net(x)</div><div class="line">plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/model_save_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 只保存网络的参数, 官方推荐的方式</span></div><div class="line">torch.save(net.state_dict(), <span class="string">'net_params.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line">net = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># 加载网络参数</span></div><div class="line">net.load_state_dict(torch.load(<span class="string">'net_params.pkl'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 用新加载的参数进行预测</span></div><div class="line">prediction = net(x)</div><div class="line">plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/model_save_3_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(七)——模型的保存与加载
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(六)——快速搭建网络</title>
    <link href="noahsnail.com/2017/09/20/2017-9-20PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AD)%E2%80%94%E2%80%94%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C/"/>
    <id>noahsnail.com/2017/09/20/2017-9-20PyTorch基本用法(六)——快速搭建网络/</id>
    <published>2017-09-20T11:19:11.000Z</published>
    <updated>2017-09-20T11:23:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 许多没解释的东西可以去查文档, 文档中都有, 已查过</span></div><div class="line"><span class="comment"># pytorch文档: http://pytorch.org/docs/master/index.html</span></div><div class="line"><span class="comment"># matplotlib文档: https://matplotlib.org/</span></div><div class="line"></div><div class="line"><span class="comment"># 随机算法的生成种子</span></div><div class="line">torch.manual_seed(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 类别一的数据</span></div><div class="line">x0 = torch.normal(<span class="number">2</span> * n_data, <span class="number">1</span>)</div><div class="line"><span class="comment"># 类别一的标签</span></div><div class="line">y0 = torch.zeros(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># 类别二的数据</span></div><div class="line">x1 = torch.normal(<span class="number">-2</span> * n_data, <span class="number">1</span>)</div><div class="line"><span class="comment"># 类别二的标签</span></div><div class="line">y1 = torch.ones(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># x0, x1连接起来, 按维度0连接, 并指定数据的类型</span></div><div class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.FloatTensor)</div><div class="line"><span class="comment"># y0, y1连接, 由于只有一维, 因此没有指定维度, torch中标签类型必须为LongTensor</span></div><div class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># x,y 转为变量, torch只支持变量的训练, 因为Variable中有grad</span></div><div class="line">x, y = Variable(x), Variable(y)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制数据散点图</span></div><div class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c = y.data.numpy(), s = <span class="number">100</span>, lw = <span class="number">0</span>, cmap = <span class="string">'RdYlGn'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/classification_1_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 快速搭建分类网络</span></div><div class="line">net = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(<span class="number">2</span>, <span class="number">10</span>),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">2</span>))</div><div class="line">print(net)</div></pre></td></tr></table></figure>
<pre><code>Sequential (
  (0): Linear (2 -&gt; 10)
  (1): ReLU ()
  (2): Linear (10 -&gt; 2)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义优化方法</span></div><div class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.02</span>)</div><div class="line"><span class="comment"># 定义损失函数</span></div><div class="line">loss_func = torch.nn.CrossEntropyLoss()</div><div class="line"></div><div class="line">plt.ion()</div><div class="line"></div><div class="line"><span class="comment"># 训练过程</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">100</span>):</div><div class="line">    prediction = net(x)</div><div class="line">    loss = loss_func(prediction, y)</div><div class="line"></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</div><div class="line">        plt.cla()</div><div class="line">        <span class="comment"># 获取概率最大的类别的索引</span></div><div class="line">        prediction = torch.max(F.softmax(prediction), <span class="number">1</span>)[<span class="number">1</span>]</div><div class="line">        <span class="comment"># 将输出结果变为一维</span></div><div class="line">        pred_y = prediction.data.numpy().squeeze()</div><div class="line">        target_y = y.data.numpy()</div><div class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c = pred_y, s = <span class="number">100</span>, lw = <span class="number">0</span>, cmap = <span class="string">'RdYlGn'</span>)</div><div class="line">        <span class="comment"># 计算准确率</span></div><div class="line">        accuracy = sum(pred_y == target_y) / <span class="number">200.0</span></div><div class="line">        plt.text(<span class="number">1.5</span>, <span class="number">-4</span>, <span class="string">'Accuracy=%.2f'</span> % accuracy, fontdict = &#123;<span class="string">'size'</span>: <span class="number">10</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</div><div class="line">        plt.pause(<span class="number">0.1</span>)</div><div class="line"></div><div class="line">plt.ioff()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/classification_4_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(六)——快速搭建网络
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(五)——分类</title>
    <link href="noahsnail.com/2017/09/19/2017-9-19-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%94)%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/"/>
    <id>noahsnail.com/2017/09/19/2017-9-19-PyTorch基本用法(五)——分类/</id>
    <published>2017-09-19T12:43:42.000Z</published>
    <updated>2017-09-19T12:46:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 许多没解释的东西可以去查文档, 文档中都有, 已查过</span></div><div class="line"><span class="comment"># pytorch文档: http://pytorch.org/docs/master/index.html</span></div><div class="line"><span class="comment"># matplotlib文档: https://matplotlib.org/</span></div><div class="line"></div><div class="line"><span class="comment"># 随机算法的生成种子</span></div><div class="line">torch.manual_seed(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 类别一的数据</span></div><div class="line">x0 = torch.normal(<span class="number">2</span> * n_data, <span class="number">1</span>)</div><div class="line"><span class="comment"># 类别一的标签</span></div><div class="line">y0 = torch.zeros(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># 类别二的数据</span></div><div class="line">x1 = torch.normal(<span class="number">-2</span> * n_data, <span class="number">1</span>)</div><div class="line"><span class="comment"># 类别二的标签</span></div><div class="line">y1 = torch.ones(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># x0, x1连接起来, 按维度0连接, 并指定数据的类型</span></div><div class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.FloatTensor)</div><div class="line"><span class="comment"># y0, y1连接, 由于只有一维, 因此没有指定维度, torch中标签类型必须为LongTensor</span></div><div class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># x,y 转为变量, torch只支持变量的训练, 因为Variable中有grad</span></div><div class="line">x, y = Variable(x), Variable(y)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制数据散点图</span></div><div class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c = y.data.numpy(), s = <span class="number">100</span>, lw = <span class="number">0</span>, cmap = <span class="string">'RdYlGn'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/classification_1_0.png" alt="png"></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 定义分类网络</div><div class="line">class Net(torch.nn.Module):</div><div class="line">    </div><div class="line">    def __init__(self, n_feature, n_hidden, n_output):</div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</div><div class="line">        self.prediction = torch.nn.Linear(n_hidden, n_output)</div><div class="line"></div><div class="line">    def forward(self, x)</div><div class="line">        x = F.relu(self.hidden(x))</div><div class="line">        x = self.prediction(x)</div><div class="line">        return x</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义网络</span></div><div class="line">net = Net(n_feature = <span class="number">2</span>, n_hidden = <span class="number">10</span>, n_output = <span class="number">2</span>)</div><div class="line">print(net)</div></pre></td></tr></table></figure>
<pre><code>Net (
  (hidden): Linear (2 -&gt; 10)
  (prediction): Linear (10 -&gt; 2)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义优化方法</span></div><div class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.02</span>)</div><div class="line"><span class="comment"># 定义损失函数</span></div><div class="line">loss_func = torch.nn.CrossEntropyLoss()</div><div class="line"></div><div class="line">plt.ion()</div><div class="line"></div><div class="line"><span class="comment"># 训练过程</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">100</span>):</div><div class="line">    prediction = net(x)</div><div class="line">    loss = loss_func(prediction, y)</div><div class="line"></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</div><div class="line">        plt.cla()</div><div class="line">        <span class="comment"># 获取概率最大的类别的索引</span></div><div class="line">        prediction = torch.max(F.softmax(prediction), <span class="number">1</span>)[<span class="number">1</span>]</div><div class="line">        <span class="comment"># 将输出结果变为一维</span></div><div class="line">        pred_y = prediction.data.numpy().squeeze()</div><div class="line">        target_y = y.data.numpy()</div><div class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c = pred_y, s = <span class="number">100</span>, lw = <span class="number">0</span>, cmap = <span class="string">'RdYlGn'</span>)</div><div class="line">        <span class="comment"># 计算准确率</span></div><div class="line">        accuracy = sum(pred_y == target_y) / <span class="number">200.0</span></div><div class="line">        plt.text(<span class="number">1.5</span>, <span class="number">-4</span>, <span class="string">'Accuracy=%.2f'</span> % accuracy, fontdict = &#123;<span class="string">'size'</span>: <span class="number">10</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</div><div class="line">        plt.pause(<span class="number">0.1</span>)</div><div class="line"></div><div class="line">plt.ioff()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/classification_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># torch.max用法</span></div><div class="line">a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</div><div class="line"><span class="keyword">print</span> a</div><div class="line"><span class="keyword">print</span> torch.max(a, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<pre><code>-1.8524 -1.0491  0.5382 -0.5129
 0.1233 -0.1821  2.1519 -1.4547
-1.0267  0.2644 -0.8832 -0.2647
 0.3944 -1.2512 -0.1158  0.5071
[torch.FloatTensor of size 4x4]

(
 0.5382
 2.1519
 0.2644
 0.5071
[torch.FloatTensor of size 4]
, 
 2
 2
 1
 3
[torch.LongTensor of size 4]
)
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(五)——分类
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(四)——回归</title>
    <link href="noahsnail.com/2017/09/19/2017-9-19-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%9B%9B)%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/"/>
    <id>noahsnail.com/2017/09/19/2017-9-19-PyTorch基本用法(四)——回归/</id>
    <published>2017-09-19T07:47:28.000Z</published>
    <updated>2017-09-19T07:49:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim = <span class="number">1</span>)</div><div class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())</div><div class="line"></div><div class="line"><span class="comment"># 变为Variable</span></div><div class="line">x, y = Variable(x), Variable(y)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制数据图像</span></div><div class="line">plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/output_1_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义pytorch网络</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, n_hidden, n_output)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.hidden = torch.nn.Linear(n_features, n_hidden)</div><div class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.relu(self.hidden(x))</div><div class="line">        y = self.predict(x)</div><div class="line">        <span class="keyword">return</span> y</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 构建网络</span></div><div class="line">net = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> net</div></pre></td></tr></table></figure>
<pre><code>Net (
  (hidden): Linear (1 -&gt; 10)
  (predict): Linear (10 -&gt; 1)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 选择优化方法</span></div><div class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># 选择损失函数</span></div><div class="line">loss_func = torch.nn.MSELoss()</div><div class="line"></div><div class="line">plt.ion()</div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">100</span>):</div><div class="line">    <span class="comment"># 对x进行预测</span></div><div class="line">    prediction = net(x)</div><div class="line">    <span class="comment"># 计算损失</span></div><div class="line">    loss = loss_func(prediction, y)</div><div class="line">    <span class="comment"># 每次迭代清空上一次的梯度</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    <span class="comment"># 反向传播</span></div><div class="line">    loss.backward()</div><div class="line">    <span class="comment"># 更新梯度</span></div><div class="line">    optimizer.step()</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</div><div class="line">        plt.cla()</div><div class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.4f'</span> % loss.data[<span class="number">0</span>], fontdict=&#123;<span class="string">'size'</span>: <span class="number">10</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</div><div class="line">        plt.pause(<span class="number">0.1</span>)</div><div class="line">plt.ioff()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/output_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># unsqueeze用法, 一维变二维</span></div><div class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</div><div class="line"><span class="keyword">print</span> x</div><div class="line"><span class="keyword">print</span> torch.unsqueeze(x, <span class="number">0</span>)</div><div class="line"><span class="keyword">print</span> torch.unsqueeze(x, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<pre><code> 1
 2
 3
 4
[torch.FloatTensor of size 4]


 1  2  3  4
[torch.FloatTensor of size 1x4]


 1
 2
 3
 4
[torch.FloatTensor of size 4x1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># rand用法, rand返回的是[0,1)之间的均匀分布</span></div><div class="line"><span class="keyword">print</span> torch.rand(<span class="number">4</span>)</div><div class="line"><span class="keyword">print</span> torch.rand(<span class="number">2</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure>
<pre><code> 0.8473
 0.2252
 0.0599
 0.0777
[torch.FloatTensor of size 4]


 0.2864  0.1693  0.1261
 0.9013  0.2009  0.9854
[torch.FloatTensor of size 2x3]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(四)——回归
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improving Deep Neural Networks学习笔记(二)</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-Improving%20Deep%20Neural%20Networks%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%BA%8C)/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-Improving Deep Neural Networks学习笔记(二)/</id>
    <published>2017-09-18T13:53:18.000Z</published>
    <updated>2017-09-21T14:32:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="4-Optimization-algorithms"><a href="#4-Optimization-algorithms" class="headerlink" title="4. Optimization algorithms"></a>4. Optimization algorithms</h2><h4 id="4-1-Mini-batch-gradient-descent"><a href="#4-1-Mini-batch-gradient-descent" class="headerlink" title="4.1 Mini-batch gradient descent"></a>4.1 Mini-batch gradient descent</h4><p>$x^{\{t\}}$，$y^{\{t\}}$ is used to index into different mini batches. $x^{[t]}$，$y^{[t]}$ is used to index into different layer. $x^{(t)}$，$y^{(t)}$ is used to index into different examples.</p>
<p>Batch gradient descent is to process entire training set at the same time. Mini-batch gradient descent is to process single mini batch $x^{\{t\}}$，$y^{\{t\}}$ at the same time.</p>
<p>Run forward propagation and back propagation once on mini batch is called one iteration.</p>
<p>Mini-batch gradient descent runs much faster than batch gradient descent.</p>
<h4 id="4-2-Understanding-mini-batch-gradient-descent"><a href="#4-2-Understanding-mini-batch-gradient-descent" class="headerlink" title="4.2 Understanding mini-batch gradient descent"></a>4.2 Understanding mini-batch gradient descent</h4><p>If mini-batch size = m, it’s batch gradient descend.<br>If mini-batch size = 1, it’s stochastic gradient descend.<br>In pracice, mini-batch size between 1 and m.</p>
<p>Batch gradient descend: too long per iteration.<br>Stochastic gradient descend: lose speed up from vectorization.<br>Mini-batch gradient descend: Faster learning, 1. vectorization 2. Make progress without needing to wait.</p>
<p>Choosing mini-batch size:</p>
<p>If small training set(m &lt;= 2000), use batch gradient descend.<br>Typical mini-batch size: 64, 128, 256, 512, 1024(rare).</p>
<h4 id="4-3-Exponentially-weighted-averages"><a href="#4-3-Exponentially-weighted-averages" class="headerlink" title="4.3 Exponentially weighted averages"></a>4.3 Exponentially weighted averages</h4><p>$$V_t = \beta V_{t-1} + (1-\beta)\theta_t$$</p>
<p>View $V_t$ as approximately averaging over $\frac {1} {1 - \beta}$.</p>
<p>It’s called moving average in the statistics literature.</p>
<p>$\beta = 0.9$：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/ewa_1.png" alt="Figure 1"></p>
<p>$\beta = 0.9(red)$，$\beta = 0.98(green)$，$\beta = 0.5(yellow)$：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/ewa_2.png" alt="Figure 2"></p>
<h4 id="4-4-Understanding-exponentially-weighted-averages"><a href="#4-4-Understanding-exponentially-weighted-averages" class="headerlink" title="4.4 Understanding exponentially weighted averages"></a>4.4 Understanding exponentially weighted averages</h4><p>$\theta$ is the temperature of the day.</p>
<p>$$v_{100} = 0.9v_{99} + 0.1 \theta_{100}$$$$v_{99} = 0.9v_{98} + 0.1 \theta_{99}$$$$…$$</p>
<p>So $$v_{100} = 0.1 * \theta _{100} + 0.1 * 0.9 * \theta _{99} + … + 0.1 * 0.9^{i} * \theta _{100-i} + …$$</p>
<p>Th coefficients is $$0.1 + 0.1 * 0.9 + 0.1 * 0.9^2 + …$$</p>
<p>All of these coefficients, add up to one or add up to very close to one. It is called bias correction.</p>
<p>$$(1 - \epsilon)^{\frac {1} {\epsilon}} \approx \frac {1} {e}$$ $$\frac {1} {e} \approx 0.3679$$</p>
<p>Implement exponentially weighted average:</p>
<p>$$v_0 = 0$$$$v_1 = \beta v_0 + (1- \beta) \theta _1$$$$v_2 = \beta v_1 + (1- \beta) \theta _2$$$$…$$</p>
<p>Exponentially weighted average takes very low memory.</p>
<h4 id="4-5-Bias-correction-in-exponentially-weighted-averages"><a href="#4-5-Bias-correction-in-exponentially-weighted-averages" class="headerlink" title="4.5 Bias correction in exponentially weighted averages"></a>4.5 Bias correction in exponentially weighted averages</h4><p>It’s not a very good estimate of the first several day’s temperature. Bias correction is used to mofity this estimate that makes it much better. The formula is: $$\frac {v_t} {1 - \beta^t} = \beta v_{t-1} + (1- \beta) \theta _t.$$</p>
<h4 id="4-6-Gradient-descent-with-momentum"><a href="#4-6-Gradient-descent-with-momentum" class="headerlink" title="4.6 Gradient descent with momentum"></a>4.6 Gradient descent with momentum</h4><p>Gradient descent with momentum almost always works faster than the standard gradient descent algorithm. The basic idea is to compute an exponentially weighted average of gradients, and then use that gradient to update weights instead.</p>
<p>On iteration t:</p>
<ol>
<li>compute $dw$, db on current mini-batch.</li>
<li>compute $v_{dw}$, $v_{db}$<br>$$v_{dw} = \beta v_{dw} + (1 - \beta)dw$$$$v_{db} = \beta v_{db} + (1 - \beta)db$$</li>
<li>update dw, db<br>$$w = w - \alpha v_{dw}$$$$b = b - \alpha v_{db}$$</li>
</ol>
<p>There are two hyperparameters, the most common value for $\beta$ is 0.9.</p>
<p>Another formula is $v_{dw} = \beta v_{dw} + dw$, you need to modify corresponding $\alpha$.</p>
<h4 id="4-7-RMSprop"><a href="#4-7-RMSprop" class="headerlink" title="4.7 RMSprop"></a>4.7 RMSprop</h4><p>RMSprop stands for root mean square prop, that can also speed up gradient descent.</p>
<p>On iteration t:</p>
<ol>
<li>compute $dw$, db on current mini-batch.</li>
<li>compute $s_{dw}$, $s_{db}$<br>$$s_{dw} = \beta s_{dw} + (1 - \beta){dw}^2$$$$s_{db} = \beta s_{db} + (1 - \beta){db}^2$$</li>
<li>update dw, db<br>$$w = w - \alpha \frac {dw} {\sqrt {s_{dw}}}$$$$b = b - \alpha \frac {db} {\sqrt {s_{db}}}$$</li>
</ol>
<p>In practice, in order to avoid $\sqrt {s_{dw}}$ being very close zero:</p>
<p>$$w = w - \alpha \frac {dw} {\sqrt {s_{dw}} + \epsilon}$$$$b = b - \alpha \frac {db} {\sqrt {s_{db}} + \epsilon}$$</p>
<p>Usually $$\epsilon = 10^{-8}$$</p>
<h4 id="4-8-Adam-optimization-algorithm"><a href="#4-8-Adam-optimization-algorithm" class="headerlink" title="4.8 Adam optimization algorithm"></a>4.8 Adam optimization algorithm</h4><p>$$v_{dw}=0, s_{dw}=0,v_{db},s_{db}=0$$</p>
<p>On iteration t:</p>
<p>$$v_{dw} = \beta_1 v_{dw} + (1 - \beta_1)dw$$$$v_{db} = \beta_1 v_{db} + (1 - \beta_1)db$$</p>
<p>$$s_{dw} = \beta_2 s_{dw} + (1 - \beta_2){dw}^2$$$$s_{db} = \beta_2 s_{db} + (1 - \beta_2){db}^2$$</p>
<p>Bias correction:</p>
<p>$$v_{dw}^{bc} = \frac {v_{dw}} {1 - \beta_1^t}, v_{db}^{bc} = \frac {v_{db}} {1 - \beta_1^t}$$$$s_{dw}^{bc} = \frac {s_{dw}} {1 - \beta_2^t}, s_{db}^{bc} = \frac {s_{db}} {1 - \beta_2^t}$$</p>
<p>Update weight:</p>
<p>$$w = w - \alpha \frac {v_{dw}^{bc}} {\sqrt {s_{dw}^{bc}} + \epsilon}$$$$b = b - \alpha \frac {v_{db}^{bc}} {\sqrt {s_{db}^{bc}} + \epsilon}$$</p>
<p>Adam combines the effect of gradient descent with momentum together with gradient descent with RMSprop. It’s a commonly used learning algorithm that is proven to be very effective for many different neural networks of a very wide variety of architectures.\</p>
<p>$\alpha$ needs to be tuned. $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$.</p>
<p>Adam stands for Adaptive Moment Estimation.</p>
<h4 id="4-9-Learning-rate-decay"><a href="#4-9-Learning-rate-decay" class="headerlink" title="4.9 Learning rate decay"></a>4.9 Learning rate decay</h4><p>Learning rate decay is slowly reduce the learning rate.</p>
<p>$$\alpha = \frac {1} {1 + {decay rate} * epochs} \alpha_0$$</p>
<p>$\alpha_0$ is the initial learning rate.</p>
<p>Other learning rate decay methods:</p>
<p>$\alpha = 0.95^{epochs}\alpha_0$, this is called exponentially decay.</p>
<p>$\alpha = \frac {k} {\sqrt {epochs} } \alpha_0$, $\alpha = \frac {k} {\sqrt t} \alpha_0$.</p>
<p>$\alpha = {\frac {1} {2}}^{epochs} \alpha _0$, this is called a discrete staircase.</p>
<h4 id="4-10-The-problem-of-local-optima"><a href="#4-10-The-problem-of-local-optima" class="headerlink" title="4.10 The problem of local optima"></a>4.10 The problem of local optima</h4><p>In very high-dimensional spaces you’re actually much more likely to run into a saddle point, rather than local optimum.</p>
<ul>
<li>Unlikely to get stuck in a bad local optima.</li>
<li>Plateaus can make learning slow.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Improving Deep Neural Networks学习笔记(二)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(三)——激活函数</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%89)%E2%80%94%E2%80%94%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(三)——激活函数/</id>
    <published>2017-09-18T12:25:07.000Z</published>
    <updated>2017-09-18T13:15:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> func</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义数据x</span></div><div class="line">x = torch.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)</div><div class="line">x = Variable(x)</div><div class="line">np_x = x.data.numpy()</div><div class="line"></div><div class="line"><span class="comment"># 通过激活函数处理x</span></div><div class="line">y_relu = func.relu(x).data.numpy()</div><div class="line">y_sigmoid = func.sigmoid(x).data.numpy()</div><div class="line">y_tanh = func.tanh(x).data.numpy()</div><div class="line">y_softmax = func.softplus(x).data.numpy()</div><div class="line"></div><div class="line"><span class="comment"># 绘制激活函数图</span></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">221</span>)</div><div class="line">plt.plot(np_x, y_relu, c = <span class="string">'red'</span>, label = <span class="string">'relu'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">222</span>)</div><div class="line">plt.plot(np_x, y_sigmoid, c = <span class="string">'red'</span>, label = <span class="string">'sigmoid'</span>)</div><div class="line">plt.ylim((<span class="number">0</span>, <span class="number">1</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">223</span>)</div><div class="line">plt.plot(np_x, y_tanh, c = <span class="string">'red'</span>, label = <span class="string">'tanh'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">224</span>)</div><div class="line">plt.plot(np_x, y_softmax, c = <span class="string">'red'</span>, label = <span class="string">'softmax'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/ac_func.png" alt="Figure"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(三)——激活函数
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(二)——Variable</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%8C)%E2%80%94%E2%80%94Variable/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(二)——Variable/</id>
    <published>2017-09-18T10:46:11.000Z</published>
    <updated>2017-09-18T12:24:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是PyTorch中Variable变量的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">tensor = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</div><div class="line"></div><div class="line"><span class="comment"># 定义Variable, requires_grad用来指定是否需要计算梯度</span></div><div class="line">variable = Variable(tensor, requires_grad = <span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> tensor</div><div class="line"><span class="keyword">print</span> variable</div></pre></td></tr></table></figure>
<pre><code> 1  2
 3  4
[torch.FloatTensor of size 2x2]

Variable containing:
 1  2
 3  4
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算x^2的均值</span></div><div class="line">tensor_mean = torch.mean(tensor * tensor)</div><div class="line">variable_mean = torch.mean(variable * variable)</div><div class="line"><span class="keyword">print</span> tensor_mean</div><div class="line"><span class="keyword">print</span> variable_mean</div></pre></td></tr></table></figure>
<pre><code>7.5
Variable containing:
 7.5000
[torch.FloatTensor of size 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># variable进行反向传播</span></div><div class="line"><span class="comment"># 梯度计算如下：</span></div><div class="line"><span class="comment"># variable_mean = 1/4 * sum(variable * variable)</span></div><div class="line"><span class="comment"># d(variable_mean)/d(variable) = 1/4 * 2 * variable = 1/2 * variable</span></div><div class="line">variable_mean.backward()</div><div class="line"></div><div class="line"><span class="comment"># 输出variable中的梯度</span></div><div class="line"><span class="keyword">print</span> variable.grad</div></pre></td></tr></table></figure>
<pre><code>Variable containing:
 0.5000  1.0000
 1.5000  2.0000
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># *表示逐元素点乘,不是矩阵乘法</span></div><div class="line"><span class="keyword">print</span> tensor * tensor</div><div class="line"><span class="keyword">print</span> variable * variable</div></pre></td></tr></table></figure>
<pre><code>  1   4
  9  16
[torch.FloatTensor of size 2x2]

Variable containing:
  1   4
  9  16
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输出variable中的data, data是tensor</span></div><div class="line"><span class="keyword">print</span> variable.data</div></pre></td></tr></table></figure>
<pre><code> 1  2
 3  4
[torch.FloatTensor of size 2x2]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(二)——Variable
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python性能优化</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-Python%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-Python性能优化/</id>
    <published>2017-09-18T02:15:08.000Z</published>
    <updated>2017-09-18T10:40:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Python使用非常方便、灵活，因此很受欢迎。但正因为如此，导致实现同一功能时，Python代码有很多写法，但不同的写法有不同的性能。因此写Python代码要有良好的习惯，多写高性能的代码。作者原来平常写Python代码也很随意，直到某天处理大量数据时半天看不到结果，究其原因，是Python代码的性能问题导致的。</p>
<h2 id="1-列表解析与列表重建"><a href="#1-列表解析与列表重建" class="headerlink" title="1. 列表解析与列表重建"></a>1. 列表解析与列表重建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">fr = open(<span class="string">'words.txt'</span>)</div><div class="line">t1 = time.time()</div><div class="line">word_list = fr.readlines()</div><div class="line">t2 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'read file time: '</span>, t2 -t1</div><div class="line">fr.close()</div><div class="line"></div><div class="line"><span class="comment"># for循环构建列表</span></div><div class="line">keywords = []</div><div class="line">t1 = time.time()</div><div class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_list:</div><div class="line">    word = word.strip()</div><div class="line">    keywords.append(word)</div><div class="line">t2 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'for loop time: '</span>, t2 - t1</div><div class="line"></div><div class="line"><span class="comment"># 列表解析</span></div><div class="line">t3 = time.time()</div><div class="line">keywords = [word.strip() <span class="keyword">for</span> word <span class="keyword">in</span> word_list]</div><div class="line">t4 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'list pars time: '</span>, t4 - t3</div><div class="line"></div><div class="line">fr = open(<span class="string">'words.txt'</span>)</div><div class="line">t5 = time.time()</div><div class="line">keywords = [word.strip() <span class="keyword">for</span> word <span class="keyword">in</span> fr.readlines()]</div><div class="line">t6 = time.time()</div><div class="line">fr.close()</div><div class="line"><span class="keyword">print</span> <span class="string">'read file and list parse time: '</span>, t6 - t5</div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'list length: '</span>, len(word_list)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">read file time:  0.0318450927734</div><div class="line">for loop time:  0.137716054916</div><div class="line">list pars time:  0.0910630226135</div><div class="line">read file and list parse time:  0.124923944473</div><div class="line">list length:  441669</div></pre></td></tr></table></figure>
<p>结论：本次测试中，列表解析时间是for循环时间的<code>2/3</code>。</p>
<h2 id="2-字符串拼接"><a href="#2-字符串拼接" class="headerlink" title="2. 字符串拼接"></a>2. 字符串拼接</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import time</div><div class="line"></div><div class="line">fr = open(&apos;words.txt&apos;)</div><div class="line">keywords = [word.strip() for word in fr.readlines()]</div><div class="line">fr.close()</div><div class="line"></div><div class="line"># 加号拼接字符串</div><div class="line">t1 = time.time()</div><div class="line">str1 = &apos;&apos;</div><div class="line">for word in keywords:</div><div class="line">    str1 += word</div><div class="line">t2 = time.time()</div><div class="line">print &apos;string concat time: &apos;, t2 - t1</div><div class="line"></div><div class="line"># join拼接字符串</div><div class="line">t1 = time.time()</div><div class="line">str2 = &apos;&apos;.join(keywords)</div><div class="line">t2 = time.time()</div><div class="line">print &apos;string join time: &apos;, t2 - t1</div><div class="line"></div><div class="line">print &apos;list length: &apos;, len(keywords)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">string concat time:  0.0814869403839</div><div class="line">string join time:  0.0123951435089</div><div class="line">list length:  441669</div></pre></td></tr></table></figure>
<p>结论：本次测试中，<code>join</code>函数拼接字符串比<code>+=</code>拼接字符串快<code>6倍多</code>。</p>
<h2 id="3-range与xrange"><a href="#3-range与xrange" class="headerlink" title="3. range与xrange"></a>3. range与xrange</h2><ul>
<li>range</li>
</ul>
<p>python中range会直接生成一个list对象。</p>
<ul>
<li>xrange</li>
</ul>
<p>用法与range完全相同，所不同的是生成的不是一个数组，而是一个生成器，它的类型为<code>xrange</code>。在生成非常大的数字序列时，xrange不会马上开辟很大的一块内存空间。如果不是需要返回列表，则尽可能使用<code>xrange</code>。</p>
<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import time</div><div class="line"></div><div class="line">t1 = time.time()</div><div class="line">for i in range(1000000):</div><div class="line">    pass</div><div class="line">t2 = time.time()</div><div class="line">print &apos;range time: &apos;, t2 -t1</div><div class="line"></div><div class="line">t1 = time.time()</div><div class="line">for i in xrange(1000000):</div><div class="line">    pass</div><div class="line">t2 = time.time()</div><div class="line">print &apos;xrange time: &apos;, t2 -t1</div></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">range time:  0.0680990219116</div><div class="line">xrange time:  0.0329170227051</div></pre></td></tr></table></figure>
<p>结论：本次测试中，<code>xrange</code>比<code>range</code>快一倍多。</p>
<h2 id="4-待续。"><a href="#4-待续。" class="headerlink" title="4. 待续。"></a>4. 待续。</h2>]]></content>
    
    <summary type="html">
    
      Python性能优化
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(一)——Numpy，Torch对比</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94Numpy%EF%BC%8CTorch%E5%AF%B9%E6%AF%94/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(一)——Numpy，Torch对比/</id>
    <published>2017-09-18T01:08:35.000Z</published>
    <updated>2017-09-18T03:47:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是对比Torch与Numpy的一些操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># numpy的array与torch的tensor的转换</span></div><div class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</div><div class="line">torch_data = torch.from_numpy(np_data)</div><div class="line">tensor2array = torch_data.numpy() </div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'numpy data: '</span>, np_data</div><div class="line"><span class="keyword">print</span> <span class="string">'torch data: '</span>, torch_data</div><div class="line"><span class="keyword">print</span> <span class="string">'tensor2array: '</span>, tensor2array</div></pre></td></tr></table></figure>
<pre><code>numpy data:  [[0 1 2]
 [3 4 5]]
torch data:  
 0  1  2
 3  4  5
[torch.LongTensor of size 2x3]

tensor2array:  [[0 1 2]
 [3 4 5]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Tensor的文档：http://pytorch.org/docs/master/tensors.html</span></div><div class="line">data = [<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</div><div class="line">float_data = torch.FloatTensor(data)</div><div class="line"><span class="keyword">print</span> float_data</div></pre></td></tr></table></figure>
<pre><code>-2
-1
 0
 1
 2
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># abs操作</span></div><div class="line"><span class="keyword">print</span> np.abs(data)</div><div class="line"><span class="keyword">print</span> torch.abs(float_data)</div></pre></td></tr></table></figure>
<pre><code>[2 1 0 1 2]

 2
 1
 0
 1
 2
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># sin操作</span></div><div class="line"><span class="keyword">print</span> np.sin(data)</div><div class="line"><span class="keyword">print</span> torch.sin(float_data)</div></pre></td></tr></table></figure>
<pre><code>[-0.90929743 -0.84147098  0.          0.84147098  0.90929743]

-0.9093
-0.8415
 0.0000
 0.8415
 0.9093
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># mean操作</span></div><div class="line"><span class="keyword">print</span> np.mean(data)</div><div class="line"><span class="keyword">print</span> torch.mean(float_data)</div></pre></td></tr></table></figure>
<pre><code>0.0
0.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 矩阵相乘</span></div><div class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</div><div class="line">tensor = torch.FloatTensor(data)</div><div class="line"></div><div class="line"><span class="keyword">print</span> np.matmul(data, data)</div><div class="line"><span class="comment"># torch.mm不支持广播形式</span></div><div class="line"><span class="keyword">print</span> torch.mm(tensor, tensor)</div><div class="line"><span class="comment"># torch.matmul支持广播形式</span></div><div class="line"><span class="keyword">print</span> torch.matmul(tensor, tensor)</div></pre></td></tr></table></figure>
<pre><code>[[ 7 10]
 [15 22]]

  7  10
 15  22
[torch.FloatTensor of size 2x2]


  7  10
 15  22
[torch.FloatTensor of size 2x2]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(一)——Numpy，Torch对比
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习——第一课笔记(上)</title>
    <link href="noahsnail.com/2017/09/17/2017-9-17-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E8%AF%BE/"/>
    <id>noahsnail.com/2017/09/17/2017-9-17-动手学深度学习——第一课/</id>
    <published>2017-09-17T10:09:47.000Z</published>
    <updated>2017-09-17T14:22:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是学习李沐直播课程的笔记。视频及内容的具体地址可参考：<a href="https://zhuanlan.zhihu.com/p/29125290" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29125290</a>。</p>
<h2 id="第一课：从上手到多类分类"><a href="#第一课：从上手到多类分类" class="headerlink" title="第一课：从上手到多类分类"></a>第一课：从上手到多类分类</h2><p>课程首先介绍了深度学习的很多应用：例如增强学习、物体识别、语音识别、机器翻译、推荐系统、广告点击预测等。</p>
<p>课程目的：通过动手实现来理解深度学习，跟工业界应用相比，主要只是数据规模和模型复杂度的区别。</p>
<p>深度学习的轮子很多，例如Caffe，TensorFlow，mxnet，PyTorch，CNTK等。它们之间的主要区别在于：1.便利的开发；2.方便的部署。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/chapter1_1.png" alt="Figure 1"></p>
<p>mxnet之上的一个package是Gluon，主要目的是一次解决开发和部署。课程主要分为以下三个部分：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/chapter1_2.png" alt="Figure 2"></p>
<h3 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1. 环境配置"></a>1. 环境配置</h3><p>我的配置环境是Mac，Linux平台类似。</p>
<p>mxnet安装命令如下，前提是已经安装好了Anaconda，Anaconda的安装可以参考官网：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install mxnet</div></pre></td></tr></table></figure>
<p>测试mxnet：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import mxnet</div><div class="line">&gt;&gt;&gt; print mxnet.__version__</div><div class="line">0.11.0</div></pre></td></tr></table></figure>
<p>然后安装notedown，运行Jupyter并加载notedown插件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install https://github.com/mli/notedown/tarball/master</div><div class="line">jupyter notebook --NotebookApp.contents_manager_class=&apos;notedown.NotedownContentsManager&apos;</div></pre></td></tr></table></figure>
<p>通过ExecutionTime插件来对每个cell的运行计时，国内使用豆瓣源。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pip install jupyter_contrib_nbextensions -i https://pypi.douban.com/simple</div><div class="line">jupyter contrib nbextension install --user</div><div class="line">jupyter nbextension enable execute_time/ExecuteTime</div></pre></td></tr></table></figure>
<h3 id="2-NDArray"><a href="#2-NDArray" class="headerlink" title="2. NDArray"></a>2. NDArray</h3><p>NDArray是MXNet储存和变换数据的主要工具，它与numpy非常类似。NDArray提供了CPU和GPU的异步计算，还提供了自动求导。NDArray的基本用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"></div><div class="line"><span class="comment"># 创建矩阵</span></div><div class="line">nd.zeros((<span class="number">3</span>, <span class="number">4</span>))</div><div class="line">x = nd.ones((<span class="number">3</span>, <span class="number">4</span>))</div><div class="line">nd.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>]])</div><div class="line">y = nd.random_normal(<span class="number">0</span>, <span class="number">1</span>, shape=(<span class="number">3</span>, <span class="number">4</span>))</div><div class="line"></div><div class="line"><span class="comment"># 查看矩阵大小</span></div><div class="line">y.shape</div><div class="line"></div><div class="line"><span class="comment"># 查看矩阵元素个数</span></div><div class="line">y.size</div><div class="line"></div><div class="line"><span class="comment"># 矩阵加法</span></div><div class="line">x + y</div><div class="line"></div><div class="line"><span class="comment"># 矩阵乘法</span></div><div class="line">x * y</div><div class="line"></div><div class="line"><span class="comment"># 指数运算</span></div><div class="line">nd.exp(y)</div><div class="line"></div><div class="line"><span class="comment"># 矩阵乘法</span></div><div class="line">nd.dot(x, y.T)</div><div class="line"></div><div class="line"><span class="comment"># 广播操作</span></div><div class="line">a = nd.arange(<span class="number">3</span>).reshape((<span class="number">3</span>,<span class="number">1</span>))</div><div class="line">b = nd.arange(<span class="number">2</span>).reshape((<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">print(<span class="string">'a:'</span>, a)</div><div class="line">print(<span class="string">'b:'</span>, b)</div><div class="line">print(<span class="string">'a+b:'</span>, a+b)</div><div class="line"></div><div class="line"><span class="comment"># NDArray与Numpy的转换</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.ones((<span class="number">2</span>,<span class="number">3</span>))</div><div class="line">y = nd.array(x)  <span class="comment"># numpy -&gt; mxnet</span></div><div class="line">z = y.asnumpy()  <span class="comment"># mxnet -&gt; numpy</span></div><div class="line">print([z, y])</div></pre></td></tr></table></figure>
<p>NDArray的自动求导：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet.ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">import</span> mxnet.autograd <span class="keyword">as</span> ag</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义矩阵</span></div><div class="line">x = nd.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</div><div class="line"></div><div class="line"><span class="comment"># 添加自动求导</span></div><div class="line">x.attach_grad()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 记录x的变化</span></div><div class="line"><span class="keyword">with</span> ag.record():</div><div class="line">    y = x * <span class="number">2</span></div><div class="line">    z = y * x</div><div class="line"></div><div class="line"><span class="comment"># 求导</span></div><div class="line">z.backward()</div><div class="line"></div><div class="line"><span class="comment"># 判断导数是否相等</span></div><div class="line">x.grad == <span class="number">4</span>*x</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      动手学深度学习——第一课笔记(上)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improving Deep Neural Networks学习笔记(一)</title>
    <link href="noahsnail.com/2017/09/16/2017-9-16-Improving%20Deep%20Neural%20Networks%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
    <id>noahsnail.com/2017/09/16/2017-9-16-Improving Deep Neural Networks学习笔记(一)/</id>
    <published>2017-09-16T01:21:06.000Z</published>
    <updated>2017-09-16T14:21:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-Setting-up-your-Machine-Learning-Application"><a href="#1-Setting-up-your-Machine-Learning-Application" class="headerlink" title="1. Setting up your Machine Learning Application"></a>1. Setting up your Machine Learning Application</h2><h4 id="1-1-Train-Dev-Test-sets"><a href="#1-1-Train-Dev-Test-sets" class="headerlink" title="1.1 Train/Dev/Test sets"></a>1.1 Train/Dev/Test sets</h4><p>Make sure that the dev and test sets come from the same distribution。</p>
<p>Not having a test set might be okay.(Only dev set.)</p>
<p>So having set up a train dev and test set will allow you to integrate more quickly. It will also allow you to more efficiently measure the bias and variance of your algorithm, so you can more efficiently select ways to improve your algorithm.</p>
<h4 id="1-2-Bias-Variance"><a href="#1-2-Bias-Variance" class="headerlink" title="1.2 Bias/Variance"></a>1.2 Bias/Variance</h4><p>High Bias: underfitting<br>High Variance: overfitting</p>
<p>Assumption——human: 0% (Optimal/Bayes error), train set and dev set are drawn from the same distribution.</p>
<table>
<thead>
<tr>
<th>Train set error</th>
<th>Dev set error</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>1%</td>
<td>11%</td>
<td>high variance</td>
</tr>
<tr>
<td>15%</td>
<td>16%</td>
<td>high bias</td>
</tr>
<tr>
<td>15%</td>
<td>30%</td>
<td>high bias and high variance</td>
</tr>
<tr>
<td>0.5%</td>
<td>1%</td>
<td>low bias and low variance</td>
</tr>
</tbody>
</table>
<h4 id="1-3-Basic-Recipe-for-Machine-Learning"><a href="#1-3-Basic-Recipe-for-Machine-Learning" class="headerlink" title="1.3 Basic Recipe for Machine Learning"></a>1.3 Basic Recipe for Machine Learning</h4><p>High bias –&gt; Bigger network, Training longer, Advanced optimization algorithms, Try different netword.</p>
<p>High variance –&gt; More data, Try regularization, Find a more appropriate neural network architecture.</p>
<h2 id="2-Regularizing-your-neural-network"><a href="#2-Regularizing-your-neural-network" class="headerlink" title="2. Regularizing your neural network"></a>2. Regularizing your neural network</h2><h4 id="2-1-Regularization"><a href="#2-1-Regularization" class="headerlink" title="2.1 Regularization"></a>2.1 Regularization</h4><p>In logistic regression, $$w \in R^{n_x}, b \in R$$$$J(w, b) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} ||w||_2^2$$$$||w||_2^2 = \sum _{j=1} ^{n_x} w_j^2 = w^Tw$$<br>This is called L2 regularization.</p>
<p>$$J(w, b) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} ||w||_1$$<br>This is called L1 regularization. <code>w</code> will end up being sparse. $\lambda$ is called regularization parameter.</p>
<p>In neural network, the formula is $$J(w^{[1]},b^{[1]},…,w^{[L]},b^{[L]}) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} \sum _{l=1}^L ||w^{[l]}||^2$$$$||w^{[l]}||^2 = \sum_{i=1}^{n^{[l-1]}}\sum _{j=1}^{n^{[l]}} (w_{ij}^{[l]})^2, w:(n^{[l-1]}, n^{[l]})$$</p>
<p>This matrix norm, it turns out is called the <code>Frobenius Norm</code> of the matrix, denoted with a <code>F</code> in the subscript.</p>
<p>L2 norm regularization is also called <code>weight decay</code>.</p>
<h4 id="2-2-Why-regularization-reduces-overfitting"><a href="#2-2-Why-regularization-reduces-overfitting" class="headerlink" title="2.2 Why regularization reduces overfitting?"></a>2.2 Why regularization reduces overfitting?</h4><p>If $\lambda$ is set too large, matrices <code>W</code> is set to be reasonabley close to zero, and it will zero out the impact of these hidden units. And that’s the case, then this much simplified neural network becomes a much smaller neural network. It will take you from overfitting to underfitting, but there is a <code>just right case</code> in the middle.</p>
<h4 id="2-3-Dropout-regularization"><a href="#2-3-Dropout-regularization" class="headerlink" title="2.3 Dropout regularization"></a>2.3 Dropout regularization</h4><p>Dropout will go through each of the layers of the network, and set some probability of eliminating a node in neural network. By far the most common implementation of dropouts today is inverted dropouts.</p>
<p>Inverted dropout, <code>kp</code> stands for <code>keep-prob</code>:</p>
<p>$$z^{[i + 1]} = w^{[i + 1]} a^{[i]} + b^{[i + 1]}$$$$a^{[i]} = a^{[i]} / kp$$</p>
<p>In test phase, we don’t use dropout and <code>keep-prob</code>.</p>
<h4 id="2-4-Understanding-dropout"><a href="#2-4-Understanding-dropout" class="headerlink" title="2.4 Understanding dropout"></a>2.4 Understanding dropout</h4><p>Why does dropout workd? Intuition: Can’t rely on any one feature, so have to spread out weights.</p>
<p>By spreading all the weights, this will tend to have an effect of shrinking the squared norm of the weights.</p>
<h4 id="2-5-Other-regularization-methods"><a href="#2-5-Other-regularization-methods" class="headerlink" title="2.5 Other regularization methods"></a>2.5 Other regularization methods</h4><ul>
<li>Data augmentation.</li>
<li>Early stopping</li>
</ul>
<h2 id="3-Setting-up-your-optimization-problem"><a href="#3-Setting-up-your-optimization-problem" class="headerlink" title="3. Setting up your optimization problem"></a>3. Setting up your optimization problem</h2><h4 id="3-1-Normalizing-inputs"><a href="#3-1-Normalizing-inputs" class="headerlink" title="3.1 Normalizing inputs"></a>3.1 Normalizing inputs</h4><p>Normalizing inputs can speed up training. Normalizing inputs corresponds to two steps. The first is to subtract out or to zero out the mean. And then the second step is to normalize the variances.</p>
<h4 id="3-2-Vanishing-Exploding-gradients"><a href="#3-2-Vanishing-Exploding-gradients" class="headerlink" title="3.2 Vanishing/Exploding gradients"></a>3.2 Vanishing/Exploding gradients</h4><p>If the network is very deeper, deep network suffer from the problems of vanishing or exploding gradients.</p>
<h4 id="3-3-Weight-initialization-for-deep-networks"><a href="#3-3-Weight-initialization-for-deep-networks" class="headerlink" title="3.3 Weight initialization for deep networks"></a>3.3 Weight initialization for deep networks</h4><p>If activation function is <code>ReLU</code> or <code>tanh</code>, <code>w</code> initialization is: $$w^{[l]} = np.random.randn(shape) * np.sqrt(\frac {2} {n^{[l-1]}}).$$ This is called Xavier initalization. </p>
<p>Another formula is $$w^{[l]} = np.random.randn(shape) * np.sqrt(\frac {2} {n^{[l-1]} + n^{[l]}}).$$</p>
<h4 id="3-4-Numberical-approximation-of-gradients"><a href="#3-4-Numberical-approximation-of-gradients" class="headerlink" title="3.4 Numberical approximation of gradients"></a>3.4 Numberical approximation of gradients</h4><p>In order to build up to gradient checking, you need to numerically approximate computatiions of gradients.</p>
<p>$$g(\theta) \approx \frac {f(\theta + \epsilon) - f(\theta - \epsilon)} {2 \epsilon}$$</p>
<h4 id="3-5-Gradient-checking"><a href="#3-5-Gradient-checking" class="headerlink" title="3.5 Gradient checking"></a>3.5 Gradient checking</h4><p>Take matrix <code>W</code>, vector <code>b</code> and reshape them into vectors, and then concatenate them, you have a giant vector $\theta$. For each <code>i</code>:</p>
<p>$$d\theta _{approx}[i]= \frac {J(\theta_1,…,\theta_i + \epsilon,…)-J(\theta_1,…,\theta_i - \epsilon,…)} {2\epsilon} \approx d\theta_i=\frac {\partial J} {\partial \theta_i}$$</p>
<p>If $$\frac {||d\theta_{approx} - d\theta ||_2} {||d\theta_{approx}||_2 + ||\theta||_2} \approx 10^{-7}$$, that’s great. If $\approx 10^{-5}$, you need to do double check, if $\approx 10^{-5}$, there may be a bug.</p>
<h4 id="3-6-Gradient-checking-implementation-notes"><a href="#3-6-Gradient-checking-implementation-notes" class="headerlink" title="3.6 Gradient checking implementation notes"></a>3.6 Gradient checking implementation notes</h4><ul>
<li>Don’t use gradient check in training, only to debug.</li>
<li>If algorithm fails gradient check, look at components to try to identify bug.</li>
<li>Remember regularization.</li>
<li>Doesn’t work with dropout.</li>
<li>Run at random initialization; perhaps again after some training.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Improving Deep Neural Networks学习笔记(一)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python的命令行参数解析</title>
    <link href="noahsnail.com/2017/09/13/2017-9-13-Python%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/"/>
    <id>noahsnail.com/2017/09/13/2017-9-13-Python的命令行参数解析/</id>
    <published>2017-09-13T02:22:00.000Z</published>
    <updated>2017-09-13T02:59:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>命令行参数解析在编程语言中基本都会碰到，Python中内置了一个用于命令项选项与参数解析的模块<code>argparse</code>。下面主要介绍两种解析Python命令行参数的方式。</p>
<h2 id="1-sys-argv"><a href="#1-sys-argv" class="headerlink" title="1. sys.argv"></a>1. sys.argv</h2><p>解析Python中命令行参数的最传统的方法是通过<code>sys.argv</code>。Demo如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#!/usr/env/python python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import sys</div><div class="line"></div><div class="line">param1 = sys.argv[1]</div><div class="line">param2 = sys.argv[2]</div><div class="line"></div><div class="line">print sys.argv</div><div class="line">print param1</div><div class="line">print param2</div><div class="line">print type(param1)</div><div class="line">print type(param2)</div></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ python test.py 1 2</div><div class="line">[&apos;test.py&apos;, &apos;1&apos;, &apos;2&apos;]</div><div class="line">1</div><div class="line">2</div></pre></td></tr></table></figure>
<p>这种方法比较古老，灵活性很差，同时解析出来的参数都是<code>str</code>类型。但在编写简单脚本，参数较少且固定时比较方便。</p>
<h2 id="2-argparse"><a href="#2-argparse" class="headerlink" title="2. argparse"></a>2. argparse</h2><p><code>argparse</code>模块是Python内置的参数解析模块，使用起来比较简单且功能强大。Demo如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">#!/usr/env/python python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import argparse</div><div class="line"></div><div class="line"># Create ArgumentParser() object</div><div class="line">parser = argparse.ArgumentParser()</div><div class="line"></div><div class="line"># Add argument</div><div class="line">parser.add_argument(&apos;--train&apos;, required=True, help=&apos;path to dataset&apos;)</div><div class="line">parser.add_argument(&apos;--val&apos;, required=True, help=&apos;path to dataset&apos;)</div><div class="line">parser.add_argument(&apos;--total&apos;, type=int, help=&apos;number of dataset&apos;, default=100)</div><div class="line">parser.add_argument(&apos;--lr&apos;, type=float, default=0.01, help=&apos;learning rate&apos;)</div><div class="line"></div><div class="line"># Print usage</div><div class="line">parser.print_help()</div><div class="line"></div><div class="line"># Parse argument</div><div class="line">args = parser.parse_args()</div><div class="line"></div><div class="line"># Print args</div><div class="line">print args</div><div class="line"></div><div class="line">print args.train</div><div class="line">print type(args.train)</div><div class="line">print args.val</div><div class="line">print type(args.val)</div><div class="line">print args.total</div><div class="line">print type(args.total)</div><div class="line">print args.lr</div><div class="line">print type(args.lr)</div></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"># Test 1</div><div class="line">python test.py --train train_lmdb --val val_lmdb --total 10000 --lr 0.001</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.001, total=10000, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">10000</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.001</div><div class="line">&lt;type &apos;float&apos;&gt;</div><div class="line"></div><div class="line"># Test 2</div><div class="line">python test.py --train train_lmdb --val val_lmdb</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.01, total=100, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">100</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.01</div><div class="line">&lt;type &apos;float&apos;&gt;</div><div class="line"></div><div class="line"></div><div class="line"># Test 3</div><div class="line">python test.py --val val_lmdb --train train_lmdb</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.01, total=100, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">100</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.01</div><div class="line">&lt;type &apos;float&apos;&gt;</div></pre></td></tr></table></figure>
<p><code>ArgumentParser</code>类创建时的参数如下：</p>
<ul>
<li>prog - 程序的名字（默认：sys.argv[0]）</li>
<li>usage - 描述程序用法的字符串（默认：从解析器的参数生成）</li>
<li>description - 参数帮助信息之前的文本（默认：空）</li>
<li>epilog - 参数帮助信息之后的文本（默认：空）</li>
<li>parents - ArgumentParser 对象的一个列表，这些对象的参数应该包括进去</li>
<li>formatter_class - 定制化帮助信息的类</li>
<li>prefix_chars - 可选参数的前缀字符集（默认：‘-‘）</li>
<li>fromfile_prefix_chars - 额外的参数应该读取的文件的前缀字符集（默认：None）</li>
<li>argument_default - 参数的全局默认值（默认：None）</li>
<li>conflict_handler - 解决冲突的可选参数的策略（通常没有必要）</li>
<li>add_help - 给解析器添加-h/–help 选项（默认：True）</li>
</ul>
<p><code>add_argument</code>函数的参数如下：</p>
<ul>
<li>name or flags - 选项字符串的名字或者列表，例如foo 或者-f, –foo。</li>
<li>action - 在命令行遇到该参数时采取的基本动作类型。</li>
<li>nargs - 应该读取的命令行参数数目。</li>
<li>const - 某些action和nargs选项要求的常数值。</li>
<li>default - 如果命令行中没有出现该参数时的默认值。</li>
<li>type - 命令行参数应该被转换成的类型。</li>
<li>choices - 参数可允许的值的一个容器。</li>
<li>required - 该命令行选项是否可以省略（只针对可选参数）。</li>
<li>help - 参数的简短描述。</li>
<li>metavar - 参数在帮助信息中的名字。</li>
<li>dest - 给parse_args()返回的对象要添加的属性名称。</li>
</ul>
<p>参考资料：</p>
<ol>
<li><a href="http://python.usyiyi.cn/translate/python_278/library/argparse.html" target="_blank" rel="external">http://python.usyiyi.cn/translate/python_278/library/argparse.html</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/explore-python/Standard-Modules/argparse.html" target="_blank" rel="external">http://wiki.jikexueyuan.com/project/explore-python/Standard-Modules/argparse.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python的命令行参数解析
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python中的编码</title>
    <link href="noahsnail.com/2017/09/07/2017-9-7-Python%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E7%A0%81/"/>
    <id>noahsnail.com/2017/09/07/2017-9-7-Python中的字符串编码/</id>
    <published>2017-09-07T03:26:27.000Z</published>
    <updated>2017-09-07T08:48:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Python处理字符串，写文件时会碰到许多的编码问题，特别是涉及到中文的时候，非常烦人，但又不得不学。下面主要记录工作过程中碰到的Python编码问题。</p>
<h2 id="1-字符串编码"><a href="#1-字符串编码" class="headerlink" title="1. 字符串编码"></a>1. 字符串编码</h2><p>Python的字符串类型为<code>str</code>，可以通过<code>type</code>函数查看返回的类型。Python中字符串默认的编码方式需要通过<code>sys.getfilesystemencoding()</code>查看，通常是<code>utf-8</code>。<code>u&#39;中文&#39;</code>构造出来的是<code>unicode</code>类型，不是<code>str</code>类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 查看字符串编码方式</div><div class="line">&gt;&gt;&gt; import sys</div><div class="line">&gt;&gt;&gt; print sys.getfilesystemencoding()</div><div class="line">utf-8</div><div class="line"></div><div class="line">&gt;&gt;&gt; s1 = &apos;中国&apos;</div><div class="line">&gt;&gt;&gt; s2 = u&apos;中国&apos;</div><div class="line">&gt;&gt;&gt; type(s1)</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">&gt;&gt;&gt; type(s2)</div><div class="line">&lt;type &apos;unicode&apos;&gt;</div></pre></td></tr></table></figure>
<p><code>str</code>类型和<code>unicode</code>类型分别有<code>decode</code>和<code>encode</code>函数。<code>str.decode</code>用来将<code>str</code>转为<code>unicode</code>，<code>unicode.encode</code>用来将<code>unicdoe</code>转为<code>str</code>。用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># decode</div><div class="line">&gt;&gt;&gt; s1.decode(&apos;utf8&apos;)</div><div class="line">u&apos;\u4e2d\u56fd&apos;</div><div class="line">&gt;&gt;&gt; type(s1.decode(&apos;utf8&apos;))</div><div class="line">&lt;type &apos;unicode&apos;&gt;</div><div class="line"></div><div class="line"># encode</div><div class="line">&gt;&gt;&gt; s2.encode(&apos;utf8&apos;)</div><div class="line">&apos;\xe4\xb8\xad\xe5\x9b\xbd&apos;</div><div class="line">&gt;&gt;&gt; type(s2.encode(&apos;utf8&apos;))</div><div class="line">&lt;type &apos;str&apos;&gt;</div></pre></td></tr></table></figure>
<h2 id="2-代码文件编码"><a href="#2-代码文件编码" class="headerlink" title="2. 代码文件编码"></a>2. 代码文件编码</h2><p><code>py</code>文件默认的编码是ASCII编码，中文显示时会进行ASCII编码到系统默认编码的转换，在运行Python文件时经常会报错。因此需要设置<code>py</code>文件的编码为<code>utf-8</code>。设置方式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># _*_ coding: utf-8 _*_</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      Python中的编码
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
</feed>
