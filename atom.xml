<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="noahsnail.com/"/>
  <updated>2017-09-18T13:15:51.000Z</updated>
  <id>noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch基本用法(三)——激活函数</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%89)%E2%80%94%E2%80%94%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(三)——激活函数/</id>
    <published>2017-09-18T12:25:07.000Z</published>
    <updated>2017-09-18T13:15:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> func</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义数据x</span></div><div class="line">x = torch.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)</div><div class="line">x = Variable(x)</div><div class="line">np_x = x.data.numpy()</div><div class="line"></div><div class="line"><span class="comment"># 通过激活函数处理x</span></div><div class="line">y_relu = func.relu(x).data.numpy()</div><div class="line">y_sigmoid = func.sigmoid(x).data.numpy()</div><div class="line">y_tanh = func.tanh(x).data.numpy()</div><div class="line">y_softmax = func.softplus(x).data.numpy()</div><div class="line"></div><div class="line"><span class="comment"># 绘制激活函数图</span></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">221</span>)</div><div class="line">plt.plot(np_x, y_relu, c = <span class="string">'red'</span>, label = <span class="string">'relu'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">222</span>)</div><div class="line">plt.plot(np_x, y_sigmoid, c = <span class="string">'red'</span>, label = <span class="string">'sigmoid'</span>)</div><div class="line">plt.ylim((<span class="number">0</span>, <span class="number">1</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">223</span>)</div><div class="line">plt.plot(np_x, y_tanh, c = <span class="string">'red'</span>, label = <span class="string">'tanh'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">224</span>)</div><div class="line">plt.plot(np_x, y_softmax, c = <span class="string">'red'</span>, label = <span class="string">'softmax'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/ac_func.png" alt="Figure"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(三)——激活函数
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(二)——Variable</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%8C)%E2%80%94%E2%80%94Variable/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(二)——Variable/</id>
    <published>2017-09-18T10:46:11.000Z</published>
    <updated>2017-09-18T12:24:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是PyTorch中Variable变量的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">tensor = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</div><div class="line"></div><div class="line"><span class="comment"># 定义Variable, requires_grad用来指定是否需要计算梯度</span></div><div class="line">variable = Variable(tensor, requires_grad = <span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> tensor</div><div class="line"><span class="keyword">print</span> variable</div></pre></td></tr></table></figure>
<pre><code> 1  2
 3  4
[torch.FloatTensor of size 2x2]

Variable containing:
 1  2
 3  4
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算x^2的均值</span></div><div class="line">tensor_mean = torch.mean(tensor * tensor)</div><div class="line">variable_mean = torch.mean(variable * variable)</div><div class="line"><span class="keyword">print</span> tensor_mean</div><div class="line"><span class="keyword">print</span> variable_mean</div></pre></td></tr></table></figure>
<pre><code>7.5
Variable containing:
 7.5000
[torch.FloatTensor of size 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># variable进行反向传播</span></div><div class="line"><span class="comment"># 梯度计算如下：</span></div><div class="line"><span class="comment"># variable_mean = 1/4 * sum(variable * variable)</span></div><div class="line"><span class="comment"># d(variable_mean)/d(variable) = 1/4 * 2 * variable = 1/2 * variable</span></div><div class="line">variable_mean.backward()</div><div class="line"></div><div class="line"><span class="comment"># 输出variable中的梯度</span></div><div class="line"><span class="keyword">print</span> variable.grad</div></pre></td></tr></table></figure>
<pre><code>Variable containing:
 0.5000  1.0000
 1.5000  2.0000
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># *表示逐元素点乘,不是矩阵乘法</span></div><div class="line"><span class="keyword">print</span> tensor * tensor</div><div class="line"><span class="keyword">print</span> variable * variable</div></pre></td></tr></table></figure>
<pre><code>  1   4
  9  16
[torch.FloatTensor of size 2x2]

Variable containing:
  1   4
  9  16
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输出variable中的data, data是tensor</span></div><div class="line"><span class="keyword">print</span> variable.data</div></pre></td></tr></table></figure>
<pre><code> 1  2
 3  4
[torch.FloatTensor of size 2x2]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(二)——Variable
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python性能优化</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-Python%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-Python性能优化/</id>
    <published>2017-09-18T02:15:08.000Z</published>
    <updated>2017-09-18T10:40:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Python使用非常方便、灵活，因此很受欢迎。但正因为如此，导致实现同一功能时，Python代码有很多写法，但不同的写法有不同的性能。因此写Python代码要有良好的习惯，多写高性能的代码。作者原来平常写Python代码也很随意，直到某天处理大量数据时半天看不到结果，究其原因，是Python代码的性能问题导致的。</p>
<h2 id="1-列表解析与列表重建"><a href="#1-列表解析与列表重建" class="headerlink" title="1. 列表解析与列表重建"></a>1. 列表解析与列表重建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">fr = open(<span class="string">'words.txt'</span>)</div><div class="line">t1 = time.time()</div><div class="line">word_list = fr.readlines()</div><div class="line">t2 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'read file time: '</span>, t2 -t1</div><div class="line">fr.close()</div><div class="line"></div><div class="line"><span class="comment"># for循环构建列表</span></div><div class="line">keywords = []</div><div class="line">t1 = time.time()</div><div class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_list:</div><div class="line">    word = word.strip()</div><div class="line">    keywords.append(word)</div><div class="line">t2 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'for loop time: '</span>, t2 - t1</div><div class="line"></div><div class="line"><span class="comment"># 列表解析</span></div><div class="line">t3 = time.time()</div><div class="line">keywords = [word.strip() <span class="keyword">for</span> word <span class="keyword">in</span> word_list]</div><div class="line">t4 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'list pars time: '</span>, t4 - t3</div><div class="line"></div><div class="line">fr = open(<span class="string">'words.txt'</span>)</div><div class="line">t5 = time.time()</div><div class="line">keywords = [word.strip() <span class="keyword">for</span> word <span class="keyword">in</span> fr.readlines()]</div><div class="line">t6 = time.time()</div><div class="line">fr.close()</div><div class="line"><span class="keyword">print</span> <span class="string">'read file and list parse time: '</span>, t6 - t5</div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'list length: '</span>, len(word_list)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">read file time:  0.0318450927734</div><div class="line">for loop time:  0.137716054916</div><div class="line">list pars time:  0.0910630226135</div><div class="line">read file and list parse time:  0.124923944473</div><div class="line">list length:  441669</div></pre></td></tr></table></figure>
<p>结论：本次测试中，列表解析时间是for循环时间的<code>2/3</code>。</p>
<h2 id="2-字符串拼接"><a href="#2-字符串拼接" class="headerlink" title="2. 字符串拼接"></a>2. 字符串拼接</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import time</div><div class="line"></div><div class="line">fr = open(&apos;words.txt&apos;)</div><div class="line">keywords = [word.strip() for word in fr.readlines()]</div><div class="line">fr.close()</div><div class="line"></div><div class="line"># 加号拼接字符串</div><div class="line">t1 = time.time()</div><div class="line">str1 = &apos;&apos;</div><div class="line">for word in keywords:</div><div class="line">    str1 += word</div><div class="line">t2 = time.time()</div><div class="line">print &apos;string concat time: &apos;, t2 - t1</div><div class="line"></div><div class="line"># join拼接字符串</div><div class="line">t1 = time.time()</div><div class="line">str2 = &apos;&apos;.join(keywords)</div><div class="line">t2 = time.time()</div><div class="line">print &apos;string join time: &apos;, t2 - t1</div><div class="line"></div><div class="line">print &apos;list length: &apos;, len(keywords)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">string concat time:  0.0814869403839</div><div class="line">string join time:  0.0123951435089</div><div class="line">list length:  441669</div></pre></td></tr></table></figure>
<p>结论：本次测试中，<code>join</code>函数拼接字符串比<code>+=</code>拼接字符串快<code>6倍多</code>。</p>
<h2 id="3-range与xrange"><a href="#3-range与xrange" class="headerlink" title="3. range与xrange"></a>3. range与xrange</h2><ul>
<li>range</li>
</ul>
<p>python中range会直接生成一个list对象。</p>
<ul>
<li>xrange</li>
</ul>
<p>用法与range完全相同，所不同的是生成的不是一个数组，而是一个生成器，它的类型为<code>xrange</code>。在生成非常大的数字序列时，xrange不会马上开辟很大的一块内存空间。如果不是需要返回列表，则尽可能使用<code>xrange</code>。</p>
<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import time</div><div class="line"></div><div class="line">t1 = time.time()</div><div class="line">for i in range(1000000):</div><div class="line">    pass</div><div class="line">t2 = time.time()</div><div class="line">print &apos;range time: &apos;, t2 -t1</div><div class="line"></div><div class="line">t1 = time.time()</div><div class="line">for i in xrange(1000000):</div><div class="line">    pass</div><div class="line">t2 = time.time()</div><div class="line">print &apos;xrange time: &apos;, t2 -t1</div></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">range time:  0.0680990219116</div><div class="line">xrange time:  0.0329170227051</div></pre></td></tr></table></figure>
<p>结论：本次测试中，<code>xrange</code>比<code>range</code>快一倍多。</p>
<h2 id="4-待续。"><a href="#4-待续。" class="headerlink" title="4. 待续。"></a>4. 待续。</h2>]]></content>
    
    <summary type="html">
    
      Python性能优化
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(一)——Numpy，Torch对比</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94Numpy%EF%BC%8CTorch%E5%AF%B9%E6%AF%94/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(一)——Numpy，Torch对比/</id>
    <published>2017-09-18T01:08:35.000Z</published>
    <updated>2017-09-18T03:47:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是对比Torch与Numpy的一些操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># numpy的array与torch的tensor的转换</span></div><div class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</div><div class="line">torch_data = torch.from_numpy(np_data)</div><div class="line">tensor2array = torch_data.numpy() </div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'numpy data: '</span>, np_data</div><div class="line"><span class="keyword">print</span> <span class="string">'torch data: '</span>, torch_data</div><div class="line"><span class="keyword">print</span> <span class="string">'tensor2array: '</span>, tensor2array</div></pre></td></tr></table></figure>
<pre><code>numpy data:  [[0 1 2]
 [3 4 5]]
torch data:  
 0  1  2
 3  4  5
[torch.LongTensor of size 2x3]

tensor2array:  [[0 1 2]
 [3 4 5]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Tensor的文档：http://pytorch.org/docs/master/tensors.html</span></div><div class="line">data = [<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</div><div class="line">float_data = torch.FloatTensor(data)</div><div class="line"><span class="keyword">print</span> float_data</div></pre></td></tr></table></figure>
<pre><code>-2
-1
 0
 1
 2
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># abs操作</span></div><div class="line"><span class="keyword">print</span> np.abs(data)</div><div class="line"><span class="keyword">print</span> torch.abs(float_data)</div></pre></td></tr></table></figure>
<pre><code>[2 1 0 1 2]

 2
 1
 0
 1
 2
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># sin操作</span></div><div class="line"><span class="keyword">print</span> np.sin(data)</div><div class="line"><span class="keyword">print</span> torch.sin(float_data)</div></pre></td></tr></table></figure>
<pre><code>[-0.90929743 -0.84147098  0.          0.84147098  0.90929743]

-0.9093
-0.8415
 0.0000
 0.8415
 0.9093
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># mean操作</span></div><div class="line"><span class="keyword">print</span> np.mean(data)</div><div class="line"><span class="keyword">print</span> torch.mean(float_data)</div></pre></td></tr></table></figure>
<pre><code>0.0
0.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 矩阵相乘</span></div><div class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</div><div class="line">tensor = torch.FloatTensor(data)</div><div class="line"></div><div class="line"><span class="keyword">print</span> np.matmul(data, data)</div><div class="line"><span class="comment"># torch.mm不支持广播形式</span></div><div class="line"><span class="keyword">print</span> torch.mm(tensor, tensor)</div><div class="line"><span class="comment"># torch.matmul支持广播形式</span></div><div class="line"><span class="keyword">print</span> torch.matmul(tensor, tensor)</div></pre></td></tr></table></figure>
<pre><code>[[ 7 10]
 [15 22]]

  7  10
 15  22
[torch.FloatTensor of size 2x2]


  7  10
 15  22
[torch.FloatTensor of size 2x2]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(一)——Numpy，Torch对比
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习——第一课笔记(上)</title>
    <link href="noahsnail.com/2017/09/17/2017-9-17-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E8%AF%BE/"/>
    <id>noahsnail.com/2017/09/17/2017-9-17-动手学深度学习——第一课/</id>
    <published>2017-09-17T10:09:47.000Z</published>
    <updated>2017-09-17T14:22:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是学习李沐直播课程的笔记。视频及内容的具体地址可参考：<a href="https://zhuanlan.zhihu.com/p/29125290" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29125290</a>。</p>
<h2 id="第一课：从上手到多类分类"><a href="#第一课：从上手到多类分类" class="headerlink" title="第一课：从上手到多类分类"></a>第一课：从上手到多类分类</h2><p>课程首先介绍了深度学习的很多应用：例如增强学习、物体识别、语音识别、机器翻译、推荐系统、广告点击预测等。</p>
<p>课程目的：通过动手实现来理解深度学习，跟工业界应用相比，主要只是数据规模和模型复杂度的区别。</p>
<p>深度学习的轮子很多，例如Caffe，TensorFlow，mxnet，PyTorch，CNTK等。它们之间的主要区别在于：1.便利的开发；2.方便的部署。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/chapter1_1.png" alt="Figure 1"></p>
<p>mxnet之上的一个package是Gluon，主要目的是一次解决开发和部署。课程主要分为以下三个部分：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/chapter1_2.png" alt="Figure 2"></p>
<h3 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1. 环境配置"></a>1. 环境配置</h3><p>我的配置环境是Mac，Linux平台类似。</p>
<p>mxnet安装命令如下，前提是已经安装好了Anaconda，Anaconda的安装可以参考官网：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install mxnet</div></pre></td></tr></table></figure>
<p>测试mxnet：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import mxnet</div><div class="line">&gt;&gt;&gt; print mxnet.__version__</div><div class="line">0.11.0</div></pre></td></tr></table></figure>
<p>然后安装notedown，运行Jupyter并加载notedown插件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install https://github.com/mli/notedown/tarball/master</div><div class="line">jupyter notebook --NotebookApp.contents_manager_class=&apos;notedown.NotedownContentsManager&apos;</div></pre></td></tr></table></figure>
<p>通过ExecutionTime插件来对每个cell的运行计时，国内使用豆瓣源。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pip install jupyter_contrib_nbextensions -i https://pypi.douban.com/simple</div><div class="line">jupyter contrib nbextension install --user</div><div class="line">jupyter nbextension enable execute_time/ExecuteTime</div></pre></td></tr></table></figure>
<h3 id="2-NDArray"><a href="#2-NDArray" class="headerlink" title="2. NDArray"></a>2. NDArray</h3><p>NDArray是MXNet储存和变换数据的主要工具，它与numpy非常类似。NDArray提供了CPU和GPU的异步计算，还提供了自动求导。NDArray的基本用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"></div><div class="line"><span class="comment"># 创建矩阵</span></div><div class="line">nd.zeros((<span class="number">3</span>, <span class="number">4</span>))</div><div class="line">x = nd.ones((<span class="number">3</span>, <span class="number">4</span>))</div><div class="line">nd.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>]])</div><div class="line">y = nd.random_normal(<span class="number">0</span>, <span class="number">1</span>, shape=(<span class="number">3</span>, <span class="number">4</span>))</div><div class="line"></div><div class="line"><span class="comment"># 查看矩阵大小</span></div><div class="line">y.shape</div><div class="line"></div><div class="line"><span class="comment"># 查看矩阵元素个数</span></div><div class="line">y.size</div><div class="line"></div><div class="line"><span class="comment"># 矩阵加法</span></div><div class="line">x + y</div><div class="line"></div><div class="line"><span class="comment"># 矩阵乘法</span></div><div class="line">x * y</div><div class="line"></div><div class="line"><span class="comment"># 指数运算</span></div><div class="line">nd.exp(y)</div><div class="line"></div><div class="line"><span class="comment"># 矩阵乘法</span></div><div class="line">nd.dot(x, y.T)</div><div class="line"></div><div class="line"><span class="comment"># 广播操作</span></div><div class="line">a = nd.arange(<span class="number">3</span>).reshape((<span class="number">3</span>,<span class="number">1</span>))</div><div class="line">b = nd.arange(<span class="number">2</span>).reshape((<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">print(<span class="string">'a:'</span>, a)</div><div class="line">print(<span class="string">'b:'</span>, b)</div><div class="line">print(<span class="string">'a+b:'</span>, a+b)</div><div class="line"></div><div class="line"><span class="comment"># NDArray与Numpy的转换</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.ones((<span class="number">2</span>,<span class="number">3</span>))</div><div class="line">y = nd.array(x)  <span class="comment"># numpy -&gt; mxnet</span></div><div class="line">z = y.asnumpy()  <span class="comment"># mxnet -&gt; numpy</span></div><div class="line">print([z, y])</div></pre></td></tr></table></figure>
<p>NDArray的自动求导：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet.ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">import</span> mxnet.autograd <span class="keyword">as</span> ag</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义矩阵</span></div><div class="line">x = nd.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</div><div class="line"></div><div class="line"><span class="comment"># 添加自动求导</span></div><div class="line">x.attach_grad()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 记录x的变化</span></div><div class="line"><span class="keyword">with</span> ag.record():</div><div class="line">    y = x * <span class="number">2</span></div><div class="line">    z = y * x</div><div class="line"></div><div class="line"><span class="comment"># 求导</span></div><div class="line">z.backward()</div><div class="line"></div><div class="line"><span class="comment"># 判断导数是否相等</span></div><div class="line">x.grad == <span class="number">4</span>*x</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      动手学深度学习——第一课笔记(上)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improving Deep Neural Networks学习笔记(一)</title>
    <link href="noahsnail.com/2017/09/16/2017-9-16-Improving%20Deep%20Neural%20Networks%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
    <id>noahsnail.com/2017/09/16/2017-9-16-Improving Deep Neural Networks学习笔记(一)/</id>
    <published>2017-09-16T01:21:06.000Z</published>
    <updated>2017-09-16T14:21:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-Setting-up-your-Machine-Learning-Application"><a href="#1-Setting-up-your-Machine-Learning-Application" class="headerlink" title="1. Setting up your Machine Learning Application"></a>1. Setting up your Machine Learning Application</h2><h4 id="1-1-Train-Dev-Test-sets"><a href="#1-1-Train-Dev-Test-sets" class="headerlink" title="1.1 Train/Dev/Test sets"></a>1.1 Train/Dev/Test sets</h4><p>Make sure that the dev and test sets come from the same distribution。</p>
<p>Not having a test set might be okay.(Only dev set.)</p>
<p>So having set up a train dev and test set will allow you to integrate more quickly. It will also allow you to more efficiently measure the bias and variance of your algorithm, so you can more efficiently select ways to improve your algorithm.</p>
<h4 id="1-2-Bias-Variance"><a href="#1-2-Bias-Variance" class="headerlink" title="1.2 Bias/Variance"></a>1.2 Bias/Variance</h4><p>High Bias: underfitting<br>High Variance: overfitting</p>
<p>Assumption——human: 0% (Optimal/Bayes error), train set and dev set are drawn from the same distribution.</p>
<table>
<thead>
<tr>
<th>Train set error</th>
<th>Dev set error</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>1%</td>
<td>11%</td>
<td>high variance</td>
</tr>
<tr>
<td>15%</td>
<td>16%</td>
<td>high bias</td>
</tr>
<tr>
<td>15%</td>
<td>30%</td>
<td>high bias and high variance</td>
</tr>
<tr>
<td>0.5%</td>
<td>1%</td>
<td>low bias and low variance</td>
</tr>
</tbody>
</table>
<h4 id="1-3-Basic-Recipe-for-Machine-Learning"><a href="#1-3-Basic-Recipe-for-Machine-Learning" class="headerlink" title="1.3 Basic Recipe for Machine Learning"></a>1.3 Basic Recipe for Machine Learning</h4><p>High bias –&gt; Bigger network, Training longer, Advanced optimization algorithms, Try different netword.</p>
<p>High variance –&gt; More data, Try regularization, Find a more appropriate neural network architecture.</p>
<h2 id="2-Regularizing-your-neural-network"><a href="#2-Regularizing-your-neural-network" class="headerlink" title="2. Regularizing your neural network"></a>2. Regularizing your neural network</h2><h4 id="2-1-Regularization"><a href="#2-1-Regularization" class="headerlink" title="2.1 Regularization"></a>2.1 Regularization</h4><p>In logistic regression, $$w \in R^{n_x}, b \in R$$$$J(w, b) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} ||w||_2^2$$$$||w||_2^2 = \sum _{j=1} ^{n_x} w_j^2 = w^Tw$$<br>This is called L2 regularization.</p>
<p>$$J(w, b) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} ||w||_1$$<br>This is called L1 regularization. <code>w</code> will end up being sparse. $\lambda$ is called regularization parameter.</p>
<p>In neural network, the formula is $$J(w^{[1]},b^{[1]},…,w^{[L]},b^{[L]}) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} \sum _{l=1}^L ||w^{[l]}||^2$$$$||w^{[l]}||^2 = \sum_{i=1}^{n^{[l-1]}}\sum _{j=1}^{n^{[l]}} (w_{ij}^{[l]})^2, w:(n^{[l-1]}, n^{[l]})$$</p>
<p>This matrix norm, it turns out is called the <code>Frobenius Norm</code> of the matrix, denoted with a <code>F</code> in the subscript.</p>
<p>L2 norm regularization is also called <code>weight decay</code>.</p>
<h4 id="2-2-Why-regularization-reduces-overfitting"><a href="#2-2-Why-regularization-reduces-overfitting" class="headerlink" title="2.2 Why regularization reduces overfitting?"></a>2.2 Why regularization reduces overfitting?</h4><p>If $\lambda$ is set too large, matrices <code>W</code> is set to be reasonabley close to zero, and it will zero out the impact of these hidden units. And that’s the case, then this much simplified neural network becomes a much smaller neural network. It will take you from overfitting to underfitting, but there is a <code>just right case</code> in the middle.</p>
<h4 id="2-3-Dropout-regularization"><a href="#2-3-Dropout-regularization" class="headerlink" title="2.3 Dropout regularization"></a>2.3 Dropout regularization</h4><p>Dropout will go through each of the layers of the network, and set some probability of eliminating a node in neural network. By far the most common implementation of dropouts today is inverted dropouts.</p>
<p>Inverted dropout, <code>kp</code> stands for <code>keep-prob</code>:</p>
<p>$$z^{[i + 1]} = w^{[i + 1]} a^{[i]} + b^{[i + 1]}$$$$a^{[i]} = a^{[i]} / kp$$</p>
<p>In test phase, we don’t use dropout and <code>keep-prob</code>.</p>
<h4 id="2-4-Understanding-dropout"><a href="#2-4-Understanding-dropout" class="headerlink" title="2.4 Understanding dropout"></a>2.4 Understanding dropout</h4><p>Why does dropout workd? Intuition: Can’t rely on any one feature, so have to spread out weights.</p>
<p>By spreading all the weights, this will tend to have an effect of shrinking the squared norm of the weights.</p>
<h4 id="2-5-Other-regularization-methods"><a href="#2-5-Other-regularization-methods" class="headerlink" title="2.5 Other regularization methods"></a>2.5 Other regularization methods</h4><ul>
<li>Data augmentation.</li>
<li>Early stopping</li>
</ul>
<h2 id="3-Setting-up-your-optimization-problem"><a href="#3-Setting-up-your-optimization-problem" class="headerlink" title="3. Setting up your optimization problem"></a>3. Setting up your optimization problem</h2><h4 id="3-1-Normalizing-inputs"><a href="#3-1-Normalizing-inputs" class="headerlink" title="3.1 Normalizing inputs"></a>3.1 Normalizing inputs</h4><p>Normalizing inputs can speed up training. Normalizing inputs corresponds to two steps. The first is to subtract out or to zero out the mean. And then the second step is to normalize the variances.</p>
<h4 id="3-2-Vanishing-Exploding-gradients"><a href="#3-2-Vanishing-Exploding-gradients" class="headerlink" title="3.2 Vanishing/Exploding gradients"></a>3.2 Vanishing/Exploding gradients</h4><p>If the network is very deeper, deep network suffer from the problems of vanishing or exploding gradients.</p>
<h4 id="3-3-Weight-initialization-for-deep-networks"><a href="#3-3-Weight-initialization-for-deep-networks" class="headerlink" title="3.3 Weight initialization for deep networks"></a>3.3 Weight initialization for deep networks</h4><p>If activation function is <code>ReLU</code> or <code>tanh</code>, <code>w</code> initialization is: $$w^{[l]} = np.random.randn(shape) * np.sqrt(\frac {2} {n^{[l-1]}}).$$ This is called Xavier initalization. </p>
<p>Another formula is $$w^{[l]} = np.random.randn(shape) * np.sqrt(\frac {2} {n^{[l-1]} + n^{[l]}}).$$</p>
<h4 id="3-4-Numberical-approximation-of-gradients"><a href="#3-4-Numberical-approximation-of-gradients" class="headerlink" title="3.4 Numberical approximation of gradients"></a>3.4 Numberical approximation of gradients</h4><p>In order to build up to gradient checking, you need to numerically approximate computatiions of gradients.</p>
<p>$$g(\theta) \approx \frac {f(\theta + \epsilon) - f(\theta - \epsilon)} {2 \epsilon}$$</p>
<h4 id="3-5-Gradient-checking"><a href="#3-5-Gradient-checking" class="headerlink" title="3.5 Gradient checking"></a>3.5 Gradient checking</h4><p>Take matrix <code>W</code>, vector <code>b</code> and reshape them into vectors, and then concatenate them, you have a giant vector $\theta$. For each <code>i</code>:</p>
<p>$$d\theta _{approx}[i]= \frac {J(\theta_1,…,\theta_i + \epsilon,…)-J(\theta_1,…,\theta_i - \epsilon,…)} {2\epsilon} \approx d\theta_i=\frac {\partial J} {\partial \theta_i}$$</p>
<p>If $$\frac {||d\theta_{approx} - d\theta ||_2} {||d\theta_{approx}||_2 + ||\theta||_2} \approx 10^{-7}$$, that’s great. If $\approx 10^{-5}$, you need to do double check, if $\approx 10^{-5}$, there may be a bug.</p>
<h4 id="3-6-Gradient-checking-implementation-notes"><a href="#3-6-Gradient-checking-implementation-notes" class="headerlink" title="3.6 Gradient checking implementation notes"></a>3.6 Gradient checking implementation notes</h4><ul>
<li>Don’t use gradient check in training, only to debug.</li>
<li>If algorithm fails gradient check, look at components to try to identify bug.</li>
<li>Remember regularization.</li>
<li>Doesn’t work with dropout.</li>
<li>Run at random initialization; perhaps again after some training.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Improving Deep Neural Networks学习笔记(一)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python的命令行参数解析</title>
    <link href="noahsnail.com/2017/09/13/2017-9-13-Python%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/"/>
    <id>noahsnail.com/2017/09/13/2017-9-13-Python的命令行参数解析/</id>
    <published>2017-09-13T02:22:00.000Z</published>
    <updated>2017-09-13T02:59:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>命令行参数解析在编程语言中基本都会碰到，Python中内置了一个用于命令项选项与参数解析的模块<code>argparse</code>。下面主要介绍两种解析Python命令行参数的方式。</p>
<h2 id="1-sys-argv"><a href="#1-sys-argv" class="headerlink" title="1. sys.argv"></a>1. sys.argv</h2><p>解析Python中命令行参数的最传统的方法是通过<code>sys.argv</code>。Demo如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#!/usr/env/python python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import sys</div><div class="line"></div><div class="line">param1 = sys.argv[1]</div><div class="line">param2 = sys.argv[2]</div><div class="line"></div><div class="line">print sys.argv</div><div class="line">print param1</div><div class="line">print param2</div><div class="line">print type(param1)</div><div class="line">print type(param2)</div></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ python test.py 1 2</div><div class="line">[&apos;test.py&apos;, &apos;1&apos;, &apos;2&apos;]</div><div class="line">1</div><div class="line">2</div></pre></td></tr></table></figure>
<p>这种方法比较古老，灵活性很差，同时解析出来的参数都是<code>str</code>类型。但在编写简单脚本，参数较少且固定时比较方便。</p>
<h2 id="2-argparse"><a href="#2-argparse" class="headerlink" title="2. argparse"></a>2. argparse</h2><p><code>argparse</code>模块是Python内置的参数解析模块，使用起来比较简单且功能强大。Demo如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">#!/usr/env/python python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import argparse</div><div class="line"></div><div class="line"># Create ArgumentParser() object</div><div class="line">parser = argparse.ArgumentParser()</div><div class="line"></div><div class="line"># Add argument</div><div class="line">parser.add_argument(&apos;--train&apos;, required=True, help=&apos;path to dataset&apos;)</div><div class="line">parser.add_argument(&apos;--val&apos;, required=True, help=&apos;path to dataset&apos;)</div><div class="line">parser.add_argument(&apos;--total&apos;, type=int, help=&apos;number of dataset&apos;, default=100)</div><div class="line">parser.add_argument(&apos;--lr&apos;, type=float, default=0.01, help=&apos;learning rate&apos;)</div><div class="line"></div><div class="line"># Print usage</div><div class="line">parser.print_help()</div><div class="line"></div><div class="line"># Parse argument</div><div class="line">args = parser.parse_args()</div><div class="line"></div><div class="line"># Print args</div><div class="line">print args</div><div class="line"></div><div class="line">print args.train</div><div class="line">print type(args.train)</div><div class="line">print args.val</div><div class="line">print type(args.val)</div><div class="line">print args.total</div><div class="line">print type(args.total)</div><div class="line">print args.lr</div><div class="line">print type(args.lr)</div></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"># Test 1</div><div class="line">python test.py --train train_lmdb --val val_lmdb --total 10000 --lr 0.001</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.001, total=10000, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">10000</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.001</div><div class="line">&lt;type &apos;float&apos;&gt;</div><div class="line"></div><div class="line"># Test 2</div><div class="line">python test.py --train train_lmdb --val val_lmdb</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.01, total=100, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">100</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.01</div><div class="line">&lt;type &apos;float&apos;&gt;</div><div class="line"></div><div class="line"></div><div class="line"># Test 3</div><div class="line">python test.py --val val_lmdb --train train_lmdb</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.01, total=100, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">100</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.01</div><div class="line">&lt;type &apos;float&apos;&gt;</div></pre></td></tr></table></figure>
<p><code>ArgumentParser</code>类创建时的参数如下：</p>
<ul>
<li>prog - 程序的名字（默认：sys.argv[0]）</li>
<li>usage - 描述程序用法的字符串（默认：从解析器的参数生成）</li>
<li>description - 参数帮助信息之前的文本（默认：空）</li>
<li>epilog - 参数帮助信息之后的文本（默认：空）</li>
<li>parents - ArgumentParser 对象的一个列表，这些对象的参数应该包括进去</li>
<li>formatter_class - 定制化帮助信息的类</li>
<li>prefix_chars - 可选参数的前缀字符集（默认：‘-‘）</li>
<li>fromfile_prefix_chars - 额外的参数应该读取的文件的前缀字符集（默认：None）</li>
<li>argument_default - 参数的全局默认值（默认：None）</li>
<li>conflict_handler - 解决冲突的可选参数的策略（通常没有必要）</li>
<li>add_help - 给解析器添加-h/–help 选项（默认：True）</li>
</ul>
<p><code>add_argument</code>函数的参数如下：</p>
<ul>
<li>name or flags - 选项字符串的名字或者列表，例如foo 或者-f, –foo。</li>
<li>action - 在命令行遇到该参数时采取的基本动作类型。</li>
<li>nargs - 应该读取的命令行参数数目。</li>
<li>const - 某些action和nargs选项要求的常数值。</li>
<li>default - 如果命令行中没有出现该参数时的默认值。</li>
<li>type - 命令行参数应该被转换成的类型。</li>
<li>choices - 参数可允许的值的一个容器。</li>
<li>required - 该命令行选项是否可以省略（只针对可选参数）。</li>
<li>help - 参数的简短描述。</li>
<li>metavar - 参数在帮助信息中的名字。</li>
<li>dest - 给parse_args()返回的对象要添加的属性名称。</li>
</ul>
<p>参考资料：</p>
<ol>
<li><a href="http://python.usyiyi.cn/translate/python_278/library/argparse.html" target="_blank" rel="external">http://python.usyiyi.cn/translate/python_278/library/argparse.html</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/explore-python/Standard-Modules/argparse.html" target="_blank" rel="external">http://wiki.jikexueyuan.com/project/explore-python/Standard-Modules/argparse.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python的命令行参数解析
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python中的编码</title>
    <link href="noahsnail.com/2017/09/07/2017-9-7-Python%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E7%A0%81/"/>
    <id>noahsnail.com/2017/09/07/2017-9-7-Python中的字符串编码/</id>
    <published>2017-09-07T03:26:27.000Z</published>
    <updated>2017-09-07T08:48:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Python处理字符串，写文件时会碰到许多的编码问题，特别是涉及到中文的时候，非常烦人，但又不得不学。下面主要记录工作过程中碰到的Python编码问题。</p>
<h2 id="1-字符串编码"><a href="#1-字符串编码" class="headerlink" title="1. 字符串编码"></a>1. 字符串编码</h2><p>Python的字符串类型为<code>str</code>，可以通过<code>type</code>函数查看返回的类型。Python中字符串默认的编码方式需要通过<code>sys.getfilesystemencoding()</code>查看，通常是<code>utf-8</code>。<code>u&#39;中文&#39;</code>构造出来的是<code>unicode</code>类型，不是<code>str</code>类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 查看字符串编码方式</div><div class="line">&gt;&gt;&gt; import sys</div><div class="line">&gt;&gt;&gt; print sys.getfilesystemencoding()</div><div class="line">utf-8</div><div class="line"></div><div class="line">&gt;&gt;&gt; s1 = &apos;中国&apos;</div><div class="line">&gt;&gt;&gt; s2 = u&apos;中国&apos;</div><div class="line">&gt;&gt;&gt; type(s1)</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">&gt;&gt;&gt; type(s2)</div><div class="line">&lt;type &apos;unicode&apos;&gt;</div></pre></td></tr></table></figure>
<p><code>str</code>类型和<code>unicode</code>类型分别有<code>decode</code>和<code>encode</code>函数。<code>str.decode</code>用来将<code>str</code>转为<code>unicode</code>，<code>unicode.encode</code>用来将<code>unicdoe</code>转为<code>str</code>。用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># decode</div><div class="line">&gt;&gt;&gt; s1.decode(&apos;utf8&apos;)</div><div class="line">u&apos;\u4e2d\u56fd&apos;</div><div class="line">&gt;&gt;&gt; type(s1.decode(&apos;utf8&apos;))</div><div class="line">&lt;type &apos;unicode&apos;&gt;</div><div class="line"></div><div class="line"># encode</div><div class="line">&gt;&gt;&gt; s2.encode(&apos;utf8&apos;)</div><div class="line">&apos;\xe4\xb8\xad\xe5\x9b\xbd&apos;</div><div class="line">&gt;&gt;&gt; type(s2.encode(&apos;utf8&apos;))</div><div class="line">&lt;type &apos;str&apos;&gt;</div></pre></td></tr></table></figure>
<h2 id="2-代码文件编码"><a href="#2-代码文件编码" class="headerlink" title="2. 代码文件编码"></a>2. 代码文件编码</h2><p><code>py</code>文件默认的编码是ASCII编码，中文显示时会进行ASCII编码到系统默认编码的转换，在运行Python文件时经常会报错。因此需要设置<code>py</code>文件的编码为<code>utf-8</code>。设置方式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># _*_ coding: utf-8 _*_</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      Python中的编码
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="noahsnail.com/2017/09/05/2017-9-5-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>noahsnail.com/2017/09/05/2017-9-5-逻辑回归/</id>
    <published>2017-09-05T01:04:24.000Z</published>
    <updated>2017-09-06T02:25:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="0-逻辑回归"><a href="#0-逻辑回归" class="headerlink" title="0. 逻辑回归"></a>0. 逻辑回归</h2><p>逻辑回归，英文名为Logistic Regression，简称LR。逻辑回归主要用于二分类问题，当然也可用于多分类问题。逻辑回归在数据挖掘，广告预测，疾病诊断等领域得到了广泛应用。</p>
<h2 id="1-问题背景"><a href="#1-问题背景" class="headerlink" title="1. 问题背景"></a>1. 问题背景</h2><p>以癌症分类为例，判断一个病人是否患有癌症，分为两种情况，“有”和“没有”，这是问题的输出。在判断时需要知道病人的年龄，性别，饮食习惯，是否抽烟等因素，这是问题的输入。为了学习癌症分类的模型，需要知道多个癌症病人和非癌症病人的数据，这是训练数据。</p>
<h2 id="2-符号表示"><a href="#2-符号表示" class="headerlink" title="2. 符号表示"></a>2. 符号表示</h2><h2 id="3-损失函数"><a href="#3-损失函数" class="headerlink" title="3. 损失函数"></a>3. 损失函数</h2><h2 id="4-求解方法"><a href="#4-求解方法" class="headerlink" title="4. 求解方法"></a>4. 求解方法</h2><h2 id="5-代码实现"><a href="#5-代码实现" class="headerlink" title="5. 代码实现"></a>5. 代码实现</h2>]]></content>
    
    <summary type="html">
    
      逻辑回归
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/09/04/2017-9-4-Batch%20Normalization%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2017/09/04/2017-9-4-Batch Normalization论文翻译——中英文对照/</id>
    <published>2017-09-04T02:02:52.000Z</published>
    <updated>2017-09-07T08:49:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift"><a href="#Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift" class="headerlink" title="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"></a>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as <em>internal covariate shift</em>, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization <em>for each training mini-batch</em>. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout.  Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching <code>4.9% top-5</code> validation error (and <code>4.8%</code> test error), exceeding the accuracy of human raters.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>训练深度神经网络的复杂性在于，每层输入的分布在训练过程中会发生变化，因为前面的层的参数发生变化。通过要求较低的学习率和仔细的参数初始化减慢了训练，并且使具有饱和非线性的模型训练起来非常困难。我们将这种现象称为<em>内部协变量转移</em>，并通过归一化层输入来解决这个问题。我们的方法力图使归一化成为模型架构的一部分，并为<em>每个训练小批量</em>执行归一化。批量归一化使我们能够使用更高的学习率，并且不用太注意初始化。它也作为一个正则化项，在某些情况下不需要Dropout。将批量归一化应用到最先进的图像分类模型上，批量归一化在取得相同的精度的情况下，减少了14倍的训练步骤，并以显著的差距击败了原始模型。使用批量归一化网络的组合，我们改进了在ImageNet分类上公布的最佳结果：达到了<code>4.9％ top-5</code>的验证误差（和<code>4.8％</code>测试误差），超过了人类评估者的准确性。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Deep learning has dramatically advanced the state of the art in vision, speech, and many other areas. Stochastic gradient descent (SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum (Sutskever et al., 2013) and Adagrad (Duchi et al., 2011) have been used to achieve state of the art performance. SGD optimizes the parameters $\Theta$ of the network, so as to minimize the loss </p>
<p>$$\Theta = \arg \min_\Theta \frac{1}{N}\sum_{i=1}^N \ell(x_i, \Theta)$$</p>
<p>where $x_{1\ldots N}$ is the training data set. With SGD, the training proceeds in steps, and at each step we consider a <em>mini-batch</em> $x_{1\ldots m}$ of size $m$. The mini-batch is used to approximate the gradient of the loss function with respect to the parameters, by computing $\frac {1} {m} \sum _{i=1} ^m \frac {\partial \ell(x_i, \Theta)} {\partial \Theta}$. Using mini-batches of examples, as opposed to one example at a time, is helpful in several ways. First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. Second, computation over a batch can be much more efficient than $m$ computations for individual examples, due to the parallelism afforded by the modern computing platforms.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>深度学习在视觉、语音等诸多方面显著提高了现有技术的水平。随机梯度下降（SGD）已经被证明是训练深度网络的有效方式，并且已经使用诸如动量（Sutskever等，2013）和Adagrad（Duchi等人，2011）等SGD变种取得了最先进的性能。SGD优化网络参数$\Theta$，以最小化损失</p>
<p>$$\Theta = \arg \min_\Theta \frac{1}{N}\sum_{i=1}^N \ell(x_i, \Theta)$$</p>
<p>$x_{1\ldots N}$是训练数据集。使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为$m$的<em>小批量</em>$x_{1 \ldots m}$。通过计算$\frac {1} {m} \sum _{i=1} ^m \frac {\partial \ell(x_i, \Theta)} {\partial \Theta}$，使用小批量来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个例子，在一些方面是有帮助的。首先，小批量的梯度损失是训练集中的梯度估计，其质量随着批量增加而改善。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个示例的$m$次的计算效率更高。</p>
<p>While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers —— so that small changes to the network parameters amplify as the network becomes deeper.</p>
<p>虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。</p>
<p>The change in the distributions of layers’ inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience <em>covariate shift</em> (Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. Consider a network computing $$\ell = F_2(F_1(u, \Theta_1), \Theta_2)$$ where $F_1$ and $F_2$ are arbitrary transformations, and the parameters $\Theta_1, \Theta_2$ are to be learned so as to minimize the loss $\ell$.  Learning $\Theta_2$ can be viewed as if the inputs $x=F_1(u,\Theta_1)$ are fed into the sub-network $$\ell = F_2(x, \Theta_2).$$</p>
<p>层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。当学习系统的输入分布发生变化时，据说会经历<em>协变量转移</em>（Shimodaira，2000）。这通常是通过域适应（Jiang，2008）来处理的。然而，协变量漂移的概念可以扩展到整个学习系统之外，适用于学习系统的一部分，例如子网络或层。考虑网络计算$$\ell = F_2(F_1(u, \Theta_1), \Theta_2)$$ $F_1$和$F_2$是任意变换，学习参数$\Theta_1，\Theta_2$以便最小化损失$\ell$。学习$\Theta_2$可以看作输入$x=F_1(u,\Theta_1)$反馈到子网络$$\ell = F_2(x, \Theta_2)。$$</p>
<p>For example, a gradient descent step $$\Theta_2\leftarrow \Theta_2 - \frac {\alpha} {m} \sum_{i=1}^m \frac {\partial F_2(x_i,\Theta_2)} {\partial \Theta_2}$$ (for batch size $m$ and learning rate $\alpha$) is exactly equivalent to that for a stand-alone network $F_2$ with input $x$.  Therefore, the input distribution properties that make training more efficient —— such as having the same distribution between the training and test data —— apply to training the sub-network as well.  As such it is advantageous for the distribution of $\vx$ to remain fixed over time. Then, $\Theta_2$ does not have to readjust to compensate for the change in the distribution of $x$.</p>
<p>例如，梯度下降步骤$$\Theta_2\leftarrow \Theta_2 - \frac {\alpha} {m} \sum_{i=1}^m \frac {\partial F_2(x_i,\Theta_2)} {\partial \Theta_2}$$（对于批大小$m$和学习率$\alpha$）与输入为$x$的单网络$F_2$完全等价。</p>
<p>Fixed distribution of inputs to a sub-network would have positive consequences for the layers {\em outside} the sub-network, as well. Consider a layer with a sigmoid activation function $\vz = g(W\vu+\vb)$ where $\vu$ is the layer input, the weight matrix $W$ andbias vector $\vb$ are the layer parameters to be learned, and $g(x) = \frac{1}{1+\exp(-x)}$. As $|x|$ increases, $g’(x)$ tends to zero. This means that for all dimensions of $\vx=W\vu+\vb$ except those with small absolute values, the gradient flowing down to $\vu$ will vanish and the model will train slowly. However, since $\vx$ is affected by $W, \vb$ and the parameters of all the layers below, changes to those parameters during training will likely move many dimensions of $\vx$ into the saturated regime of the nonlinearity and slow down the convergence. This effect is amplified as the network depth increases. In practice, the saturation problem and the resulting vanishing gradients are usually addressed by using Rectified Linear Units \cite{relu} $ReLU(x)=\max(x,0)$, careful initialization \cite{glorot-difficulty,iclr-dynamics}, and small learning rates.  If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate. </p>
<p>We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as {\em Internal Covariate Shift}. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call {\em Batch Normalization}, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization<br>step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow<br>through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout \cite{dropout}.  Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.</p>
<p>In Sec.~\ref{sec-results}, we apply Batch Normalization to the best-performing ImageNet classification network, and show that we can match its performance using only 7\% of the training steps, and can further exceed its accuracy by a substantial margin.  Using an ensemble of such networks trained with Batch Normalization, we achieve the top-5 error rate that improves upon the best known results on ImageNet classification.</p>
]]></content>
    
    <summary type="html">
    
      Batch Normalization论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization论文翻译——中文版</title>
    <link href="noahsnail.com/2017/09/04/2017-9-4-Batch%20Normalization%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/09/04/2017-9-4-Batch Normalization论文翻译——中文版/</id>
    <published>2017-09-04T02:02:23.000Z</published>
    <updated>2017-09-06T02:34:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>训练深度神经网络的复杂性在于，每层输入的分布在训练过程中会发生变化，因为前面的层的参数发生变化。通过要求较低的学习率和仔细的参数初始化减慢了训练，并且使具有饱和非线性的模型训练起来非常困难。我们将这种现象称为<em>内部协变量转移</em>，并通过归一化层输入来解决这个问题。我们的方法力图使归一化成为模型架构的一部分，并为<em>每个训练小批量</em>执行归一化。批量归一化使我们能够使用更高的学习率，并且不用太注意初始化。它也作为一个正则化项，在某些情况下不需要Dropout。将批量归一化应用到最先进的图像分类模型上，批量归一化在取得相同的精度的情况下，减少了14倍的训练步骤，并以显著的差距击败了原始模型。使用批量归一化网络的组合，我们改进了在ImageNet分类上公布的最佳结果：达到了<code>4.9％ top-5</code>的验证误差（和<code>4.8％</code>测试误差），超过了人类评估者的准确性。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>深度学习在视觉、语音等诸多方面显著提高了现有技术的水平。随机梯度下降（SGD）已经被证明是训练深度网络的有效方式，并且已经使用诸如动量（Sutskever等，2013）和Adagrad（Duchi等人，2011）等SGD变种取得了最先进的性能。SGD优化网络参数$\Theta$，以最小化损失</p>
<p>$$\Theta = \arg \min_\Theta \frac{1}{N}\sum_{i=1}^N \ell(x_i, \Theta)$$</p>
<p>$x_{1\ldots N}$是训练数据集。使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为$m$的<em>小批量</em>$x_{1 \ldots m}$。通过计算$\frac {1} {m} \sum _{i=1} ^m \frac {\partial \ell(x_i, \Theta)} {\partial \Theta}$，使用小批量来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个例子，在一些方面是有帮助的。首先，小批量的梯度损失是训练集中的梯度估计，其质量随着批量增加而改善。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个示例的$m$次的计算效率更高。</p>
<p>虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。</p>
]]></content>
    
    <summary type="html">
    
      Batch Normalization论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>CRNN论文翻译——中文版</title>
    <link href="noahsnail.com/2017/08/21/2017-8-21-CRNN%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/08/21/2017-8-21-CRNN论文翻译——中文版/</id>
    <published>2017-08-21T05:57:25.000Z</published>
    <updated>2017-09-05T02:56:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="An-End-to-End-Trainable-Neural-Network-for-Image-based-Sequence-Recognition-and-Its-Application-to-Scene-Text-Recognition"><a href="#An-End-to-End-Trainable-Neural-Network-for-Image-based-Sequence-Recognition-and-Its-Application-to-Scene-Text-Recognition" class="headerlink" title="An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition"></a>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>基于图像的序列识别一直是计算机视觉中长期存在的研究课题。在本文中，我们研究了场景文本识别的问题，这是基于图像的序列识别中最重要和最具挑战性的任务之一。提出了一种将特征提取，序列建模和转录整合到统一框架中的新型神经网络架构。与以前的场景文本识别系统相比，所提出的架构具有四个不同的特性：（1）与大多数现有的组件需要单独训练和协调的算法相比，它是端对端训练的。（2）它自然地处理任意长度的序列，不涉及字符分割或水平尺度归一化。（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。（4）它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。在包括IIIT-5K，Street View Text和ICDAR数据集在内的标准基准数据集上的实验证明了提出的算法比现有的技术更有优势。此外，提出的算法在基于图像的音乐乐谱识别任务中表现良好，这显然证实了它的泛化性。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。然而，最近大多数与深度神经网络相关的工作主要致力于检测或分类对象类别[12,25]。在本文中，我们关注计算机视觉中的一个经典问题：基于图像的序列识别。在现实世界中，稳定的视觉对象，如场景文字，手写字符和乐谱，往往以序列的形式出现，而不是孤立地出现。与一般的对象识别不同，识别这样的类序列对象通常需要系统预测一系列对象标签，而不是单个标签。因此，可以自然地将这样的对象的识别作为序列识别问题。类序列对象的另一个独特之处在于它们的长度可能会有很大变化。例如，英文单词可以由2个字符组成，如“OK”，或由15个字符组成，如“congratulations”。因此，最流行的深度模型像DCNN[25,26]不能直接应用于序列预测，因为DCNN模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。</p>
<p>已经针对特定的类似序列的对象（例如场景文本）进行了一些尝试来解决该问题。例如，[35,8]中的算法首先检测单个字符，然后用DCNN模型识别这些检测到的字符，并使用标注的字符图像进行训练。这些方法通常需要训练强字符检测器，以便从原始单词图像中准确地检测和裁剪每个字符。一些其他方法（如[22]）将场景文本识别视为图像分类问题，并为每个英文单词（总共9万个词）分配一个类标签。结果是一个大的训练模型中有很多类，这很难泛化到其它类型的类序列对象，如中文文本，音乐配乐等，因为这种序列的基本组合数目可能大于100万。总之，目前基于DCNN的系统不能直接用于基于图像的序列识别。</p>
<p>循环神经网络（RNN）模型是深度神经网络家族中的另一个重要分支，主要是设计来处理序列。RNN的优点之一是在训练和测试中不需要序列目标图像中每个元素的位置。然而，将输入目标图像转换成图像特征序列的预处理步骤通常是必需的。例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。预处理步骤独立于流程中的后续组件，因此基于RNN的现有系统不能以端到端的方式进行训练和优化。</p>
<p>一些不是基于神经网络的传统场景文本识别方法也为这一领域带来了有见地的想法和新颖的表现。例如，Almaza`n等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。Yao等人[36]和Gordo等人[14]使用中层特征进行场景文本识别。虽然在标准基准数据集上取得了有效的性能，但是前面的基于神经网络的算法[8,22]以及本文提出的方法通常都优于这些方法。</p>
<p>本文的主要贡献是一种新颖的神经网络模型，其网络架构设计专门用于识别图像中的类序列对象。所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是DCNN和RNN的组合。对于类序列对象，CRNN与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；3）具有与RNN相同的性质，能够产生一系列标签；4）对类序列对象的长度无约束，只需要在训练阶段和测试阶段对高度进行归一化；5）与现有技术相比，它在场景文本（字识别）上获得更好或更具竞争力的表现[23,8]。6）它比标准DCNN模型包含的参数要少得多，占用更少的存储空间。</p>
<h2 id="2-提出的网络架构"><a href="#2-提出的网络架构" class="headerlink" title="2. 提出的网络架构"></a>2. 提出的网络架构</h2><p>如图1所示，CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层，从底向上。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig1.png" alt="Figure 1"></p>
<p>图1。网络架构。架构包括三部分：1) 卷积层，从输入图像中提取特征序列；2) 循环层，预测每一帧的标签分布；3) 转录层，将每一帧的预测变为最终的标签序列。</p>
<p>在CRNN的底部，卷积层自动从每个输入图像中提取特征序列。在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。</p>
<h3 id="2-1-特征序列提取"><a href="#2-1-特征序列提取" class="headerlink" title="2.1. 特征序列提取"></a>2.1. 特征序列提取</h3><p>在CRNN模型中，通过采用标准CNN模型（去除全连接层）中的卷积层和最大池化层来构造卷积层的组件。这样的组件用于从输入图像中提取序列特征表示。在进入网络之前，所有的图像需要缩放到相同的高度。然后从卷积层组件产生的特征图中提取特征向量序列，这些特征向量序列作为循环层的输入。具体地，特征序列的每一个特征向量在特征图上按列从左到右生成。这意味着第i个特征向量是所有特征图第i列的连接。在我们的设置中每列的宽度固定为单个像素。</p>
<p>由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。因此，特征图的每列对应于原始图像的一个矩形区域（称为感受野），并且这些矩形区域与特征图上从左到右的相应列具有相同的顺序。如图2所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig2.png" alt="Figure 2"></p>
<p>图2。感受野。提取的特征序列中的每一个向量关联输入图像的一个感受野，可认为是该区域的特征向量。</p>
<h3 id="2-2-序列标注"><a href="#2-2-序列标注" class="headerlink" title="2.2. 序列标注"></a>2.2. 序列标注</h3><p>一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。循环层预测特征序列$x = x_1,…,x_T$中每一帧$x_t$的标签分布$y_t$。循环层的优点是三重的。首先，RNN具有很强的捕获序列内上下文信息的能力。对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。以场景文本识别为例，宽字符可能需要一些连续的帧来完全描述（参见图2）。此外，一些模糊的字符在观察其上下文时更容易区分，例如，通过对比字符高度更容易识别“il”而不是分别识别它们中的每一个。其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。第三，RNN能够从头到尾对任意长度的序列进行操作。</p>
<p>传统的RNN单元在其输入和输出层之间具有自连接的隐藏层。每次接收到序列中的帧$x_t$时，它将使用非线性函数来更新其内部状态$h_t$，该非线性函数同时接收当前输入$x_t$和过去状态$h_{t−1}$作为其输入：$h_t = g(x_t, h_{t−1})$。那么预测$y_t$是基于$h_t$的。以这种方式，过去的上下文{$\lbrace x_{t\prime} \rbrace _{t \prime &lt; t}$被捕获并用于预测。然而，传统的RNN单元有梯度消失的问题[7]，这限制了其可以存储的上下文范围，并给训练过程增加了负担。长短时记忆[18,11]（LSTM）是一种专门设计用于解决这个问题的RNN单元。LSTM（图3所示）由一个存储单元和三个多重门组成，即输入，输出和遗忘门。在概念上，存储单元存储过去的上下文，并且输入和输出门允许单元长时间地存储上下文。同时，单元中的存储可以被遗忘门清除。LSTM的特殊设计允许它捕获长距离依赖，这经常发生在基于图像的序列中。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig3.png" alt="Figure 3"></p>
<p>图3。(a) 基本的LSTM单元的结构。LSTM包括单元模块和三个门，即输入门，输出门和遗忘门。（b）我们论文中使用的深度双向LSTM结构。合并前向（从左到右）和后向（从右到左）LSTM的结果到双向LSTM中。在深度双向LSTM中堆叠多个双向LSTM结果。</p>
<p>LSTM是定向的，它只使用过去的上下文。然而，在基于图像的序列中，两个方向的上下文是相互有用且互补的。因此，我们遵循[17]，将两个LSTM，一个向前和一个向后组合到一个双向LSTM中。此外，可以堆叠多个双向LSTM，得到如图3.b所示的深双向LSTM。深层结构允许比浅层抽象更高层次的抽象，并且在语音识别任务中取得了显著的性能改进[17]。</p>
<p>在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。</p>
<h3 id="2-3-转录"><a href="#2-3-转录" class="headerlink" title="2.3. 转录"></a>2.3. 转录</h3><p>转录是将RNN所做的每帧预测转换成标签序列的过程。数学上，转录是根据每帧预测找到具有最高概率的标签序列。在实践中，存在两种转录模式，即无词典转录和基于词典的转录。词典是一组标签序列，预测受拼写检查字典约束。在无词典模式中，预测时没有任何词典。在基于词典的模式中，通过选择具有最高概率的标签序列进行预测。</p>
<h4 id="2-3-1-标签序列的概率"><a href="#2-3-1-标签序列的概率" class="headerlink" title="2.3.1 标签序列的概率"></a>2.3.1 标签序列的概率</h4><p>我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。按照每帧预测$y=y_1,…,y_T$对标签序列$l$定义概率，并忽略$l$中每个标签所在的位置。因此，当我们使用这种概率的负对数似然作为训练网络的目标函数时，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。</p>
<p>条件概率的公式简要描述如下：输入是序列$y = y_1,…,y_T$，其中$T$是序列长度。这里，每个$y_t \in\Re^{|{\cal L}’|}$是在集合${\cal L}’ = {\cal L} \cup$上的概率分布，其中${\cal L}$包含了任务中的所有标签（例如，所有英文字符），以及由<code>-</code>表示的“空白”标签。序列到序列的映射函数${\cal B}$定义在序列$\boldsymbol{\pi}\in{\cal L}’^{T}$上，其中$T$是长度。${\cal B}$将$\boldsymbol{\pi}$映射到$\mathbf{l}$上，首先删除重复的标签，然后删除<code>blank</code>。例如，${\cal B}$将“–hh-e-l-ll-oo–”（<code>-</code>表示<code>blank</code>）映射到“hello”。然后，条件概率被定义为由${\cal B}$映射到$\mathbf{l}$上的所有$\boldsymbol{\pi}$的概率之和：</p>
<p>$$<br>\begin{equation}<br>p(\mathbf{l}|\mathbf{y})=\sum_{\boldsymbol{\pi}:{\cal B}(\boldsymbol{\pi})=\mathbf{l}}p(\boldsymbol{\pi}|\mathbf{y}),\tag{1}<br>\end{equation}<br>$$</p>
<p>$\boldsymbol{\pi}$的概率定义为$p(\boldsymbol{\pi}|\mathbf{y})=\prod_{t=1}^{T}y_{\pi_{t}}^{t}$，$y_{\pi_{t}}^{t}$是时刻$t$时有标签$\pi_{t}$的概率。由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。然而，使用[15]中描述的前向算法可以有效计算方程1。</p>
<h4 id="2-3-2-无字典转录"><a href="#2-3-2-无字典转录" class="headerlink" title="2.3.2 无字典转录"></a>2.3.2 无字典转录</h4><p>在这种模式下，将具有方程1中定义的最高概率的序列$\mathbf{l}^{*}$作为预测。由于不存在用于精确找到解的可行方法，我们采用[15]中的策略。序列$\mathbf{l}^{*}$通过$\mathbf{l}^{*}\approx{\cal B}(\arg\max_{\boldsymbol{\pi}}p(\boldsymbol{\pi}|\mathbf{y}))$近似发现，即在每个时间戳$t$采用最大概率的标签$\pi_{t}$，并将结果序列映射到$\mathbf{l}^{*}$。</p>
<h4 id="2-3-3-基于词典的转录"><a href="#2-3-3-基于词典的转录" class="headerlink" title="2.3.3 基于词典的转录"></a>2.3.3 基于词典的转录</h4><p>在基于字典的模式中，每个测试采样与词典${\cal D}$相关联。基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。然而，对于大型词典，例如5万个词的Hunspell拼写检查词典[1]，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</p>
<p>$$<br>\begin{equation}<br>\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal N}_{\delta}(\mathbf{l}’)}p(\mathbf{l}|\mathbf{y}).\tag{2}<br>\end{equation}<br>$$</p>
<p>可以使用BK树数据结构[9]有效地找到候选目标${\cal N}_{\delta}(\mathbf{l}’)$，这是一种专门适用于离散度量空间的度量树。BK树的搜索时间复杂度为$O(\log|{\cal D}|)$，其中$|{\cal D}|$是词典大小。因此，这个方案很容易扩展到非常大的词典。在我们的方法中，一个词典离线构造一个BK树。然后，我们使用树执行快速在线搜索，通过查找具有小于或等于$\delta$编辑距离来查询序列。</p>
<h3 id="2-4-网络训练"><a href="#2-4-网络训练" class="headerlink" title="2.4. 网络训练"></a>2.4. 网络训练</h3><p>${\cal X}= \lbrace I_i,\mathbf{l}_i \rbrace _i $表示训练集，$I_{i}$是训练图像，$\mathbf{l}_{i}$是真实的标签序列。目标是最小化真实条件概率的负对数似然：</p>
<p>$$<br>\begin{equation}<br>{\cal O}=-\sum_{I_{i},\mathbf{l}_{i}\in{\cal X}}\log p(\mathbf{l}_{i}|\mathbf{y}_{i}),\tag{3}<br>\end{equation}<br>$$</p>
<p>$\mathbf{y}_{i}$是循环层和卷积层从$I_{i}$生成的序列。目标函数直接从图像和它的真实标签序列计算代价值。因此，网络可以在成对的图像和序列上进行端对端训练，去除了在训练图像中手动标记所有单独组件的过程。</p>
<p>网络使用随机梯度下降（SGD）进行训练。梯度由反向传播算法计算。特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。在循环层中，应用随时间反向传播（BPTT）来计算误差。</p>
<p>为了优化，我们使用ADADELTA[37]自动计算每维的学习率。与传统的动量[31]方法相比，ADADELTA不需要手动设置学习率。更重要的是，我们发现使用ADADELTA的优化收敛速度比动量方法快。</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><p>为了评估提出的CRNN模型的有效性，我们在场景文本识别和乐谱识别的标准基准数据集上进行了实验，这些都是具有挑战性的视觉任务。数据集和训练测试的设置见3.1小节，场景文本图像中CRNN的详细设置见3.2小节，综合比较的结果在3.3小节报告。为了进一步证明CRNN的泛化性，在3.4小节我们在乐谱识别任务上验证了提出的算法。</p>
<h3 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1. 数据集"></a>3.1. 数据集</h3><p>对于场景文本识别的所有实验，我们使用Jaderberg等人[20]发布的合成数据集（Synth）作为训练数据。数据集包含8百万训练图像及其对应的实际单词。这样的图像由合成文本引擎生成并且是非常现实的。我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。即使CRNN模型是在纯合成文本数据上训练，但它在标准文本识别基准数据集的真实图像上工作良好。</p>
<p>有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</p>
<p><strong>IC03</strong>[27]测试数据集包含251个具有标记文本边界框的场景图像。王等人[34]，我们忽略包含非字母数字字符或少于三个字符的图像，并获得具有860个裁剪的文本图像的测试集。每张测试图像与由Wang等人[34]定义的50词的词典相关联。通过组合所有的每张图像词汇构建完整的词典。此外，我们使用由Hunspell拼写检查字典[1]中的单词组成的5万个词的词典。</p>
<p><strong>IC13</strong>[24]测试数据集继承了IC03中的大部分数据。它包含1015个实际的裁剪单词图像。</p>
<p><strong>IIIT5k</strong>[28]包含从互联网收集的3000张裁剪的词测试图像。每张图像关联一个50词的词典和一个1000词的词典。</p>
<p><strong>SVT</strong>[34]测试数据集由从Google街景视图收集的249张街景图像组成。从它们中裁剪出了647张词图像。每张单词图像都有一个由Wang等人[34]定义的50个词的词典。</p>
<h3 id="3-2-实现细节"><a href="#3-2-实现细节" class="headerlink" title="3.2. 实现细节"></a>3.2. 实现细节</h3><p>在实验中我们使用的网络配置总结在表1中。卷积层的架构是基于VGG-VeryDeep的架构[32]。为了使其适用于识别英文文本，对其进行了调整。在第3和第4个最大池化层中，我们采用1×2大小的矩形池化窗口而不是传统的平方形。这种调整产生宽度较大的特征图，因此具有更长的特征序列。例如，包含10个字符的图像通常为大小为100×32，可以从其生成25帧的特征序列。这个长度超过了大多数英文单词的长度。最重要的是，矩形池窗口产生矩形感受野（如图2所示），这有助于识别一些具有窄形状的字符，例如<code>i</code>和<code>l</code>。</p>
<p>表1。网络配置总结。第一行是顶层。<code>k</code>，<code>s</code>，<code>p</code>分别表示核大小，步长和填充大小。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table1.png" alt="Table 1"></p>
<p>网络不仅有深度卷积层，而且还有循环层。众所周知两者都难以训练。我们发现批归一化[19]技术对于训练这种深度网络非常有用。分别在第5和第6卷积层之后插入两个批归一化层。使用批归一化层训练过程大大加快。</p>
<p>我们在Torch7[10]框架内实现了网络，使用定制实现的LSTM单元（Torch7/CUDA），转录层（C++）和BK树数据结构（C++）。实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。网络用ADADELTA训练，将参数ρ设置为0.9。在训练期间，所有图像都被缩放为100×32，以加快训练过程。训练过程大约需要50个小时才能达到收敛。测试图像缩放的高度为32。宽度与高度成比例地缩放，但至少为100像素。平均测试时间为0.16s/样本，在IC03上测得的，没有词典。近似词典搜索应用于IC03的50k词典，参数δ设置为3。测试每个样本平均花费0.53s。</p>
<h3 id="3-3-比较评估"><a href="#3-3-比较评估" class="headerlink" title="3.3. 比较评估"></a>3.3. 比较评估</h3><p>提出的CRNN模型在上述四个公共数据集上获得的所有识别精度以及最近的最新技术，包括基于深度模型[23,22,21]的方法如表2所示。</p>
<p>表2。四个数据集上识别准确率(%)。在第二行，“50”，“1k”，“50k”和“Full”表示使用的字典，“None”表示识别没有字典。*[22]严格意义上讲不是无字典的，因为它的输出限制在90K的字典。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table2.png" alt="Table 2"></p>
<p>在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。请注意，[22]中的模型是在特定字典上训练的，即每个单词都与一个类标签相关联。与[22]不同，CRNN不限于识别已知字典中的单词，并且能够处理随机字符串（例如电话号码），句子或其他诸如中文单词的脚本。 因此，CRNN的结果在所有测试数据集上都具有竞争力。</p>
<p>在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。注意，表2的“none”列中的空白表示这种方法不能应用于没有词典的识别，或者在无约束的情况下不能报告识别精度。我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。在这个意义上，我们在无限制词典表中的结果仍然是有前途的。</p>
<p>为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</p>
<p>表3。各种方法的对比。比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table3.png" alt="Table 3"></p>
<p><strong>E2E Train</strong>：这一列是为了显示某种文字阅读模型是否可以进行端到端的训练，无需任何预处理或经过几个分离的步骤，这表明这种方法对于训练是优雅且干净的。从表3可以看出，只有基于深度神经网络的模型，包括[22,21]以及CRNN具有这种性质。</p>
<p><strong>Conv Ftrs</strong>：这一列表明一个方法是否使用从训练图像直接学习到的卷积特征或手动特征作为基本的表示。</p>
<p><strong>CharGT-Free</strong>：这一列用来表明字符级标注对于训练模型是否是必要的。由于CRNN的输入和输出标签是序列，因此字符级标注是不必要的。</p>
<p><strong>Unconstrained</strong>：这一列用来表明训练模型是否受限于一个特定的字典，是否不能处理字典之外的单词或随机序列。注意尽管最近通过标签嵌入[5, 14]和增强学习[22]学习到的模型取得了非常有竞争力的性能，但它们受限于一个特定的字典。</p>
<p><strong>Model Size</strong>：这一列报告了学习模型的存储空间。在CRNN中，所有的层有权重共享连接，不需要全连接层。因此，CRNN的参数数量远小于CNN变体[22,21]所得到的模型，导致与[22,21]相比，模型要小得多。我们的模型有830万个参数，只有33MB RAM（每个参数使用4字节单精度浮点数），因此可以轻松地移植到移动设备上。</p>
<p>表3详细列出了不同方法之间的差异，充分展示了CRNN与其它竞争方法的优势。</p>
<p>另外，为了测试参数$\delta$的影响，我们在方程2中实验了$\delta$的不同值。在图4中，我们将识别精度绘制为$\delta$的函数。更大的$\delta$导致更多的候选目标，从而基于词典的转录更准确。另一方面，由于更长的BK树搜索时间，以及更大数量的候选序列用于测试，计算成本随着$\delta$的增大而增加。实际上，我们选择$\delta=3$作为精度和速度之间的折衷。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig4.png" alt="Figure 4"></p>
<p>图4。蓝线图：识别准确率作为$\delta$的函数。红条：每个样本的词典搜索时间。在IC03数据集上使用50k词典进行的测试。</p>
<h3 id="3-4-乐谱识别"><a href="#3-4-乐谱识别" class="headerlink" title="3.4. 乐谱识别"></a>3.4. 乐谱识别</h3><p>乐谱通常由排列在五线谱的音符序列组成。识别图像中的乐谱被称为光学音乐识别（OMR）问题。以前的方法通常需要图像预处理（主要是二值化），五线谱检测和单个音符识别[29]。我们将OMR作为序列识别问题，直接用CRNN从图像中预测音符的序列。为了简单起见，我们仅认识音调，忽略所有和弦，并假定所有乐谱具有相同的大调音阶（C大调）。</p>
<p>据我们所知，没有用于评估音调识别算法的公共数据集。为了准备CRNN所需的训练数据，我们从[2]中收集了2650张图像。每个图像中有一个包含3到20个音符的乐谱片段。我们手动标记所有图像的真实标签序列（不是的音调序列）。收集到的图像通过旋转，缩放和用噪声损坏增强到了265k个训练样本，并用自然图像替换它们的背景。对于测试，我们创建了三个数据集：1）“纯净的”，其中包含从[2]收集的260张图像。实例如图5.a所示；2）“合成的”，使用“纯净的”创建的，使用了上述的增强策略。它包含200个样本，其中一些如图5.b所示；3）“现实世界”，其中包含用手机相机拍摄的音乐书籍中的200张图像。例子如图5.c所示。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig5.png" alt="Figure 5"></p>
<p>图5。(a)从[2]中收集的干净的乐谱图像。(b)合成的乐谱图像。(c)用手机相机拍摄的现实世界的乐谱图像。</p>
<p>由于我们的训练数据有限，因此我们使用简化的CRNN配置来减少模型容量。与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。网络对图像对和对应的标签序列进行训练。使用两种方法来评估识别性能：1）片段准确度，即正确识别的乐谱片段的百分比；2）平均编辑距离，即预测音调序列与真实值之间的平均编辑距离。为了比较，我们评估了两种商用OMR引擎，即Capella Scan[3]和PhotoScore[4]。</p>
<p>表4总结了结果。CRNN大大优于两个商业系统。Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。另一方面，CRNN使用对噪声和扭曲具有鲁棒性的卷积特征。此外，CRNN中的循环层可以利用乐谱中的上下文信息。每个音符不仅自身被识别，而且被附近的音符识别。因此，通过将一些音符与附近的音符进行比较可以识别它们，例如对比他们的垂直位置。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table4.png" alt="Table 4"></p>
<p>表4。在我们收集的数据集上，CRNN和两个商业OMR系统对音调识别准确率的对比。通过片段准确率和平均编辑距离(“片段准确率/平均编辑距离”)来评估性能。</p>
<p>结果显示了CRNN的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。但它为OMR提供了一个新的方案，并且在音高识别方面表现出有前途的能力。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><p>在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。CRNN能够获取不同尺寸的输入图像，并产生不同长度的预测。它直接在粗粒度的标签（例如单词）上运行，在训练阶段不需要详细标注每一个单独的元素（例如字符）。此外，由于CRNN放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。所有这些属性使得CRNN成为一种基于图像序列识别的极好方法。</p>
<p>在场景文本识别基准数据集上的实验表明，与传统方法以及其它基于CNN和RNN的算法相比，CRNN实现了优异或极具竞争力的性能。这证实了所提出的算法的优点。此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</p>
<p>实际上，CRNN是一个通用框架，因此可以应用于其它的涉及图像序列预测的领域和问题（如汉字识别）。进一步加快CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>这项工作主要是由中国国家自然科学基金(NSFC)支持 (No. 61222308)。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://hunspell.sourceforge.net/" target="_blank" rel="external">http://hunspell.sourceforge.net/</a>. 4, 5</p>
<p>[2] <a href="https://musescore.com/sheetmusic" target="_blank" rel="external">https://musescore.com/sheetmusic</a>. 7, 8</p>
<p>[3] <a href="http://www.capella.de/us/index.cfm/products/capella-scan/info-capella-scan/" target="_blank" rel="external">http://www.capella.de/us/index.cfm/products/capella-scan/info-capella-scan/</a>. 8</p>
<p>[4] <a href="http://www.sibelius.com/products/photoscore/ultimate.html" target="_blank" rel="external">http://www.sibelius.com/products/photoscore/ultimate.html</a>. 8</p>
<p>[5] J. Almaza ́n, A. Gordo, A. Forne ́s, and E. Valveny. Word spotting and recognition with embedded attributes. PAMI, 36(12):2552–2566, 2014. 2, 6, 7</p>
<p>[6] O. Alsharif and J. Pineau. End-to-end text recognition with hybrid HMM maxout models. ICLR, 2014. 6, 7</p>
<p>[7] Y. Bengio, P. Y. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. NN, 5(2):157–166, 1994. 3</p>
<p>[8] A. Bissacco, M. Cummins, Y. Netzer, and H. Neven. Photoocr: Reading text in uncontrolled conditions. In ICCV, 2013. 1, 2, 6, 7</p>
<p>[9] W. A. Burkhard and R. M. Keller. Some approaches to best-match file searching. Commun. ACM, 16(4):230–236, 1973.4</p>
<p>[10] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, 2011. 6</p>
<p>[11] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber. Learning precise timing with LSTM recurrent networks. JMLR, 3:115–143, 2002. 3</p>
<p>[12] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 1, 3</p>
<p>[13] V. Goel, A. Mishra, K. Alahari, and C. V. Jawahar. Whole is greater than sum of parts: Recognizing scene text words. In ICDAR, 2013. 6, 7</p>
<p>[14] A. Gordo. Supervised mid-level features for word image representation. In CVPR, 2015. 2, 6, 7</p>
<p>[15] A. Graves, S. Ferna ́ndez, F. J. Gomez, and J. Schmidhuber. Connectionist temporal classification: labelling unseg- mented sequence data with recurrent neural networks. In ICML, 2006. 4, 5</p>
<p>[16] A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. PAMI, 31(5):855–868, 2009. 2</p>
<p>[17] A. Graves, A. Mohamed, and G. E. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, 2013. 3</p>
<p>[18] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997. 3</p>
<p>[19] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 6</p>
<p>[20] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. NIPS Deep Learning Workshop, 2014. 5</p>
<p>[21] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Deep structured output learning for unconstrained text recognition. In ICLR, 2015. 6, 7</p>
<p>[22] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Reading text in the wild with convolutional neural networks. IJCV (Accepted), 2015. 1, 2, 3, 6, 7</p>
<p>[23] M. Jaderberg, A. Vedaldi, and A. Zisserman. Deep features for text spotting. In ECCV, 2014. 2, 6, 7</p>
<p>[24] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Bigorda, S. R. Mestre, J. Mas, D. F. Mota, J. Almaza ́n, and L. de las Heras. ICDAR 2013 robust reading competition. In ICDAR, 2013. 5</p>
<p>[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1, 3</p>
<p>[26] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 1</p>
<p>[27] S. M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, R. Young, K. Ashida, H. Nagai, M. Okamoto, H. Yamamoto, H. Miyao, J. Zhu, W. Ou, C. Wolf, J. Jolion, L. Todoran, M. Worring, and X. Lin. ICDAR 2003 robust reading competitions: entries, results, and future directions. IJDAR, 7(2-3):105–122, 2005. 5</p>
<p>[28] A. Mishra, K. Alahari, and C. V. Jawahar. Scene text recognition using higher order language priors. In BMVC, 2012. 5, 6, 7</p>
<p>[29] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S. Marc ̧al, C. Guedes, and J. S. Cardoso. Optical music recognition: state-of-the-art and open issues. IJMIR, 1(3):173–190, 2012. 7</p>
<p>[30] J. A. Rodr ́ıguez-Serrano, A. Gordo, and F. Perronnin. Label embedding: A frugal baseline for text recognition. IJCV, 113(3):193–207, 2015. 2, 6, 7</p>
<p>[31] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696–699. MIT Press, 1988. 5</p>
<p>[32] K. Simonyan and A. Zisserman. Very deep convolu- tional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 5</p>
<p>[33] B. Su and S. Lu. Accurate scene text recognition based on recurrent neural network. In ACCV, 2014. 2, 6, 7</p>
<p>[34] K. Wang, B. Babenko, and S. Belongie. End-to-end scene text recognition. In ICCV, 2011. 5, 6, 7</p>
<p>[35] T. Wang, D. J. Wu, A. Coates, and A. Y. Ng. End-to-end text recognition with convolutional neural networks. In ICPR, 2012. 1, 6, 7</p>
<p>[36] C. Yao, X. Bai, B. Shi, and W. Liu. Strokelets: A learned multi-scale representation for scene text recognition. In CVPR, 2014. 2, 6, 7</p>
<p>[37] M. D. Zeiler. ADADELTA: anadaptive learning rate method. CoRR, abs/1212.5701, 2012. 5</p>
]]></content>
    
    <summary type="html">
    
      CRNN论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>CRNN论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/08/21/2017-8-21-CRNN%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2017/08/21/2017-8-21-CRNN论文翻译——中英文对照/</id>
    <published>2017-08-21T05:57:13.000Z</published>
    <updated>2017-09-06T05:18:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="An-End-to-End-Trainable-Neural-Network-for-Image-based-Sequence-Recognition-and-Its-Application-to-Scene-Text-Recognition"><a href="#An-End-to-End-Trainable-Neural-Network-for-Image-based-Sequence-Recognition-and-Its-Application-to-Scene-Text-Recognition" class="headerlink" title="An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition"></a>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>基于图像的序列识别一直是计算机视觉中长期存在的研究课题。在本文中，我们研究了场景文本识别的问题，这是基于图像的序列识别中最重要和最具挑战性的任务之一。提出了一种将特征提取，序列建模和转录整合到统一框架中的新型神经网络架构。与以前的场景文本识别系统相比，所提出的架构具有四个不同的特性：（1）与大多数现有的组件需要单独训练和协调的算法相比，它是端对端训练的。（2）它自然地处理任意长度的序列，不涉及字符分割或水平尺度归一化。（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。（4）它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。在包括IIIT-5K，Street View Text和ICDAR数据集在内的标准基准数据集上的实验证明了提出的算法比现有的技术更有优势。此外，提出的算法在基于图像的音乐乐谱识别任务中表现良好，这显然证实了它的泛化性。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Recently, the community has seen a strong revival of neural networks, which is mainly stimulated by the great success of deep neural network models, specifically Deep Convolutional Neural Networks (DCNN), in various vision tasks. However, majority of the recent works related to deep neural networks have devoted to detection or classification of object categories [12, 25]. In this paper, we are concerned with a classic problem in computer vision: image-based sequence recognition. In real world, a stable of visual objects, such as scene text, handwriting and musical score, tend to occur in the form of sequence, not in isolation. Unlike general object recognition, recognizing such sequence-like objects often requires the system to predict a series of object labels, instead of a single label. Therefore, recognition of such objects can be naturally cast as a sequence recognition problem. Another unique property of sequence-like objects is that their lengths may vary drastically. For instance, English words can either consist of 2 characters such as “OK” or 15 characters such as “congratulations”. Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。然而，最近大多数与深度神经网络相关的工作主要致力于检测或分类对象类别[12,25]。在本文中，我们关注计算机视觉中的一个经典问题：基于图像的序列识别。在现实世界中，稳定的视觉对象，如场景文字，手写字符和乐谱，往往以序列的形式出现，而不是孤立地出现。与一般的对象识别不同，识别这样的类序列对象通常需要系统预测一系列对象标签，而不是单个标签。因此，可以自然地将这样的对象的识别作为序列识别问题。类序列对象的另一个独特之处在于它们的长度可能会有很大变化。例如，英文单词可以由2个字符组成，如“OK”，或由15个字符组成，如“congratulations”。因此，最流行的深度模型像DCNN[25,26]不能直接应用于序列预测，因为DCNN模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。</p>
<p>Some attempts have been made to address this problem for a specific sequence-like object (e.g. scene text). For example, the algorithms in [35, 8] firstly detect individual characters and then recognize these detected characters with DCNN models, which are trained using labeled character images. Such methods often require training a strong character detector for accurately detecting and cropping each character out from the original word image. Some other approaches (such as [22]) treat scene text recognition as an image classification problem, and assign a class label to each English word (90K words in total). It turns out a large trained model with a huge number of classes, which is difficult to be generalized to other types of sequence-like objects, such as Chinese texts, musical scores, etc., because the numbers of basic combinations of such kind of sequences can be greater than 1 million. In summary, current systems based on DCNN can not be directly used for image-based sequence recognition.</p>
<p>已经针对特定的类似序列的对象（例如场景文本）进行了一些尝试来解决该问题。例如，[35,8]中的算法首先检测单个字符，然后用DCNN模型识别这些检测到的字符，并使用标注的字符图像进行训练。这些方法通常需要训练强字符检测器，以便从原始单词图像中准确地检测和裁剪每个字符。一些其他方法（如[22]）将场景文本识别视为图像分类问题，并为每个英文单词（总共9万个词）分配一个类标签。结果是一个大的训练模型中有很多类，这很难泛化到其它类型的类序列对象，如中文文本，音乐配乐等，因为这种序列的基本组合数目可能大于100万。总之，目前基于DCNN的系统不能直接用于基于图像的序列识别。</p>
<p>Recurrent neural networks (RNN) models, another important branch of the deep neural networks family, were mainly designed for handling sequences. One of the advantages of RNN is that it does not need the position of each element in a sequence object image in both training and testing. However, a preprocessing step that converts an input object image into a sequence of image features, is usually essential. For example, Graves et al. [16] extract a set of geometrical or image features from handwritten texts, while Su and Lu [33] convert word images into sequential HOG features. The preprocessing step is independent of the subsequent components in the pipeline, thus the existing systems based on RNN can not be trained and optimized in an end-to-end fashion.</p>
<p>循环神经网络（RNN）模型是深度神经网络家族中的另一个重要分支，主要是设计来处理序列。RNN的优点之一是在训练和测试中不需要序列目标图像中每个元素的位置。然而，将输入目标图像转换成图像特征序列的预处理步骤通常是必需的。例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。预处理步骤独立于流程中的后续组件，因此基于RNN的现有系统不能以端到端的方式进行训练和优化。</p>
<p>Several conventional scene text recognition methods that are not based on neural networks also brought insightful ideas and novel representations into this field. For example, Almaza`n et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a retrieval problem. Yao et al. [36] and Gordo et al. [14] used mid-level features for scene text recognition. Though achieved promising performance on standard benchmarks, these methods are generally outperformed by previous algorithms based on neural networks [8, 22], as well as the approach proposed in this paper.</p>
<p>一些不是基于神经网络的传统场景文本识别方法也为这一领域带来了有见地的想法和新颖的表现。例如，Almaza`n等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。Yao等人[36]和Gordo等人[14]使用中层特征进行场景文本识别。虽然在标准基准数据集上取得了有效的性能，但是前面的基于神经网络的算法[8,22]以及本文提出的方法通常都优于这些方法。</p>
<p>The main contribution of this paper is a novel neural network model, whose network architecture is specifically designed for recognizing sequence-like objects in images. The proposed neural network model is named as Convolutional Recurrent Neural Network (CRNN), since it is a combination of DCNN and RNN. For sequence-like objects, CRNN possesses several distinctive advantages over conventional neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed annotations (for instance, characters); 2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component localization, etc.; 3) It has the same property of RNN, being able to produce a sequence of labels; 4) It is unconstrained to the lengths of sequence-like objects, requiring only height normalization in both training and testing phases; 5) It achieves better or highly competitive performance on scene texts (word recognition) than the prior arts [23, 8]; 6) It contains much less parameters than a standard DCNN model, consuming less storage space.</p>
<p>本文的主要贡献是一种新颖的神经网络模型，其网络架构设计专门用于识别图像中的类序列对象。所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是DCNN和RNN的组合。对于类序列对象，CRNN与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；3）具有与RNN相同的性质，能够产生一系列标签；4）对类序列对象的长度无约束，只需要在训练阶段和测试阶段对高度进行归一化；5）与现有技术相比，它在场景文本（字识别）上获得更好或更具竞争力的表现[23,8]。6）它比标准DCNN模型包含的参数要少得多，占用更少的存储空间。</p>
<h2 id="2-The-Proposed-Network-Architecture"><a href="#2-The-Proposed-Network-Architecture" class="headerlink" title="2. The Proposed Network Architecture"></a>2. The Proposed Network Architecture</h2><p>The network architecture of CRNN, as shown in Fig. 1, consists of three components, including the convolutional layers, the recurrent layers, and a transcription layer, from bottom to top.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig1.png" alt="Figure 1"></p>
<p>Figure 1. The network architecture. The architecture consists of three parts: 1) convolutional layers, which extract a feature sequence from the input image; 2) recurrent layers, which predict a label distribution for each frame; 3) transcription layer, which translates the per-frame predictions into the final label sequence.</p>
<h2 id="2-提出的网络架构"><a href="#2-提出的网络架构" class="headerlink" title="2. 提出的网络架构"></a>2. 提出的网络架构</h2><p>如图1所示，CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层，从底向上。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig1.png" alt="Figure 1"></p>
<p>图1。网络架构。架构包括三部分：1) 卷积层，从输入图像中提取特征序列；2) 循环层，预测每一帧的标签分布；3) 转录层，将每一帧的预测变为最终的标签序列。</p>
<p>At the bottom of CRNN, the convolutional layers automatically extract a feature sequence from each input image. On top of the convolutional network, a recurrent network is built for making prediction for each frame of the feature sequence, outputted by the convolutional layers. The transcription layer at the top of CRNN is adopted to translate the per-frame predictions by the recurrent layers into a label sequence. Though CRNN is composed of different kinds of network architectures (eg. CNN and RNN), it can be jointly trained with one loss function.</p>
<p>在CRNN的底部，卷积层自动从每个输入图像中提取特征序列。在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。</p>
<h3 id="2-1-Feature-Sequence-Extraction"><a href="#2-1-Feature-Sequence-Extraction" class="headerlink" title="2.1. Feature Sequence Extraction"></a>2.1. Feature Sequence Extraction</h3><p>In CRNN model, the component of convolutional layers is constructed by taking the convolutional and max-pooling layers from a standard CNN model (fully-connected layers are removed). Such component is used to extract a sequential feature representation from an input image. Before being fed into the network, all the images need to be scaled to the same height. Then a sequence of feature vectors is extracted from the feature maps produced by the component of convolutional layers, which is the input for the recurrent layers. Specifically, each feature vector of a feature sequence is generated from left to right on the feature maps by column. This means the i-th feature vector is the concatenation of the i-th columns of all the maps. The width of each column in our settings is fixed to single pixel.</p>
<h3 id="2-1-特征序列提取"><a href="#2-1-特征序列提取" class="headerlink" title="2.1. 特征序列提取"></a>2.1. 特征序列提取</h3><p>在CRNN模型中，通过采用标准CNN模型（去除全连接层）中的卷积层和最大池化层来构造卷积层的组件。这样的组件用于从输入图像中提取序列特征表示。在进入网络之前，所有的图像需要缩放到相同的高度。然后从卷积层组件产生的特征图中提取特征向量序列，这些特征向量序列作为循环层的输入。具体地，特征序列的每一个特征向量在特征图上按列从左到右生成。这意味着第i个特征向量是所有特征图第i列的连接。在我们的设置中每列的宽度固定为单个像素。</p>
<p>As the layers of convolution, max-pooling, and element-wise activation function operate on local regions, they are translation invariant. Therefore, each column of the feature maps corresponds to a rectangle region of the original image (termed the receptive field), and such rectangle regions are in the same order to their corresponding columns on the feature maps from left to right. As illustrated in Fig. 2, each vector in the feature sequence is associated with a receptive field, and can be considered as the image descriptor for that region.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig2.png" alt="Figure 2"></p>
<p>Figure 2. The receptive field. Each vector in the extracted feature sequence is associated with a receptive field on the input image, and can be considered as the feature vector of that field.</p>
<p>由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。因此，特征图的每列对应于原始图像的一个矩形区域（称为感受野），并且这些矩形区域与特征图上从左到右的相应列具有相同的顺序。如图2所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig2.png" alt="Figure 2"></p>
<p>图2。感受野。提取的特征序列中的每一个向量关联输入图像的一个感受野，可认为是该区域的特征向量。</p>
<p>Being robust, rich and trainable, deep convolutional features have been widely adopted for different kinds of visual recognition tasks [25, 12]. Some previous approaches have employed CNN to learn a robust representation for sequence-like objects such as scene text [22]. However, these approaches usually extract holistic representation of the whole image by CNN, then the local deep features are collected for recognizing each component of a sequence-like object. Since CNN requires the input images to be scaled to a fixed size in order to satisfy with its fixed input dimension, it is not appropriate for sequence-like objects due to their large length variation. In CRNN, we convey deep features into sequential representations in order to be invariant to the length variation of sequence-like objects.</p>
<p>鲁棒的，丰富的和可训练的深度卷积特征已被广泛应用于各种视觉识别任务[25,12]。一些以前的方法已经使用CNN来学习诸如场景文本之类的类序列对象的鲁棒表示[22]。然而，这些方法通常通过CNN提取整个图像的整体表示，然后收集局部深度特征来识别类序列对象的每个分量。由于CNN要求将输入图像缩放到固定尺寸，以满足其固定的输入尺寸，因为它们的长度变化很大，因此不适合类序列对象。在CRNN中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。</p>
<h3 id="2-2-Sequence-Labeling"><a href="#2-2-Sequence-Labeling" class="headerlink" title="2.2. Sequence Labeling"></a>2.2. Sequence Labeling</h3><p>A deep bidirectional Recurrent Neural Network is built on the top of the convolutional layers, as the recurrent layers. The recurrent layers predict a label distribution $y_t$ for each frame $x_t$ in the feature sequence $x = x_1,…,x_T$. The advantages of the recurrent layers are three-fold. Firstly, RNN has a strong capability of capturing contextual information within a sequence. Using contextual cues for image-based sequence recognition is more stable and helpful than treating each symbol independently. Taking scene text recognition as an example, wide characters may require several successive frames to fully describe (refer to Fig. 2). Besides, some ambiguous characters are easier to distinguish when observing their contexts, e.g. it is easier to recognize “il” by contrasting the character heights than by recognizing each of them separately. Secondly, RNN can back-propagates error differentials to its input, i.e. the convolutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a unified network. Thirdly, RNN is able to operate on sequences of arbitrary lengths, traversing from starts to ends.</p>
<h3 id="2-2-序列标注"><a href="#2-2-序列标注" class="headerlink" title="2.2. 序列标注"></a>2.2. 序列标注</h3><p>一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。循环层预测特征序列$x = x_1,…,x_T$中每一帧$x_t$的标签分布$y_t$。循环层的优点是三重的。首先，RNN具有很强的捕获序列内上下文信息的能力。对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。以场景文本识别为例，宽字符可能需要一些连续的帧来完全描述（参见图2）。此外，一些模糊的字符在观察其上下文时更容易区分，例如，通过对比字符高度更容易识别“il”而不是分别识别它们中的每一个。其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。第三，RNN能够从头到尾对任意长度的序列进行操作。</p>
<p>A traditional RNN unit has a self-connected hidden layer between its input and output layers. Each time it receives a frame $x_t$ in the sequence, it updates its internal state $h_t$ with a non-linear function that takes both current input $x_t$ and past state $h_{t−1}$ as its inputs: $h_t = g(x_t, h_{t−1})$. Then the prediction $y_t$ is made based on $h_t$. In this way, past contexts $\lbrace x_{t\prime} \rbrace _{t \prime &lt; t}$ are captured and utilized for prediction. Traditional RNN unit, however, suffers from the vanishing gradient problem [7], which limits the range of context it can store, and adds burden to the training process. Long-Short Term Memory [18, 11] (LSTM) is a type of RNN unit that is specially designed to address this problem. An LSTM (illustrated in Fig. 3) consists of a memory cell and three multiplicative gates, namely the input, output and forget gates. Conceptually, the memory cell stores the past contexts, and the input and output gates allow the cell to store contexts for a long period of time. Meanwhile, the memory in the cell can be cleared by the forget gate. The special design of LSTM allows it to capture long-range dependencies, which often occur in image-based sequences.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig3.png" alt="Figure 3"></p>
<p>Figure 3. (a) The structure of a basic LSTM unit. An LSTM consists of a cell module and three gates, namely the input gate, the output gate and the forget gate. (b) The structure of deep bidirectional LSTM we use in our paper. Combining a forward (left to right) and a backward (right to left) LSTMs results in a bidirectional LSTM. Stacking multiple bidirectional LSTM results in a deep bidirectional LSTM.</p>
<p>传统的RNN单元在其输入和输出层之间具有自连接的隐藏层。每次接收到序列中的帧$x_t$时，它将使用非线性函数来更新其内部状态$h_t$，该非线性函数同时接收当前输入$x_t$和过去状态$h_{t−1}$作为其输入：$h_t = g(x_t, h_{t−1})$。那么预测$y_t$是基于$h_t$的。以这种方式，过去的上下文{$\lbrace x_{t\prime} \rbrace _{t \prime &lt; t}$被捕获并用于预测。然而，传统的RNN单元有梯度消失的问题[7]，这限制了其可以存储的上下文范围，并给训练过程增加了负担。长短时记忆[18,11]（LSTM）是一种专门设计用于解决这个问题的RNN单元。LSTM（图3所示）由一个存储单元和三个多重门组成，即输入，输出和遗忘门。在概念上，存储单元存储过去的上下文，并且输入和输出门允许单元长时间地存储上下文。同时，单元中的存储可以被遗忘门清除。LSTM的特殊设计允许它捕获长距离依赖，这经常发生在基于图像的序列中。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig3.png" alt="Figure 3"></p>
<p>图3。(a) 基本的LSTM单元的结构。LSTM包括单元模块和三个门，即输入门，输出门和遗忘门。（b）我们论文中使用的深度双向LSTM结构。合并前向（从左到右）和后向（从右到左）LSTM的结果到双向LSTM中。在深度双向LSTM中堆叠多个双向LSTM结果。</p>
<p>LSTM is directional, it only uses past contexts. However, in image-based sequences, contexts from both directions are useful and complementary to each other. Therefore, we follow [17] and combine two LSTMs, one forward and one backward, into a bidirectional LSTM. Furthermore, multiple bidirectional LSTMs can be stacked, resulting in a deep bidirectional LSTM as illustrated in Fig. 3.b. The deep structure allows higher level of abstractions than a shallow one, and has achieved significant performance improvements in the task of speech recognition [17].</p>
<p>LSTM是定向的，它只使用过去的上下文。然而，在基于图像的序列中，两个方向的上下文是相互有用且互补的。因此，我们遵循[17]，将两个LSTM，一个向前和一个向后组合到一个双向LSTM中。此外，可以堆叠多个双向LSTM，得到如图3.b所示的深双向LSTM。深层结构允许比浅层抽象更高层次的抽象，并且在语音识别任务中取得了显著的性能改进[17]。</p>
<p>In recurrent layers, error differentials are propagated in the opposite directions of the arrows shown in Fig. 3.b, i.e. Back-Propagation Through Time (BPTT). At the bottom of the recurrent layers, the sequence of propagated differentials are concatenated into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers. In practice, we create a custom network layer, called “Map-to-Sequence”, as the bridge between convolutional layers and recurrent layers.</p>
<p>在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。</p>
<h3 id="2-3-Transcription"><a href="#2-3-Transcription" class="headerlink" title="2.3. Transcription"></a>2.3. Transcription</h3><p>Transcription is the process of converting the per-frame predictions made by RNN into a label sequence. Mathematically, transcription is to find the label sequence with the highest probability conditioned on the per-frame predictions. In practice, there exists two modes of transcription, namely the lexicon-free and lexicon-based transcriptions. A lexicon is a set of label sequences that prediction is constraint to, e.g. a spell checking dictionary. In lexicon-free mode, predictions are made without any lexicon. In lexicon-based mode, predictions are made by choosing the label sequence that has the highest probability.</p>
<h3 id="2-3-转录"><a href="#2-3-转录" class="headerlink" title="2.3. 转录"></a>2.3. 转录</h3><p>转录是将RNN所做的每帧预测转换成标签序列的过程。数学上，转录是根据每帧预测找到具有最高概率的标签序列。在实践中，存在两种转录模式，即无词典转录和基于词典的转录。词典是一组标签序列，预测受拼写检查字典约束。在无词典模式中，预测时没有任何词典。在基于词典的模式中，通过选择具有最高概率的标签序列进行预测。</p>
<h4 id="2-3-1-Probability-of-label-sequence"><a href="#2-3-1-Probability-of-label-sequence" class="headerlink" title="2.3.1 Probability of label sequence"></a>2.3.1 Probability of label sequence</h4><p>We adopt the conditional probability defined in the Connectionist Temporal Classification (CTC) layer proposed by Graves et al. [15]. The probability is defined for label sequence $l$ conditioned on the per-frame predictions $y=y_1,…,y_T$, and it ignores the position where each label in $l$ is located. Consequently, when we use the negative log-likelihood of this probability as the objective to train the network, we only need images and their corresponding label sequences, avoiding the labor of labeling positions of individual characters.</p>
<h4 id="2-3-1-标签序列的概率"><a href="#2-3-1-标签序列的概率" class="headerlink" title="2.3.1 标签序列的概率"></a>2.3.1 标签序列的概率</h4><p>我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。按照每帧预测$y=y_1,…,y_T$对标签序列$l$定义概率，并忽略$l$中每个标签所在的位置。因此，当我们使用这种概率的负对数似然作为训练网络的目标函数时，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。</p>
<p>The formulation of the conditional probability is briefly described as follows: The input is a sequence $y = y_1,…,y_T$ where $T$ is the sequence length. Here, each $ y_t \in\Re^{|{\cal L}’|}$ is a probability distribution over the set ${\cal L}’ = {\cal L} \cup$, where ${\cal L}$ contains all labels in the task (e.g. all English characters), as well as a ’blank’ label denoted by <code>-</code>. A sequence-to-sequence mapping function ${\cal B}$ is defined on sequence $\boldsymbol{\pi}\in{\cal L}’^{T}$, where $T$ is the length. ${\cal B}$ maps $\boldsymbol{\pi}$ onto $\mathbf{l}$ by firstly removing the repeated labels, then removing the <code>blank</code>s. For example, B maps “–hh-e-l-ll-oo–” (’-’ represents ’blank’) onto “hello”. Then, the conditional probability is defined as the sum of probabilities of all $\boldsymbol{\pi}$ that are mapped by ${\cal B}$ onto $\mathbf{l}$:</p>
<p>$$<br>\begin{equation}<br>p(\mathbf{l}|\mathbf{y})=\sum_{\boldsymbol{\pi}:{\cal B}(\boldsymbol{\pi})=\mathbf{l}}p(\boldsymbol{\pi}|\mathbf{y}),\tag{1}<br>\end{equation}<br>$$</p>
<p>where the probability of $\boldsymbol{\pi}$ is defined as $p(\boldsymbol{\pi}|\mathbf{y})=\prod_{t=1}^{T}y_{\pi_{t}}^{t}$, $y_{\pi_{t}}^{t}$ is the probability of having label $\pi_{t}$ at time stamp $t$. Directly computing Eq.1 would be computationally infeasible due to the exponentially large number of summation items. However, Eq.1 can be efficiently computed using the forward-backward algorithm described in [15].</p>
<p>条件概率的公式简要描述如下：输入是序列$y = y_1,…,y_T$，其中$T$是序列长度。这里，每个$y_t \in\Re^{|{\cal L}’|}$是在集合${\cal L}’ = {\cal L} \cup$上的概率分布，其中${\cal L}$包含了任务中的所有标签（例如，所有英文字符），以及由<code>-</code>表示的“空白”标签。序列到序列的映射函数${\cal B}$定义在序列$\boldsymbol{\pi}\in{\cal L}’^{T}$上，其中$T$是长度。${\cal B}$将$\boldsymbol{\pi}$映射到$\mathbf{l}$上，首先删除重复的标签，然后删除<code>blank</code>。例如，${\cal B}$将“–hh-e-l-ll-oo–”（<code>-</code>表示<code>blank</code>）映射到“hello”。然后，条件概率被定义为由${\cal B}$映射到$\mathbf{l}$上的所有$\boldsymbol{\pi}$的概率之和：</p>
<p>$$<br>\begin{equation}<br>p(\mathbf{l}|\mathbf{y})=\sum_{\boldsymbol{\pi}:{\cal B}(\boldsymbol{\pi})=\mathbf{l}}p(\boldsymbol{\pi}|\mathbf{y}),\tag{1}<br>\end{equation}<br>$$</p>
<p>$\boldsymbol{\pi}$的概率定义为$p(\boldsymbol{\pi}|\mathbf{y})=\prod_{t=1}^{T}y_{\pi_{t}}^{t}$，$y_{\pi_{t}}^{t}$是时刻$t$时有标签$\pi_{t}$的概率。由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。然而，使用[15]中描述的前向算法可以有效计算方程1。</p>
<h4 id="2-3-2-Lexicon-free-transcription"><a href="#2-3-2-Lexicon-free-transcription" class="headerlink" title="2.3.2 Lexicon-free transcription"></a>2.3.2 Lexicon-free transcription</h4><p>In this mode, the sequence $\mathbf{l}^{*}$ that has the highest probability as defined in Eq.1 is taken as the prediction. Since there exists no tractable algorithm to precisely find the solution, we use the strategy adopted in [15]. The sequence $\mathbf{l}^{*}$ is approximately found by $\mathbf{l}^{*}\approx{\cal B}(\arg\max_{\boldsymbol{\pi}}p(\boldsymbol{\pi}|\mathbf{y}))$, i.e. taking the most probable label $\pi_{t}$ at each time stamp $t$, and map the resulted sequence onto $\mathbf{l}^{*}$.</p>
<h4 id="2-3-2-无字典转录"><a href="#2-3-2-无字典转录" class="headerlink" title="2.3.2 无字典转录"></a>2.3.2 无字典转录</h4><p>在这种模式下，将具有方程1中定义的最高概率的序列$\mathbf{l}^{*}$作为预测。由于不存在用于精确找到解的可行方法，我们采用[15]中的策略。序列$\mathbf{l}^{*}$通过$\mathbf{l}^{*}\approx{\cal B}(\arg\max_{\boldsymbol{\pi}}p(\boldsymbol{\pi}|\mathbf{y}))$近似发现，即在每个时间戳$t$采用最大概率的标签$\pi_{t}$，并将结果序列映射到$\mathbf{l}^{*}$。</p>
<h4 id="2-3-3-Lexicon-based-transcription"><a href="#2-3-3-Lexicon-based-transcription" class="headerlink" title="2.3.3 Lexicon-based transcription"></a>2.3.3 Lexicon-based transcription</h4><p>In lexicon-based mode, each test sample is associated with a lexicon ${\cal D}$. Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.1 for all sequences in the lexicon and choose the one with the highest probability. To solve this problem, we observe that the label sequences predicted via lexicon-free transcription, described in 2.3.2, are often close to the ground-truth under the edit distance metric. This indicates that we can limit our search to the nearest-neighbor candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the maximal edit distance and $\mathbf{l}’$ is the sequence transcribed from $\mathbf{y}$ in lexicon-free mode:</p>
<p>$$<br>\begin{equation}<br>\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal N}_{\delta}(\mathbf{l}’)}p(\mathbf{l}|\mathbf{y}).\tag{2}<br>\end{equation}<br>$$</p>
<h4 id="2-3-3-基于词典的转录"><a href="#2-3-3-基于词典的转录" class="headerlink" title="2.3.3 基于词典的转录"></a>2.3.3 基于词典的转录</h4><p>在基于字典的模式中，每个测试采样与词典${\cal D}$相关联。基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。然而，对于大型词典，例如5万个词的Hunspell拼写检查词典[1]，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</p>
<p>$$<br>\begin{equation}<br>\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal N}_{\delta}(\mathbf{l}’)}p(\mathbf{l}|\mathbf{y}).\tag{2}<br>\end{equation}<br>$$</p>
<p>The candidates ${\cal N}_{\delta}(\mathbf{l}’)$ can be found efficiently with the BK-tree data structure[9], which is a metric tree specifically adapted to discrete metric spaces. The search time complexity of BK-tree is $O(\log|{\cal D}|)$, where $|{\cal D}|$ is the lexicon size. Therefore this scheme readily extends to very large lexicons. In our approach, a BK-tree is constructed offline for a lexicon. Then we perform fast online search with the tree, by finding sequences that have less or equal to $\delta$ edit distance to the query sequence.</p>
<p>可以使用BK树数据结构[9]有效地找到候选目标${\cal N}_{\delta}(\mathbf{l}’)$，这是一种专门适用于离散度量空间的度量树。BK树的搜索时间复杂度为$O(\log|{\cal D}|)$，其中$|{\cal D}|$是词典大小。因此，这个方案很容易扩展到非常大的词典。在我们的方法中，一个词典离线构造一个BK树。然后，我们使用树执行快速在线搜索，通过查找具有小于或等于$\delta$编辑距离来查询序列。</p>
<h3 id="2-4-Network-Training"><a href="#2-4-Network-Training" class="headerlink" title="2.4. Network Training"></a>2.4. Network Training</h3><p>Denote the training dataset by ${\cal X}= \lbrace I_i,\mathbf{l}_i \rbrace _i $, where $I_{i}$ is the training image and $\mathbf{l}_{i}$ is the ground truth label sequence. The objective is to minimize the negative log-likelihood of conditional probability of ground truth:</p>
<p>$$<br>\begin{equation}<br>{\cal O}=-\sum_{I_{i},\mathbf{l}_{i}\in{\cal X}}\log p(\mathbf{l}_{i}|\mathbf{y}_{i}),\tag{3}<br>\end{equation}<br>$$</p>
<p>where $\mathbf{y}_{i}$ is the sequence produced by the recurrent and convolutional layers from $I_{i}$. This objective function calculates a cost value directly from an image and its ground truth label sequence. Therefore, the network can be end-to-end trained on pairs of images and sequences, eliminating the procedure of manually labeling all individual components in training images.</p>
<h3 id="2-4-网络训练"><a href="#2-4-网络训练" class="headerlink" title="2.4. 网络训练"></a>2.4. 网络训练</h3><p>${\cal X}= \lbrace I_i,\mathbf{l}_i \rbrace _i $表示训练集，$I_{i}$是训练图像，$\mathbf{l}_{i}$是真实的标签序列。目标是最小化真实条件概率的负对数似然：</p>
<p>$$<br>\begin{equation}<br>{\cal O}=-\sum_{I_{i},\mathbf{l}_{i}\in{\cal X}}\log p(\mathbf{l}_{i}|\mathbf{y}_{i}),\tag{3}<br>\end{equation}<br>$$</p>
<p>$\mathbf{y}_{i}$是循环层和卷积层从$I_{i}$生成的序列。目标函数直接从图像和它的真实标签序列计算代价值。因此，网络可以在成对的图像和序列上进行端对端训练，去除了在训练图像中手动标记所有单独组件的过程。</p>
<p>The network is trained with stochastic gradient descent (SGD). Gradients are calculated by the back-propagation algorithm. In particular, in the transcription layer, error differentials are back-propagated with the forward-backward algorithm, as described in [15]. In the recurrent layers, the Back-Propagation Through Time (BPTT) is applied to calculate the error differentials.</p>
<p>网络使用随机梯度下降（SGD）进行训练。梯度由反向传播算法计算。特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。在循环层中，应用随时间反向传播（BPTT）来计算误差。</p>
<p>For optimization, we use the ADADELTA [37] to automatically calculate per-dimension learning rates. Compared with the conventional momentum [31] method, ADADELTA requires no manual setting of a learning rate. More importantly, we find that optimization using ADADELTA converges faster than the momentum method.</p>
<p>为了优化，我们使用ADADELTA[37]自动计算每维的学习率。与传统的动量[31]方法相比，ADADELTA不需要手动设置学习率。更重要的是，我们发现使用ADADELTA的优化收敛速度比动量方法快。</p>
<h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><p>To evaluate the effectiveness of the proposed CRNN model, we conducted experiments on standard benchmarks for scene text recognition and musical score recognition, which are both challenging vision tasks. The datasets and setting for training and testing are given in Sec. 3.1, the detailed settings of CRNN for scene text images is provided in Sec. 3.2, and the results with the comprehensive comparisons are reported in Sec. 3.3. To further demonstrate the generality of CRNN, we verify the proposed algorithm on a music score recognition task in Sec. 3.4.</p>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h2><p>为了评估提出的CRNN模型的有效性，我们在场景文本识别和乐谱识别的标准基准数据集上进行了实验，这些都是具有挑战性的视觉任务。数据集和训练测试的设置见3.1小节，场景文本图像中CRNN的详细设置见3.2小节，综合比较的结果在3.3小节报告。为了进一步证明CRNN的泛化性，在3.4小节我们在乐谱识别任务上验证了提出的算法。</p>
<h3 id="3-1-Datasets"><a href="#3-1-Datasets" class="headerlink" title="3.1. Datasets"></a>3.1. Datasets</h3><p>For all the experiments for scene text recognition, we use the synthetic dataset (Synth) released by Jaderberg et al. [20] as the training data. The dataset contains 8 millions training images and their corresponding ground truth words. Such images are generated by a synthetic text engine and are highly realistic. Our network is trained on the synthetic data once, and tested on all other real-world test datasets without any fine-tuning on their training data. Even though the CRNN model is purely trained with synthetic text data, it works well on real images from standard text recognition benchmarks.</p>
<h3 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1. 数据集"></a>3.1. 数据集</h3><p>对于场景文本识别的所有实验，我们使用Jaderberg等人[20]发布的合成数据集（Synth）作为训练数据。数据集包含8百万训练图像及其对应的实际单词。这样的图像由合成文本引擎生成并且是非常现实的。我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。即使CRNN模型是在纯合成文本数据上训练，但它在标准文本识别基准数据集的真实图像上工作良好。</p>
<p>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).</p>
<p>有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</p>
<p><strong>IC03</strong> [27] test dataset contains 251 scene images with labeled text bounding boxes. Following Wang et al. [34], we ignore images that either contain non-alphanumeric characters or have less than three characters, and get a test set with 860 cropped text images. Each test image is associated with a 50-words lexicon which is defined by Wang et al. [34]. A full lexicon is built by combining all the per-image lexicons. In addition, we use a 50k words lexicon consisting of the words in the Hunspell spell-checking dictionary [1].</p>
<p><strong>IC03</strong>[27]测试数据集包含251个具有标记文本边界框的场景图像。王等人[34]，我们忽略包含非字母数字字符或少于三个字符的图像，并获得具有860个裁剪的文本图像的测试集。每张测试图像与由Wang等人[34]定义的50词的词典相关联。通过组合所有的每张图像词汇构建完整的词典。此外，我们使用由Hunspell拼写检查字典[1]中的单词组成的5万个词的词典。</p>
<p><strong>IC13</strong> [24] test dataset inherits most of its data from IC03. It contains 1,015 ground truths cropped word images.</p>
<p><strong>IC13</strong>[24]测试数据集继承了IC03中的大部分数据。它包含1015个实际的裁剪单词图像。</p>
<p><strong>IIIT5k</strong> [28] contains 3,000 cropped word test images collected from the Internet. Each image has been associated to a 50-words lexicon and a 1k-words lexicon.</p>
<p><strong>IIIT5k</strong>[28]包含从互联网收集的3000张裁剪的词测试图像。每张图像关联一个50词的词典和一个1000词的词典。</p>
<p><strong>SVT</strong> [34] test dataset consists of 249 street view images collected from Google Street View. From them 647 word images are cropped. Each word image has a 50 words lexicon defined by Wang et al. [34].</p>
<p><strong>SVT</strong>[34]测试数据集由从Google街景视图收集的249张街景图像组成。从它们中裁剪出了647张词图像。每张单词图像都有一个由Wang等人[34]定义的50个词的词典。</p>
<h3 id="3-2-Implementation-Details"><a href="#3-2-Implementation-Details" class="headerlink" title="3.2. Implementation Details"></a>3.2. Implementation Details</h3><p>The network configuration we use in our experiments is summarized in Table 1. The architecture of the convolutional layers is based on the VGG-VeryDeep architectures [32]. A tweak is made in order to make it suitable for recognizing English texts. In the 3rd and the 4th max-pooling layers, we adopt 1 × 2 sized rectangular pooling windows instead of the conventional squared ones. This tweak yields feature maps with larger width, hence longer feature sequence. For example, an image containing 10 characters is typically of size 100 × 32, from which a feature sequence 25 frames can be generated. This length exceeds the lengths of most English words. On top of that, the rectangular pooling windows yield rectangular receptive fields (illustrated in Fig. 2), which are beneficial for recognizing some characters that have narrow shapes, such as ’i’ and ’l’.</p>
<p>Table 1. Network configuration summary. The first row is the top layer. ‘k’, ‘s’ and ‘p’ stand for kernel size, stride and padding size respectively.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table1.png" alt="Table 1"></p>
<h3 id="3-2-实现细节"><a href="#3-2-实现细节" class="headerlink" title="3.2. 实现细节"></a>3.2. 实现细节</h3><p>在实验中我们使用的网络配置总结在表1中。卷积层的架构是基于VGG-VeryDeep的架构[32]。为了使其适用于识别英文文本，对其进行了调整。在第3和第4个最大池化层中，我们采用1×2大小的矩形池化窗口而不是传统的平方形。这种调整产生宽度较大的特征图，因此具有更长的特征序列。例如，包含10个字符的图像通常为大小为100×32，可以从其生成25帧的特征序列。这个长度超过了大多数英文单词的长度。最重要的是，矩形池窗口产生矩形感受野（如图2所示），这有助于识别一些具有窄形状的字符，例如<code>i</code>和<code>l</code>。</p>
<p>表1。网络配置总结。第一行是顶层。<code>k</code>，<code>s</code>，<code>p</code>分别表示核大小，步长和填充大小。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table1.png" alt="Table 1"></p>
<p>The network not only has deep convolutional layers, but also has recurrent layers. Both are known to be hard to train. We find that the batch normalization [19] technique is extremely useful for training network of such depth. Two batch normalization layers are inserted after the 5th and 6th convolutional layers respectively. With the batch normalization layers, the training process is greatly accelerated.</p>
<p>网络不仅有深度卷积层，而且还有循环层。众所周知两者都难以训练。我们发现批归一化[19]技术对于训练这种深度网络非常有用。分别在第5和第6卷积层之后插入两个批归一化层。使用批归一化层训练过程大大加快。</p>
<p>We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/CUDA), the transcription layer (in C++) and the BK-tree data structure (in C++). Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU. Networks are trained with ADADELTA, setting the parameter ρ to 0.9. During training, all images are scaled to 100 × 32 in order to accelerate the training process. The training process takes about 50 hours to reach convergence. Testing images are scaled to have height 32. Widths are proportionally scaled with heights, but at least 100 pixels. The average testing time is 0.16s/sample, as measured on IC03 without a lexicon. The approximate lexicon search is applied to the 50k lexicon of IC03, with the parameter δ set to 3. Testing each sample takes 0.53s on average.</p>
<p>我们在Torch7[10]框架内实现了网络，使用定制实现的LSTM单元（Torch7/CUDA），转录层（C++）和BK树数据结构（C++）。实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。网络用ADADELTA训练，将参数ρ设置为0.9。在训练期间，所有图像都被缩放为100×32，以加快训练过程。训练过程大约需要50个小时才能达到收敛。测试图像缩放的高度为32。宽度与高度成比例地缩放，但至少为100像素。平均测试时间为0.16s/样本，在IC03上测得的，没有词典。近似词典搜索应用于IC03的50k词典，参数δ设置为3。测试每个样本平均花费0.53s。</p>
<h3 id="3-3-Comparative-Evaluation"><a href="#3-3-Comparative-Evaluation" class="headerlink" title="3.3. Comparative Evaluation"></a>3.3. Comparative Evaluation</h3><p>All the recognition accuracies on the above four public datasets, obtained by the proposed CRNN model and the recent state-of-the-arts techniques including the approaches based on deep models [23, 22, 21], are shown in Table 2.</p>
<p>Table 2. Recognition accuracies (%) on four datasets. In the second row, “50”, “1k”, “50k” and “Full” denote the lexicon used, and “None” denotes recognition without a lexicon. *[22] is not lexicon-free in the strict sense, as its outputs are constrained to a 90k dictionary.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table2.png" alt="Table 2"></p>
<h3 id="3-3-比较评估"><a href="#3-3-比较评估" class="headerlink" title="3.3. 比较评估"></a>3.3. 比较评估</h3><p>提出的CRNN模型在上述四个公共数据集上获得的所有识别精度以及最近的最新技术，包括基于深度模型[23,22,21]的方法如表2所示。</p>
<p>表2。四个数据集上识别准确率(%)。在第二行，“50”，“1k”，“50k”和“Full”表示使用的字典，“None”表示识别没有字典。*[22]严格意义上讲不是无字典的，因为它的输出限制在90K的字典。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table2.png" alt="Table 2"></p>
<p>In the constrained lexicon cases, our method consistently outperforms most state-of-the-arts approaches, and in average beats the best text reader proposed in [22]. Specifically, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” lexicon. Note that the model in[22] is trained on a specific dictionary, namely that each word is associated to a class label. Unlike [22], CRNN is not limited to recognize a word in a known dictionary, and able to handle random strings (e.g. telephone numbers), sentences or other scripts like Chinese words. Therefore, the results of CRNN are competitive on all the testing datasets.</p>
<p>在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。请注意，[22]中的模型是在特定字典上训练的，即每个单词都与一个类标签相关联。与[22]不同，CRNN不限于识别已知字典中的单词，并且能够处理随机字符串（例如电话号码），句子或其他诸如中文单词的脚本。 因此，CRNN的结果在所有测试数据集上都具有竞争力。</p>
<p>In the unconstrained lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13. Note that the blanks in the “none” columns of Table 2 denote that such approaches are unable to be applied to recognition without lexicon or did not report the recognition accuracies in the unconstrained cases. Our method uses only synthetic text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with character-level annotations for training. The best persformance is reported by [22] in the unconstrained lexicon cases, benefiting from its large dictionary, however, it is not a model strictly unconstrained to a lexicon as mentioned before. In this sense, our results in the unconstrained lexicon case are still promising.</p>
<p>在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。注意，表2的“none”列中的空白表示这种方法不能应用于没有词典的识别，或者在无约束的情况下不能报告识别精度。我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。在这个意义上，我们在无限制词典表中的结果仍然是有前途的。</p>
<p>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv Ftrs, CharGT-Free, Unconstrained, and Model Size, as summarized in Table 3.</p>
<p>Table 3. Comparison among various methods. Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table3.png" alt="Table 3"></p>
<p>为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</p>
<p>表3。各种方法的对比。比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table3.png" alt="Table 3"></p>
<p><strong>E2E Train</strong>: This column is to show whether a certain text reading model is end-to-end trainable, without any preprocess or through several separated steps, which indicates such approaches are elegant and clean for training. As can be observed from Table 3, only the models based on deep neural networks including [22, 21] as well as CRNN have this property.</p>
<p><strong>E2E Train</strong>：这一列是为了显示某种文字阅读模型是否可以进行端到端的训练，无需任何预处理或经过几个分离的步骤，这表明这种方法对于训练是优雅且干净的。从表3可以看出，只有基于深度神经网络的模型，包括[22,21]以及CRNN具有这种性质。</p>
<p><strong>Conv Ftrs</strong>: This column is to indicate whether an approach uses the convolutional features learned from training images directly or handcraft features as the basic representations.</p>
<p><strong>Conv Ftrs</strong>：这一列用来表明一个方法是否使用从训练图像直接学习到的卷积特征或手动特征作为基本的表示。</p>
<p><strong>CharGT-Free</strong>: This column is to indicate whether the character-level annotations are essential for training the model. As the input and output labels of CRNN can be a sequence, character-level annotations are not necessary.</p>
<p><strong>CharGT-Free</strong>：这一列用来表明字符级标注对于训练模型是否是必要的。由于CRNN的输入和输出标签是序列，因此字符级标注是不必要的。</p>
<p><strong>Unconstrained</strong>: This column is to indicate whether the trained model is constrained to a specific dictionary, unable to handling out-of-dictionary words or random sequences. Notice that though the recent models learned by label embedding [5, 14] and incremental learning [22] achieved highly competitive performance, they are constrained to a specific dictionary.</p>
<p><strong>Unconstrained</strong>：这一列用来表明训练模型是否受限于一个特定的字典，是否不能处理字典之外的单词或随机序列。注意尽管最近通过标签嵌入[5, 14]和增强学习[22]学习到的模型取得了非常有竞争力的性能，但它们受限于一个特定的字典。</p>
<p><strong>Model Size</strong>: This column is to report the storage space of the learned model. In CRNN, all layers have weight-sharing connections, and the fully-connected layers are not needed. Consequently, the number of parameters of CRNN is much less than the models learned on the variants of CNN [22, 21], resulting in a much smaller model compared with [22, 21]. Our model has 8.3 million parameters, taking only 33MB RAM (using 4-bytes single-precision float for each parameter), thus it can be easily ported to mobile devices.</p>
<p><strong>Model Size</strong>：这一列报告了学习模型的存储空间。在CRNN中，所有的层有权重共享连接，不需要全连接层。因此，CRNN的参数数量远小于CNN变体[22,21]所得到的模型，导致与[22,21]相比，模型要小得多。我们的模型有830万个参数，只有33MB RAM（每个参数使用4字节单精度浮点数），因此可以轻松地移植到移动设备上。</p>
<p>Table 3 clearly shows the differences among different approaches in details, and fully demonstrates the advantages of CRNN over other competing methods.</p>
<p>表3详细列出了不同方法之间的差异，充分展示了CRNN与其它竞争方法的优势。</p>
<p>In addition, to test the impact of parameter $\delta$, we experiment different values of $\delta$ in Eq.2. In Fig.4 we plot the recognition accuracy as a function of $\delta$. Larger $\delta$ results in more candidates, thus more accurate lexicon-based transcription. On the other hand, the computational cost grows with larger $\delta$, due to longer BK-tree search time, as well as larger number of candidate sequences for testing. In practice, we choose $\delta=3$ as a tradeoff between accuracy and speed.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig4.png" alt="Figure 4"></p>
<p>Figure 4. Blue line graph: recognition accuracy as a function parameter $\delta$. Red bars: lexicon search time per sample. Tested on the IC03 dataset with the 50k lexicon.</p>
<p>另外，为了测试参数$\delta$的影响，我们在方程2中实验了$\delta$的不同值。在图4中，我们将识别精度绘制为$\delta$的函数。更大的$\delta$导致更多的候选目标，从而基于词典的转录更准确。另一方面，由于更长的BK树搜索时间，以及更大数量的候选序列用于测试，计算成本随着$\delta$的增大而增加。实际上，我们选择$\delta=3$作为精度和速度之间的折衷。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig4.png" alt="Figure 4"></p>
<p>图4。蓝线图：识别准确率作为$\delta$的函数。红条：每个样本的词典搜索时间。在IC03数据集上使用50k词典进行的测试。</p>
<h3 id="3-4-Musical-Score-Recognition"><a href="#3-4-Musical-Score-Recognition" class="headerlink" title="3.4. Musical Score Recognition"></a>3.4. Musical Score Recognition</h3><p>A musical score typically consists of sequences of musical notes arranged on staff lines. Recognizing musical scores in images is known as the Optical Music Recognition (OMR) problem. Previous methods often requires image preprocessing (mostly binirization), staff lines detection and individual notes recognition [29]. We cast the OMR as a sequence recognition problem, and predict a sequence of musical notes directly from the image with CRNN. For simplicity, we recognize pitches only, ignore all chords and assume the same major scales (C major) for all scores.</p>
<h3 id="3-4-乐谱识别"><a href="#3-4-乐谱识别" class="headerlink" title="3.4. 乐谱识别"></a>3.4. 乐谱识别</h3><p>乐谱通常由排列在五线谱的音符序列组成。识别图像中的乐谱被称为光学音乐识别（OMR）问题。以前的方法通常需要图像预处理（主要是二值化），五线谱检测和单个音符识别[29]。我们将OMR作为序列识别问题，直接用CRNN从图像中预测音符的序列。为了简单起见，我们仅认识音调，忽略所有和弦，并假定所有乐谱具有相同的大调音阶（C大调）。</p>
<p>To the best of our knowledge, there exists no public datasets for evaluating algorithms on pitch recognition. To prepare the training data needed by CRNN, we collect 2650 images from [2]. Each image contains a fragment of score containing 3 to 20 notes. We manually label the ground truth label sequences (sequences of not ezpitches) for all the images. The collected images are augmented to 265k training samples by being rotated, scaled and corrupted with noise, and by replacing their backgrounds with natural images. For testing, we create three datasets: 1) “Clean”, which contains 260 images collected from [2]. Examples are shown in Fig. 5.a; 2) “Synthesized”, which is created from “Clean”, using the augmentation strategy mentioned above. It contains 200 samples, some of which are shown in Fig. 5.b; 3) “Real-World”, which contains 200 images of score fragments taken from music books with a phone camera. Examples are shown in Fig. 5.c.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig5.png" alt="Figure 5"></p>
<p>Figure 5. (a) Clean musical scores images collected from [2] (b) Synthesized musical score images. (c) Real-world score images taken with a mobile phone camera.</p>
<p>据我们所知，没有用于评估音调识别算法的公共数据集。为了准备CRNN所需的训练数据，我们从[2]中收集了2650张图像。每个图像中有一个包含3到20个音符的乐谱片段。我们手动标记所有图像的真实标签序列（不是的音调序列）。收集到的图像通过旋转，缩放和用噪声损坏增强到了265k个训练样本，并用自然图像替换它们的背景。对于测试，我们创建了三个数据集：1）“纯净的”，其中包含从[2]收集的260张图像。实例如图5.a所示；2）“合成的”，使用“纯净的”创建的，使用了上述的增强策略。它包含200个样本，其中一些如图5.b所示；3）“现实世界”，其中包含用手机相机拍摄的音乐书籍中的200张图像。例子如图5.c所示。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-fig5.png" alt="Figure 5"></p>
<p>图5。(a)从[2]中收集的干净的乐谱图像。(b)合成的乐谱图像。(c)用手机相机拍摄的现实世界的乐谱图像。</p>
<p>Since we have limited training data, we use a simplified CRNN configuration in order to reduce model capacity. Different from the configuration specified in Tab. 1, the 4th and 6th convolution layers are removed, and the 2-layer bidirectional LSTM is replaced by a 2-layer single directional LSTM. The network is trained on the pairs of images and corresponding label sequences. Two measures are used for evaluating the recognition performance: 1) fragment accuracy, i.e. the percentage of score fragments correctly recognized; 2) average edit distance, i.e. the average edit distance between predicted pitch sequences and the ground truths. For comparison, we evaluate two commercial OMR engines, namely the Capella Scan [3] and the PhotoScore [4].</p>
<p>由于我们的训练数据有限，因此我们使用简化的CRNN配置来减少模型容量。与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。网络对图像对和对应的标签序列进行训练。使用两种方法来评估识别性能：1）片段准确度，即正确识别的乐谱片段的百分比；2）平均编辑距离，即预测音调序列与真实值之间的平均编辑距离。为了比较，我们评估了两种商用OMR引擎，即Capella Scan[3]和PhotoScore[4]。</p>
<p>Tab. 4 summarizes the results. The CRNN outperforms the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data. The main reason is that they rely on robust binarization to detect staff lines and notes, but the binarization step often fails on synthesized and real-world data due to bad lighting condition, noise corruption and cluttered background. The CRNN, on the other hand, uses convolutional features that are highly robust to noises and distortions. Besides, recurrent layers in CRNN can utilize contextual information in the score. Each note is recognized not only itself, but also by the nearby notes. Consequently, some notes can be recognized by comparing them with the nearby notes, e.g. contrasting their vertical positions.</p>
<p>Table 4. Comparison of pitch recognition accuracies, among CRNN and two commercial OMR systems, on the three datasets we have collected. Performances are evaluated by fragment accuracies and average edit distance (“fragment accuracy/average edit distance”).</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table4.png" alt="Table 4"></p>
<p>表4总结了结果。CRNN大大优于两个商业系统。Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。另一方面，CRNN使用对噪声和扭曲具有鲁棒性的卷积特征。此外，CRNN中的循环层可以利用乐谱中的上下文信息。每个音符不仅自身被识别，而且被附近的音符识别。因此，通过将一些音符与附近的音符进行比较可以识别它们，例如对比他们的垂直位置。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/crnn-table4.png" alt="Table 4"></p>
<p>表4。在我们收集的数据集上，CRNN和两个商业OMR系统对音调识别准确率的对比。通过片段准确率和平均编辑距离(“片段准确率/平均编辑距离”)来评估性能。</p>
<p>The results have shown the generality of CRNN, in that it can be readily applied to other image-based sequence recognition problems, requiring minimal domain knowledge. Compared with Capella Scan and PhotoScore, our CRNN-based system is still preliminary and misses many functionalities. But it provides a new scheme for OMR, and has shown promising capabilities in pitch recognition.</p>
<p>结果显示了CRNN的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。但它为OMR提供了一个新的方案，并且在音高识别方面表现出有前途的能力。</p>
<h2 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4. Conclusion"></a>4. Conclusion</h2><p>In this paper, we have presented a novel neural network architecture, called Convolutional Recurrent Neural Network (CRNN), which integrates the advantages of both Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). CRNN is able to take input images of varying dimensions and produces predictions with different lengths. It directly runs on coarse level labels (e.g. words), requiring no detailed annotations for each individual element (e.g. characters) in the training phase. Moreover, as CRNN abandons fully connected layers used in conventional neural networks, it results in a much more compact and efficient model. All these properties make CRNN an excellent approach for image-based sequence recognition.</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><p>在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。CRNN能够获取不同尺寸的输入图像，并产生不同长度的预测。它直接在粗粒度的标签（例如单词）上运行，在训练阶段不需要详细标注每一个单独的元素（例如字符）。此外，由于CRNN放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。所有这些属性使得CRNN成为一种基于图像序列识别的极好方法。</p>
<p>The experiments on the scene text recognition benchmarks demonstrate that CRNN achieves superior or highly competitive performance, compared with conventional methods as well as other CNN and RNN based algorithms. This confirms the advantages of the proposed algorithm. In addition, CRNN significantly outperforms other competitors on a benchmark for Optical Music Recognition (OMR), which verifies the generality of CRNN.</p>
<p>在场景文本识别基准数据集上的实验表明，与传统方法以及其它基于CNN和RNN的算法相比，CRNN实现了优异或极具竞争力的性能。这证实了所提出的算法的优点。此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</p>
<p>Actually, CRNN is a general framework, thus it can be applied to other domains and problems (such as Chinese character recognition), which involve sequence prediction in images. To further speed up CRNN and make it more practical in real-world applications is another direction that is worthy of exploration in the future.</p>
<p>实际上，CRNN是一个通用框架，因此可以应用于其它的涉及图像序列预测的领域和问题（如汉字识别）。进一步加快CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。</p>
<h2 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h2><p>This work was primarily supported by National Natural Science Foundation of China (NSFC) (No. 61222308).</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>这项工作主要是由中国国家自然科学基金(NSFC)支持 (No. 61222308)。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://hunspell.sourceforge.net/" target="_blank" rel="external">http://hunspell.sourceforge.net/</a>. 4, 5</p>
<p>[2] <a href="https://musescore.com/sheetmusic" target="_blank" rel="external">https://musescore.com/sheetmusic</a>. 7, 8</p>
<p>[3] <a href="http://www.capella.de/us/index.cfm/products/capella-scan/info-capella-scan/" target="_blank" rel="external">http://www.capella.de/us/index.cfm/products/capella-scan/info-capella-scan/</a>. 8</p>
<p>[4] <a href="http://www.sibelius.com/products/photoscore/ultimate.html" target="_blank" rel="external">http://www.sibelius.com/products/photoscore/ultimate.html</a>. 8</p>
<p>[5] J. Almaza ́n, A. Gordo, A. Forne ́s, and E. Valveny. Word spotting and recognition with embedded attributes. PAMI, 36(12):2552–2566, 2014. 2, 6, 7</p>
<p>[6] O. Alsharif and J. Pineau. End-to-end text recognition with hybrid HMM maxout models. ICLR, 2014. 6, 7</p>
<p>[7] Y. Bengio, P. Y. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. NN, 5(2):157–166, 1994. 3</p>
<p>[8] A. Bissacco, M. Cummins, Y. Netzer, and H. Neven. Photoocr: Reading text in uncontrolled conditions. In ICCV, 2013. 1, 2, 6, 7</p>
<p>[9] W. A. Burkhard and R. M. Keller. Some approaches to best-match file searching. Commun. ACM, 16(4):230–236, 1973.4</p>
<p>[10] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, 2011. 6</p>
<p>[11] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber. Learning precise timing with LSTM recurrent networks. JMLR, 3:115–143, 2002. 3</p>
<p>[12] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 1, 3</p>
<p>[13] V. Goel, A. Mishra, K. Alahari, and C. V. Jawahar. Whole is greater than sum of parts: Recognizing scene text words. In ICDAR, 2013. 6, 7</p>
<p>[14] A. Gordo. Supervised mid-level features for word image representation. In CVPR, 2015. 2, 6, 7</p>
<p>[15] A. Graves, S. Fernández, F. J. Gomez, and J. Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In ICML, 2006. 4, 5</p>
<p>[16] A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. PAMI, 31(5):855–868, 2009. 2</p>
<p>[17] A. Graves, A. Mohamed, and G. E. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, 2013. 3</p>
<p>[18] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997. 3</p>
<p>[19] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 6</p>
<p>[20] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. NIPS Deep Learning Workshop, 2014. 5</p>
<p>[21] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Deep structured output learning for unconstrained text recog- nition. In ICLR, 2015. 6, 7</p>
<p>[22] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Reading text in the wild with convolutional neural networks. IJCV (Accepted), 2015. 1, 2, 3, 6, 7</p>
<p>[23] M. Jaderberg, A. Vedaldi, and A. Zisserman. Deep features for text spotting. In ECCV, 2014. 2, 6, 7</p>
<p>[24] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Bigorda, S. R. Mestre, J. Mas, D. F. Mota, J. Almaza ́n, and L. de las Heras. ICDAR 2013 robust reading competition. In ICDAR, 2013. 5</p>
<p>[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1, 3</p>
<p>[26] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998. 1</p>
<p>[27] S. M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, R. Young, K. Ashida, H. Nagai, M. Okamoto, H. Yamamoto, H. Miyao, J. Zhu, W. Ou, C. Wolf, J. Jolion, L. Todoran, M. Worring, and X. Lin. ICDAR 2003 robust reading competitions: entries, results, and future directions. IJDAR, 7(2-3):105–122, 2005. 5</p>
<p>[28] A. Mishra, K. Alahari, and C. V. Jawahar. Scene text recognition using higher order language priors. In BMVC, 2012. 5, 6, 7</p>
<p>[29] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S. Marc ̧al, C. Guedes, and J. S. Cardoso. Optical music recognition: state-of-the-art and open issues. IJMIR, 1(3):173–190, 2012. 7</p>
<p>[30] J. A. Rodr ́ıguez-Serrano, A. Gordo, and F. Perronnin. Label embedding: A frugal baseline for text recognition. IJCV, 113(3):193–207, 2015. 2, 6, 7</p>
<p>[31] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696–699. MIT Press, 1988. 5</p>
<p>[32] K. Simonyan and A. Zisserman. Very deep convolu- tional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 5</p>
<p>[33] B. Su and S. Lu. Accurate scene text recognition based on recurrent neural network. In ACCV, 2014. 2, 6, 7</p>
<p>[34] K. Wang, B. Babenko, and S. Belongie. End-to-end scene text recognition. In ICCV, 2011. 5, 6, 7</p>
<p>[35] T. Wang, D. J. Wu, A. Coates, and A. Y. Ng. End-to-end text recognition with convolutional neural networks. In ICPR, 2012. 1, 6, 7</p>
<p>[36] C. Yao, X. Bai, B. Shi, and W. Liu. Strokelets: A learned multi-scale representation for scene text recognition. In CVPR, 2014. 2, 6, 7</p>
<p>[37] M. D. Zeiler. ADADELTA: anadaptive learning rate method. CoRR, abs/1212.5701, 2012. 5</p>
]]></content>
    
    <summary type="html">
    
      CRNN论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe模型的Forward时间及GPU显存对比</title>
    <link href="noahsnail.com/2017/08/21/2017-8-21-Caffe%E6%A8%A1%E5%9E%8B%E7%9A%84Forward%E6%97%B6%E9%97%B4%E5%8F%8AGPU%E6%98%BE%E5%AD%98%E5%AF%B9%E6%AF%94/"/>
    <id>noahsnail.com/2017/08/21/2017-8-21-Caffe模型的Forward时间及GPU显存对比/</id>
    <published>2017-08-21T02:17:20.000Z</published>
    <updated>2017-08-21T05:55:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>Caffe模型训练完成后，在实际生产环境中部署时需要对Caffe模型使用的显存（使用CPU时是内存）及模型分类的时间进行评估，下面是对比结果。测试使用的GPU为NVIDIA TESLA M40。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/model_evaluation.png" alt="Model Evaluation"></p>
]]></content>
    
    <summary type="html">
    
      Caffe模型的Forward时间及GPU显存对比
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>VGG论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/08/17/2017-8-17-VGG%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2017/08/17/2017-8-17-VGG论文翻译——中英文对照/</id>
    <published>2017-08-17T02:01:04.000Z</published>
    <updated>2017-08-29T08:11:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<h1 id="Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition"><a href="#Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition" class="headerlink" title="Very Deep Convolutional Networks for Large-Scale Image Recognition"></a>Very Deep Convolutional Networks for Large-Scale Image Recognition</h1><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在这项工作中，我们研究了卷积网络深度在大规模的图像识别环境下对准确性的影响。我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。这些发现是我们的ImageNet Challenge 2014提交的基础，我们的团队在定位和分类过程中分别获得了第一名和第二名。我们还表明，我们的表示对于其他数据集泛化的很好，在其它数据集上取得了最好的结果。我们使我们的两个性能最好的ConvNet模型可公开获得，以便进一步研究计算机视觉中深度视觉表示的使用。</p>
<h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale image and video recognition (Krizhevsky et al., 2012; Zeiler &amp; Fergus, 2013; Sermanet et al., 2014; Simonyan &amp; Zisserman, 2014) which has become possible due to the large public image repositories, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al., 2012) (the winner of ILSVRC-2012).</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>卷积网络（ConvNets）近来在大规模图像和视频识别方面取得了巨大成功（Krizhevsky等，2012；Zeiler＆Fergus，2013；Sermanet等，2014；Simonyan＆Zisserman，2014）由于大的公开图像存储库，例如ImageNet，以及高性能计算系统的出现，例如GPU或大规模分布式集群（Dean等，2012），使这成为可能。特别是，在深度视觉识别架构的进步中，ImageNet大型视觉识别挑战（ILSVRC）（Russakovsky等，2014）发挥了重要作用，它已经成为几代大规模图像分类系统的测试台，从高维度浅层特征编码（Perronnin等，2010）（ILSVRC-2011的获胜者）到深层ConvNets（Krizhevsky等，2012）（ILSVRC-2012的获奖者）。</p>
<p>With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al., 2014; Howard, 2014). In this paper, we address another important aspect of ConvNet architecture design —— its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers.</p>
<p>随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。在本文中，我们解决了ConvNet架构设计的另一个重要方面——其深度。为此，我们修正了架构的其它参数，并通过添加更多的卷积层来稳定地增加网络的深度，这是可行的，因为在所有层中使用非常小的（3×3）卷积滤波器。</p>
<p>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing models to facilitate further research.</p>
<p>因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。我们发布了两款表现最好的模型1，以便进一步研究。</p>
<p>The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect. 3, and the configurations are compared on the ILSVRC classification task in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions.</p>
<p>本文的其余部分组织如下。在第2节，我们描述了我们的ConvNet配置。图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。第5节总结了论文。为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。最后，附录C包含了主要的论文修订列表。</p>
<h2 id="2-CONVNET-CONFIGURATIONS"><a href="#2-CONVNET-CONFIGURATIONS" class="headerlink" title="2 CONVNET CONFIGURATIONS"></a>2 CONVNET CONFIGURATIONS</h2><p>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3.</p>
<h2 id="2-ConvNet配置"><a href="#2-ConvNet配置" class="headerlink" title="2. ConvNet配置"></a>2. ConvNet配置</h2><p>为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。最后，我们的设计选择将在2.3节进行讨论并与现有技术进行比较。</p>
<h3 id="2-1-ARCHITECTURE"><a href="#2-1-ARCHITECTURE" class="headerlink" title="2.1 ARCHITECTURE"></a>2.1 ARCHITECTURE</h3><p>During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.</p>
<p>在训练期间，我们的ConvNet的输入是固定大小的224×224 RGB图像。我们唯一的预处理是从每个像素中减去在训练集上计算的RGB均值。图像通过一堆卷积（conv.）层，我们使用感受野很小的滤波器：3×3（这是捕获左/右，上/下，中心概念的最小尺寸）。在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。卷积步长固定为1个像素；卷积层输入的空间填充要满足卷积之后保留空间分辨率，即3×3卷积层的填充为1个像素。空间池化由五个最大池化层进行，这些层在一些卷积层之后（不是所有的卷积层之后都是最大池化）。在2×2像素窗口上进行最大池化，步长为2。</p>
<p>A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.</p>
<p>一堆卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。最后一层是soft-max层。所有网络中全连接层的配置是相同的。</p>
<p>All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).</p>
<p>所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</p>
<h3 id="2-2-CONFIGURATIONS"><a href="#2-2-CONFIGURATIONS" class="headerlink" title="2.2 CONFIGURATIONS"></a>2.2 CONFIGURATIONS</h3><p>The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A–E). All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.</p>
<p>Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “conv⟨receptive field size⟩-⟨number of channels⟩”. The ReLU activation function is not shown for brevity.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table1.png" alt="Table 1"></p>
<h3 id="2-2-配置"><a href="#2-2-配置" class="headerlink" title="2.2 配置"></a>2.2 配置</h3><p>本文中评估的ConvNet配置在表1中列出，每列一个。接下来我们将按网站名称（A-E）来提及网络。所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池化层之后增加2倍，直到达到512。</p>
<p>表1：ConvNet配置（以列显示）。随着更多的层被添加，配置的深度从左（A）增加到右（E）（添加的层以粗体显示）。卷积层参数表示为“conv⟨感受野大小⟩-通道数⟩”。为了简洁起见，不显示ReLU激活功能。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table1.png" alt="Table 1"></p>
<p>In Table 2 we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (Sermanet et al., 2014)).</p>
<p>Table 2: Number of parameters (in millions).</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table2.png" alt="Table 2"></p>
<p>在表2中，我们报告了每个配置的参数数量。尽管深度很大，我们的网络中权重数量并不大于具有更大卷积层宽度和感受野的较浅网络中的权重数量（144M的权重在（Sermanet等人，2014）中）。</p>
<p>表2：参数数量（百万级别）</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table2.png" alt="Table 2"></p>
<h3 id="2-3-DISCUSSION"><a href="#2-3-DISCUSSION" class="headerlink" title="2.3 DISCUSSION"></a>2.3 DISCUSSION</h3><p>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014). Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al., 2012), or 7 × 7 with stride 2 in (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 × 5; three such layers have a 7 × 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 × 3 conv. layers instead of a single 7 × 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has $C$ channels, the stack is parametrised by $3(3^2C^2)=27C^2$ weights; at the same time, a single 7 × 7 conv. layer would require $7^2C^2=49C^2$ parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between).</p>
<h3 id="2-3-讨论"><a href="#2-3-讨论" class="headerlink" title="2.3 讨论"></a>2.3 讨论</h3><p>我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。很容易看到两个3×3卷积层堆叠（没有空间池化）有5×5的有效感受野；三个这样的层具有7×7的有效感受野。那么我们获得了什么？例如通过使用三个3×3卷积层的堆叠来替换单个7×7层。首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。其次，我们减少参数的数量：假设三层3×3卷积堆叠的输入和输出有$C$个通道，堆叠卷积层的参数为$3(3^2C^2)=27C^2$个权重；同时，单个7×7卷积层将需要$7^2C^2=49C^2$个参数，即参数多81％。这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</p>
<p>The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 × 1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).</p>
<p>结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。即使在我们的案例下，1×1卷积基本上是在相同维度空间上的线性投影（输入和输出通道的数量相同），由修正函数引入附加的非线性。应该注意的是1×1卷积层最近在Lin等人(2014)的“Network in Network”架构中已经得到了使用。</p>
<p>Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). Their network topology is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classification accuracy.</p>
<p>Ciresan等人（2011）以前使用小尺寸的卷积滤波器，但是他们的网络深度远远低于我们的网络，他们并没有在大规模的ILSVRC数据集上进行评估。Goodfellow等人（2014）在街道号识别任务中采用深层ConvNets（11个权重层），显示出增加的深度导致了更好的性能。GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。然而，它们的网络拓扑结构比我们的更复杂，并且在第一层中特征图的空间分辨率被更积极地减少，以减少计算量。正如将在第4.5节显示的那样，我们的模型在单网络分类精度方面胜过Szegedy等人（2014）。</p>
<h2 id="3-CLASSIFICATION-FRAMEWORK"><a href="#3-CLASSIFICATION-FRAMEWORK" class="headerlink" title="3 CLASSIFICATION FRAMEWORK"></a>3 CLASSIFICATION FRAMEWORK</h2><p>In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation.</p>
<h2 id="3-分类框架"><a href="#3-分类框架" class="headerlink" title="3 分类框架"></a>3 分类框架</h2><p>在上一节中，我们介绍了我们的网络配置的细节。在本节中，我们将介绍分类ConvNet训练和评估的细节。</p>
<h3 id="3-1-TRAINING"><a href="#3-1-TRAINING" class="headerlink" title="3.1 TRAINING"></a>3.1 TRAINING</h3><p>The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to $5·10^{−4}$) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to $10^{−2}$, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.</p>
<h3 id="3-1-训练"><a href="#3-1-训练" class="headerlink" title="3.1 训练"></a>3.1 训练</h3><p>ConvNet训练过程通常遵循Krizhevsky等人（2012）（除了从多尺度训练图像中对输入裁剪图像进行采样外，如下文所述）。也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。批量大小设为256，动量为0.9。训练通过权重衰减（L2惩罚乘子设定为$5·10^{−4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。学习率初始设定为$10^{−2}$，然后当验证集准确率停止改善时，减少10倍。学习率总共降低3次，学习在37万次迭代后停止（74个epochs）。我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</p>
<p>The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully-connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and $10^{−2}$ variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot &amp; Bengio (2010).</p>
<p>网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。为了规避这个问题，我们开始训练配置A（表1），足够浅以随机初始化进行训练。然后，当训练更深的架构时，我们用网络A的层初始化前四个卷积层和最后三个全连接层（中间层被随机初始化）。我们没有减少预初始化层的学习率，允许他们在学习过程中改变。对于随机初始化（如果应用），我们从均值为0和方差为$10^{−2}$的正态分布中采样权重。偏置初始化为零。值得注意的是，在提交论文之后，我们发现可以通过使用Glorot＆Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</p>
<p>To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al., 2012). Training image rescaling is explained below.</p>
<p>为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。为了进一步增强训练集，裁剪图像经过了随机水平翻转和随机RGB颜色偏移（Krizhevsky等，2012）。下面解释训练图像归一化。</p>
<p><strong>Training image size</strong>. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S as the training scale). While the crop size is fixed to 224 × 224, in principle S can take on any value not less than 224: for $S = 224$ the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for $S ≫ 224$ the crop will correspond to a small part of the image, containing a small object or an object part.</p>
<p><strong>训练图像大小</strong>。令S是等轴归一化的训练图像的最小边，ConvNet输入从S中裁剪（我们也将S称为训练尺度）。虽然裁剪尺寸固定为224×224，但原则上S可以是不小于224的任何值：对于$S=224$，裁剪图像将捕获整个图像的统计数据，完全扩展训练图像的最小边；对于$S»224$，裁剪图像将对应于图像的一小部分，包含小对象或对象的一部分。</p>
<p>We consider two approaches for setting the training scale S. The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi-scale image statistics). In our experiments, we evaluated models trained at two fixed scales: $S = 256$ (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler &amp; Fergus, 2013; Sermanet et al., 2014)) and $S = 384$. Given a ConvNet configuration, we first trained the network using $S = 256$. To speed-up training of the $S = 384$ network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of $10^{−3}$.</p>
<p>我们考虑两种方法来设置训练尺度S。第一种是修正对应单尺度训练的S（注意，采样裁剪图像中的图像内容仍然可以表示多尺度图像统计）。在我们的实验中，我们评估了以两个固定尺度训练的模型：$S = 256$（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和$S = 384$。给定ConvNet配置，我们首先使用$S=256$来训练网络。为了加速$S = 384$网络的训练，用$S = 256$预训练的权重来进行初始化，我们使用较小的初始学习率$10^{−3}$。</p>
<p>The second approach to setting S is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range $[S_{min},S_{max}]$ (we used $S_{min} = 256$ and $S_{max} = 512$). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed $S = 384$.</p>
<p>设置S的第二种方法是多尺度训练，其中每个训练图像通过从一定范围$[S_{min}，S_{max}]$（我们使用$S_{min} = 256$和$S_{max} = 512$）随机采样S来单独进行归一化。由于图像中的目标可能具有不同的大小，因此在训练期间考虑到这一点是有益的。这也可以看作是通过尺度抖动进行训练集增强，其中单个模型被训练在一定尺度范围内识别对象。为了速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调，训练了多尺度模型，并用固定的$S = 384$进行预训练。</p>
<h3 id="3-2-TESTING"><a href="#3-2-TESTING" class="headerlink" title="3.2 TESTING"></a>3.2 TESTING</h3><p>At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.</p>
<h3 id="3-2-测试"><a href="#3-2-测试" class="headerlink" title="3.2 测试"></a>3.2 测试</h3><p>在测试时，给出训练的ConvNet和输入图像，它按以下方式分类。首先，将其等轴地归一化到预定义的最小图像边，表示为Q（我们也将其称为测试尺度）。我们注意到，Q不一定等于训练尺度S（正如我们在第4节中所示，每个S使用Q的几个值会导致性能改进）。然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。即，全连接层首先被转换成卷积层（第一FC层转换到7×7卷积层，最后两个FC层转换到1×1卷积层）。然后将所得到的全卷积网络应用于整个（未裁剪）图像上。结果是类得分图的通道数等于类别的数量，以及取决于输入图像大小的可变空间分辨率。最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</p>
<p>Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).</p>
<p>由于全卷积网络被应用在整个图像上，所以不需要在测试时对采样多个裁剪图像（Krizhevsky等，2012），因为它需要网络重新计算每个裁剪图像，这样效率较低。同时，如Szegedy等人（2014）所做的那样，使用大量的裁剪图像可以提高准确度，因为与全卷积网络相比，它使输入图像的采样更精细。此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</p>
<h3 id="3-3-IMPLEMENTATION-DETAILS"><a href="#3-3-IMPLEMENTATION-DETAILS" class="headerlink" title="3.3 IMPLEMENTATION DETAILS"></a>3.3 IMPLEMENTATION DETAILS</h3><p>Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU.</p>
<h3 id="3-3-实现细节"><a href="#3-3-实现细节" class="headerlink" title="3.3 实现细节"></a>3.3 实现细节</h3><p>我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。多GPU训练利用数据并行性，通过将每批训练图像分成几个GPU批次，每个GPU并行处理。在计算GPU批次梯度之后，将其平均以获得完整批次的梯度。梯度计算在GPU之间是同步的，所以结果与在单个GPU上训练完全一样。</p>
<p>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.</p>
<p>最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。在配备四个NVIDIA Titan Black GPU的系统上，根据架构训练单个网络需要2-3周时间。</p>
<h2 id="4-CLASSIFICATION-EXPERIMENTS"><a href="#4-CLASSIFICATION-EXPERIMENTS" class="headerlink" title="4 CLASSIFICATION EXPERIMENTS"></a>4 CLASSIFICATION EXPERIMENTS</h2><p>Dataset. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The classification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.</p>
<h2 id="4-分类实验"><a href="#4-分类实验" class="headerlink" title="4 分类实验"></a>4 分类实验</h2><p>数据集。在本节中，我们介绍了描述的ConvNet架构（用于ILSVRC 2012-2014挑战）在ILSVRC-2012数据集上实现的图像分类结果。数据集包括1000个类别的图像，并分为三组：训练（130万张图像），验证（5万张图像）和测试（留有类标签的10万张图像）。使用两个措施评估分类性能：top-1和top-5错误率。前者是多类分类误差，即不正确分类图像的比例；后者是ILSVRC中使用的主要评估标准，并且计算为图像真实类别在前5个预测类别之外的图像比例。</p>
<p>For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al., 2014).</p>
<p>对于大多数实验，我们使用验证集作为测试集。在测试集上也进行了一些实验，并将其作为ILSVRC-2014竞赛（Russakovsky等，2014）“VGG”小组的输入提交到了官方的ILSVRC服务器。</p>
<h3 id="4-1-SINGLE-SCALE-EVALUATION"><a href="#4-1-SINGLE-SCALE-EVALUATION" class="headerlink" title="4.1 SINGLE SCALE EVALUATION"></a>4.1 SINGLE SCALE EVALUATION</h3><p>We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect. 2.2. The test image size was set as follows: $Q = S$ for fixed S, and $Q = 0.5(S_{min} + S_{max})$ for jittered $S ∈ [S_{min}, S_{max}]$. The results of are shown in Table 3.</p>
<p>Table 3: ConvNet performance at a single test scale.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table3.png" alt="Table 3"></p>
<h3 id="4-1-单尺度评估"><a href="#4-1-单尺度评估" class="headerlink" title="4.1 单尺度评估"></a>4.1 单尺度评估</h3><p>我们首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。测试图像大小设置如下：对于固定S的$Q = S$，对于抖动$S ∈ [S_{min}, S_{max}]$，$Q = 0.5(S_{min} + S_{max})$。结果如表3所示。</p>
<p>表3：在单测试尺度的ConvNet性能</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table3.png" alt="Table 3"></p>
<p>First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E).</p>
<p>首先，我们注意到，使用局部响应归一化（A-LRN网络）在没有任何归一化层的情况下，对模型A没有改善。因此，我们在较深的架构（B-E）中不采用归一化。</p>
<p>Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which uses 3 × 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.</p>
<p>第二，我们观察到分类误差随着ConvNet深度的增加而减小：从A中的11层到E中的19层。值得注意的是，尽管深度相同，配置C（包含三个1×1卷积层）比在整个网络层中使用3×3卷积的配置D更差。这表明，虽然额外的非线性确实有帮助（C优于B），但也可以通过使用具有非平凡感受野（D比C好）的卷积滤波器来捕获空间上下文。当深度达到19层时，我们架构的错误率饱和，但更深的模型可能有益于较大的数据集。我们还将网络B与具有5×5卷积层的浅层网络进行了比较，浅层网络可以通过用单个5×5卷积层替换B中每对3×3卷积层得到（其具有相同的感受野如第2.3节所述）。测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。</p>
<p>Finally, scale jittering at training time ($S ∈ [256; 512]$) leads to significantly better results than training on images with fixed smallest side ($S = 256$ or $S = 384$), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics.</p>
<p>最后，训练时的尺度抖动（$S∈[256; 512]$）得到了与固定最小边（$S = 256$或$S = 384$）的图像训练相比更好的结果，即使在测试时使用单尺度。这证实了通过尺度抖动进行的训练集增强确实有助于捕获多尺度图像统计。</p>
<h3 id="4-2-MULTI-SCALE-EVALUATION"><a href="#4-2-MULTI-SCALE-EVALUATION" class="headerlink" title="4.2 MULTI-SCALE EVALUATION"></a>4.2 MULTI-SCALE EVALUATION</h3><p>Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: $Q = {S − 32, S, S + 32}$. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable $S ∈ [S_{min}; S_{max}]$ was evaluated over a larger range of sizes $Q = {S_{min}, 0.5(S_{min} + S_{max}), S_{max}$.</p>
<h3 id="4-2-多尺度评估"><a href="#4-2-多尺度评估" class="headerlink" title="4.2 多尺度评估"></a>4.2 多尺度评估</h3><p>在单尺度上评估ConvNet模型后，我们现在评估测试时尺度抖动的影响。它包括在一张测试图像的几个归一化版本上运行模型（对应于不同的Q值），然后对所得到的类别后验进行平均。考虑到训练和测试尺度之间的巨大差异会导致性能下降，用固定S训练的模型在三个测试图像尺度上进行了评估，接近于训练一次：$Q = {S − 32, S, S + 32}$。同时，训练时的尺度抖动允许网络在测试时应用于更广的尺度范围，所以用变量$S ∈ [S_{min}; S_{max}]$训练的模型在更大的尺寸范围$Q = {S_{min}, 0.5(S_{min} + S_{max}), S_{max}$上进行评估。</p>
<p>The results, presented in Table 4, indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is <code>24.8%/7.5% top-1/top-5</code> error (highlighted in bold in Table 4). On the test set, the configuration E achieves 7.3% top-5 error.</p>
<p>Table 4: ConvNet performance at multiple test scales.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table4.png" alt="Table 4"></p>
<p>表4中给出的结果表明，测试时的尺度抖动导致了更好的性能（与在单一尺度上相同模型的评估相比，如表3所示）。如前所述，最深的配置（D和E）执行最佳，并且尺度抖动优于使用固定最小边S的训练。我们在验证集上的最佳单网络性能为<code>24.8％/7.5％ top-1/top-5</code>的错误率（在表4中用粗体突出显示）。在测试集上，配置E实现了<code>7.3％ top-5</code>的错误率。</p>
<p>表4：在多个测试尺度上的ConvNet性能</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table4.png" alt="Table 4"></p>
<h3 id="4-3-MULTI-CROP-EVALUATION"><a href="#4-3-MULTI-CROP-EVALUATION" class="headerlink" title="4.3 MULTI-CROP EVALUATION"></a>4.3 MULTI-CROP EVALUATION</h3><p>In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for details). We also assess the complementarity of the two evaluation techniques by averaging their soft-max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions.</p>
<p>Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384, 512}.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table5.png" alt="Table 5"></p>
<h3 id="4-3-多裁剪图像评估"><a href="#4-3-多裁剪图像评估" class="headerlink" title="4.3 多裁剪图像评估"></a>4.3 多裁剪图像评估</h3><p>在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。我们还通过平均其soft-max输出来评估两种评估技术的互补性。可以看出，使用多裁剪图像表现比密集评估略好，而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。如上所述，我们假设这是由于卷积边界条件的不同处理。</p>
<p>表5：ConvNet评估技术比较。在所有的实验中训练尺度S从[256；512]采样，三个测试适度Q考虑：{256, 384, 512}。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table5.png" alt="Table 5"></p>
<h3 id="4-4-CONVNET-FUSION"><a href="#4-4-CONVNET-FUSION" class="headerlink" title="4.4 CONVNET FUSION"></a>4.4 CONVNET FUSION</h3><p>Up until now, we evaluated the performance of individual ConvNet models. In this part of the experiments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014).</p>
<h3 id="4-4-卷积网络融合"><a href="#4-4-卷积网络融合" class="headerlink" title="4.4 卷积网络融合"></a>4.4 卷积网络融合</h3><p>到目前为止，我们评估了ConvNet模型的性能。在这部分实验中，我们通过对soft-max类别后验进行平均，结合了几种模型的输出。由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</p>
<p>The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5).</p>
<p>Table 6: Multiple ConvNet fusion results.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table6.png" alt="Table 6"></p>
<p>结果如表6所示。在ILSVRC提交的时候，我们只训练了单规模网络，以及一个多尺度模型D（仅在全连接层进行微调而不是所有层）。由此产生的7个网络组合具有7.3％的ILSVRC测试误差。在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。作为参考，我们表现最佳的单模型达到7.1％的误差（模型E，表5）。</p>
<p>表6：多个卷积网络融合结果</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table6.png" alt="Table 6"></p>
<h3 id="4-5-COMPARISON-WITH-THE-STATE-OF-THE-ART"><a href="#4-5-COMPARISON-WITH-THE-STATE-OF-THE-ART" class="headerlink" title="4.5 COMPARISON WITH THE STATE OF THE ART"></a>4.5 COMPARISON WITH THE STATE OF THE ART</h3><p>Finally, we compare our results with the state of the art in Table 7. In the classification task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with<br>7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models.</p>
<p>Table 7: Comparison with the state of the art in ILSVRC classification. Our method is denoted as “VGG”. Only the results obtained without outside training data are reported.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table7.png" alt="Table 7"></p>
<h3 id="4-5-与最新技术比较"><a href="#4-5-与最新技术比较" class="headerlink" title="4.5 与最新技术比较"></a>4.5 与最新技术比较</h3><p>最后，我们在表7中与最新技术比较我们的结果。在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，<br>使用7个模型的组合取得了7.3％测试误差。提交后，我们使用2个模型的组合将错误率降低到6.8％。</p>
<p>表7：在ILSVRC分类中与最新技术比较。我们的方法表示为“VGG”。报告的结果没有使用外部数据。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table7.png" alt="Table 7"></p>
<p>As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models —— significantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.</p>
<p>从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。这是非常显著的，考虑到我们最好的结果是仅通过组合两个模型实现的——明显少于大多数ILSVRC提交。在单网络性能方面，我们的架构取得了最好节果（7.0％测试误差），超过单个GoogLeNet 0.9％。值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</p>
<h2 id="5-CONCLUSION"><a href="#5-CONCLUSION" class="headerlink" title="5 CONCLUSION"></a>5 CONCLUSION</h2><p>In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large-scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations.</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>在这项工作中，我们评估了非常深的卷积网络（最多19个权重层）用于大规模图像分类。已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。在附录中，我们还显示了我们的模型很好地泛化到各种各样的任务和数据集上，可以匹敌或超越更复杂的识别流程，其构建围绕不深的图像表示。我们的结果再次证实了深度在视觉表示中的重要性。</p>
<h2 id="ACKNOWLEDGEMENTS"><a href="#ACKNOWLEDGEMENTS" class="headerlink" title="ACKNOWLEDGEMENTS"></a>ACKNOWLEDGEMENTS</h2><p>This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</p>
<h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><p>Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014.</p>
<p>Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc. BMVC., 2014.</p>
<p>Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014.</p>
<p>Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In IJCAI, pp. 1237–1242, 2011.</p>
<p>Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232–1240, 2012.</p>
<p>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR, 2009.</p>
<p>Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.</p>
<p>Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. IJCV, 111(1):98–136, 2015.</p>
<p>Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE CVPR Workshop of Generative Model Based Vision, 2004.</p>
<p>Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014.</p>
<p>Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014.</p>
<p>Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249–256, 2010.</p>
<p>Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc. ICLR, 2014.</p>
<p>Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014.</p>
<p>Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014.</p>
<p>Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014.</p>
<p>Jia, Y. Caffe: An open source convolutional architecture for fast feature embedding. <a href="http://caffe.berkeleyvision.org/" target="_blank" rel="external">http://caffe.berkeleyvision.org/</a>, 2013.</p>
<p>Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR, abs/1412.2306, 2014.</p>
<p>Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014.</p>
<p>Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014.</p>
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.</p>
<p>LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.</p>
<p>Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014.</p>
<p>Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014.</p>
<p>Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014.</p>
<p>Perronnin, F., Sa ́nchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010.</p>
<p>Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014.</p>
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014.</p>
<p>Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014.</p>
<p>Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014.</p>
<p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.</p>
<p>Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014.</p>
<p>Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014.</p>
]]></content>
    
    <summary type="html">
    
      VGG论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>VGG论文翻译——中文版</title>
    <link href="noahsnail.com/2017/08/17/2017-8-17-VGG%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/08/17/2017-8-17-VGG论文翻译——中文版/</id>
    <published>2017-08-17T02:00:25.000Z</published>
    <updated>2017-08-29T08:11:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<h1 id="Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition"><a href="#Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition" class="headerlink" title="Very Deep Convolutional Networks for Large-Scale Image Recognition"></a>Very Deep Convolutional Networks for Large-Scale Image Recognition</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在这项工作中，我们研究了卷积网络深度在大规模的图像识别环境下对准确性的影响。我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。这些发现是我们的ImageNet Challenge 2014提交的基础，我们的团队在定位和分类过程中分别获得了第一名和第二名。我们还表明，我们的表示对于其他数据集泛化的很好，在其它数据集上取得了最好的结果。我们使我们的两个性能最好的ConvNet模型可公开获得，以便进一步研究计算机视觉中深度视觉表示的使用。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>卷积网络（ConvNets）近来在大规模图像和视频识别方面取得了巨大成功（Krizhevsky等，2012；Zeiler＆Fergus，2013；Sermanet等，2014；Simonyan＆Zisserman，2014）由于大的公开图像存储库，例如ImageNet，以及高性能计算系统的出现，例如GPU或大规模分布式集群（Dean等，2012），使这成为可能。特别是，在深度视觉识别架构的进步中，ImageNet大型视觉识别挑战（ILSVRC）（Russakovsky等，2014）发挥了重要作用，它已经成为几代大规模图像分类系统的测试台，从高维度浅层特征编码（Perronnin等，2010）（ILSVRC-2011的获胜者）到深层ConvNets（Krizhevsky等，2012）（ILSVRC-2012的获奖者）。</p>
<p>随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。在本文中，我们解决了ConvNet架构设计的另一个重要方面——其深度。为此，我们修正了架构的其它参数，并通过添加更多的卷积层来稳定地增加网络的深度，这是可行的，因为在所有层中使用非常小的（3×3）卷积滤波器。</p>
<p>因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。我们发布了两款表现最好的模型1，以便进一步研究。</p>
<p>本文的其余部分组织如下。在第2节，我们描述了我们的ConvNet配置。图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。第5节总结了论文。为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。最后，附录C包含了主要的论文修订列表。</p>
<h2 id="2-ConvNet配置"><a href="#2-ConvNet配置" class="headerlink" title="2. ConvNet配置"></a>2. ConvNet配置</h2><p>为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。最后，我们的设计选择将在2.3节进行讨论并与现有技术进行比较。</p>
<p>在训练期间，我们的ConvNet的输入是固定大小的224×224 RGB图像。我们唯一的预处理是从每个像素中减去在训练集上计算的RGB均值。图像通过一堆卷积（conv.）层，我们使用感受野很小的滤波器：3×3（这是捕获左/右，上/下，中心概念的最小尺寸）。在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。卷积步长固定为1个像素；卷积层输入的空间填充要满足卷积之后保留空间分辨率，即3×3卷积层的填充为1个像素。空间池化由五个最大池化层进行，这些层在一些卷积层之后（不是所有的卷积层之后都是最大池化）。在2×2像素窗口上进行最大池化，步长为2。</p>
<p>一堆卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。最后一层是soft-max层。所有网络中全连接层的配置是相同的。</p>
<p>所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</p>
<h3 id="2-2-配置"><a href="#2-2-配置" class="headerlink" title="2.2 配置"></a>2.2 配置</h3><p>本文中评估的ConvNet配置在表1中列出，每列一个。接下来我们将按网站名称（A-E）来提及网络。所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池化层之后增加2倍，直到达到512。</p>
<p>表1：ConvNet配置（以列显示）。随着更多的层被添加，配置的深度从左（A）增加到右（E）（添加的层以粗体显示）。卷积层参数表示为“conv⟨感受野大小⟩-通道数⟩”。为了简洁起见，不显示ReLU激活功能。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table1.png" alt="Table 1"></p>
<p>在表2中，我们报告了每个配置的参数数量。尽管深度很大，我们的网络中权重数量并不大于具有更大卷积层宽度和感受野的较浅网络中的权重数量（144M的权重在（Sermanet等人，2014）中）。</p>
<p>表2：参数数量（百万级别）</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table2.png" alt="Table 2"></p>
<h3 id="2-3-讨论"><a href="#2-3-讨论" class="headerlink" title="2.3 讨论"></a>2.3 讨论</h3><p>我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。很容易看到两个3×3卷积层堆叠（没有空间池化）有5×5的有效感受野；三个这样的层具有7×7的有效感受野。那么我们获得了什么？例如通过使用三个3×3卷积层的堆叠来替换单个7×7层。首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。其次，我们减少参数的数量：假设三层3×3卷积堆叠的输入和输出有$C$个通道，堆叠卷积层的参数为$3(3^2C^2)=27C^2$个权重；同时，单个7×7卷积层将需要$7^2C^2=49C^2$个参数，即参数多81％。这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</p>
<p>结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。即使在我们的案例下，1×1卷积基本上是在相同维度空间上的线性投影（输入和输出通道的数量相同），由修正函数引入附加的非线性。应该注意的是1×1卷积层最近在Lin等人(2014)的“Network in Network”架构中已经得到了使用。</p>
<p>Ciresan等人（2011）以前使用小尺寸的卷积滤波器，但是他们的网络深度远远低于我们的网络，他们并没有在大规模的ILSVRC数据集上进行评估。Goodfellow等人（2014）在街道号识别任务中采用深层ConvNets（11个权重层），显示出增加的深度导致了更好的性能。GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。然而，它们的网络拓扑结构比我们的更复杂，并且在第一层中特征图的空间分辨率被更积极地减少，以减少计算量。正如将在第4.5节显示的那样，我们的模型在单网络分类精度方面胜过Szegedy等人（2014）。</p>
<h2 id="3-分类框架"><a href="#3-分类框架" class="headerlink" title="3 分类框架"></a>3 分类框架</h2><p>在上一节中，我们介绍了我们的网络配置的细节。在本节中，我们将介绍分类ConvNet训练和评估的细节。</p>
<h3 id="3-1-训练"><a href="#3-1-训练" class="headerlink" title="3.1 训练"></a>3.1 训练</h3><p>ConvNet训练过程通常遵循Krizhevsky等人（2012）（除了从多尺度训练图像中对输入裁剪图像进行采样外，如下文所述）。也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。批量大小设为256，动量为0.9。训练通过权重衰减（L2惩罚乘子设定为$5·10^{−4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。学习率初始设定为$10^{−2}$，然后当验证集准确率停止改善时，减少10倍。学习率总共降低3次，学习在37万次迭代后停止（74个epochs）。我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</p>
<p>网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。为了规避这个问题，我们开始训练配置A（表1），足够浅以随机初始化进行训练。然后，当训练更深的架构时，我们用网络A的层初始化前四个卷积层和最后三个全连接层（中间层被随机初始化）。我们没有减少预初始化层的学习率，允许他们在学习过程中改变。对于随机初始化（如果应用），我们从均值为0和方差为$10^{−2}$的正态分布中采样权重。偏置初始化为零。值得注意的是，在提交论文之后，我们发现可以通过使用Glorot＆Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</p>
<p><strong>训练图像大小</strong>。令S是等轴归一化的训练图像的最小边，ConvNet输入从S中裁剪（我们也将S称为训练尺度）。虽然裁剪尺寸固定为224×224，但原则上S可以是不小于224的任何值：对于$S=224$，裁剪图像将捕获整个图像的统计数据，完全扩展训练图像的最小边；对于$S»224$，裁剪图像将对应于图像的一小部分，包含小对象或对象的一部分。</p>
<p>我们考虑两种方法来设置训练尺度S。第一种是修正对应单尺度训练的S（注意，采样裁剪图像中的图像内容仍然可以表示多尺度图像统计）。在我们的实验中，我们评估了以两个固定尺度训练的模型：$S = 256$（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和$S = 384$。给定ConvNet配置，我们首先使用$S=256$来训练网络。为了加速$S = 384$网络的训练，用$S = 256$预训练的权重来进行初始化，我们使用较小的初始学习率$10^{−3}$。</p>
<p>设置S的第二种方法是多尺度训练，其中每个训练图像通过从一定范围$[S_{min}，S_{max}]$（我们使用$S_{min} = 256$和$S_{max} = 512$）随机采样S来单独进行归一化。由于图像中的目标可能具有不同的大小，因此在训练期间考虑到这一点是有益的。这也可以看作是通过尺度抖动进行训练集增强，其中单个模型被训练在一定尺度范围内识别对象。为了速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调，训练了多尺度模型，并用固定的$S = 384$进行预训练。</p>
<h3 id="3-2-测试"><a href="#3-2-测试" class="headerlink" title="3.2 测试"></a>3.2 测试</h3><p>在测试时，给出训练的ConvNet和输入图像，它按以下方式分类。首先，将其等轴地归一化到预定义的最小图像边，表示为Q（我们也将其称为测试尺度）。我们注意到，Q不一定等于训练尺度S（正如我们在第4节中所示，每个S使用Q的几个值会导致性能改进）。然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。即，全连接层首先被转换成卷积层（第一FC层转换到7×7卷积层，最后两个FC层转换到1×1卷积层）。然后将所得到的全卷积网络应用于整个（未裁剪）图像上。结果是类得分图的通道数等于类别的数量，以及取决于输入图像大小的可变空间分辨率。最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</p>
<p>由于全卷积网络被应用在整个图像上，所以不需要在测试时对采样多个裁剪图像（Krizhevsky等，2012），因为它需要网络重新计算每个裁剪图像，这样效率较低。同时，如Szegedy等人（2014）所做的那样，使用大量的裁剪图像可以提高准确度，因为与全卷积网络相比，它使输入图像的采样更精细。此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</p>
<h3 id="3-3-实现细节"><a href="#3-3-实现细节" class="headerlink" title="3.3 实现细节"></a>3.3 实现细节</h3><p>我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。多GPU训练利用数据并行性，通过将每批训练图像分成几个GPU批次，每个GPU并行处理。在计算GPU批次梯度之后，将其平均以获得完整批次的梯度。梯度计算在GPU之间是同步的，所以结果与在单个GPU上训练完全一样。</p>
<p>最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。在配备四个NVIDIA Titan Black GPU的系统上，根据架构训练单个网络需要2-3周时间。</p>
<h2 id="4-分类实验"><a href="#4-分类实验" class="headerlink" title="4 分类实验"></a>4 分类实验</h2><p>数据集。在本节中，我们介绍了描述的ConvNet架构（用于ILSVRC 2012-2014挑战）在ILSVRC-2012数据集上实现的图像分类结果。数据集包括1000个类别的图像，并分为三组：训练（130万张图像），验证（5万张图像）和测试（留有类标签的10万张图像）。使用两个措施评估分类性能：top-1和top-5错误率。前者是多类分类误差，即不正确分类图像的比例；后者是ILSVRC中使用的主要评估标准，并且计算为图像真实类别在前5个预测类别之外的图像比例。</p>
<p>对于大多数实验，我们使用验证集作为测试集。在测试集上也进行了一些实验，并将其作为ILSVRC-2014竞赛（Russakovsky等，2014）“VGG”小组的输入提交到了官方的ILSVRC服务器。</p>
<h3 id="4-1-单尺度评估"><a href="#4-1-单尺度评估" class="headerlink" title="4.1 单尺度评估"></a>4.1 单尺度评估</h3><p>我们首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。测试图像大小设置如下：对于固定S的$Q = S$，对于抖动$S ∈ [S_{min}, S_{max}]$，$Q = 0.5(S_{min} + S_{max})$。结果如表3所示。</p>
<p>表3：在单测试尺度的ConvNet性能</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table3.png" alt="Table 3"></p>
<p>首先，我们注意到，使用局部响应归一化（A-LRN网络）在没有任何归一化层的情况下，对模型A没有改善。因此，我们在较深的架构（B-E）中不采用归一化。</p>
<p>第二，我们观察到分类误差随着ConvNet深度的增加而减小：从A中的11层到E中的19层。值得注意的是，尽管深度相同，配置C（包含三个1×1卷积层）比在整个网络层中使用3×3卷积的配置D更差。这表明，虽然额外的非线性确实有帮助（C优于B），但也可以通过使用具有非平凡感受野（D比C好）的卷积滤波器来捕获空间上下文。当深度达到19层时，我们架构的错误率饱和，但更深的模型可能有益于较大的数据集。我们还将网络B与具有5×5卷积层的浅层网络进行了比较，浅层网络可以通过用单个5×5卷积层替换B中每对3×3卷积层得到（其具有相同的感受野如第2.3节所述）。测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。</p>
<p>最后，训练时的尺度抖动（$S∈[256; 512]$）得到了与固定最小边（$S = 256$或$S = 384$）的图像训练相比更好的结果，即使在测试时使用单尺度。这证实了通过尺度抖动进行的训练集增强确实有助于捕获多尺度图像统计。</p>
<h3 id="4-2-多尺度评估"><a href="#4-2-多尺度评估" class="headerlink" title="4.2 多尺度评估"></a>4.2 多尺度评估</h3><p>在单尺度上评估ConvNet模型后，我们现在评估测试时尺度抖动的影响。它包括在一张测试图像的几个归一化版本上运行模型（对应于不同的Q值），然后对所得到的类别后验进行平均。考虑到训练和测试尺度之间的巨大差异会导致性能下降，用固定S训练的模型在三个测试图像尺度上进行了评估，接近于训练一次：$Q = {S − 32, S, S + 32}$。同时，训练时的尺度抖动允许网络在测试时应用于更广的尺度范围，所以用变量$S ∈ [S_{min}; S_{max}]$训练的模型在更大的尺寸范围$Q = {S_{min}, 0.5(S_{min} + S_{max}), S_{max}$上进行评估。</p>
<p>表4中给出的结果表明，测试时的尺度抖动导致了更好的性能（与在单一尺度上相同模型的评估相比，如表3所示）。如前所述，最深的配置（D和E）执行最佳，并且尺度抖动优于使用固定最小边S的训练。我们在验证集上的最佳单网络性能为<code>24.8％/7.5％ top-1/top-5</code>的错误率（在表4中用粗体突出显示）。在测试集上，配置E实现了<code>7.3％ top-5</code>的错误率。</p>
<p>表4：在多个测试尺度上的ConvNet性能</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table4.png" alt="Table 4"></p>
<h3 id="4-3-多裁剪图像评估"><a href="#4-3-多裁剪图像评估" class="headerlink" title="4.3 多裁剪图像评估"></a>4.3 多裁剪图像评估</h3><p>在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。我们还通过平均其soft-max输出来评估两种评估技术的互补性。可以看出，使用多裁剪图像表现比密集评估略好，而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。如上所述，我们假设这是由于卷积边界条件的不同处理。</p>
<p>表5：ConvNet评估技术比较。在所有的实验中训练尺度S从[256；512]采样，三个测试适度Q考虑：{256, 384, 512}。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table5.png" alt="Table 5"></p>
<h3 id="4-4-卷积网络融合"><a href="#4-4-卷积网络融合" class="headerlink" title="4.4 卷积网络融合"></a>4.4 卷积网络融合</h3><p>到目前为止，我们评估了ConvNet模型的性能。在这部分实验中，我们通过对soft-max类别后验进行平均，结合了几种模型的输出。由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</p>
<p>结果如表6所示。在ILSVRC提交的时候，我们只训练了单规模网络，以及一个多尺度模型D（仅在全连接层进行微调而不是所有层）。由此产生的7个网络组合具有7.3％的ILSVRC测试误差。在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。作为参考，我们表现最佳的单模型达到7.1％的误差（模型E，表5）。</p>
<p>表6：多个卷积网络融合结果</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table6.png" alt="Table 6"></p>
<h3 id="4-5-与最新技术比较"><a href="#4-5-与最新技术比较" class="headerlink" title="4.5 与最新技术比较"></a>4.5 与最新技术比较</h3><p>最后，我们在表7中与最新技术比较我们的结果。在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，<br>使用7个模型的组合取得了7.3％测试误差。提交后，我们使用2个模型的组合将错误率降低到6.8％。</p>
<p>表7：在ILSVRC分类中与最新技术比较。我们的方法表示为“VGG”。报告的结果没有使用外部数据。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/vgg-table7.png" alt="Table 7"></p>
<p>从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。这是非常显著的，考虑到我们最好的结果是仅通过组合两个模型实现的——明显少于大多数ILSVRC提交。在单网络性能方面，我们的架构取得了最好节果（7.0％测试误差），超过单个GoogLeNet 0.9％。值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>在这项工作中，我们评估了非常深的卷积网络（最多19个权重层）用于大规模图像分类。已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。在附录中，我们还显示了我们的模型很好地泛化到各种各样的任务和数据集上，可以匹敌或超越更复杂的识别流程，其构建围绕不深的图像表示。我们的结果再次证实了深度在视觉表示中的重要性。</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</p>
<h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><p>Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014.</p>
<p>Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc. BMVC., 2014.</p>
<p>Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014.</p>
<p>Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In IJCAI, pp. 1237–1242, 2011.</p>
<p>Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232–1240, 2012.</p>
<p>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR, 2009.</p>
<p>Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.</p>
<p>Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. IJCV, 111(1):98–136, 2015.</p>
<p>Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE CVPR Workshop of Generative Model Based Vision, 2004.</p>
<p>Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014.</p>
<p>Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014.</p>
<p>Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249–256, 2010.</p>
<p>Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc. ICLR, 2014.</p>
<p>Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014.</p>
<p>Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014.</p>
<p>Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014.</p>
<p>Jia, Y. Caffe: An open source convolutional architecture for fast feature embedding. <a href="http://caffe.berkeleyvision.org/" target="_blank" rel="external">http://caffe.berkeleyvision.org/</a>, 2013.</p>
<p>Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR, abs/1412.2306, 2014.</p>
<p>Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014.</p>
<p>Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014.</p>
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.</p>
<p>LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.</p>
<p>Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014.</p>
<p>Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014.</p>
<p>Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014.</p>
<p>Perronnin, F., Sa ́nchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010.</p>
<p>Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014.</p>
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014.</p>
<p>Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014.</p>
<p>Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014.</p>
<p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.</p>
<p>Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014.</p>
<p>Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014.</p>
]]></content>
    
    <summary type="html">
    
      VGG论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet论文总结</title>
    <link href="noahsnail.com/2017/08/07/2017-8-7-AlexNet%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/"/>
    <id>noahsnail.com/2017/08/07/2017-8-7-AlexNet论文总结/</id>
    <published>2017-08-07T09:17:33.000Z</published>
    <updated>2017-08-07T10:09:20.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h3 id="Preconditions"><a href="#Preconditions" class="headerlink" title="Preconditions"></a>Preconditions</h3><ul>
<li><p>ImageNet<br>Objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. <strong>ImageNet</strong> consists of over 15 million labeled high-resolution images in over 22,000 categories.</p>
</li>
<li><p>CNNs<br>To learn about thousands of objects from millions of images, we need a model with a large learning capacity. <strong>Convolutional neural networks (CNNs)</strong> constitute one such class of models. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). </p>
</li>
</ul>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p>The specific contributions of this paper are as follows:</p>
<ul>
<li>We trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions and achieved by far the best results ever reported on these datasets. </li>
<li>We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly. </li>
<li>Our network contains a number of new and unusual features which improve its performance and reduce its training time. </li>
<li>The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting. </li>
<li>Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance.</li>
</ul>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p><img src="http://ocs628urt.bkt.clouddn.com/Fig%202.png" alt="Figure"></p>
<p>The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. </p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><strong>Motivation</strong>: We are not the first to consider alternatives to traditional neuron models in CNNs. </p>
<p>The standard way to model a neuron’s output f as a function of its input x is with $f(x)=tanh(x)$ or $f(x)=(1+e_{−x})^{−1}$. In terms of training time with gradient descent, these <strong>saturating nonlinearities</strong> are much slower than the <strong>non-saturating nonlinearity</strong> $f(x)=max(0,x)$. Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units.</p>
<p><strong>Note</strong>: Saturation of Sigmoid f(x) refers to the time when the output is almost zero or 1, where the gradient in the region is almost zero, causing the local gradient to disappear.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%201.png" alt="Figure"></p>
<p>A four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). </p>
<h3 id="Training-on-Multiple-GPUs"><a href="#Training-on-Multiple-GPUs" class="headerlink" title="Training on Multiple GPUs"></a>Training on Multiple GPUs</h3><p>A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: <strong>the GPUs communicate only in certain layers</strong>.<br><strong>Result</strong>: This scheme reduces our <code>top-1</code> and <code>top-5</code> error rates by <code>1.7%</code> and <code>1.2%</code>, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU. The two-GPU net takes <strong>slightly less time</strong> to train than the one-GPU net.</p>
<h3 id="Local-Response-Normalization"><a href="#Local-Response-Normalization" class="headerlink" title="Local Response Normalization"></a>Local Response Normalization</h3><p>Motivation: This sort of response normalization implements a form of <strong>lateral inhibition</strong> inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels.</p>
<p>We still find that the following local normalization scheme aids generalization. Denoting by $a_{x,y}^i$ the activity of a neuron computed by applying kernel $i$ at position $(x, y)$ and then applying the ReLU nonlinearity, the response-normalized activity $b^i_{x,y}$ is given by the expression</p>
<p>$$b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta$$</p>
<p>where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer. The constants k, n, α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = 0.0001, and β = 0.75.</p>
<p><strong>Result</strong>: Response normalization reduces our <code>top-1</code> and <code>top-5</code> error rates by <code>1.4%</code> and <code>1.2%</code>, respectively.</p>
<h3 id="Overlapping-Pooling"><a href="#Overlapping-Pooling" class="headerlink" title="Overlapping Pooling"></a>Overlapping Pooling</h3><p>A pooling layer can be thought of as consisting of a grid of pooling units spaced $s$ pixels apart, each summarizing a neighborhood of size $z × z$ centered at the location of the pooling unit. If we set $s = z$, we obtain traditional local pooling as commonly employed in CNNs. If we set $s &lt; z$, we obtain overlapping pooling. This is what we use throughout our network, with $s = 2$ and $z = 3$.</p>
<p><strong>Result</strong>: This scheme reduces the <code>top-1</code> and <code>top-5</code> error rates by <code>0.4%</code> and <code>0.3%</code>, respectively. We generally observe during training that models with overlapping pooling find it slightly <strong>more difficult to overfit</strong>.</p>
<h3 id="Overall-Architecture"><a href="#Overall-Architecture" class="headerlink" title="Overall Architecture"></a>Overall Architecture</h3><p><img src="http://ocs628urt.bkt.clouddn.com/architecture.png" alt="Architecture"></p>
<h1 id="Reduce-Overfitting"><a href="#Reduce-Overfitting" class="headerlink" title="Reduce Overfitting"></a>Reduce Overfitting</h1><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><p>The first form of data augmentation consists of generating image translations and horizontal reflections.</p>
<p>The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set. To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel $I_{xy} = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$ we add the following quantity: </p>
<p>$$[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T$$</p>
<p>where $p_i$ and $\lambda_i$ are $i$th eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and $\alpha_i$ is the aforementioned random variable.</p>
<p><strong>Result</strong>: This scheme reduces the <code>top-1</code> error rate by over <code>1%</code>.</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p><strong>Motivation</strong>: Combining the predictions of many different models is a very successful way to reduce test errors, but it appears to be too expensive for big neural networks that already take several days to train.</p>
<p>There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called <strong>“dropout”</strong>, consists of setting to zero the output of each hidden neuron with probability 0.5. We use dropout in the first two fully-connected layers. Without dropout, our network exhibits substantial overfitting. Dropout roughly <strong>doubles</strong> the number of iterations required to converge.</p>
<h1 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h1><p><img src="http://ocs628urt.bkt.clouddn.com/exp_1.png" alt="Table 1"></p>
<p><img src="http://ocs628urt.bkt.clouddn.com/exp_2.png" alt="Table 2"></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks</a></p>
<h1 id="Supplement-material"><a href="#Supplement-material" class="headerlink" title="Supplement material"></a>Supplement material</h1><p><img src="http://ocs628urt.bkt.clouddn.com/Alex_2.png" alt="Fig 1"></p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Alex_sup_2.png" alt="Fig 2"></p>
]]></content>
    
    <summary type="html">
    
      AlexNet论文总结
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Yolo-Darknet的安装和使用</title>
    <link href="noahsnail.com/2017/08/02/2017-8-2-Yolo-Darknet%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>noahsnail.com/2017/08/02/2017-8-2-Yolo-Darknet的安装和使用/</id>
    <published>2017-08-02T03:02:30.000Z</published>
    <updated>2017-08-03T03:10:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Yolo-Darknet介绍"><a href="#1-Yolo-Darknet介绍" class="headerlink" title="1. Yolo-Darknet介绍"></a>1. Yolo-Darknet介绍</h2><p>YOLO是基于深度学习方法的端到端实时目标检测系统，目前有三个版本，Yolo-v1，Yolo-9000，Yolo-v2。Darknet是Yolo的实现，但Darknet不仅包含Yolo的实现，还包括其它内容。</p>
<h2 id="2-Darknet安装"><a href="#2-Darknet安装" class="headerlink" title="2. Darknet安装"></a>2. Darknet安装</h2><p>安装过程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># 代码下载</div><div class="line">git clone https://github.com/pjreddie/darknet.git</div><div class="line"> </div><div class="line"># 修改Makefile</div><div class="line">cd darknet</div><div class="line">sed -i &apos;1s/GPU=0/GPU=1/&apos; Makefile</div><div class="line">sed -i &apos;2s/CUDNN=0/CUDNN=1/&apos; Makefile</div><div class="line">sed -i &apos;3s/OPENCV=0/OPENCV=1/&apos; Makefile</div><div class="line"> </div><div class="line"># 安装</div><div class="line">make</div><div class="line"> </div><div class="line"># 下载预训练的模型</div><div class="line">wget https://pjreddie.com/media/files/yolo.weights</div><div class="line">wget https://pjreddie.com/media/files/tiny-yolo-voc.weights</div><div class="line">wget http://pjreddie.com/media/files/yolov1.weights</div><div class="line">wget http://pjreddie.com/media/files/tiny-yolo.weights</div><div class="line">wget http://pjreddie.com/media/files/tiny-coco.weights</div><div class="line">wget http://pjreddie.com/media/files/yolo-coco.weights</div></pre></td></tr></table></figure>
<h2 id="3-Yolo-v2用法"><a href="#3-Yolo-v2用法" class="headerlink" title="3. Yolo-v2用法"></a>3. Yolo-v2用法</h2><ul>
<li>使用预训练的模型进行目标检测</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/predictions.jpg" alt="Result"></p>
<ul>
<li>输入图像名称进行检测</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">$ ./darknet detect cfg/yolo.cfg yolo.weights</div><div class="line"># 输入 data/horses.jpg</div><div class="line"># 执行结果如下:</div><div class="line">layer     filters    size              input                output</div><div class="line">    0 conv     32  3 x 3 / 1   608 x 608 x   3   -&gt;   608 x 608 x  32</div><div class="line">    1 max          2 x 2 / 2   608 x 608 x  32   -&gt;   304 x 304 x  32</div><div class="line">    2 conv     64  3 x 3 / 1   304 x 304 x  32   -&gt;   304 x 304 x  64</div><div class="line">    3 max          2 x 2 / 2   304 x 304 x  64   -&gt;   152 x 152 x  64</div><div class="line">    4 conv    128  3 x 3 / 1   152 x 152 x  64   -&gt;   152 x 152 x 128</div><div class="line">    5 conv     64  1 x 1 / 1   152 x 152 x 128   -&gt;   152 x 152 x  64</div><div class="line">    6 conv    128  3 x 3 / 1   152 x 152 x  64   -&gt;   152 x 152 x 128</div><div class="line">    7 max          2 x 2 / 2   152 x 152 x 128   -&gt;    76 x  76 x 128</div><div class="line">    8 conv    256  3 x 3 / 1    76 x  76 x 128   -&gt;    76 x  76 x 256</div><div class="line">    9 conv    128  1 x 1 / 1    76 x  76 x 256   -&gt;    76 x  76 x 128</div><div class="line">   10 conv    256  3 x 3 / 1    76 x  76 x 128   -&gt;    76 x  76 x 256</div><div class="line">   11 max          2 x 2 / 2    76 x  76 x 256   -&gt;    38 x  38 x 256</div><div class="line">   12 conv    512  3 x 3 / 1    38 x  38 x 256   -&gt;    38 x  38 x 512</div><div class="line">   13 conv    256  1 x 1 / 1    38 x  38 x 512   -&gt;    38 x  38 x 256</div><div class="line">   14 conv    512  3 x 3 / 1    38 x  38 x 256   -&gt;    38 x  38 x 512</div><div class="line">   15 conv    256  1 x 1 / 1    38 x  38 x 512   -&gt;    38 x  38 x 256</div><div class="line">   16 conv    512  3 x 3 / 1    38 x  38 x 256   -&gt;    38 x  38 x 512</div><div class="line">   17 max          2 x 2 / 2    38 x  38 x 512   -&gt;    19 x  19 x 512</div><div class="line">   18 conv   1024  3 x 3 / 1    19 x  19 x 512   -&gt;    19 x  19 x1024</div><div class="line">   19 conv    512  1 x 1 / 1    19 x  19 x1024   -&gt;    19 x  19 x 512</div><div class="line">   20 conv   1024  3 x 3 / 1    19 x  19 x 512   -&gt;    19 x  19 x1024</div><div class="line">   21 conv    512  1 x 1 / 1    19 x  19 x1024   -&gt;    19 x  19 x 512</div><div class="line">   22 conv   1024  3 x 3 / 1    19 x  19 x 512   -&gt;    19 x  19 x1024</div><div class="line">   23 conv   1024  3 x 3 / 1    19 x  19 x1024   -&gt;    19 x  19 x1024</div><div class="line">   24 conv   1024  3 x 3 / 1    19 x  19 x1024   -&gt;    19 x  19 x1024</div><div class="line">   25 route  16</div><div class="line">   26 conv     64  1 x 1 / 1    38 x  38 x 512   -&gt;    38 x  38 x  64</div><div class="line">   27 reorg              / 2    38 x  38 x  64   -&gt;    19 x  19 x 256</div><div class="line">   28 route  27 24</div><div class="line">   29 conv   1024  3 x 3 / 1    19 x  19 x1280   -&gt;    19 x  19 x1024</div><div class="line">   30 conv    425  1 x 1 / 1    19 x  19 x1024   -&gt;    19 x  19 x 425</div><div class="line">   31 detection</div><div class="line">mask_scale: Using default &apos;1.000000&apos;</div><div class="line">Loading weights from yolo.weights...Done!</div><div class="line">Enter Image Path: data/horses.jpg</div><div class="line">data/horses.jpg: Predicted in 0.030211 seconds.</div><div class="line">horse: 46%</div><div class="line">horse: 59%</div><div class="line">horse: 91%</div><div class="line"> </div><div class="line">(predictions:31): Gtk-WARNING **: cannot open display:</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/horse-predictions.jpg" alt="result"></p>
<ul>
<li>设置检测阈值</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg -thresh 0.1</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/thres-predictions.jpg" alt="result"></p>
<ul>
<li>检测视频</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet detector demo cfg/coco.data cfg/yolo.cfg yolo.weights &lt;video file&gt;</div></pre></td></tr></table></figure>
<p>参考资料</p>
<ol>
<li><p><a href="https://pjreddie.com/darknet/install/" target="_blank" rel="external">https://pjreddie.com/darknet/install/</a></p>
</li>
<li><p><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="external">https://pjreddie.com/darknet/yolo/</a></p>
</li>
<li><p><a href="https://pjreddie.com/darknet/yolov1/" target="_blank" rel="external">https://pjreddie.com/darknet/yolov1/</a></p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Yolo-Darknet的安装和使用
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>YOLO论文翻译——中文版</title>
    <link href="noahsnail.com/2017/08/02/2017-8-2-You%20Only%20Look%20Once-%20Unified,%20Real-Time%20Object%20Detection%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/08/02/2017-8-2-You Only Look Once- Unified, Real-Time Object Detection论文翻译——中文版/</id>
    <published>2017-08-02T02:45:29.000Z</published>
    <updated>2017-08-07T10:29:36.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<h1 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection"></a>You Only Look Once: Unified, Real-Time Object Detection</h1>]]></content>
    
    <summary type="html">
    
      You Only Look Once——Unified, Real-Time Object Detection论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>YOLO论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/08/02/2017-8-2-You%20Only%20Look%20Once-%20Unified,%20Real-Time%20Object%20Detection%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2017/08/02/2017-8-2-You Only Look Once- Unified, Real-Time Object Detection论文翻译——中英文对照/</id>
    <published>2017-08-02T02:44:49.000Z</published>
    <updated>2017-08-07T10:29:42.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<h1 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection"></a>You Only Look Once: Unified, Real-Time Object Detection</h1>]]></content>
    
    <summary type="html">
    
      You Only Look Once——Unified, Real-Time Object Detection论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
