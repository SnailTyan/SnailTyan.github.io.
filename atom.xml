<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://noahsnail.com/"/>
  <updated>2018-06-18T07:20:18.000Z</updated>
  <id>http://noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>An end-to-end TextSpotter with Explicit Alignment and Attention论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2018/06/15/2018-06-15-An%20end-to-end%20TextSpotter%20with%20Explicit%20Alignment%20and%20Attention%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2018/06/15/2018-06-15-An end-to-end TextSpotter with Explicit Alignment and Attention论文翻译——中英文对照/</id>
    <published>2018-06-15T01:52:19.000Z</published>
    <updated>2018-06-18T07:20:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="An-end-to-end-TextSpotter-with-Explicit-Alignment-and-Attention"><a href="#An-end-to-end-TextSpotter-with-Explicit-Alignment-and-Attention" class="headerlink" title="An end-to-end TextSpotter with Explicit Alignment and Attention"></a>An end-to-end TextSpotter with Explicit Alignment and Attention</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Training of two tasks in a unified framework is non-trivial due to significant differences in optimisation difficulties. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in one shot. Our main contributions are three-fold: 1) we propose a novel text-alignment layer that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance; 2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; 3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by sharing convolutional features, which is critical to identify challenging text instances. Our model achieves impressive results in end-to-end recognition on the ICDAR2015 [1] dataset, significantly advancing most recent results [2], with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detector by achieving a new state-of-the-art detection performance on two datasets.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>长期以来，自然图像中的文本检测和识别被视为按顺序处理的两个独立任务。由于优化难度存在显着差异，在统一框架下培训两项任务并不重要。在这项工作中，我们提出了一个概念简单而高效的框架，可以同时处理两个任务。我们的主要贡献有三个：1）我们提出了一种新颖的文本对齐层，允许它精确地计算任意方向的文本实例的卷积特征，这是提高性能的关键；2）将字符空间信息作为显式监督引入角色注意机制，导致识别率大幅提高; 3）两种技术，连同一个新的RNN分支用于单词识别，可以无缝集成到一个端到端可训练的单一模型中。这允许两个任务通过共享卷积特征来协作工作，这对识别具有挑战性的文本实例是至关重要的。我们的模型在ICDAR2015 [1]数据集端到端识别方面取得了令人印象深刻的结果，显着推进了最新的研究成果[2]，F-度量从（0.54,0.51,0.47）提高到（0.82,0.77,0.63 ），分别使用强，弱和通用的词典。得益于联合训练，我们的方法还可以作为一个很好的检测器，通过在两个数据集上实现新的最先进的检测性能。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>The goal of text spotting is to map an input natural image into a set of character sequences or word transcripts and corresponding location. It has attracted increasing attention in the vision community, due to its numerous potential applications. It has made rapid progress riding on the wave of recent deep learning technologies, as substantiated by recent works [3, 4, 2, 5, 6, 7, 8, 9, 10, 11]. However, text spotting in the wild still remains an open problem, since text instances often exhibit vast diversity in font, scale and orientation with various illumination affects, which often come with a highly complicated background.</p>
<p>Past works in text spotting often consider it as two individual tasks: text detection and word recognition, which are implemented sequentially. The goal of text detection is to precisely localize all text instances (e.g., words) in a natural image, and then a recognition model is processed repeatedly through all detected regions for recognizing corresponding text transcripts. Recent approaches for text detection are mainly extended from general object detectors (such as Faster R-CNN [12] and SSD [13]) by directly regressing a bounding box for each text instance, or from semantic segmentation methods (e.g., Fully Convolutional Networks (FCN) [14]) by predicting a text/non-text probability at each pixel. With careful model design and development, these approaches can be customized properly towards this highly domain-specific task, and achieve the state-of-the-art performance [4, 6, 7, 8, 9, 15]. The word recognition can be cast into a sequence labeling problem where convolutional recurrent models have been developed recently [9, 16]. Some of them were further incorporated with an attention mechanism for improving the performance [17, 18]. However, training two tasks separately does not exploit the full potential of convolutional networks, where the convolutional features are not shared. It is natural for us to make a more reliable decision if we clearly understand or recognize the meaning of a word and all characters within it. Besides, it is also possible to introduce a number of heuristic rules and hyper-parameters that are costly to tune, making the whole system highly complicated.</p>
<p>Recent Mask R-CNN [19] incorporates an instance segmentation task into the Faster R-CNN [12] detection framework, resulting in a multi-task learning model that jointly predicts a bounding box and a segmentation mask for each object instance. Our work draws inspiration from this pipeline, but has a different goal of learning a direct mapping between an input image and a set of character sequences. We create a recurrent sequence modeling branch for word recognition within a text detection framework, where the RNN based word recognition is processed in parallel to the detection task.</p>
<p>However, the RNN branch, where the gradients are back-propagated through time, is clearly much more difficult to optimize than the task of bounding box regression in detection. This naturally leads to significant differences in learning difficulties and convergence rates between two tasks, making the model particularly hard to be trained jointly. For example, the magnitude of images for training a text detection model is about 103 (e.g., 1000 training images in the ICDAR 2015 [1]) , but the number is increased significantly by many orders of magnitude when a RNN based text recognition model is trained, such as the 800K synthetic images used in [20]. Furthermore, simply using a set of character sequences as direct supervision may be too abstractive (high-level) to provide meaningful detailed information for training such an integrated model effectively, which will make the model difficult to convergence. In this work, we introduce strong spatial constraints in both word and character levels, which allows the model to be optimized gradually by reducing the search space at each step.</p>
<p>Contributions In this work, we present a single-shot textspotter capable of learning a direct mapping between an input image and a set of character sequences or word transcripts. We propose a solution that combines a text-alignment layer tailed for multi-orientation text detection, together with a character attention mechanism that explicitly encodes strong spatial information of characters into the RNN branch, as shown in Fig. 1. These two technologies faithfully preserve the exact spatial information in both text instance and character levels, playing a key role in boosting the overall performance. We develop a principled learning strategy that allows the two tasks to be trained collaboratively by sharing convolutional features. Our main contributions are described as follows.</p>
<p>Firstly, we develop a text-alignment layer by introducing a grid sampling scheme instead of conventional RoI pooling. It computes fixed-length convolutional features that precisely align to a detected text region of arbitrary orientation, successfully avoiding the negative effects caused by orientation changing and quantization factor of the RoI pooling.</p>
<p>Secondly, we introduce a character attention mechanism by using character spatial information as an addition supervision. This explicitly encodes strong spatial attentions of characters into the model, which allows the RNN to focus on current attentional features in decoding, leading to performance boost in word recognition.</p>
<p>Thirdly, both approaches, together with a new RNN branch for word recognition, are integrated elegantly into a CNN detection framework, resulting in a single model that can be trained in an end-to-end manner. We develop a principled and intuitive learning strategy that allows the two tasks to be trained effectively by sharing features, with fast convergence.</p>
<p>Finally, we show by experiments that word recognition can significantly improve detection accuracy in our model, demonstrating strong complementary nature of them, which is unique to this highly domain-specific application. Our model achieves new state-of-the-art results on the ICDAR2015 in end-to-end recognition of multi-orientation texts, largely outperforming the most recent results in [2], with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63) in terms of using a strong, weak and generic lexicon. Code is avail- able at <a href="https://github.com/tonghe90/textspotter" target="_blank" rel="external">https://github.com/tonghe90/textspotter</a></p>
<p><strong>Related work</strong> Here we briefly introduce some related works on text detection, recognition and end-to-end wordspotting.</p>
<p><em>Scene text detection</em> Recently, some methods cast previous character based detection [21, 22, 23, 24] into direct text region estimation [25, 8, 15, 26, 4, 27, 28], avoiding multiple bottom-up post-processing steps by taking word or text-line as a whole. Tian et al. [7] modified Faster-RCNN [12] by applying a recurrent structure on the convolution feature maps of the top layer horizontally. The methods in [4, 25] were inspired from [13]. They both explored the framework from generic objects and convert to scene text detection by adjusting the feature extraction process to this domain-specific task. However, these methods are based on prior boxes, which need to be carefully designed in order to fulfill the requirements for training. Methods of direct regression for inclined bounding boxes, instead of offsets to fixed prior boxes, have been proposed recently. EAST [8] designed a fully convolutional network structure which outputs a pixel-wise prediction map for text/non-text and five values for every point of text region, i.e., distances from the current point to the four edges with an inclined angle. He et al. [6] proposed a method to generate arbitrary quadrilaterals by calculating offsets between every point of text region and vertex coordinates.</p>
<p>Scene text recognition With the success of recurrent neural networks on digit recognition and speech translation, a lot of works have been proposed for text recognition. He et al. [16] and Shi et al. [9, 29] treat text recognition as a sequence labeling problem by introducing LSTM [30] and connectionist temporal classification (CTC) [31] into a unified framework. [17] proposed an attention-based LSTM for text recognition, which mainly contains two parts: encoder and decoder. In the encoding stage, text images are transformed into a sequence of feature vectors by CNN/LSTM. Attention weights, indicating relative importance for recognition, will be learned during the decoding stage. However, these weights are totally learned by the distribution of data and no supervision is provided to guide the learning process.</p>
<p><em>End-to-end wordspotting</em> End-to-end wordspotting is an emerging research area. Previous methods usually try to solve it by splitting the whole process into two independent problems: training two cascade models, one for detection and one for recognition. Detected text regions are firstly cropped from original image, followed by affine transforming and rescaling. Corrected images are repeatedly precessed by recognition model to get corresponding transcripts. However, training errors will be accumulated due to cascading models without sharable features. Li et al. [5] proposed a unified network that simultaneously localizes and recognizes text in one forward pass by sharing convolution features under a curriculum strategy. But the existing RoI pooling operation limits it to detect and recognize only horizontal examples. Busta et al. [2] brought up deep text spotter, which can solve wordspotting of multi-orientation problem. However, the method does not have sharable feature, meaning that the recognition loss of the later stage has no influence on the former localization results.</p>
<h2 id="2-Single-Shot-TextSpotter-by-Joint-Detection-and-Recognition"><a href="#2-Single-Shot-TextSpotter-by-Joint-Detection-and-Recognition" class="headerlink" title="2. Single Shot TextSpotter by Joint Detection and Recognition"></a>2. Single Shot TextSpotter by Joint Detection and Recognition</h2><p>In this section, we present the details of the proposed textspotter which learns a direct mapping between an input image and a set of word transcripts with corresponding bounding boxes of arbitrary orientations. Our model is a fully convolutional architecture built on the PVAnet framework [32]. As shown in Fig. 2, we introduce a new recurrent branch for word recognition, which is integrated into our CNN model in parallel with the existing detection branch for text bounding box regression. The RNN branch is composed of a new text-alignment layer and a LSTM-based recurrent module with a novel character attention embedding mechanism. The text-alignment layer extracts precise sequence feature within the detected region, preventing encoding irrelevant texts or background information. The character attention embedding mechanism regulates the decoding process by providing more detailed supervisions of characters. Our textspotter directly outputs final results in one shot, without any post-processing step except for a simple non-maximum suppression (NMS).</p>
<p>Network architecture Our model is a fully convolutional architecture inspired by [8], where a PVA network [32] is utilized as backbone due to its significantly low computational cost. Unlike generic objects, texts often have a much larger variations in both sizes and aspect ratios. Thus it not only needs to preserve local details for small-scale text instances, but also should maintain a large receptive field for very long instances. Inspired by the success in semantic segmentation [33], we exploit feature fusion by combining convolutional features of conv5, conv4, conv3 and conv2 layers gradually, with the goal of maintaining both local detailed features and high-level context information. This results in more reliable predictions on multi-scale text instances. Size of the top layer is $\frac {1} {4}$ of the input image for simplicity.</p>
<p><strong>Text detection</strong> This branch is similar to that of [8], where a multi-task prediction is implemented at each spatial location on the top convolutional maps, by adopting an Intersection over Union (IoU) loss described in [34]. It contains two sub-branches on the top convolutional layer designed for joint text/non-text classification and multi-orientation bounding boxes regression. The first sub-branch returns a classification map with an equal spatial size of the top feature maps, indicating the predicted text/non-text probabilities using a softmax function. The second sub-branch outputs five localization maps with the same spatial size, which estimate five parameters for each bounding box with arbitrary orientation at each spatial location of text regions. The five parameters represent the distances of the current point to the top, bottom, left and right sides of an associated bounding box, together with its inclined orientation. With these configurations, the detection branch is able to predict a quadrilateral of arbitrary orientation for each text instance. The feature of the detected quadrilateral region is then feed into the RNN branch for word recognition via a <em>text-alignment</em> layer which is described below.</p>
<h3 id="2-1-Text-Alignment-Layer"><a href="#2-1-Text-Alignment-Layer" class="headerlink" title="2.1. Text-Alignment Layer"></a>2.1. Text-Alignment Layer</h3><p>We create a new recurrent branch for word recognition, where a text-alignment layer is proposed to pre- cisely compute fixed-size convolutional features from a quadrilateral region of arbitrary size. The text-alignment layer is extended from RoI pooling [35] which is widely used for general objects detection. The RoI pooling computes a fixed-size convolutional features (e.g., 7 × 7) from a rectangle region of arbitrary size, by performing quantization operation. It can be integrated into the convolutional layers for in-network region cropping, which is a key component for end-to-end training a detection framework. However, directly applying the RoI pooling to a text region will lead to a significant performance drop in word recognition due to the issue of misalignment.</p>
<p>– First, unlike object detection and classification where the RoI pooling computes global features of a RoI region for discriminating an object, word recognition requires more detailed and accurate local features and spatial information for predicting each character sequentially. As pointed out in [19], the RoI pooling performs quantizations which inevitably introduce misalignments between the original RoI region and the extracted features. Such misalignments have a significant negative effect on predicting characters, particularly on some small-scale ones such as ‘i’, ‘l’.</p>
<ul>
<li>Second, RoI pooling was designed for a rectangle region which is only capable of localizing horizontal instances. It will make larger misalignments when applied to multi-orientation text instances. Furthermore, a large amount of background information and irrelevant texts are easily encoded when a rectangle RoI region is applied to a highly inclined text instance, as shown in Fig. 3. This severely reduces the performance on RNN decoding process for recognizing sequential characters.</li>
</ul>
<p>Recent Mask R-CNN considers explicit per-pixel spatial correspondence by introducing RoIAlign pooling [19]. This inspires current work that develops a new text-alignment layer tailored for text instance which is a quadrilateral shape with arbitrary orientation. It provides strong word-level alignment with accurate per-pixel correspondence, which is of critical importance to extract exact text information from the convolutional maps, as shown in Fig. 3.</p>
]]></content>
    
    <summary type="html">
    
      An end-to-end TextSpotter with Explicit Alignment and Attention论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>An end-to-end TextSpotter with Explicit Alignment and Attention论文翻译——中文版</title>
    <link href="http://noahsnail.com/2018/06/15/2018-06-15-An%20end-to-end%20TextSpotter%20with%20Explicit%20Alignment%20and%20Attention%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2018/06/15/2018-06-15-An end-to-end TextSpotter with Explicit Alignment and Attention论文翻译——中文版/</id>
    <published>2018-06-15T01:51:20.000Z</published>
    <updated>2018-06-15T01:51:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="An-end-to-end-TextSpotter-with-Explicit-Alignment-and-Attention"><a href="#An-end-to-end-TextSpotter-with-Explicit-Alignment-and-Attention" class="headerlink" title="An end-to-end TextSpotter with Explicit Alignment and Attention"></a>An end-to-end TextSpotter with Explicit Alignment and Attention</h1>]]></content>
    
    <summary type="html">
    
      An end-to-end TextSpotter with Explicit Alignment and Attention论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>MobileNetV2——Inverted Residuals and Linear Bottlenecks论文翻译——中文版</title>
    <link href="http://noahsnail.com/2018/06/06/2018-06-06-MobileNetV2-%20Inverted%20Residuals%20and%20Linear%20Bottlenecks%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2018/06/06/2018-06-06-MobileNetV2- Inverted Residuals and Linear Bottlenecks论文翻译——中文版/</id>
    <published>2018-06-06T07:22:49.000Z</published>
    <updated>2018-06-19T10:30:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks"><a href="#MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks" class="headerlink" title="MobileNetV2: Inverted Residuals and Linear Bottlenecks"></a>MobileNetV2: Inverted Residuals and Linear Bottlenecks</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在本文中，我们描述了一种新的移动架构MobileNetV2，该架构提高了移动模型在多个任务和多个基准数据集上以及在不同模型尺寸范围内的最佳性能。我们还描述了在我们称之为SSDLite的新框架中将这些移动模型应用于目标检测的有效方法。此外，我们还演示了如何通过DeepLabv3的简化形式，我们称之为Mobile DeepLabv3来构建移动语义分割模型。</p>
<p>MobileNetV2架构基于倒置的残差结构，其中快捷连接位于窄的瓶颈层之间。中间展开层使用轻量级的深度卷积作为非线性源来过滤特征。此外，我们发现为了保持表示能力，去除窄层中的非线性是非常重要的。我们证实了这可以提高性能并提供了产生此设计的直觉。</p>
<p>最后，我们的方法允许将输入/输出域与变换的表现力解耦，这为进一步分析提供了便利的框架。我们在ImageNet[1]分类，COCO目标检测[2]，VOC图像分割[3]上评估了我们的性能。我们评估了在精度、通过乘加（MAdd）度量的操作次数，以及实际的延迟和参数的数量之间的权衡。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>神经网络已经彻底改变了机器智能的许多领域，使具有挑战性的图像识别任务获得了超过常人的准确性。然而，提高准确性的驱动力往往需要付出代价：现代先进网络需要超出许多移动和嵌入式应用能力之外的高计算资源。</p>
<p>本文介绍了一种专为移动和资源受限环境量身定制的新型神经网络架构。我们的网络通过显著减少所需操作和内存的数量，同时保持相同的精度推进了移动定制计算机视觉模型的最新水平。</p>
<p>我们的主要贡献是一个新的层模块：具有线性瓶颈的倒置残差。该模块将输入的低维压缩表示首先扩展到高维并用轻量级深度卷积进行过滤。随后用线性卷积将特征投影回低维表示。官方实现可作为[4]中TensorFlow-Slim模型库的一部分。</p>
<p>这个模块可以使用任何现代框架中的标准操作来高效地实现，并允许我们的模型使用标准基线沿多个性能点击败最先进的技术。此外，这种卷积模块特别适用于移动设计，因为它可以通过从不完全实现大型中间张量来显著减少推断过程中所需的内存占用。这减少了许多嵌入式硬件设计中对主存储器访问的需求，这些设计提供了少量高速软件控制缓存。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p>调整深层神经架构以在精确性和性能之间达到最佳平衡已成为过去几年研究活跃的一个领域。由许多团队进行的手动架构搜索和训练算法的改进，已经比早期的设计（如AlexNet[5]，VGGNet [6]，GoogLeNet[7]和ResNet[8]）有了显著的改进。最近在算法架构探索方面取得了很多进展，包括超参数优化[9，10，11]、各种网络修剪方法[12，13，14，15，16，17]和连接学习[18，19]。 也有大量的工作致力于改变内部卷积块的连接结构如ShuffleNet[20]或引入稀疏性[21]和其他[22]。</p>
<p>最近，[23,24,25,26]开辟了了一个新的方向，将遗传算法和强化学习等优化方法带入架构搜索。然而，一个缺点是最终所得到的网络非常复杂。在本文中，我们追求的目标是发展了解神经网络如何运行的更好直觉，并使用它来指导最简单可能的网络设计。我们的方法应该被视为[23]中描述的方法和相关工作的补充。在这种情况下，我们的方法与[20，22]所采用的方法类似，并且可以进一步提高性能，同时可以一睹其内部的运行。我们的网络设计基于MobileNetV1[27]。它保留了其简单性，并且不需要任何特殊的运算符，同时显著提高了它的准确性，为移动应用实现了在多种图像分类和检测任务上的最新技术。</p>
<h2 id="3-准备，讨论和直觉"><a href="#3-准备，讨论和直觉" class="headerlink" title="3. 准备，讨论和直觉"></a>3. 准备，讨论和直觉</h2><h3 id="3-1-深度可分卷积"><a href="#3-1-深度可分卷积" class="headerlink" title="3.1. 深度可分卷积"></a>3.1. 深度可分卷积</h3><p>深度可分卷积是许多高效神经网络架构的关键组成部分[27，28，20]，我们在目前的工作中也使用它们。其基本思想是用分解版本替换完整的卷积运算符，将卷积拆分为两个单独的层。第一层称为深度卷积，它通过对每个输入通道应用单个卷积滤波器来执行轻量级滤波。第二层是1×1卷积，称为逐点卷积，它负责通过计算输入通道的线性组合来构建新特征。</p>
<p>标准卷积使用$K\in \mathbf{R}^{k\times k \times d_i \times d_j}$维的输入张量$L_i$，并对其应用卷积核$K\in \mathbf{R}^{k\times k \times d_i \times d_j}$来产生$h_i\times w_i\times d_j$维的输出张量$L_j$。标准卷积层的计算代价为$h_i \cdot w_i \cdot d_i \cdot d_j \cdot k \cdot k$。</p>
<p>深度可分卷积是标准卷积层的直接替换。经验上，它们几乎与常规卷积一样工作，但其成本为：$$\begin{equation}h_i \cdot w_i \cdot d_i (k^2 + d_j) \tag{1}\end{equation}$$它是深度方向和$1\times 1$逐点卷积的总和。深度可分卷积与传统卷积层相比有效地减少了几乎$k^2$倍的计算量。MobileNetV2使用$k=3$（$3\times 3$的深度可分卷积），因此计算成本比标准卷积小$8$到$9$倍，但精度只有很小的降低[27]。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vision, 115(3):211–252, December 2015.</p>
<p>[2] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[3] Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserma. The pascal visual object classes challenge a retrospective. IJCV, 2014.</p>
<p>[4] Mobilenetv2 source code. Available from <a href="https://github.com/tensorflow/" target="_blank" rel="external">https://github.com/tensorflow/</a> models/tree/master/research/slim/nets/mobilenet.</p>
<p>[5] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Bartlett et al. [48], pages 1106–1114.</p>
<p>[6] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.</p>
<p>[7] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 1–9. IEEE Computer Society, 2015.</p>
<p>[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.</p>
<p>[9] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281–305, 2012.</p>
<p>[10] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms. In Bartlett et al. [48], pages 2960–2968.</p>
<p>[11] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using deep neural networks. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 2171–2180. JMLR.org, 2015.</p>
<p>[12] Babak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December 3, 1992], pages 164–171. Morgan Kaufmann, 1992.</p>
<p>[13] Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky, editor, Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pages 598–605. Morgan Kaufmann, 1989.</p>
<p>[14] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural network. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1135–1143, 2015.</p>
<p>[15] Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John Tran, and William J. Dally. DSD: regularizing deep neural networks with dense-sparse-dense training flow. CoRR, abs/1607.04381, 2016.</p>
<p>[16] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1379–1387, 2016.</p>
<p>[17] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. CoRR, abs/1608.08710, 2016.</p>
<p>[18] Karim Ahmed and Lorenzo Torresani. Connectivity learning in multi-branch networks. CoRR, abs/1709.09582, 2017.</p>
<p>[19] Tom Veniat and Ludovic Denoyer. Learning time-efficient deep architectures with budgeted super networks. CoRR, abs/1706.00046, 2017.</p>
<p>[20] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. CoRR, abs/1707.01083, 2017.</p>
<p>[21] Soravit Changpinyo, Mark Sandler, and Andrey Zhmoginov. The power of sparsity in convolutional neural networks. CoRR, abs/1702.06257, 2017.</p>
<p>[22] Min Wang, Baoyuan Liu, and Hassan Foroosh. Design of efficient convolutional layers using single intra-channel convolution, topological subdivisioning and spatial ”bottleneck” structure. CoRR, abs/1608.04337, 2016.</p>
<p>[23] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures for scalable image recognition. CoRR, abs/1707.07012, 2017.</p>
<p>[24] Lingxi Xie and Alan L. Yuille. Genetic CNN. CoRR, abs/1703.01513, 2017.</p>
<p>[25] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 2902–2911. PMLR, 2017.</p>
<p>[26] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR, abs/1611.01578, 2016.</p>
<p>[27] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.<br>Mobilenets: Efficient convolutional neural networks for mobile vision applications. CoRR, abs/1704.04861, 2017.</p>
<p>[28] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.</p>
<p>[29] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. CoRR, abs/1610.02915, 2016.</p>
<p>[30] Saining Xie, Ross B. Girshick, Piotr Dolla ́r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. CoRR, abs/1611.05431, 2016.</p>
<p>[31] Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.</p>
<p>[32] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed- ding. arXiv preprint arXiv:1408.5093, 2014.</p>
<p>[33] Jonathan Huang, Vivek Rathod, Chen Sun, Men- glong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. In CVPR, 2017.</p>
<p>[34] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, 2016.</p>
<p>[35] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016.</p>
<p>[36] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.</p>
<p>[37] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks. In Advances in neural information processing systems, pages 379–387, 2016.</p>
<p>[38] Jonathan Huang, Vivek Rathod, Derek Chow, Chen Sun, and Menglong Zhu. Tensorflow object detection api, 2017.</p>
<p>[39] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017.</p>
<p>[40] Matthias Holschneider, Richard Kronland-Martinet, Jean Morlet, and Ph Tchamitchian. A real-time algorithm for signal analysis with the help of the wavelet transform. In Wavelets: Time-Frequency Methods and Phase Space, pages 289–297. 1989.</p>
<p>[41] Pierre Sermanet, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv:1312.6229, 2013.</p>
<p>[42] George Papandreou,Iasonas Kokkinos, and Pierre Andre Savalle. Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection. In CVPR, 2015.</p>
<p>[43] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2017.</p>
<p>[44] Wei Liu, Andrew Rabinovich, and Alexander C. Berg. Parsenet: Looking wider to see better. CoRR, abs/1506.04579, 2015.</p>
<p>[45] Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, 2011.</p>
<p>[46] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR, abs/1602.07261, 2016.</p>
<p>[47] Guido Montúfar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS’14, pages 2924–2932, Cambridge, MA, USA, 2014. MIT Press.</p>
<p>[48] Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, editors. Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, 2012.</p>
]]></content>
    
    <summary type="html">
    
      MobileNetV2——Inverted Residuals and Linear Bottlenecks论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>MobileNetV2——Inverted Residuals and Linear Bottlenecks论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2018/06/06/2018-06-06-MobileNetV2-%20Inverted%20Residuals%20and%20Linear%20Bottlenecks%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2018/06/06/2018-06-06-MobileNetV2- Inverted Residuals and Linear Bottlenecks论文翻译——中英文对照/</id>
    <published>2018-06-06T07:22:11.000Z</published>
    <updated>2018-06-19T10:30:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks"><a href="#MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks" class="headerlink" title="MobileNetV2: Inverted Residuals and Linear Bottlenecks"></a>MobileNetV2: Inverted Residuals and Linear Bottlenecks</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在本文中，我们描述了一种新的移动架构MobileNetV2，该架构提高了移动模型在多个任务和多个基准数据集上以及在不同模型尺寸范围内的最佳性能。我们还描述了在我们称之为SSDLite的新框架中将这些移动模型应用于目标检测的有效方法。此外，我们还演示了如何通过DeepLabv3的简化形式，我们称之为Mobile DeepLabv3来构建移动语义分割模型。</p>
<p>The MobileNetV2 architecture is based on an inverted residual structure where the shortcut connections are between the thin bottle-neck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design.</p>
<p>MobileNetV2架构基于倒置的残差结构，其中快捷连接位于窄的瓶颈层之间。中间展开层使用轻量级的深度卷积作为非线性源来过滤特征。此外，我们发现为了保持表示能力，去除窄层中的非线性是非常重要的。我们证实了这可以提高性能并提供了产生此设计的直觉。</p>
<p>Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.</p>
<p>最后，我们的方法允许将输入/输出域与变换的表现力解耦，这为进一步分析提供了便利的框架。我们在ImageNet[1]分类，COCO目标检测[2]，VOC图像分割[3]上评估了我们的性能。我们评估了在精度、通过乘加（MAdd）度量的操作次数，以及实际的延迟和参数的数量之间的权衡。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Neural networks have revolutionized many areas of machine intelligence, enabling superhuman accuracy for challenging image recognition tasks. However, the drive to improve accuracy often comes at a cost: modern state of the art networks require high computational resources beyond the capabilities of many mobile and embedded applications.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>神经网络已经彻底改变了机器智能的许多领域，使具有挑战性的图像识别任务获得了超过常人的准确性。然而，提高准确性的驱动力往往需要付出代价：现代先进网络需要超出许多移动和嵌入式应用能力之外的高计算资源。</p>
<p>This paper introduces a new neural network architecture that is specifically tailored for mobile and resource constrained environments. Our network pushes the state of the art for mobile tailored computer vision models, by significantly decreasing the number of operations and memory needed while retaining the same accuracy.</p>
<p>本文介绍了一种专为移动和资源受限环境量身定制的新型神经网络架构。我们的网络通过显著减少所需操作和内存的数量，同时保持相同的精度推进了移动定制计算机视觉模型的最新水平。</p>
<p>Our main contribution is a novel layer module: the inverted residual with linear bottleneck. This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution. The official implementation is available as part of TensorFlow-Slim model library in [4].</p>
<p>我们的主要贡献是一个新的层模块：具有线性瓶颈的倒置残差。该模块将输入的低维压缩表示首先扩展到高维并用轻量级深度卷积进行过滤。随后用线性卷积将特征投影回低维表示。官方实现可作为[4]中TensorFlow-Slim模型库的一部分。</p>
<p>This module can be efficiently implemented using standard operations in any modern framework and allows our models to beat state of the art along multiple performance points using standard benchmarks. Furthermore, this convolutional module is particularly suitable for mobile designs, because it allows to significantly reduce the memory footprint needed during inference by never fully materializing large intermediate tensors. This reduces the need for main memory access in many embedded hardware designs, that provide small amounts of very fast software controlled cache memory.</p>
<p>这个模块可以使用任何现代框架中的标准操作来高效地实现，并允许我们的模型使用标准基线沿多个性能点击败最先进的技术。此外，这种卷积模块特别适用于移动设计，因为它可以通过从不完全实现大型中间张量来显著减少推断过程中所需的内存占用。这减少了许多嵌入式硬件设计中对主存储器访问的需求，这些设计提供了少量高速软件控制缓存。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>Tuning deep neural architectures to strike an optimal balance between accuracy and performance has been an area of active research for the last several years. Both manual architecture search and improvements in training algorithms, carried out by numerous teams has lead to dramatic improvements over early designs such as AlexNet [5], VGGNet [6], GoogLeNet [7]. , and ResNet [8]. Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [9, 10, 11] as well as various methods of network pruning [12, 13, 14, 15, 16, 17] and connectivity learning [18, 19]. A substantial amount of work has also been dedicated to changing the connectivity structure of the internal convolutional blocks such as in ShuffleNet [20] or introducing sparsity [21] and others [22].</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p>调整深层神经架构以在精确性和性能之间达到最佳平衡已成为过去几年研究活跃的一个领域。由许多团队进行的手动架构搜索和训练算法的改进，已经比早期的设计（如AlexNet[5]，VGGNet [6]，GoogLeNet[7]和ResNet[8]）有了显著的改进。最近在算法架构探索方面取得了很多进展，包括超参数优化[9，10，11]、各种网络修剪方法[12，13，14，15，16，17]和连接学习[18，19]。 也有大量的工作致力于改变内部卷积块的连接结构如ShuffleNet[20]或引入稀疏性[21]和其他[22]。</p>
<p>Recently, [23, 24, 25, 26], opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search. However one drawback is that the resulting networks end up very complex. In this paper, we pursue the goal of developing better intuition about how neural networks operate and use that to guide the simplest possible network design. Our approach should be seen as complimentary to the one described in [23] and related work. In this vein our approach is similar to those taken by [20, 22] and allows to further improve the performance, while providing a glimpse on its internal operation. Our network design is based on MobileNetV1 [27]. It retains its simplicity and does not require any special operators while significantly improves its accuracy, achieving state of the art on multiple image classification and detection tasks for mobile applications.</p>
<p>最近，[23,24,25,26]开辟了了一个新的方向，将遗传算法和强化学习等优化方法带入架构搜索。然而，一个缺点是最终所得到的网络非常复杂。在本文中，我们追求的目标是发展了解神经网络如何运行的更好直觉，并使用它来指导最简单可能的网络设计。我们的方法应该被视为[23]中描述的方法和相关工作的补充。在这种情况下，我们的方法与[20，22]所采用的方法类似，并且可以进一步提高性能，同时可以一睹其内部的运行。我们的网络设计基于MobileNetV1[27]。它保留了其简单性，并且不需要任何特殊的运算符，同时显著提高了它的准确性，为移动应用实现了在多种图像分类和检测任务上的最新技术。</p>
<h2 id="3-Preliminaries-discussion-and-intuition"><a href="#3-Preliminaries-discussion-and-intuition" class="headerlink" title="3. Preliminaries, discussion and intuition"></a>3. Preliminaries, discussion and intuition</h2><h3 id="3-1-Depthwise-Separable-Convolutions"><a href="#3-1-Depthwise-Separable-Convolutions" class="headerlink" title="3.1. Depthwise Separable Convolutions"></a>3.1. Depthwise Separable Convolutions</h3><p>Depthwise Separable Convolutions are a key building block for many efficient neural network architectures [27, 28, 20] and we use them in the present work as well. The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a 1 × 1 convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the input channels.</p>
<h2 id="3-准备，讨论和直觉"><a href="#3-准备，讨论和直觉" class="headerlink" title="3. 准备，讨论和直觉"></a>3. 准备，讨论和直觉</h2><h3 id="3-1-深度可分卷积"><a href="#3-1-深度可分卷积" class="headerlink" title="3.1. 深度可分卷积"></a>3.1. 深度可分卷积</h3><p>深度可分卷积是许多高效神经网络架构的关键组成部分[27，28，20]，我们在目前的工作中也使用它们。其基本思想是用分解版本替换完整的卷积运算符，将卷积拆分为两个单独的层。第一层称为深度卷积，它通过对每个输入通道应用单个卷积滤波器来执行轻量级滤波。第二层是1×1卷积，称为逐点卷积，它负责通过计算输入通道的线性组合来构建新特征。</p>
<p>Standard convolution takes an $h_i\times w_i\times d_i$ input tensor $L_i$, and applies convolutional  kernel $K\in \mathbf{R}^{k\times k \times d_i \times d_j}$ to produce an $h_i\times w_i\times d_j$ output tensor $L_j$. Standard convolutional layers have the computational cost of $h_i \cdot w_i \cdot d_i \cdot d_j \cdot k \cdot k$.</p>
<p>标准卷积使用$K\in \mathbf{R}^{k\times k \times d_i \times d_j}$维的输入张量$L_i$，并对其应用卷积核$K\in \mathbf{R}^{k\times k \times d_i \times d_j}$来产生$h_i\times w_i\times d_j$维的输出张量$L_j$。标准卷积层的计算代价为$h_i \cdot w_i \cdot d_i \cdot d_j \cdot k \cdot k$。</p>
<p>Depthwise separable convolutions are a drop-in replacement for standard convolutional layers. Empirically they work almost as well as regular convolutions but only cost: $$\begin{equation}h_i \cdot w_i \cdot d_i (k^2 + d_j) \tag{1}\end{equation}$$ which is the sum of the depthwise and $1 \times 1$ pointwise convolutions. Effectively depthwise separable convolution reduces computation compared to traditional layers by almost a factor of $k^2$. MobileNetV2 uses $k=3$ ($3 \times 3$ depthwise separable convolutions) so the computational cost is $8$ to $9$ times smaller than that of standard convolutions at only a small reduction in accuracy [27].</p>
<p>深度可分卷积是标准卷积层的直接替换。经验上，它们几乎与常规卷积一样工作，但其成本为：$$\begin{equation}h_i \cdot w_i \cdot d_i (k^2 + d_j) \tag{1}\end{equation}$$它是深度方向和$1\times 1$逐点卷积的总和。深度可分卷积与传统卷积层相比有效地减少了几乎$k^2$倍的计算量。MobileNetV2使用$k=3$（$3\times 3$的深度可分卷积），因此计算成本比标准卷积小$8$到$9$倍，但精度只有很小的降低[27]。</p>
<h3 id="3-2-Linear-Bottlenecks"><a href="#3-2-Linear-Bottlenecks" class="headerlink" title="3.2. Linear Bottlenecks"></a>3.2. Linear Bottlenecks</h3><p>Consider a deep neural network consisting of $n$ layers $L_i$ each of which has an activation tensor of dimensions $h_i \times w_i \times d_i$. Throughout this section we will be discussing the basic properties of these activation tensors, which we will treat as containers of $h_i \times<br>w_i$ “pixels” with $d_i$ dimensions. Informally, for an input set of real images, we say that the set of layer activations (for any layer $L_i$) forms a “manifold of interest”. It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. In other words, when we look at all individual $d$-channel pixels of a deep convolutional layer, the information encoded in those values actually lie in some manifold, which in turn is embeddable into a low-dimensional subspace.</p>
<p>At a first glance, such a fact could then be captured and exploited by simply reducing the dimensionality of a layer thus reducing the dimensionality of the operating space. This has been successfully exploited by MobileNetV1 [27] to effectively trade off between computation and accuracy via a width multiplier parameter, and has been incorporated into efficient model designs of other networks as well [20]. Following that intuition, the width multiplier approach allows one to reduce the dimensionality of the activation space until the manifold of interest spans this entire space. However, this intuition breaks down when we recall that deep convolutional neural networks actually have non-linear per coordinate transformations, such as ReLU. For example, ReLU applied to a line in 1D space produces a <code>ray</code>, where as in $\mathbf{R}^n$ space, it generally results in a piece-wise linear curve with $n$-joints.</p>
<p>It is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume $S$, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain. We refer to supplemental material for a more formal statement.</p>
<p>On the other hand, when ReLU collapses the channel, it inevitably loses information in that channel. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental materials, we show that if the input manifold can be embedded into a significantly lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.</p>
<p>To summarize, we have highlighted two properties that are indicative of the requirement that the manifold of interest should lie in a low-dimensional subspace of the higher-dimensional activation space:</p>
<ol>
<li><p>If the manifold of interest remains non-zero volume after ReLU transformation, it corresponds to a linear transformation.</p>
</li>
<li><p>ReLU is capable of preserving complete information about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space.</p>
</li>
</ol>
<p>These two insights provide us with an empirical hint for optimizing existing neural architectures: assuming the manifold of interest is low-dimensional we can capture this by inserting linear bottleneck layers into the convolutional blocks. Experimental evidence suggests that using linear layers is crucial as it prevents non-linearities from destroying too much information. In Section 6, we show empirically that using non-linear layers in bottlenecks indeed hurts the performance by several percent, further validating our hypothesis3. We note that similar reports where non-linearity was helped were reported in [29] where non-linearity was removed from the input of the traditional residual block and that lead to improved performance on CIFAR dataset.</p>
<p>For the remainder of this paper we will be utilizing bottleneck convolutions. We will refer to the ratio between the size of the input bottleneck and the inner size as the expansion ratio.</p>
<h3 id="3-3-Inverted-residuals"><a href="#3-3-Inverted-residuals" class="headerlink" title="3.3. Inverted residuals"></a>3.3. Inverted residuals</h3><p>The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks. Figure 3 provides a schematic visualization of the difference in the designs. The motivation for inserting shortcuts is similar to that of classical residual connections: we want to improve the ability of a gradient to propagate across multiplier layers. However, the inverted design is considerably more memory efficient (see Section 5 for details), as well as works slightly better in our experiments.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vision, 115(3):211–252, December 2015.</p>
<p>[2] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[3] Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserma. The pascal visual object classes challenge a retrospective. IJCV, 2014.</p>
<p>[4] Mobilenetv2 source code. Available from <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</a>.</p>
<p>[5] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Bartlett et al. [48], pages 1106–1114.</p>
<p>[6] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.</p>
<p>[7] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 1–9. IEEE Computer Society, 2015.</p>
<p>[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.</p>
<p>[9] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281–305, 2012.</p>
<p>[10] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms. In Bartlett et al. [48], pages 2960–2968.</p>
<p>[11] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using deep neural networks. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 2171–2180. JMLR.org, 2015.</p>
<p>[12] Babak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December 3, 1992], pages 164–171. Morgan Kaufmann, 1992.</p>
<p>[13] Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In David S. Touretzky, editor, Advances in Neural Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA, November 27-30, 1989], pages 598–605. Morgan Kaufmann, 1989.</p>
<p>[14] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural network. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1135–1143, 2015.</p>
<p>[15] Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John Tran, and William J. Dally. DSD: regularizing deep neural networks with dense-sparse-dense training flow. CoRR, abs/1607.04381, 2016.</p>
<p>[16] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 1379–1387, 2016.</p>
<p>[17] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. CoRR, abs/1608.08710, 2016.</p>
<p>[18] Karim Ahmed and Lorenzo Torresani. Connectivity learning in multi-branch networks. CoRR, abs/1709.09582, 2017.</p>
<p>[19] Tom Veniat and Ludovic Denoyer. Learning time-efficient deep architectures with budgeted super networks. CoRR, abs/1706.00046, 2017.</p>
<p>[20] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. CoRR, abs/1707.01083, 2017.</p>
<p>[21] Soravit Changpinyo, Mark Sandler, and Andrey Zhmoginov. The power of sparsity in convolutional neural networks. CoRR, abs/1702.06257, 2017.</p>
<p>[22] Min Wang, Baoyuan Liu, and Hassan Foroosh. Design of efficient convolutional layers using single intra-channel convolution, topological subdivisioning and spatial ”bottleneck” structure. CoRR, abs/1608.04337, 2016.</p>
<p>[23] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures for scalable image recognition. CoRR, abs/1707.07012, 2017.</p>
<p>[24] Lingxi Xie and Alan L. Yuille. Genetic CNN. CoRR, abs/1703.01513, 2017.</p>
<p>[25] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V. Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 2902–2911. PMLR, 2017.</p>
<p>[26] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR, abs/1611.01578, 2016.</p>
<p>[27] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.<br>Mobilenets: Efficient convolutional neural networks for mobile vision applications. CoRR, abs/1704.04861, 2017.</p>
<p>[28] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.</p>
<p>[29] Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. CoRR, abs/1610.02915, 2016.</p>
<p>[30] Saining Xie, Ross B. Girshick, Piotr Dolla ́r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. CoRR, abs/1611.05431, 2016.</p>
<p>[31] Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.</p>
<p>[32] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed- ding. arXiv preprint arXiv:1408.5093, 2014.</p>
<p>[33] Jonathan Huang, Vivek Rathod, Chen Sun, Men- glong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. In CVPR, 2017.</p>
<p>[34] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, 2016.</p>
<p>[35] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016.</p>
<p>[36] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.</p>
<p>[37] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks. In Advances in neural information processing systems, pages 379–387, 2016.</p>
<p>[38] Jonathan Huang, Vivek Rathod, Derek Chow, Chen Sun, and Menglong Zhu. Tensorflow object detection api, 2017.</p>
<p>[39] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017.</p>
<p>[40] Matthias Holschneider, Richard Kronland-Martinet, Jean Morlet, and Ph Tchamitchian. A real-time algorithm for signal analysis with the help of the wavelet transform. In Wavelets: Time-Frequency Methods and Phase Space, pages 289–297. 1989.</p>
<p>[41] Pierre Sermanet, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv:1312.6229, 2013.</p>
<p>[42] George Papandreou,Iasonas Kokkinos, and Pierre Andre Savalle. Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection. In CVPR, 2015.</p>
<p>[43] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2017.</p>
<p>[44] Wei Liu, Andrew Rabinovich, and Alexander C. Berg. Parsenet: Looking wider to see better. CoRR, abs/1506.04579, 2015.</p>
<p>[45] Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, 2011.</p>
<p>[46] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR, abs/1602.07261, 2016.</p>
<p>[47] Guido Montúfar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS’14, pages 2924–2932, Cambridge, MA, USA, 2014. MIT Press.</p>
<p>[48] Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, editors. Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, 2012.</p>
]]></content>
    
    <summary type="html">
    
      MobileNetV2——Inverted Residuals and Linear Bottlenecks论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>IO中同步、异步、阻塞、非阻塞的形象解释</title>
    <link href="http://noahsnail.com/2018/05/24/2018-05-24-IO%E4%B8%AD%E5%90%8C%E6%AD%A5%E3%80%81%E5%BC%82%E6%AD%A5%E3%80%81%E9%98%BB%E5%A1%9E%E3%80%81%E9%9D%9E%E9%98%BB%E5%A1%9E%E7%9A%84%E5%BD%A2%E8%B1%A1%E8%A7%A3%E9%87%8A/"/>
    <id>http://noahsnail.com/2018/05/24/2018-05-24-IO中同步、异步、阻塞、非阻塞的形象解释/</id>
    <published>2018-05-24T07:41:39.000Z</published>
    <updated>2018-05-24T07:42:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：以下文字来自网络，具体作者不知。</strong></p>
<p>老张爱喝茶，废话不说，煮开水。<br>出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。</p>
<ol>
<li>老张把水壶放到火上，立等水开。（同步阻塞）<br>老张觉得自己有点傻</li>
<li>老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞）<br>老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音。</li>
<li>老张把响水壶放到火上，立等水开。（异步阻塞）<br>老张觉得这样傻等意义不大</li>
<li>老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞）<br>老张觉得自己聪明了。</li>
</ol>
<p>所谓同步异步，只是对于水壶而言。<br>普通水壶，同步；响水壶，异步。<br>虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。<br>同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。</p>
<p>所谓阻塞非阻塞，仅仅对于老张而言。<br>立等的老张，阻塞；看电视的老张，非阻塞。<br>情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的，可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用。</p>
]]></content>
    
    <summary type="html">
    
      IO中同步、异步、阻塞、非阻塞的形象解释
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>概率论中的各种分布总结</title>
    <link href="http://noahsnail.com/2018/05/24/2018-05-24-%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E5%88%86%E5%B8%83%E6%80%BB%E7%BB%93/"/>
    <id>http://noahsnail.com/2018/05/24/2018-05-24-概率论中的各种分布总结/</id>
    <published>2018-05-24T06:31:31.000Z</published>
    <updated>2018-06-20T08:15:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-伯努利分布"><a href="#1-伯努利分布" class="headerlink" title="1. 伯努利分布"></a>1. 伯努利分布</h2><p>伯努利分布，英文为Bernoulli Distribution，又称两点分布或者0-1分布，是一种离散型概率分布。</p>
<h2 id="2-均匀分布"><a href="#2-均匀分布" class="headerlink" title="2. 均匀分布"></a>2. 均匀分布</h2><h2 id="3-二项分布"><a href="#3-二项分布" class="headerlink" title="3. 二项分布"></a>3. 二项分布</h2><h2 id="4-泊松分布"><a href="#4-泊松分布" class="headerlink" title="4. 泊松分布"></a>4. 泊松分布</h2><h2 id="5-指数分布"><a href="#5-指数分布" class="headerlink" title="5. 指数分布"></a>5. 指数分布</h2><h2 id="6-正态分布"><a href="#6-正态分布" class="headerlink" title="6. 正态分布"></a>6. 正态分布</h2><h2 id="7-卡方分布"><a href="#7-卡方分布" class="headerlink" title="7. 卡方分布"></a>7. 卡方分布</h2><h2 id="8-T分布"><a href="#8-T分布" class="headerlink" title="8. T分布"></a>8. T分布</h2><h2 id="9-Gamma分布"><a href="#9-Gamma分布" class="headerlink" title="9. Gamma分布"></a>9. Gamma分布</h2><h2 id="10-Beta分布"><a href="#10-Beta分布" class="headerlink" title="10. Beta分布"></a>10. Beta分布</h2><h2 id="11-Laplace分布"><a href="#11-Laplace分布" class="headerlink" title="11. Laplace分布"></a>11. Laplace分布</h2><h2 id="12-Dirichlet分布"><a href="#12-Dirichlet分布" class="headerlink" title="12. Dirichlet分布"></a>12. Dirichlet分布</h2><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2>]]></content>
    
    <summary type="html">
    
      概率论中的各种分布总结
    
    </summary>
    
      <category term="数学之美" scheme="http://noahsnail.com/categories/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    
    
      <category term="数学之美" scheme="http://noahsnail.com/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    
  </entry>
  
  <entry>
    <title>Unbroken</title>
    <link href="http://noahsnail.com/2018/05/24/2018-05-24-Unbroken/"/>
    <id>http://noahsnail.com/2018/05/24/2018-05-24-Unbroken/</id>
    <published>2018-05-24T05:23:39.000Z</published>
    <updated>2018-05-24T06:30:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-3dd38b789a3124b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Unbroken"></p>
<p>励志视频：</p>
<p>腾讯视频：<a href="https://v.qq.com/x/page/a0346ctbk24.html" target="_blank" rel="external">https://v.qq.com/x/page/a0346ctbk24.html</a></p>
<p>Youtube：<a href="https://www.youtube.com/watch?v=26U_seo0a1g&amp;t=4s" target="_blank" rel="external">https://www.youtube.com/watch?v=26U_seo0a1g&amp;t=4s</a></p>
<p>Unbroken——坚不可摧<br>You can’t connect the dots looking forward, you can only connect them looking backwards.</p>
<p>你无法把过去的点滴与未来联系，只有透过回顾才能看见。</p>
<p>So you have to trust that the dots will somehow connect in your future.</p>
<p>所以你必须相信过去的点滴能够串联未来。</p>
<p>You have to trust in something: your gut, destiny, life, karma, whatever.</p>
<p>你必须有信念，不管那是你的胆识，命运，人生，还是因果报应。</p>
<p>Because believing that the dots will connect down the road, will give you the confidence to follow your heart, even when it lead you off the well worn path. And that will make all the difference.</p>
<p>因为唯有把过去的点滴串联起来，你才能有信心忠于自我，即使你的选择和常人不同。这会使你与众不同。</p>
<p>Your time is limited, so don’t waste it living someone else’s life.</p>
<p>你的生命有限，所以不要浪费你的时间在别人的生活里。</p>
<p>Don’t be trapped by dogma, which is living with the results of other people’s thinking.</p>
<p>不要被教条束缚，不要活在别人对你的期望之中。</p>
<p>Don’t let the noise of others’ opinions drown out your own inner voice. You’ve got to find what you love.</p>
<p>不要让来自他人的噪音抹去你自己内心的声音，你必须找到你所爱的东西。</p>
<p>And that is as true for your work as it is for your lovers.</p>
<p>包括你热爱的事业和你的伴侣。</p>
<p>Your work is going to fill a large part of your life and the only way to be truly satisfied is to do what you believe is great work.</p>
<p>事业将占据你的大半生，唯有相信你所做的工作是对的，你才能发自内心的得到满足。</p>
<p>And the only way to do great work is to love what you do.</p>
<p>而唯有爱你所做的事，才能成就不凡。</p>
<p>If you haven’t found it yet, keep looking, and don’t settle.</p>
<p>如若你还没有找到，继续寻找，别追去安逸。</p>
<p>Have the courage to follow your heart and intuition, they somehow already know what you truly want to become.</p>
<p>有勇气顺从自己的心和直觉，你的内心早就知道你未来的梦想。</p>
<p>You’re going to have some ups and you’re goning to have some downs.</p>
<p>你的旅途不可能一帆风顺，必然有起有落。</p>
<p>Most people give up on themselves easily. You know the human spirit is powerful?!</p>
<p>大多数人都轻易放弃，但你知道人的意志有多强大吗？</p>
<p>There is nothing as powerful. It’s hard to kill the human spirit!</p>
<p>人的意志是无可比拟的坚韧不屈！</p>
<p>Anybody can feel good when they have their health, their bills are paid, they have happy relationships.</p>
<p>任何人在财务、生活、感情、健康良好的环境下，都能感到幸福。</p>
<p>Anybody can be positive then, anybody can have a larger vision then, anybody can have faith under those kinds of circumstances.</p>
<p>在那样的环境下，任何人都能自得其满，任何人都能有伟大的理想，任何人都能有信念。</p>
<p>The real challenge of growth, mentally, emotionally and spiritually comes when you get knocked down. It takes courage to act.</p>
<p>在你被击倒时，你所面临的真正挑战是你的信念、信仰和意志。起身而行需要勇气。</p>
<p>Part of being hungry when you have been defeated.</p>
<p>被击倒仍能保持谦虚，</p>
<p>It takes courage to start over again.</p>
<p>需要勇气去放下并重新开始。</p>
<p>Fear kills dreams.</p>
<p>恐惧扼杀梦想。</p>
<p>Fear kills hope.Fear, put people in the hospital.</p>
<p>恐惧扼杀希望，恐惧使人一蹶不振。</p>
<p>Fear can age you, can hold you back from doing something that you know within yourself that you are capable of doing, but it will paralyze you.</p>
<p>恐惧使你衰老，阻止你去做你能做到的事，但是它也会使你麻木不仁。</p>
<p>At the end of your feelings is nothing, but at the end of every principle is a promise.</p>
<p>你的情绪跌宕起伏，但仍然一事无成，但是在每个原则之后是自我的承诺。</p>
<p>Behind your little feelings, it might not be absolutely nothing at the end of your little feelings.</p>
<p>你的喜怒哀乐，最后可能什么都没有。</p>
<p>But behind every principle is a promise. And some of you in your life, the reason why you not at your goal right now, because you just all about your feelings.</p>
<p>但是每当你下定决心，就是一种承诺。你们有些人，至今碌碌无为，原因只是你们太过于情绪化。</p>
<p>All on your feellings, you don’t feel like waking up, so who does?</p>
<p>你总被情绪所主导，早上不愿起床，谁想呢？</p>
<p>Everyday you say “no” to your dreams, you might be pushing your dreams back a whole six months, a whole year!</p>
<p>每天你不愿去面对自己的梦想，你也许会把目标延后六个月，或是一年！</p>
<p>That one single day, that one day you didn’t get up could have pushed your stuff back, I don’t know how long.</p>
<p>只是那没有起身鞭策自己的一瞬间，你不知后退了多少。</p>
<p>Don’t allow your emotions to control you.</p>
<p>别让你的情感控制你。</p>
<p>We are emotional, but what you want to begin to discipline your emotion.</p>
<p>我们是情感的动物，但是你必须控制你的情感。</p>
<p>If you don’t discipline and contain your emotion, they will use you.</p>
<p>你若无法控制他，你便将被它吞噬。</p>
<p>You want it, and you are going to go all out to have it.</p>
<p>你想要全力以赴，毫无保留。</p>
<p>It’s not going to be easy, when you want to change. It’s not easy.</p>
<p>当你尝试改变的时候，不会很容易。</p>
<p>If it were in fact easy, everybody would do it.</p>
<p>如果很容易，那人人皆可做到。</p>
<p>But if you’re serious, you’ll go all out.</p>
<p>但是你若认真对待，全力以赴。</p>
<p>I’m in control here.</p>
<p>我才是自己的主宰者。</p>
<p>I’m not going to let this get me down, I’m not going to let this destroy me.</p>
<p>我绝不会让外界事物打击我，摧毁我。</p>
<p>I’m coming back!</p>
<p>我将重新站起！</p>
<p>And I’ll be stronger and better because of it!</p>
<p>我将会变得更好，更坚强！</p>
<p>You have got to make a declaration, that this is what you stand for!</p>
<p>你必须要下定决心，这是你的意义!</p>
<p>You’re standing up for your dreams, you’re standing up for peace of mind, you’re standing up for health.</p>
<p>如果这是你想要的梦想，那不论是健康或是功成名就。</p>
<p>Take full responsibility for your life!</p>
<p>请对你的人生负完全的责任！</p>
<p>Accept where you are and the responsibility that you’re going to take yourself where you want to go.</p>
<p>接受现在的自己，并把坚信自己能够做得更好当作一种责任。</p>
<p>You can decide that I am going to live each day as if it were my last!</p>
<p>你可以选择把你的每一天当做最后一天来过！</p>
<p>Live your life with passion! With some drive!</p>
<p>活出你的激情！拿出你的魄力！</p>
<p>Decided that you are going to push yourself.</p>
<p>不断鞭策自己做得更好！</p>
<p>The last chapter to your life has not been written yet, and it doen’t matter about what happened yesterday.</p>
<p>人生的最后一章尚未写下，昨天种种的事情并不重要。</p>
<p>It doesn’t matter about what happened to you, what matter is: what are you going to do about it?</p>
<p>你发生了什么也没有关系，重要的是：你接下来打算怎么做？</p>
<p>This year I will make this goal become a reality.</p>
<p>今年我将使梦想成为现实。</p>
<p>I won’t talk about it anymore. I can! I can!! I can!!!</p>
<p>我已不想再谈论什么，我能行！我能行！！我能行！！！</p>
<p>To persevere I think is important for everybody, don’t give up, don’t give in.</p>
<p>我觉得坚持对每个人都很重要，不要放弃，不要妥协。</p>
<p>There’s always an answer to everything.</p>
<p>人生的每件事情总是有解答的。</p>
]]></content>
    
    <summary type="html">
    
      Unbroken
    
    </summary>
    
      <category term="励志" scheme="http://noahsnail.com/categories/%E5%8A%B1%E5%BF%97/"/>
    
    
      <category term="励志" scheme="http://noahsnail.com/tags/%E5%8A%B1%E5%BF%97/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯估计、最大似然估计、最大后验概率估计</title>
    <link href="http://noahsnail.com/2018/05/17/2018-05-17-%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/"/>
    <id>http://noahsnail.com/2018/05/17/2018-05-17-贝叶斯估计、最大似然估计、最大后验概率估计/</id>
    <published>2018-05-17T02:12:59.000Z</published>
    <updated>2018-06-01T02:06:21.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://upload-images.jianshu.io/upload_images/3232548-aa0d1d09dec1ab6d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Bayes&#39; Theorem"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>贝叶斯估计、最大似然估计(MLE)、最大后验概率估计(MAP)这几个概念在机器学习和深度学习中经常碰到，读文章的时候还感觉挺明白，但独立思考时经常会傻傻分不清楚(😭)，因此希望通过本文对其进行总结。</p>
<hr>

<h2 id="2-背景知识"><a href="#2-背景知识" class="headerlink" title="2. 背景知识"></a>2. 背景知识</h2><p><strong>注：</strong>由于概率与数理统计需要了解的背景知识很多，因此这里只列出了部分内容，且写的较简略，许多概念的学习需要根据标题自己查找答案。</p>
<h3 id="2-1-概率与统计"><a href="#2-1-概率与统计" class="headerlink" title="2.1 概率与统计"></a>2.1 概率与统计</h3><p>概率统计是很多人都学过的内容，但概率论与统计学的关系是什么？先看一下概率论与统计学在维基百科中的定义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">概率论是集中研究概率及随机现象的数学分支，是研究随机性或不确定性等现象的数学。</div><div class="line">统计学是在数据分析的基础上，研究如何测定、收集、整理、归纳和分析反映数据数据，以便给出正确消息的科学。</div></pre></td></tr></table></figure></p>
<p>下面的一段话引自LarrB Wasserman的《All of Statistics》，对概率和统计推断的研究内容进行了描述：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">The basic problem that we studB in probabilitB is: </div><div class="line">Given a data generating process, what are the properities of the outcomes?</div><div class="line"></div><div class="line">The basic problem of statistical inference is the inverse of probabilitB: </div><div class="line">Given the outcomes, what can we saB about the process that generated the data?</div></pre></td></tr></table></figure></p>
<p>概率论是在给定条件（已知模型和参数）下，对要发生的事件（新输入数据）的预测。统计推断是在给定数据（训练数据）下，对数据生成方式（模型和参数）的归纳总结。概率论是统计学的数学基础，统计学是对概率论的应用。</p>
<h3 id="2-2-描述统计和推断统计"><a href="#2-2-描述统计和推断统计" class="headerlink" title="2.2 描述统计和推断统计"></a>2.2 描述统计和推断统计</h3><p>统计学分为描述统计学和推断统计学。描述统计，是统计学中描绘或总结观察量基本情况的统计总称。推断统计指统计学中研究如何根据样本数据去推断总体数量特征的方法。</p>
<p>描述统计是对数据的一种概括。描述统计是罗列所有数据，然后选择一些特征量（例如均值、方差、中位数、四分中位数等）对总体数据进行描述。推断统计是一种对数据的推测。推断统计无法获取所有数据，只能得到部分数据，然后根据得到的数据推测总体数据的情况。</p>
<h3 id="2-3-联合概率和边缘概率"><a href="#2-3-联合概率和边缘概率" class="headerlink" title="2.3 联合概率和边缘概率"></a>2.3 联合概率和边缘概率</h3><p>假设有随机变量$A$和$B$，此时$P(A=a,B=b)$用于表示$A=a$且$B=b$同时发生的概率。这类包含多个条件且<font color="blue"><strong>所有条件同时成立</strong></font>的概率称为<strong>联合概率</strong>。请注意，联合概率并不是其中某个条件成立的概率，而是所有条件同时成立的概率。与之对应地，$P(A=a)$或$P(B=b)$这类<font color="blue"><strong>仅与单个随机变量有关</strong></font>的概率称为<strong>边缘概率</strong>。</p>
<p>联合概率与边缘概率的关系如下：</p>
<p>$$P(A=a)=\sum_{b}P(A=a,B=b)$$$$P(A=b)=\sum_{a}P(A=a,B=b)$$</p>
<h3 id="2-4-条件概率"><a href="#2-4-条件概率" class="headerlink" title="2.4 条件概率"></a>2.4 条件概率</h3><p>条件概率表示在<font color="blue">条件$B=b$成立</font>的情况下，$A=a$的概率，记作$P(A=a|B=b)$，或者说条件概率是指事件$A=a$在另外一个事件$B=b$已经发生条件下的发生概率。为了简洁表示，后面省略a，b。</p>
<p>联合概率、边缘概率、条件概率的关系如下：</p>
<p>$$P(A|B)=\frac {P(A,B)} {P(B)}$$</p>
<p>转换为乘法形式：</p>
<p>$$P(A,B)=P(B)*P(A|B)=P(A)*P(B|A)$$</p>
<h3 id="2-5-全概率公式"><a href="#2-5-全概率公式" class="headerlink" title="2.5 全概率公式"></a>2.5 全概率公式</h3><p>如果事件$A_1，A_2，A_3，\ldots，A_n$构成一个完备事件组，即它们两两互不相容（互斥），其和为全集；并且$P(A_i)$大于0，则对任意事件$B$有$$P(B)=P(B|A_1)P(A_1)+P(B|A_2)P(A_2)+\ldots+ P(B|A_n)P(A_n)=\sum^n_{i=1}P(B|A_i)P(A_i)$$上面的公式称为全概率公式。全概率公式是对复杂事件$A$的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题。</p>
<h3 id="2-6-贝叶斯公式"><a href="#2-6-贝叶斯公式" class="headerlink" title="2.6 贝叶斯公式"></a>2.6 贝叶斯公式</h3><p>由条件概率的乘法形式可得：</p>
<p>$$P(A|B)=\frac {P(B|A)} {P(B)}*P(A)$$</p>
<p>上面的式子称为贝叶斯公式，也叫做贝叶斯定理或贝叶斯法则。在贝叶斯定理中，每个名词都有约定俗成的名称：</p>
<ul>
<li>$P(A|B)$是已知$B$发生后$A$的条件概率，也由于得自$B$的取值而被称作$A$的后验概率，表示<font color="blue">事件$B$发生后，事件$A$发生的置信度</font>。</li>
<li>$P(A)$是$A$的先验概率或边缘概率，表示<font color="blue">事件$A$发生的置信度</font>。</li>
<li>$P(B|A)$是已知$A$发生后$B$的条件概率，也由于得自$A$的取值而被称作$B$的后验概率，也被称作似然函数。</li>
<li>$P(B)$是$B$的先验概率或边缘概率，称为标准化常量。</li>
<li>$\frac {P(B|A)} {P(B)}$称为标准似然比(这个叫法很多，没找到标准统一的叫法)，表示<font color="blue">事件$B$为事件$A$发生提供的支持程度</font>。</li>
</ul>
<p>因此贝叶斯公式可表示为：后验概率=似然函数*先验概率/标准化常量=标准似然比*先验概率。根据标准似然比的大小，可分为下面三种情况：</p>
<ul>
<li>如果标准似然比$&gt;1$，则先验概率$P(A)$得到增强，事件$B$的发生会增大事件$A$发生的可能性；</li>
<li>如果标准似然比$=1$，则先验概率$P(A)$保持不变，事件$B$的发生不影响事件$A$发生的可能性； </li>
<li>如果标准似然比$&lt;1$，则先验概率$P(A)$得到削弱，事件$B$的发生会降低事件$A$发生的可能性。</li>
</ul>
<p>由全概率公式、贝叶斯法则可得：<br>$$P(A_i|B)=\frac {P(B|A_i)P(A_i)} {P(B)}=\frac {P(B|A_i)P(A_i)} {\sum^n_{i=1}P(B|A_i)P(A_i)}$$</p>
<h3 id="2-7-似然与概率"><a href="#2-7-似然与概率" class="headerlink" title="2.7 似然与概率"></a>2.7 似然与概率</h3><p>在英文中，似然（likelihood）和概率（probability）是同义词，都指事件发生的可能性。但在统计中，似然与概率是不同的东西。概率是已知参数，对结果可能性的预测。似然是已知结果，对参数是某个值的可能性预测。</p>
<h3 id="2-8-似然函数与概率函数"><a href="#2-8-似然函数与概率函数" class="headerlink" title="2.8 似然函数与概率函数"></a>2.8 似然函数与概率函数</h3><p>对于函数$P(x|\theta)$，从不同的观测角度来看可以分为以下两种情况：</p>
<ul>
<li>如果$\theta$已知且保持不变，$x$是变量，则$P(x|\theta)$称为概率函数，表示不同$x$出现的概率。</li>
<li>如果$x$已知且保持不变，$\theta$是变量，则$P(x|\theta)$称为似然函数，表示不同$\theta$下，$x$出现的概率，也记作$L(\theta|x)$或$L(x;\theta)$或$f(x;\theta)$。</li>
</ul>
<p><strong>注：</strong>注意似然函数的不同写法。</p>
<h3 id="2-9-推断统计中需要了解的一些概念"><a href="#2-9-推断统计中需要了解的一些概念" class="headerlink" title="2.9 推断统计中需要了解的一些概念"></a>2.9 推断统计中需要了解的一些概念</h3><ul>
<li>假设实际观测值与真实分布相关，试图根据观测值来推测真实分布</li>
<li>由于观测值取值随机，因此由它们计算得到的估计值也是随机值</li>
<li>估计方式多种多样，且不同估计方式得到的估计值也有所不同</li>
</ul>
<p>样本、样本容量、参数统计、非参数统计、估计量、真实分布、经验分布。</p>
<h3 id="2-10-频率学派与贝叶斯学派"><a href="#2-10-频率学派与贝叶斯学派" class="headerlink" title="2.10 频率学派与贝叶斯学派"></a>2.10 频率学派与贝叶斯学派</h3><p><strong>注：</strong>频率学派与贝叶斯学派只是解决问题的角度不同。</p>
<p>频率学派与贝叶斯学派探讨「不确定性」这件事时的出发点与立足点不同。频率学派从「自然」角度出发，试图直接为「事件」本身建模，即事件$A$在独立重复试验中发生的频率趋于极限$p$，那么这个极限就是该事件的概率。</p>
<p>贝叶斯学派并不从试图刻画「事件」本身，而从「观察者」角度出发。贝叶斯学派并不试图说「事件本身是随机的」，或者「世界的本体带有某种随机性」，这套理论根本不言说关于「世界本体」的东西，而只是从「观察者知识不完备」这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。</p>
<p>频率学派的代表是最大似然估计；贝叶斯学派的代表是最大后验概率估计。</p>
<h3 id="2-11-共轭先验"><a href="#2-11-共轭先验" class="headerlink" title="2.11 共轭先验"></a>2.11 共轭先验</h3><p>在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。</p>
<h3 id="2-12-Beta分布"><a href="#2-12-Beta分布" class="headerlink" title="2.12 Beta分布"></a>2.12 Beta分布</h3><p>在概率论中，Beta分布也称Β分布，是指一组定义在$(0,1)$区间的连续概率分布，有两个参数$\alpha,\beta&gt;0$。Beta分布的概率密度为：</p>
<p>$$\begin{align}f(x;\alpha,\beta)&amp;=\frac {x^{\alpha-1}(1-x)^{\beta-1}} {\int_{0}^1 \mu^{\alpha-1}(1-\mu)^{\beta-1}d\mu} \\&amp;= \frac{\Gamma(\alpha+\beta)} {\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1} \\&amp;=\frac {1} {B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\end{align}$$其中，$\Gamma(z)$是$\Gamma$函数。随机变量$X$服从Beta分布写作$X\sim Beta(\alpha,\beta)$。</p>
<hr>

<h2 id="3-问题定义"><a href="#3-问题定义" class="headerlink" title="3. 问题定义"></a>3. 问题定义</h2><p>以抛硬币为例，假设我们有一枚硬币，现在要估计其正面朝上的概率$\theta$。为了对$\theta$进行估计，我们进行了10次实验（独立同分布，i.i.d.），这组实验记为$X=x_1，x_2，\ldots，x_{10}$，其中正面朝上的次数为6次，反面朝上的次数为4次，结果为$(1,0,1,1,0,0,0,1,1,1)$。</p>
<h2 id="4-最大似然估计-MLE"><a href="#4-最大似然估计-MLE" class="headerlink" title="4. 最大似然估计(MLE)"></a>4. 最大似然估计(MLE)</h2><p>最大似然估计，英文为Maximum Likelihood Estimation，简写为MLE，也叫极大似然估计，是用来估计概率模型参数的一种方法。最大似然估计的思想是使得观测数据（样本）发生概率最大的参数就是最好的参数。</p>
<p>对一个独立同分布的样本集来说，总体的似然就是每个样本似然的乘积。针对抛硬币的问题，似然函数可写作：$$L(X;\theta)=\prod_{i=0}^nP(x_i|\theta)=\theta^6(1-\theta)^4$$根据最大似然估计，使$L(X;\theta)$取得最大值的$\theta$即为估计结果，令$L(X;\theta)\prime =0$可得$\hat{\theta}=0.6$。似然函数图如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-c785fe210be37ef5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MLE"></p>
<p>由于总体的似然就是每个样本似然的乘积，为了求解方便，我们通常会将似然函数转成对数似然函数，然后再求解。可以转成对数似然函数的主要原因是对数函数并不影响函数的凹凸性。因此上式可变为：$$lnL(X;\theta)=ln\prod_{i=0}^nP(x_i|\theta)=\sum_{i=0}^nln(P(x_i|\theta))=6ln(\theta)+4ln(1-\theta)$$令$ln(L(X;\theta)\prime) =0$可得$\hat{\theta}=0.6$。</p>
<p><strong>正态分布的最大似然估计</strong></p>
<p>假设样本服从正态分布$N\sim(\mu,\sigma^2)$，则其似然函数为$$L(\mu,\sigma^2)=\prod_{i=0}^n \frac {1} {\sqrt{2\pi} \sigma}e^{-\frac {(x_i-\mu)^2} {2\sigma^2}}$$对其取对数得：$$lnL(\mu,\sigma^2)=-\frac {n} {2}ln(2\pi) - \frac {n} {2} ln(\sigma^2) - \frac {1} {2\sigma^2} \sum_{i=0}^n(x_i-\mu)^2$$<br>分别对$\mu，\sigma^2$求偏导，并令偏导数为0，得：$$\begin{cases}<br>\frac {\partial lnL(\mu,\sigma^2)} {\partial \mu}= \frac {1} {\sigma^2} \sum_{i=0}^n(x_i-\mu) =0\\<br>\frac {\partial lnL(\mu,\sigma^2)} {\partial \sigma^2}= -\frac {n} {2\sigma^2} + \frac {1} {2\sigma^4}\sum_{i=0}^n(x_i-\mu)^2 =0<br>\end{cases}$$</p>
<p>解得：<br>$$\begin{cases}<br>\hat{\mu}= \frac {1} {n} \sum_{i=0}^nx_i=\bar{x}\\<br>\hat{\sigma^2} = \frac {1} {n} \sum_{i=0}^n(x_i-\bar{x})^2<br>\end{cases}$$</p>
<p>$\hat{\mu}，\hat{\sigma^2}$就是正态分布中$\mu，\sigma^2$的最大似然估计。</p>
<p>最大似然估计的求解步骤：</p>
<ul>
<li>确定似然函数</li>
<li>将似然函数转换为对数似然函数</li>
<li>求对数似然函数的最大值（求导，解似然方程）</li>
</ul>
<hr>

<h2 id="5-最大后验概率估计-MAP"><a href="#5-最大后验概率估计-MAP" class="headerlink" title="5. 最大后验概率估计(MAP)"></a>5. 最大后验概率估计(MAP)</h2><p>最大后验概率估计，英文为Maximum A Posteriori Estimation，简写为MAP。回到抛硬币的问题，最大似然估计认为使似然函数$P(X|\theta)$最大的参数$\theta$即为最好的$\theta$，此时最大似然估计是将$\theta$看作固定的值，只是其值未知；最大后验概率分布认为$\theta$是一个随机变量，即$\theta$具有某种概率分布，称为先验分布，求解时除了要考虑似然函数$P(X|\theta)$之外，还要考虑$\theta$的先验分布$P(\theta)$，因此其认为使$P(X|\theta)P(\theta)$取最大值的$\theta$就是最好的$\theta$。此时要最大化的函数变为$P(X|\theta)P(\theta)$，由于$X$的先验分布$P(X)$是固定的（可通过分析数据获得，其实我们也不关心$X$的分布，我们关心的是$\theta$），因此最大化函数可变为$\frac {P(X|\theta)P(\theta)} {P(X)}$，根据贝叶斯法则，要最大化的函数$\frac {P(X|\theta)P(\theta)} {P(X)}=P(\theta|X)$，因此要最大化的函数是$P(\theta|X)$，而$P(\theta|X)$是$\theta$的后验概率。最大后验概率估计可以看作是正则化的最大似然估计，当然机器学习或深度学习中的正则项通常是加法，而在最大后验概率估计中采用的是乘法，$P(\theta)$是正则项。在最大似然估计中，由于认为$\theta$是固定的，因此$P(\theta)=1$。</p>
<p>最大后验概率估计的公式表示：$$\mathop{argmax}_{\theta}P(\theta|X)=\mathop{argmax}_{\theta}\frac {P(X|\theta)P(\theta)} {P(X)}\propto \mathop{argmax}_{\theta}P(X|\theta)P(\theta)$$</p>
<p>在抛硬币的例子中，通常认为$\theta=0.5$的可能性最大，因此我们用均值为$0.5$，方差为$0.1$的高斯分布来描述$\theta$的先验分布，当然也可以使用其它的分布来描述$\theta$的先验分布。$\theta$的先验分布为：$$\frac {1} {\sqrt{2\pi}\sigma}e^{-\frac {(\theta-\mu)^2} {2\sigma^2}} = \frac {1} {10\sqrt{2\pi}}e^{-50(\theta-0.5)^2}$$先验分布的函数图如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-258e98a5b817b74d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Gaussian"></p>
<p>在最大似然估计中，已知似然函数为$P(X|\theta)=\theta^6(1-\theta)^4$，因此：$$P(X|\theta)P(\theta)=\theta^6\times (1-\theta)^4\times \frac {1} {10\sqrt{2\pi}}\times e^{-50(\theta-0.5)^2}$$转换为对数函数：$$ln(P(X|\theta)P(\theta))=ln(\theta^6\times (1-\theta)^4 \times \frac {1} {10\sqrt{2\pi}}\times e^{-50(\theta-0.5)^2})=6ln(\theta)+4ln(1-\theta)+ln(\frac {1} {10\sqrt{2\pi}})-50(\theta-0.5)^2$$</p>
<p>令$ln(P(X|\theta)P(\theta))\prime=0$，可得：$$100\theta^3-150\theta^2+40\theta+6=0$$由于$0\le\theta\le1$，解得：$\hat{\theta}\approx0.529$。$P(X|\theta)P(\theta)$的函数图像如下，基本符合$\theta$的估计值$\hat{\theta}$：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-3ab13f8b079cc3cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MAP"></p>
<p>如果我们用均值为$0.6$，方差为$0.1$的高斯分布来描述$\theta$的先验分布，则$\hat{\theta}=0.6$。由此可见，在最大后验概率估计中，$\theta$的估计值与$\theta$的先验分布有很大的关系。这也说明一个合理的先验概率假设是非常重要的。如果先验分布假设错误，则会导致估计的参数值偏离实际的参数值。</p>
<p><strong>先验分布为Beta分布</strong></p>
<p>如果用$\alpha=3,\beta=3$的Beta分布来描述$\theta$的先验分布，则$$P(X|\theta)P(\theta)=\theta^6\times (1-\theta)^4\times \frac {1} {B(\alpha,\beta)}\times \theta^{\alpha-1}(1-\theta)^{\beta-1}$$令$P(X|\theta)P(\theta)\prime=0$求解可得：$$\hat{\theta}=\frac {\alpha+5} {\alpha + \beta +8}=\frac {8} {3 + 3 +8}\approx 0.57$$</p>
<p>$Beta(3,3)$的概率密度图像如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-3956d07a43ac0e57.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Beta(3,3)"></p>
<p>最大后验概率估计的求解步骤：</p>
<ul>
<li>确定参数的先验分布以及似然函数</li>
<li>确定参数的后验分布函数</li>
<li>将后验分布函数转换为对数函数</li>
<li>求对数函数的最大值（求导，解方程）</li>
</ul>
<hr>

<h2 id="6-贝叶斯估计"><a href="#6-贝叶斯估计" class="headerlink" title="6. 贝叶斯估计"></a>6. 贝叶斯估计</h2><p>贝叶斯估计是最大后验估计的进一步扩展，贝叶斯估计同样假定$\theta$是一个随机变量，但贝叶斯估计并不是直接估计出$\theta$的某个特定值，而是估计$\theta$的分布，这是贝叶斯估计与最大后验概率估计不同的地方。在贝叶斯估计中，先验分布$P(X)$是不可忽略的。回到抛硬币的例子中，在已知$X$的情况下，描述$\theta$的分布即描述$P(\theta|X)$，$P(\theta|X)$是一种后验分布。如果后验分布的范围较窄，则估计值的准确度相对较高，反之，如果后验分布的范围较广，则估计值的准确度就较低。</p>
<p>贝叶斯公式：$$P(\theta|X)=\frac {P(X|\theta)P(\theta)} {P(X)}$$</p>
<p>在连续型随机变量中，由于$P(X)=\int_{\Theta}P(X|\theta)P(\theta)d\theta$，因此贝叶斯公式变为：$$P(\theta|X)=\frac {P(X|\theta)P(\theta)} {\int_{\Theta}P(X|\theta)P(\theta)d\theta}$$</p>
<p>从上面的公式中可以看出，贝叶斯估计的求解非常复杂，因此选择合适的先验分布就非常重要。一般来说，计算积分$\int_{\theta}P(X|\theta)P(\theta)d\theta$是不可能的。对于这个抛硬币的例子来说，如果使用共轭先验分布，就可以更好的解决这个问题。二项分布参数的共轭先验是Beta分布，由于$\theta$的似然函数服从二项分布，因此在贝叶斯估计中，假设$\theta$的先验分布服从$P(\theta)\sim Beta(\alpha, \beta)$，Beta分布的概率密度公式为：$$f(x;\alpha,\beta)=\frac {1} {B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}$$因此，贝叶斯公式可写作：$$\begin{aligned} P(\theta|X)&amp;=\frac {P(X|\theta)P(\theta)} {\int_{\Theta}P(X|\theta)P(\theta)d\theta} \\ &amp;=\frac {\theta^6(1-\theta)^4 \frac {\theta^{\alpha-1}(1-\theta)^{\beta-1}} {B(\alpha,\beta)} } {\int_{\Theta}\theta^6(1-\theta)^4 \frac {\theta^{\alpha-1}(1-\theta)^{\beta-1}} {B(\alpha,\beta)}d\theta} \\&amp;=\frac {\theta^{\alpha+6-1}(1-\theta)^{\beta+4-1}} {\int_{\Theta}\theta^{\alpha+6-1}(1-\theta)^{\beta+4-1}d\theta} \\<br>&amp;=\frac {\theta^{\alpha+6-1}(1-\theta)^{\beta+4-1}} {B(\alpha+6-1,\beta+4-1)} \\<br>&amp;=Beta(\theta|\alpha+6-1,\beta+4-1) \\&amp;=Beta(\theta|\alpha+6,\beta+4)\end{aligned}$$从上面的公式可以看出，$P(\theta|X) \sim Beta(\theta|\alpha+6,\beta+4)$。其中$B$函数，也称$Beta$函数，是一个标准化常量，用来使整个概率的积分为1。$Beta(\theta|\alpha+6,\beta+4)$就是贝叶斯估计的结果。</p>
<p>如果使用贝叶斯估计得到的$\theta$分布存在一个有限均值，则可以用后验分布的期望作为$\theta$的估计值。假设$\alpha=3,\beta=3$，在这种情况下，先验分布会在$0.5$处取得最大值，则$P(\theta|X) \sim Beta(\theta|9,7)$，$Beta(\theta|9,7)$的曲线如下图：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-85c4b2ac27fbe731.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Beta(9,7)"></p>
<p>从上图可以看出，在$\alpha=3,\beta=3$的情况下，$\theta$的估计值$\hat{\theta}$应该在$0.6$附近。根据Beta分布的数学期望公式$E(\theta)=\frac {\alpha} {\alpha+\beta}$可得：$$\hat{\theta}=\int_{\Theta} \theta P(\theta|X)d\theta=E(\theta)=\frac {\alpha} {\alpha+\beta}=\frac {9} {9+7}=0.5625$$</p>
<p><strong>注：</strong>二项分布参数的共轭先验是Beta分布，多项式分布参数的共轭先验是Dirichlet分布，指数分布参数的共轭先验是Gamma分布，⾼斯分布均值的共轭先验是另⼀个⾼斯分布，泊松分布的共轭先验是Gamma分布。</p>
<p>贝叶斯估计要解决的不是如何估计参数，而是用来估计新测量数据出现的概率，对于新出现的数据$\tilde{x}$：</p>
<p>$$P(\tilde{x}|X)=\int_{\Theta}P(\tilde{x}|\theta)P(\theta|X)d\theta=\int_{\Theta}P(\tilde{x}|\theta)\frac {P(X|\theta)P(\theta)} {P(X)}d\theta$$</p>
<p>贝叶斯估计的求解步骤：</p>
<ul>
<li>确定参数的似然函数</li>
<li>确定参数的先验分布，应是后验分布的共轭先验</li>
<li>确定参数的后验分布函数</li>
<li>根据贝叶斯公式求解参数的后验分布</li>
</ul>
<h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><p>从最大似然估计、最大后验概率估计到贝叶斯估计，从下表可以看出$\theta$的估计值$\hat{\theta}$是逐渐接近$0.5$的。从公式的变化可以看出，使用的信息是逐渐增多的。最大似然估计、最大后验概率估计中都是假设$\theta$未知，但是确定的值，都将使函数取得最大值的$\theta$作为估计值，区别在于最大化的函数不同，最大后验概率估计使用了$\theta$的先验概率。而在贝叶斯估计中，假设参数$\theta$是未知的随机变量，不是确定值，求解的是参数$\theta$在样本$X$上的后验分布。</p>
<p><strong>注：</strong>最大后验概率估计和贝叶斯估计都采用Beta分布作为先验分布。</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>MLE</th>
<th>MAP</th>
<th>BE</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\hat{\theta}$</td>
<td>0.6</td>
<td>0.57</td>
<td>0.5625</td>
</tr>
<tr>
<td>$f$</td>
<td>$P(X&#124;\theta)$</td>
<td>$P(X&#124;\theta)P(\theta)$</td>
<td>$\frac {P(X&#124;\theta)P(\theta)} {P(X)}$</td>
</tr>
</tbody>
</table>
<hr>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>书籍：程序员的数学2——概率统计</li>
<li><a href="https://www.zhihu.com/question/20269390" target="_blank" rel="external">概率论与统计学的关系是什么？</a></li>
<li><a href="https://www.zhihu.com/question/20587681" target="_blank" rel="external">贝叶斯学派与频率学派有何不同？</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E8%AE%BA" target="_blank" rel="external">概率论</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%8E%A8%E8%AB%96%E7%B5%B1%E8%A8%88%E5%AD%B8" target="_blank" rel="external">推论统计学</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%8F%8F%E8%BF%B0%E7%BB%9F%E8%AE%A1%E5%AD%A6" target="_blank" rel="external">描述统计学</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6" target="_blank" rel="external">统计学</a></li>
<li><a href="https://blog.csdn.net/u011508640/article/details/72815981" target="_blank" rel="external">详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解</a></li>
<li><a href="https://www.zhihu.com/question/27462939" target="_blank" rel="external">如何理解条件概率？</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86" target="_blank" rel="external">贝叶斯定理</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html" target="_blank" rel="external">贝叶斯推断及其互联网应用（一）：定理简介</a></li>
<li><a href="https://baike.baidu.com/item/%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F" target="_blank" rel="external">全概率公式</a></li>
<li><a href="https://www.zhihu.com/question/19725590" target="_blank" rel="external">怎样用非数学语言讲解贝叶斯定理（Bayes’s theorem）？</a></li>
<li><a href="http://yangfangs.github.io/2018/04/06/the-different-of-likelihood-and-probability/" target="_blank" rel="external">似然（likelihood）与概率（probability）的区别</a></li>
<li><a href="https://www.zhihu.com/question/24124998" target="_blank" rel="external">如何通俗地理解概率论中的「极大似然估计法」?</a></li>
<li><a href="https://www.matongxue.com/madocs/447.html" target="_blank" rel="external">如何通俗地理解“最大似然估计法”?</a></li>
<li><a href="http://218.194.248.2/~wuxiaogang/cpcourse/other/static.pdf" target="_blank" rel="external">概率论与数理统计</a></li>
<li><a href="https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf" target="_blank" rel="external">All of Statistics: A Concise Course in Statistical Inference</a></li>
<li><a href="https://www.zhihu.com/question/19894595" target="_blank" rel="external">MLE，MAP，EM 和 point estimation 之间的关系是怎样的？</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87" target="_blank" rel="external">最大后验概率</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2018-01-09-6" target="_blank" rel="external">从最大似然估计开始，你需要打下的机器学习基石</a></li>
<li><a href="https://www.zhihu.com/question/54082000" target="_blank" rel="external">如何理解似然函数?</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C" target="_blank" rel="external">共轭先验</a></li>
<li><a href="https://blog.csdn.net/lin360580306/article/details/51289543" target="_blank" rel="external">参数估计：最大似然估计（MLE），最大后验估计(MAP)，贝叶斯估计，经验贝叶斯(Empirical Bayes)与全贝叶斯(Full Bayes)</a></li>
<li><a href="https://www.jiqizhixin.com/articles/041505" target="_blank" rel="external">什么是最大似然估计、最大后验估计以及贝叶斯参数估计</a></li>
<li><a href="https://blog.csdn.net/baimafujinji/article/details/51374202" target="_blank" rel="external">先验概率、后验概率以及共轭先验</a></li>
<li><a href="https://cosx.org/2013/01/lda-math-beta-dirichlet" target="_blank" rel="external">认识Beta/Dirichlet分布</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%CE%92%E5%88%86%E5%B8%83" target="_blank" rel="external">Β分布</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%CE%92%E5%87%BD%E6%95%B0" target="_blank" rel="external">Β函数</a></li>
<li><a href="https://en.wikipedia.org/wiki/Beta_distribution" target="_blank" rel="external">Beta distribution</a></li>
<li><a href="https://en.wikipedia.org/wiki/Beta_function" target="_blank" rel="external">Beta function</a></li>
<li><a href="http://eurekastatistics.com/beta-distribution-pdf-grapher/" target="_blank" rel="external">Beta Distribution PDF Grapher</a></li>
<li><a href="https://blog.csdn.net/yangliuy/article/details/8296481" target="_blank" rel="external">文本语言模型的参数估计-最大似然估计、MAP及贝叶斯估计</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%CE%93%E5%87%BD%E6%95%B0" target="_blank" rel="external">Γ函数</a></li>
<li><a href="http://fooplot.com/" target="_blank" rel="external">使用的绘图工具</a></li>
<li><a href="http://www.yunsuan.org/app/s2084" target="_blank" rel="external">求解一元三次方程的工具</a></li>
<li><a href="https://www.zhihu.com/question/21134457" target="_blank" rel="external">你对贝叶斯统计都有怎样的理解？</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bayesian_inference#Formal_description_of_Bayesian_inference" target="_blank" rel="external">Bayesian inference</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%A9%9F%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B8" target="_blank" rel="external">概率密度函数</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E7%B4%AF%E7%A7%AF%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0" target="_blank" rel="external">累积分布函数</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0" target="_blank" rel="external">似然函数</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E8%B4%A8%E9%87%8F%E5%87%BD%E6%95%B0" target="_blank" rel="external">概率质量函数</a></li>
<li><a href="https://www.datascience.com/blog/introduction-to-bayesian-inference-learn-data-science-tutorials" target="_blank" rel="external">Introduction to Bayesian Inference</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      贝叶斯估计、最大似然估计、最大后验概率估计
    
    </summary>
    
      <category term="数学之美" scheme="http://noahsnail.com/categories/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    
    
      <category term="数学之美" scheme="http://noahsnail.com/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/"/>
    
  </entry>
  
  <entry>
    <title>Linux的uptime命令</title>
    <link href="http://noahsnail.com/2018/05/03/2018-05-03-Linux%E7%9A%84uptime%E5%91%BD%E4%BB%A4/"/>
    <id>http://noahsnail.com/2018/05/03/2018-05-03-Linux的uptime命令/</id>
    <published>2018-05-03T09:46:00.000Z</published>
    <updated>2018-05-03T10:13:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-uptime命令介绍"><a href="#1-uptime命令介绍" class="headerlink" title="1. uptime命令介绍"></a>1. uptime命令介绍</h2><p><code>uptime</code>是Linux系统最基本的统计命令。<code>uptime</code>会提供一些我们要用到的不同基本信息：</p>
<ul>
<li>当前时间</li>
<li>系统运行的天数、小时数、分钟数</li>
<li>当前登录到系统上的用户数</li>
<li>一分钟、五分钟、十五分钟的平均负载</li>
</ul>
<h2 id="2-uptime用法"><a href="#2-uptime用法" class="headerlink" title="2. uptime用法"></a>2. uptime用法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ uptime</div><div class="line">18:12  up 70 days,  8:35, 14 users, load averages: 1.89 2.07 2.31</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Linux命令行与shell脚本编程</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Linux的uptime命令
    
    </summary>
    
      <category term="Linux" scheme="http://noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux的vmstat命令</title>
    <link href="http://noahsnail.com/2018/05/03/2018-05-03-Linux%E7%9A%84vmstat%E5%91%BD%E4%BB%A4/"/>
    <id>http://noahsnail.com/2018/05/03/2018-05-03-Linux的vmstat命令/</id>
    <published>2018-05-03T08:46:23.000Z</published>
    <updated>2018-05-03T10:42:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-vmstat命令介绍"><a href="#1-vmstat命令介绍" class="headerlink" title="1. vmstat命令介绍"></a>1. vmstat命令介绍</h2><p><code>vmstat</code>命令主要用来提取系统信息，其会生成一个详尽的系统内存和CPU使用情况报告。</p>
<h2 id="2-vmstat的符号含义介绍"><a href="#2-vmstat的符号含义介绍" class="headerlink" title="2. vmstat的符号含义介绍"></a>2. vmstat的符号含义介绍</h2><table>
<thead>
<tr>
<th>符号</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>r</td>
<td>等待CPU时间的进程数</td>
</tr>
<tr>
<td>b</td>
<td>处于不可中断休眠中的进程数</td>
</tr>
<tr>
<td>swpd</td>
<td>使用的虚拟内存总量（单位：MB）</td>
</tr>
<tr>
<td>free</td>
<td>空闲的物理内存总量（单位：MB）</td>
</tr>
<tr>
<td>buff</td>
<td>用作缓冲区的内存总量（单位：MB）</td>
</tr>
<tr>
<td>cache</td>
<td>用作高速缓存的内存总量（单位：MB）</td>
</tr>
<tr>
<td>si</td>
<td>从磁盘交换进来的内存总量（单位：MB）</td>
</tr>
<tr>
<td>so</td>
<td>交换到磁盘的内存总量（单位：MB）</td>
</tr>
<tr>
<td>bi</td>
<td>从块设备受到的块数</td>
</tr>
<tr>
<td>bo</td>
<td>发送给块设备的块数</td>
</tr>
<tr>
<td>in</td>
<td>每秒的CPU中断次数</td>
</tr>
<tr>
<td>cs</td>
<td>每秒的CPU上下文切换数</td>
</tr>
<tr>
<td>us</td>
<td>用于执行非内核代码的CPU时间所占的百分比</td>
</tr>
<tr>
<td>sy</td>
<td>用于执行内核代码的CPU时间所占的百分比</td>
</tr>
<tr>
<td>id</td>
<td>处于空闲状态的CPU时间所占的百分比</td>
</tr>
<tr>
<td>wa</td>
<td>用于等待I/O的CPU时间所占的百分比</td>
</tr>
<tr>
<td>st</td>
<td>虚拟机偷取的时间所占的百分比</td>
</tr>
</tbody>
</table>
<h2 id="3-vmstat用法"><a href="#3-vmstat用法" class="headerlink" title="3. vmstat用法"></a>3. vmstat用法</h2><p>第一次运行<code>vmstat</code>命令，它会显示自上次重启以来负载的平均负载值。用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ vmstat</div><div class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</div><div class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</div><div class="line"> 2  0   2508 144238112      0 98689096    0    0     3    13    0    0  2  6 92  0  0</div></pre></td></tr></table></figure>
<p><code>vmstat</code>工具通常使用两个数字参数来显示系统信息，第一个参数是采样时间间隔数，单位是秒，第二个参数是采样次数。用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ vmstat 2 5</div><div class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</div><div class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</div><div class="line">41  0   2508 143143008      0 98875408    0    0     3    13    0    0  2  6 92  0  0</div><div class="line"> 1  0   2508 143974560      0 98875736    0    0     0     0 37405 41706  8 26 66  0  0</div><div class="line"> 5  0   2508 143964080      0 98875184    0    0     0     0 41914 50578  9 30 61  0  0</div><div class="line">40  0   2508 143142720      0 98875656    0    0     0    92 40721 44492  8 28 64  0  0</div><div class="line"> 1  0   2508 143964080      0 98876032    0    0     0     0 35374 42967  8 23 69  0  0</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Linux命令行与shell脚本编程</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Linux的vmstat命令
    
    </summary>
    
      <category term="Linux" scheme="http://noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="http://noahsnail.com/2018/05/02/2018-05-02-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>http://noahsnail.com/2018/05/02/2018-05-02-正则表达式/</id>
    <published>2018-05-02T09:56:41.000Z</published>
    <updated>2018-05-05T14:20:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-正则表达式"><a href="#1-正则表达式" class="headerlink" title="1. 正则表达式"></a>1. 正则表达式</h2><p>正则表达式(Regular Expression)描述了一种字符串匹配模式，主要用来检索、替换匹配某种模式的字符串。</p>
<h2 id="2-正则表达式语法"><a href="#2-正则表达式语法" class="headerlink" title="2. 正则表达式语法"></a>2. 正则表达式语法</h2><p>下面以Python代码来展示正则表达式的匹配。</p>
<ul>
<li><p><code>.</code><br><code>.</code>可以匹配任意单个字符，除了换行符。例如<code>.</code>可匹配<code>abc</code>中的任意一个字符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'.'</span>, <span class="string">'abc'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>^</code><br><code>^</code>表示字符串的开始，例：<code>^Th</code>表示匹配以<code>Th</code>开头的字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'^Th'</span>, <span class="string">'This is a demo. This is a demo.'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'Th'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>$</code><br><code>$</code>表示字符串的结束，例：<code>demo$</code>表示匹配以<code>demo</code>结尾的字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'demo$'</span>, <span class="string">'This is a demo. This is a demo'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'demo'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>*</code><br><code>*</code>匹配&gt;=0个在<code>*</code>号之前的字符。例：<code>test*</code>表示匹配以<code>tes</code>为起始值，其后为0个<code>t</code>或多个<code>t</code>的字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'test*'</span>, <span class="string">'t te tes test testt'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'tes'</span>, <span class="string">'test'</span>, <span class="string">'testt'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>+</code><br><code>+</code>匹配&gt;=1个在<code>+</code>号之前的字符。例：<code>test+</code>表示匹配以<code>tes</code>为起始值，其后为1个<code>t</code>或多个<code>t</code>的字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'test+'</span>, <span class="string">'t te tes test testt'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'test'</span>, <span class="string">'testt'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>?</code><br><code>?</code>之前的字符为可选字符。例：<code>test?</code>表示匹配以<code>tes</code>为起始值，其后为1个<code>t</code>或没有<code>t</code>的字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'test?'</span>, <span class="string">'t te tes test testt'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'tes'</span>, <span class="string">'test'</span>, <span class="string">'test'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\</code><br><code>\</code>为转义字符，用于匹配一些保留的字符<code>[ ] ( ) { } . * + ? ^ $ \ |</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'test\?'</span>, <span class="string">'t te tes test? testt'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'test?'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>|</code><br><code>|</code>为或运算符，匹配符号前或后的字符。例：<code>te|st</code>表示匹配<code>te</code>或<code>st</code>的字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'te|st'</span>, <span class="string">'t te tes test'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'te'</span>, <span class="string">'te'</span>, <span class="string">'te'</span>, <span class="string">'st'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>[ ]</code><br><code>[ ]</code>表示要匹配的字符种类，匹配方括号内的任意字符。例：<code>[test]</code>匹配括号中的任意一个字符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'[test]'</span>, <span class="string">'This is a test'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'s'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'e'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>[^ ]</code><br><code>[^ ]</code>表示不进行匹配的字符种类，匹配除了方括号里字符之外的任意字符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'[^test]'</span>, <span class="string">'This is a test'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'T'</span>, <span class="string">'h'</span>, <span class="string">'i'</span>, <span class="string">' '</span>, <span class="string">'i'</span>, <span class="string">' '</span>, <span class="string">'a'</span>, <span class="string">' '</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>{m,n}</code><br><code>{m,n}</code>表示匹配<code>(n-m+1)</code>个大括号之前的字符。例：<code>test{1,2}</code>表示匹配以<code>tes</code>为起始值，其后为<code>1-2</code>个<code>t</code>的字符串。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'test&#123;1,2&#125;'</span>, <span class="string">'This is a test testt'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'test'</span>, <span class="string">'testt'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>(xyz)</code><br><code>(xyz)</code>表示匹配与<code>()</code>内容完全相同的字符串。例：<code>(test){1,2}</code>表示匹配<code>1-2</code>个<code>test</code>，<code>test</code>是一个整体。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'(test)&#123;1,2&#125;'</span>, <span class="string">'This is a test testt'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'test'</span>, <span class="string">'test'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\w</code><br><code>\w</code>匹配所有字母数字以及下划线，即<code>[a-zA-z0-9_]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\w'</span>, <span class="string">'Is this a test?_'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'I'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'h'</span>, <span class="string">'i'</span>, <span class="string">'s'</span>, <span class="string">'a'</span>, <span class="string">'t'</span>, <span class="string">'e'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\W</code><br><code>\W</code>匹配字母数字以及下划线之外的字符，即<code>[^\w]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\W'</span>, <span class="string">'Is this a test?'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">' '</span>, <span class="string">' '</span>, <span class="string">' '</span>, <span class="string">'?'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\d</code><br><code>\d</code>匹配数字，即<code>[0-9]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\d'</span>, <span class="string">'test 123'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\D</code><br><code>\D</code>匹配数字之外的字符，即<code>[^\d]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\D'</span>, <span class="string">'test 123'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'t'</span>, <span class="string">'e'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">' '</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\s</code><br><code>\s</code>匹配所有空格字符，即<code>[\t\n\f\r\p{Z}]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\s'</span>, <span class="string">'test 123\n'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">' '</span>, <span class="string">'\n'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\S</code><br><code>\S</code>匹配非空格字符，即<code>[^\s]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\S'</span>, <span class="string">'test 123\n'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'t'</span>, <span class="string">'e'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\n</code><br><code>\n</code>匹配一个换行符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\n'</span>, <span class="string">'test 123\n'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'\n'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\f</code><br><code>\f</code>匹配一个换页符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\f'</span>, <span class="string">'test 123\f'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'\x0c'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\r</code><br><code>\r</code>匹配一个回车符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\r'</span>, <span class="string">'test 123\r'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'\r'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\t</code><br><code>\t</code>匹配一个制表符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\t'</span>, <span class="string">'test 123\t'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'\t'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>\v</code><br><code>\v</code>匹配一个垂直制表符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'\v'</span>, <span class="string">'test 123\v'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果</span></div><div class="line">[<span class="string">'\x0b'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>?=</code><br><code>?=</code>是前置约束，表示要匹配的是<code>?=</code>之前的内容，但同时要匹配<code>?=</code>之后的内容，前置约束需要使用<code>()</code>。例：<code>Th(?=is)</code>表示要匹配<code>Th</code>，要找的是<code>This</code>中的<code>Th</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'Th(?=is)'</span>, <span class="string">'There or This or The?'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果，匹配的是This中的Th</span></div><div class="line">[<span class="string">'Th'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>?!</code><br><code>?!</code>也是前置约束，但与<code>?=</code>正好相反，也是要匹配<code>?!</code>之前的内容，但同时要不匹配<code>?!</code>之后的内容，前置约束需要使用<code>()</code>。例：<code>Th(?!is)</code>表示要匹配<code>Th</code>，要找的是非<code>This</code>中的<code>Th</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'Th(?!is)'</span>, <span class="string">'There or This or The?'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果，匹配的是There, The中的Th</span></div><div class="line">[<span class="string">'Th'</span>, <span class="string">'Th'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>?&lt;=</code><br><code>?&lt;=</code>是后置约束，表示要匹配的是<code>(?&lt;=)</code>之后的内容，但同时要匹配<code>(?&lt;=)</code>括号内的内容，后置约束需要使用<code>()</code>。例：<code>(?&lt;=H)e</code>表示要匹配<code>e</code>，要找的是<code>He</code>中的<code>e</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'(?&lt;=H)e'</span>, <span class="string">'The or He or She?'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果，匹配的是He中的e</span></div><div class="line">[<span class="string">'e'</span>]</div></pre></td></tr></table></figure>
</li>
<li><p><code>?&lt;!</code><br><code>?&lt;!</code>是后置约束，表示要匹配的是<code>(?&lt;!)</code>之后的内容，但同时要不匹配<code>(?&lt;!)</code>括号内的内容，后置约束需要使用<code>()</code>。例：<code>(?&lt;!H)e</code>表示要匹配<code>e</code>，要找的是非<code>He</code>中的<code>e</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">print(re.findall(<span class="string">r'(?&lt;!H)e'</span>, <span class="string">'The or He or She?'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 代码执行结果，匹配的是The, She中的e</span></div><div class="line">[<span class="string">'e'</span>, <span class="string">'e'</span>]</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://juejin.im/entry/59a651116fb9a024844938b5" target="_blank" rel="external">https://juejin.im/entry/59a651116fb9a024844938b5</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      正则表达式
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Feature Pyramid Networks for Object Detection论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2018/03/20/2018-03-20-Feature%20Pyramid%20Networks%20for%20Object%20Detection%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2018/03/20/2018-03-20-Feature Pyramid Networks for Object Detection论文翻译——中英文对照/</id>
    <published>2018-03-20T10:30:55.000Z</published>
    <updated>2018-05-09T13:08:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Feature-Pyramid-Networks-for-Object-Detection"><a href="#Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="Feature Pyramid Networks for Object Detection"></a>Feature Pyramid Networks for Object Detection</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>特征金字塔是识别系统中用于检测不同尺度目标的基本组件。但最近的深度学习目标检测器已经避免了金字塔表示，部分原因是它们是计算和内存密集型的。在本文中，我们利用深度卷积网络内在的多尺度、金字塔分级来构造具有很少额外成本的特征金字塔。开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。此外，我们的方法可以在GPU上以6FPS运行，因此是多尺度目标检测的实用和准确的解决方案。代码将公开发布。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Recognizing objects at vastly different scales is a fundamental challenge in computer vision. Feature pyramids built upon image pyramids (for short we call these featurized image pyramids) form the basis of a standard solution [1] (Fig. 1(a)). These pyramids are scale-invariant in the sense that an object’s scale change is offset by shifting its level in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-79414c80444765b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>Figure 1. (a) Using an image pyramid to build a feature pyramid. Features are computed on each of the image scales independently, which is slow. (b) Recent detection systems have opted to use only single scale features for faster detection. (c) An alternative is to reuse the pyramidal feature hierarchy computed by a ConvNet as if it were a featurized image pyramid. (d) Our proposed Feature Pyramid Network (FPN) is fast like (b) and (c), but more accurate. In this figure, feature maps are indicate by blue outlines and thicker outlines denote semantically stronger features.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>识别不同尺度的目标是计算机视觉中的一个基本挑战。建立在图像金字塔之上的特征金字塔（我们简称为特征化图像金字塔）构成了标准解决方案的基础[1]（图1（a））。这些金字塔是尺度不变的，因为目标的尺度变化是通过在金字塔中移动它的层级来抵消的。直观地说，该属性使模型能够通过在位置和金字塔等级上扫描模型来检测大范围尺度内的目标。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-79414c80444765b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1。（a）使用图像金字塔构建特征金字塔。每个图像尺度上的特征都是独立计算的，速度很慢。（b）最近的检测系统选择只使用单一尺度特征进行更快的检测。（c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。（d）我们提出的特征金字塔网络（FPN）与（b）和（c）类似，但更准确。在该图中，特征映射用蓝色轮廓表示，较粗的轮廓表示语义上较强的特征。</p>
<p>Featurized image pyramids were heavily used in the era of hand-engineered features [5, 25]. They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (e.g., 10 scales per octave). For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) [19, 20]. Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)). But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on featurized image pyramids (e.g., [16, 35]). The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.</p>
<p>特征化图像金字塔在手工设计的时代被大量使用[5，25]。它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave含义参考SIFT特征）。对于识别任务，工程特征大部分已经被深度卷积网络（ConvNets）[19，20]计算的特征所取代。除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。但即使有这种鲁棒性，金字塔仍然需要得到最准确的结果。在ImageNet[33]和COCO[21]检测挑战中，最近的所有排名靠前的输入都使用了针对特征化图像金字塔的多尺度测试（例如[16，35]）。对图像金字塔的每个层次进行特征化的主要优势在于它产生了多尺度的特征表示，其中所有层次上在语义上都很强，包括高分辨率层。</p>
<p>Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times [11]), making this approach impractical for real applications. Moreover, training deep networks end-to-end on an image pyramid is infeasible in terms of memory, and so, if exploited, image pyramids are used only at test time [15, 11, 16, 35], which creates an inconsistency between train/test-time inference. For these reasons, Fast and Faster R-CNN [11, 29] opt to not use featurized image pyramids under default settings.</p>
<p>尽管如此，特征化图像金字塔的每个层次都具有明显的局限性。推断时间显著增加（例如，四倍[11]），使得这种方法在实际应用中不切实际。此外，在图像金字塔上端对端地训练深度网络在内存方面是不可行的，所以如果被采用，图像金字塔仅在测试时被使用[15，11，16，35]，这造成了训练/测试时推断的不一致性。出于这些原因，Fast和Faster R-CNN[11，29]选择在默认设置下不使用特征化图像金字塔。</p>
<p>However, image pyramids are not the only way to compute a multi-scale feature representation. A deep ConvNet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape. This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p>
<p>但是，图像金字塔并不是计算多尺度特征表示的唯一方法。深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。这种网内特征层级产生不同空间分辨率的特征映射，但引入了由不同深度引起的较大的语义差异。高分辨率映射具有损害其目标识别表示能力的低级特征。</p>
<p>The Single Shot Detector (SSD) [22] is one of the first attempts at using a ConvNet’s pyramidal feature hierarchy as if it were a featurized image pyramid (Fig. 1(c)). Ideally, the SSD-style pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost. But to avoid using low-level features SSD foregoes reusing already computed layers and instead builds the pyramid starting from high up in the network (e.g., conv4_3 of VGG nets [36]) and then by adding several new layers. Thus it misses the opportunity to reuse the higher-resolution maps of the feature hierarchy. We show that these are important for detecting small objects.</p>
<p>单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。理想情况下，SSD风格的金字塔将重用正向传递中从不同层中计算的多尺度特征映射，因此是零成本的。但为了避免使用低级特征，SSD放弃重用已经计算好的图层，而从网络中的最高层开始构建金字塔（例如，VGG网络的conv4_3[36]），然后添加几个新层。因此它错过了重用特征层级的更高分辨率映射的机会。我们证明这些对于检测小目标很重要。</p>
<p>The goal of this paper is to naturally leverage the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales. To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections (Fig. 1(d)). The result is a feature pyramid that has rich semantics at all levels and is built quickly from a single input image scale. In other words, we show how to create in-network feature pyramids that can be used to replace featurized image pyramids without sacrificing representational power, speed, or memory.</p>
<p>本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。为了实现这个目标，我们依赖于一种结构，它将低分辨率，具有高分辨率的强大语义特征，语义上的弱特征通过自顶向下的路径和横向连接相结合（图1（d））。其结果是一个特征金字塔，在所有级别都具有丰富的语义，并且可以从单个输入图像尺度上进行快速构建。换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</p>
<p>Similar architectures adopting top-down and skip connections are popular in recent research [28, 17, 8, 26]. Their goals are to produce a single high-level feature map of a fine resolution on which the predictions are to be made (Fig. 2 top). On the contrary, our method leverages the architecture as a feature pyramid where predictions (e.g., object detections) are independently made on each level (Fig. 2 bottom). Our model echoes a featurized image pyramid, which has not been explored in these works.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e1ee021eac1b88be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>Figure 2. Top: a top-down architecture with skip connections, where predictions are made on the finest level (e.g., [28]). Bottom: our model that has a similar structure but leverages it as a feature pyramid, with predictions made independently at all levels.</p>
<p>最近的研究[28，17，8，26]中流行采用自顶向下和跳跃连接的类似架构。他们的目标是生成具有高分辨率的单个高级特征映射，并在其上进行预测（图2顶部）。相反，我们的方法利用这个架构作为特征金字塔，其中预测（例如目标检测）在每个级别上独立进行（图2底部）。我们的模型反映了一个特征化的图像金字塔，这在这些研究中还没有探索过。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e1ee021eac1b88be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2。顶部：带有跳跃连接的自顶向下的架构，在最好的级别上进行预测（例如，[28]）。底部：我们的模型具有类似的结构，但将其用作特征金字塔，并在各个层级上独立进行预测。</p>
<p>We evaluate our method, called a Feature Pyramid Network (FPN), in various systems for detection and segmentation [11, 29, 27]. Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on FPN and a basic Faster R-CNN detector [29], surpassing all existing heavily-engineered single-model entries of competition winners. In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16]. Our method is also easily extended to mask proposals and improves both instance segmentation AR and speed over state-of-the-art methods that heavily depend on image pyramids.</p>
<p>我们评估了我们称为特征金字塔网络（FPN）的方法，其在各种系统中用于检测和分割[11，29，27]。没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。我们的方法也很容易扩展掩模提议，改进实例分隔AR，加速严重依赖图像金字塔的最先进方法。</p>
<p>In addition, our pyramid structure can be trained end-to-end with all scales and is used consistently at train/test time, which would be memory-infeasible using image pyramids. As a result, FPNs are able to achieve higher accuracy than all existing state-of-the-art methods. Moreover, this improvement is achieved without increasing testing time over the single-scale baseline. We believe these advances will facilitate future research and applications. Our code will be made publicly available.</p>
<p>另外，我们的金字塔结构可以通过所有尺度进行端对端培训，并且在训练/测试时一致地使用，这在使用图像金字塔时是内存不可行的。因此，FPN能够比所有现有的最先进方法获得更高的准确度。此外，这种改进是在不增加单尺度基准测试时间的情况下实现的。我们相信这些进展将有助于未来的研究和应用。我们的代码将公开发布。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Hand-engineered features and early neural networks.</strong> SIFT features [25] were originally extracted at scale-space extrema and used for feature point matching. HOG features [5], and later SIFT features as well, were computed densely over entire image pyramids. These HOG and SIFT pyramids have been used in numerous works for image classification, object detection, human pose estimation, and more. There has also been significant interest in computing featurized image pyramids quickly. Dollar et al.[6] demonstrated fast pyramid computation by first computing a sparsely sampled (in scale) pyramid and then interpolating missing levels. Before HOG and SIFT, early work on face detection with ConvNets [38, 32] computed shallow networks over image pyramids to detect faces across scales.</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>手工设计特征和早期神经网络</strong>。SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。HOG特征[5]，以及后来的SIFT特征，都是在整个图像金字塔上密集计算的。这些HOG和SIFT金字塔已在许多工作中得到了应用，用于图像分类，目标检测，人体姿势估计等。这对快速计算特征化图像金字塔也很有意义。Dollar等人[6]通过先计算一个稀疏采样（尺度）金字塔，然后插入缺失的层级，从而演示了快速金字塔计算。在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</p>
<p><strong>Deep ConvNet object detectors</strong>. With the development of modern deep ConvNets [19], object detectors like OverFeat [34] and R-CNN [12] showed dramatic improvements in accuracy. OverFeat adopted a strategy similar to early neural network face detectors by applying a ConvNet as a sliding window detector on an image pyramid. R-CNN adopted a region proposal-based strategy [37] in which each proposal was scale-normalized before classifying with a ConvNet. SPPnet [15] demonstrated that such region-based detectors could be applied much more efficiently on feature maps extracted on a single image scale. Recent and more accurate detection methods like Fast R-CNN [11] and Faster R-CNN [29] advocate using features computed from a single scale, because it offers a good trade-off between accuracy and speed. Multi-scale detection, however, still performs better, especially for small objects.</p>
<p><strong>Deep ConvNet目标检测器</strong>。随着现代深度卷积网络[19]的发展，像OverFeat[34]和R-CNN[12]这样的目标检测器在精度上显示出了显著的提高。OverFeat采用了一种类似于早期神经网络人脸检测器的策略，通过在图像金字塔上应用ConvNet作为滑动窗口检测器。R-CNN采用了基于区域提议的策略[37]，其中每个提议在用ConvNet进行分类之前都进行了尺度归一化。SPPnet[15]表明，这种基于区域的检测器可以更有效地应用于在单个图像尺度上提取的特征映射。最近更准确的检测方法，如Fast R-CNN[11]和Faster R-CNN[29]提倡使用从单一尺度计算出的特征，因为它提供了精确度和速度之间的良好折衷。然而，多尺度检测性能仍然更好，特别是对于小型目标。</p>
<p><strong>Methods using multiple layers</strong>. A number of recent approaches improve detection and segmentation by using different layers in a ConvNet. FCN [24] sums partial scores for each category over multiple scales to compute semantic segmentations. Hypercolumns [13] uses a similar method for object instance segmentation. Several other approaches (HyperNet [18], ParseNet [23], and ION [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features. SSD [22] and MS-CNN [3] predict objects at multiple layers of the feature hierarchy without combining features or scores.</p>
<p><strong>使用多层的方法</strong>。一些最近的方法通过使用ConvNet中的不同层来改进检测和分割。FCN[24]将多个尺度上的每个类别的部分分数相加以计算语义分割。Hypercolumns[13]使用类似的方法进行目标实例分割。在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。SSD[22]和MS-CNN[3]可预测特征层级中多个层的目标，而不需要组合特征或分数。</p>
<p>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation. Ghiasi et al. [8] present a Laplacian pyramid presentation for FCNs to progressively refine segmentation. Although these methods adopt architectures with pyramidal shapes, they are unlike featurized image pyramids [5, 7, 34] where predictions are made independently at all levels, see Fig. 2. In fact, for the pyramidal architecture in Fig. 2 (top), image pyramids are still needed to recognize objects across multiple scales [28].</p>
<p>最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。尽管这些方法采用的是金字塔形状的架构，但它们不同于特征化的图像金字塔[5，7，34]，其中所有层次上的预测都是独立进行的，参见图2。事实上，对于图2（顶部）中的金字塔结构，图像金字塔仍然需要跨多个尺度上识别目标[28]。</p>
<h2 id="3-Feature-Pyramid-Networks"><a href="#3-Feature-Pyramid-Networks" class="headerlink" title="3. Feature Pyramid Networks"></a>3. Feature Pyramid Networks</h2><p>Our goal is to leverage a ConvNet’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout. The resulting Feature Pyramid Network is general-purpose and in this paper we focus on sliding window proposers (Region Proposal Network, RPN for short) [29] and region-based detectors (Fast R-CNN) [11]. We also generalize FPNs to instance segmentation proposals in Sec.6.</p>
<h2 id="3-特征金字塔网络"><a href="#3-特征金字塔网络" class="headerlink" title="3. 特征金字塔网络"></a>3. 特征金字塔网络</h2><p>我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。在第6节中我们还将FPN泛化到实例细分提议。</p>
<p>Our method takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures (e.g., [19, 36, 16]), and in this paper we present results using ResNets [16]. The construction of our pyramid involves a bottom-up pathway, a top-down pathway, and lateral connections, as introduced in the following.</p>
<p>我们的方法以任意大小的单尺度图像作为输入，并以全卷积的方式输出多层适当大小的特征映射。这个过程独立于主卷积体系结构（例如[19，36，16]），在本文中，我们呈现了使用ResNets[16]的结果。如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</p>
<p><strong>Bottom-up pathway</strong>. The bottom-up pathway is the feed-forward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. There are often many layers producing output maps of the same size and we say these layers are in the same network stage. For our feature pyramid, we define one pyramid level for each stage. We choose the output of the last layer of each stage as our reference set of feature maps, which we will enrich to create our pyramid. This choice is natural since the deepest layer of each stage should have the strongest features.</p>
<p><strong>自下而上的路径</strong>。自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。通常有许多层产生相同大小的输出映射，并且我们认为这些层位于相同的网络阶段。对于我们的特征金字塔，我们为每个阶段定义一个金字塔层。我们选择每个阶段的最后一层的输出作为我们的特征映射参考集，我们将丰富它来创建我们的金字塔。这种选择是自然的，因为每个阶段的最深层应具有最强大的特征。</p>
<p>Specifically, for ResNets [16] we use the feature activations output by each stage’s last residual block. We denote the output of these last residual blocks as $\lbrace C_2 , C_3 , C_4 , C_5 \rbrace$ for conv2, conv3, conv4, and conv5 outputs, and note that they have strides of {4, 8, 16, 32} pixels with respect to the input image. We do not include conv1 into the pyramid due to its large memory footprint.</p>
<p>具体而言，对于ResNets[16]，我们使用每个阶段的最后一个残差块输出的特征激活。对于conv2，conv3，conv4和conv5输出，我们将这些最后残差块的输出表示为$\lbrace C_2, C_3, C_4, C_5 \rbrace$，并注意相对于输入图像它们的步长为{4，8，16，32}个像素。由于其庞大的内存占用，我们不会将conv1纳入金字塔。</p>
<p><strong>Top-down pathway and lateral connections</strong>. The top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.</p>
<p><strong>自顶向下的路径和横向连接</strong>。自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。这些特征随后通过来自自下而上路径上的特征经由横向连接进行增强。每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</p>
<p>Fig. 3 shows the building block that constructs our top-down feature maps. With a coarser-resolution feature map, we upsample the spatial resolution by a factor of 2 (using nearest neighbor upsampling for simplicity). The upsampled map is then merged with the corresponding bottom-up map (which undergoes a 1×1 convolutional layer to reduce channel dimensions) by element-wise addition. This process is iterated until the finest resolution map is generated. To start the iteration, we simply attach a 1×1 convolutional layer on $C_5$ to produce the coarsest resolution map. Finally, we append a 3 × 3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling. This final set of feature maps is called $\lbrace P_2 , P_3 , P_4 , P_5 \rbrace$, corresponding to $\lbrace C_2, C_3, C_4, C_5 \rbrace$ that are respectively of the same spatial sizes.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a51d06a2a94bfec4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>Figure 3. A building block illustrating the lateral connection and the top-down pathway, merged by addition.</p>
<p>图3显示了建造我们的自顶向下特征映射的构建块。使用较粗糙分辨率的特征映射，我们将空间分辨率上采样为2倍（为了简单起见，使用最近邻上采样）。然后通过按元素相加，将上采样映射与相应的自下而上映射（其经过1×1卷积层来减少通道维度）合并。迭代这个过程，直到生成最佳分辨率映射。为了开始迭代，我们只需在$C_5$上添加一个1×1卷积层来生成最粗糙分辨率映射。最后，我们在每个合并的映射上添加一个3×3卷积来生成最终的特征映射，这是为了减少上采样的混叠效应。这个最终的特征映射集称为$\lbrace P_2 , P_3 , P_4 , P_5 \rbrace$，对应于$\lbrace C_2, C_3, C_4, C_5 \rbrace$，分别具有相同的空间大小。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a51d06a2a94bfec4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3。构建模块说明了横向连接和自顶向下路径，通过加法合并。</p>
<p>Because all levels of the pyramid use shared classifiers/regressors as in a traditional featurized image pyramid, we fix the feature dimension (numbers of channels, denoted as $d$) in all the feature maps. We set $d=256$ in this paper and thus all extra convolutional layers have 256-channel outputs. There are no non-linearities in these extra layers, which we have empirically found to have minor impacts.</p>
<p>由于金字塔的所有层都像传统的特征图像金字塔一样使用共享分类器/回归器，因此我们在所有特征映射中固定特征维度（通道数记为$d$）。我们在本文中设置$d=256$，因此所有额外的卷积层都有256个通道的输出。在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</p>
<p>Simplicity is central to our design and we have found that our model is robust to many design choices. We have experimented with more sophisticated blocks (e.g., using multi-layer residual blocks [16] as the connections) and observed marginally better results. Designing better connection modules is not the focus of this paper, so we opt for the simple design described above.</p>
<p>简洁性是我们设计的核心，我们发现我们的模型对许多设计选择都很鲁棒。我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。设计更好的连接模块并不是本文的重点，所以我们选择上述的简单设计。</p>
<h2 id="4-Applications"><a href="#4-Applications" class="headerlink" title="4. Applications"></a>4. Applications</h2><p>Our method is a generic solution for building feature pyramids inside deep ConvNets. In the following we adopt our method in RPN [29] for bounding box proposal generation and in Fast R-CNN [11] for object detection. To demonstrate the simplicity and effectiveness of our method, we make minimal modifications to the original systems of [29, 11] when adapting them to our feature pyramid.</p>
<h2 id="4-应用"><a href="#4-应用" class="headerlink" title="4. 应用"></a>4. 应用</h2><p>我们的方法是在深度ConvNets内部构建特征金字塔的通用解决方案。在下面，我们采用我们的方法在RPN[29]中进行边界框提议生成，并在Fast R-CNN[11]中进行目标检测。为了证明我们方法的简洁性和有效性，我们对[29，11]的原始系统进行最小修改，使其适应我们的特征金字塔。</p>
<h3 id="4-1-Feature-Pyramid-Networks-for-RPN"><a href="#4-1-Feature-Pyramid-Networks-for-RPN" class="headerlink" title="4.1. Feature Pyramid Networks for RPN"></a>4.1. Feature Pyramid Networks for RPN</h3><p>RPN [29] is a sliding-window class-agnostic object detector. In the original RPN design, a small subnetwork is evaluated on dense 3×3 sliding windows, on top of a single-scale convolutional feature map, performing object/non-object binary classification and bounding box regression. This is realized by a 3×3 convolutional layer followed by two sibling 1×1 convolutions for classification and regression, which we refer to as a network head. The object/non-object criterion and bounding box regression target are defined with respect to a set of reference boxes called anchors [29]. The anchors are of multiple pre-defined scales and aspect ratios in order to cover objects of different shapes.</p>
<h3 id="4-1-RPN的特征金字塔网络"><a href="#4-1-RPN的特征金字塔网络" class="headerlink" title="4.1. RPN的特征金字塔网络"></a>4.1. RPN的特征金字塔网络</h3><p>RPN[29]是一个滑动窗口类不可知的目标检测器。在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。这是通过一个3×3的卷积层实现的，后面跟着两个用于分类和回归的1×1兄弟卷积，我们称之为网络头部。目标/非目标标准和边界框回归目标的定义是关于一组称为锚点的参考框的[29]。这些锚点具有多个预定义的尺度和长宽比，以覆盖不同形状的目标。</p>
<p>We adapt RPN by replacing the single-scale feature map with our FPN. We attach a head of the same design (3×3 conv and two sibling 1×1 convs) to each level on our feature pyramid. Because the head slides densely over all locations in all pyramid levels, it is not necessary to have multi-scale anchors on a specific level. Instead, we assign anchors of a single scale to each level. Formally, we define the anchors to have areas of $\lbrace 32^2 , 64^2 , 128^2 , 256^2 , 512^2 \rbrace$ pixels on $\lbrace P_2, P_3, P_4, P_5, P_6 \rbrace$ respectively. As in [29] we also use anchors of multiple aspect ratios $\lbrace 1:2, 1:1, 2:1 \rbrace$ at each level. So in total there are 15 anchors over the pyramid.</p>
<p>我们通过用我们的FPN替换单尺度特征映射来适应RPN。我们在我们的特征金字塔的每个层级上附加一个相同设计的头部（3x3 conv和两个1x1兄弟convs）。由于头部在所有金字塔等级上的所有位置密集滑动，所以不需要在特定层级上具有多尺度锚点。相反，我们为每个层级分配单尺度的锚点。在形式上，我们定义锚点$\lbrace P_2, P_3, P_4, P_5, P_6 \rbrace$分别具有$\lbrace 32^2 , 64^2 , 128^2 , 256^2 , 512^2 \rbrace$个像素的面积。正如在[29]中，我们在每个层级上也使用了多个长宽比$\lbrace 1:2, 1:1, 2:1 \rbrace$的锚点。所以在金字塔上总共有十五个锚点。</p>
<p>We assign training labels to the anchors based on their Intersection-over-Union (IoU) ratios with ground-truth bounding boxes as in [29]. Formally, an anchor is assigned a positive label if it has the highest IoU for a given ground-truth box or an IoU over 0.7 with any ground-truth box, and a negative label if it has IoU lower than 0.3 for all ground-truth boxes. Note that scales of ground-truth boxes are not explicitly used to assign them to the levels of the pyramid; instead, ground-truth boxes are associated with anchors, which have been assigned to pyramid levels. As such, we introduce no extra rules in addition to those in [29].</p>
<p>如[29]，我们根据锚点和实际边界框的交并比（IoU）比例将训练标签分配给锚点。形式上，如果一个锚点对于一个给定的实际边界框具有最高的IoU或者与任何实际边界框的IoU超过0.7，则给其分配一个正标签，如果其与所有实际边界框的IoU都低于0.3，则为其分配一个负标签。请注意，实际边界框的尺度并未明确用于将它们分配到金字塔的层级；相反，实际边界框与已经分配给金字塔等级的锚点相关联。因此，除了[29]中的内容外，我们不引入额外的规则。</p>
<p>We note that the parameters of the heads are shared across all feature pyramid levels; we have also evaluated the alternative without sharing parameters and observed similar accuracy. The good performance of sharing parameters indicates that all levels of our pyramid share similar semantic levels. This advantage is analogous to that of using a featurized image pyramid, where a common head classifier can be applied to features computed at any image scale.</p>
<p>我们注意到头部的参数在所有特征金字塔层级上共享；我们也评估了替代方案，没有共享参数并且观察到相似的准确性。共享参数的良好性能表明我们的金字塔的所有层级共享相似的语义级别。这个优点类似于使用特征图像金字塔的优点，其中可以将常见头部分类器应用于在任何图像尺度下计算的特征。</p>
<p>With the above adaptations, RPN can be naturally trained and tested with our FPN, in the same fashion as in [29]. We elaborate on the implementation details in the experiments.</p>
<p>通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。我们在实验中详细说明实施细节。</p>
<h3 id="4-2-Feature-Pyramid-Networks-for-Fast-R-CNN"><a href="#4-2-Feature-Pyramid-Networks-for-Fast-R-CNN" class="headerlink" title="4.2. Feature Pyramid Networks for Fast R-CNN"></a>4.2. Feature Pyramid Networks for Fast R-CNN</h3><p>Fast R-CNN [11] is a region-based object detector in which Region-of-Interest (RoI) pooling is used to extract features. Fast R-CNN is most commonly performed on a single-scale feature map. To use it with our FPN, we need to assign RoIs of different scales to the pyramid levels.</p>
<h3 id="4-2-Fast-R-CNN的特征金字塔网络"><a href="#4-2-Fast-R-CNN的特征金字塔网络" class="headerlink" title="4.2. Fast R-CNN的特征金字塔网络"></a>4.2. Fast R-CNN的特征金字塔网络</h3><p>Fast R-CNN[11]是一个基于区域的目标检测器，利用感兴趣区域（RoI）池化来提取特征。Fast R-CNN通常在单尺度特征映射上执行。要将其与我们的FPN一起使用，我们需要为金字塔等级分配不同尺度的RoI。</p>
<p>We view our feature pyramid as if it were produced from an image pyramid. Thus we can adapt the assignment strategy of region-based detectors [15, 11] in the case when they are run on image pyramids. Formally, we assign an RoI of width $w$ and height $h$ (on the input image to the network) to the level $P_k$ of our feature pyramid by: $$k=\lfloor k_0+\log_2(\sqrt{wh}/224) \rfloor. \tag{1}$$ Here $224$ is the canonical ImageNet pre-training size, and $k_0$ is the target level on which an RoI with $w\times h=224^2$ should be mapped into. Analogous to the ResNet-based Faster R-CNN system [16] that uses $C_4$ as the single-scale feature map, we set $k_0$ to 4. Intuitively, Eqn.(1) means that if the RoI’s scale becomes smaller (say, 1/2 of 224), it should be mapped into a finer-resolution level (say, $k=3$).</p>
<p>我们将我们的特征金字塔看作是从图像金字塔生成的。因此，当它们在图像金字塔上运行时，我们可以适应基于区域的检测器的分配策略[15，11]。在形式上，我们通过以下公式将宽度为$w$和高度为$h$（在网络上的输入图像上）的RoI分配到特征金字塔的级别$P_k$上：$$k=\lfloor k_0+\log_2(\sqrt{wh}/224) \rfloor. \tag{1}$$ 这里$224$是规范的ImageNet预训练大小，而$k_0$是大小为$w \times h=224^2$的RoI应该映射到的目标级别。类似于基于ResNet的Faster R-CNN系统[16]使用$C_4$作为单尺度特征映射，我们将$k_0$设置为4。直觉上，方程（1）意味着如果RoI的尺寸变小了（比如224的1/2），它应该被映射到一个更精细的分辨率级别（比如$k=3$）。</p>
<p>We attach predictor heads (in Fast R-CNN the heads are class-specific classifiers and bounding box regressors) to all RoIs of all levels. Again, the heads all share parameters, regardless of their levels. In [16], a ResNet’s conv5 layers (a 9-layer deep subnetwork) are adopted as the head on top of the conv4 features, but our method has already harnessed conv5 to construct the feature pyramid. So unlike [16], we simply adopt RoI pooling to extract 7×7 features, and attach two hidden 1,024-d fully-connected ($fc$) layers (each followed by ReLU) before the final classification and bounding box regression layers. These layers are randomly initialized, as there are no pre-trained $fc$ layers available in ResNets. Note that compared to the standard conv5 head, our 2-$fc$ MLP head is lighter weight and faster.</p>
<p>我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。再次，预测器头部都共享参数，不管他们在什么层级。在[16]中，ResNet的conv5层（9层深的子网络）被用作conv4特征之上的头部，但我们的方法已经利用了conv5来构建特征金字塔。因此，与[16]不同，我们只是采用RoI池化提取7×7特征，并在最终的分类层和边界框回归层之前附加两个隐藏单元为1024维的全连接（$fc$）层（每层后都接ReLU层）。这些层是随机初始化的，因为ResNets中没有预先训练好的$fc$层。请注意，与标准的conv5头部相比，我们的2-$fc$ MLP头部更轻更快。</p>
<p>Based on these adaptations, we can train and test Fast R-CNN on top of the feature pyramid. Implementation details are given in the experimental section.</p>
<p>基于这些改编，我们可以在特征金字塔之上训练和测试Fast R-CNN。实现细节在实验部分给出。</p>
<h2 id="5-Experiments-on-Object-Detection"><a href="#5-Experiments-on-Object-Detection" class="headerlink" title="5. Experiments on Object Detection"></a>5. Experiments on Object Detection</h2><p>We perform experiments on the 80 category COCO detection dataset [21]. We train using the union of 80k train images and a 35k subset of val images (<code>trainval35k</code> [2]), and report ablations on a 5k subset of val images (<code>minival</code>). We also report final results on the standard test set (<code>test-std</code>) [21] which has no disclosed labels.</p>
<h2 id="5-目标检测实验"><a href="#5-目标检测实验" class="headerlink" title="5. 目标检测实验"></a>5. 目标检测实验</h2><p>我们在80类的COCO检测数据集[21]上进行实验。我们训练使用80k张训练图像和35k大小的验证图像子集（<code>trainval35k</code>[2]）的联合，并报告了在5k大小的验证图像子集（<code>minival</code>）上的消融实验。我们还报告了在没有公开标签的标准测试集（<code>test-std</code>）[21]上的最终结果。</p>
<p>As is common practice [12], all network backbones are pre-trained on the ImageNet1k classification set [33] and then fine-tuned on the detection dataset. We use the pre-trained ResNet-50 and ResNet-101 models that are publicly available. Our code is a reimplementation of <code>py-faster-rcnn</code> using Caffe2.</p>
<p>正如通常的做法[12]，所有的网络骨干都是在ImageNet1k分类集[33]上预先训练好的，然后在检测数据集上进行微调。我们使用公开可用的预训练的ResNet-50和ResNet-101模型。我们的代码是使用Caffe2重新实现<code>py-faster-rcnn</code>。</p>
<h3 id="5-1-Region-Proposal-with-RPN"><a href="#5-1-Region-Proposal-with-RPN" class="headerlink" title="5.1. Region Proposal with RPN"></a>5.1. Region Proposal with RPN</h3><p>We evaluate the COCO-style Average Recall (AR) and AR on small, medium, and large objects (AR$_s$, AR$_m$, and AR$_l$) following the definitions in [21]. We report results for 100 and 1000 proposals per images (AR$^{100}$ and AR$^{1k}$).</p>
<h3 id="5-1-区域提议与RPN"><a href="#5-1-区域提议与RPN" class="headerlink" title="5.1. 区域提议与RPN"></a>5.1. 区域提议与RPN</h3><p>根据[21]中的定义，我们评估了COCO类型的平均召回率（AR）和在小型，中型和大型目标(AR$_s$, AR$_m$, and AR$_l$)上的AR。我们报告了每张图像使用100个提议和1000个提议的结果(AR$^{100}$ and AR$^{1k}$)。</p>
<p><strong>Implementation details</strong>. All architectures in Table 1 are trained end-to-end. The input image is resized such that its shorter side has 800 pixels. We adopt synchronized SGD training on 8 GPUs. A mini-batch involves 2 images per GPU and 256 anchors per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the first 30k mini-batches and 0.002 for the next 10k. For all RPN experiments (including baselines), we include the anchor boxes that are outside the image for training, which is unlike [29] where these anchor boxes are ignored. Other implementation details are as in [29]. Training RPN with FPN on 8 GPUs takes about 8 hours on COCO.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e14d05236d572fe7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<p>Table 1. Bounding box proposal results using RPN [29], evaluated on the COCO <code>minival</code> set. All models are trained on <code>trainval35k</code>. The columns “lateral” and “top-down” denote the presence of lateral and top-down connections, respectively. The column “feature” denotes the feature maps on which the heads are attached. All results are based on ResNet-50 and share the same hyper-parameters.</p>
<p><strong>实施细节</strong>。表1中的所有架构都是端对端训练。输入图像的大小调整为其较短边有800像素。我们采用8个GPU进行同步SGD训练。小批量数据包括每个GPU上2张图像和每张图像上256个锚点。我们使用0.0001的权重衰减和0.9的动量。前30k次小批量数据的学习率为0.02，而下一个10k次的学习率为0.002。对于所有的RPN实验（包括基准数据集），我们都包含了图像外部的锚盒来进行训练，这不同于[29]中的忽略这些锚盒。其它实现细节如[29]中所述。使用具有FPN的RPN在8个GPU上训练COCO数据集需要约8小时。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e14d05236d572fe7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<p>表1。使用RPN[29]的边界框提议结果，在COCO的<code>minival</code>数据集上进行评估。所有模型都是通过<code>trainval35k</code>训练的。列“lateral”和“top-down”分别表示横向连接和自上而下连接的存在。列“feature”表示附着头部的特征映射。所有结果都是基于ResNet-50的并共享相同的超参数。</p>
<h4 id="5-1-1-Ablation-Experiments"><a href="#5-1-1-Ablation-Experiments" class="headerlink" title="5.1.1 Ablation Experiments"></a>5.1.1 Ablation Experiments</h4><p><strong>Comparisons with baselines.</strong> For fair comparisons with original RPNs[29], we run two baselines (Table 1(a, b)) using the single-scale map of $C_4$ (the same as [16]) or $C_5$, both using the same hyper-parameters as ours, including using 5 scale anchors of $\lbrace 32^2, 64^2, 128^2, 256^2, 512^2 \rbrace$. Table 1 (b) shows no advantage over (a), indicating that a single higher-level feature map is not enough because there is a trade-off between coarser resolutions and stronger semantics.</p>
<h4 id="5-1-1-消融实验"><a href="#5-1-1-消融实验" class="headerlink" title="5.1.1 消融实验"></a>5.1.1 消融实验</h4><p><strong>与基线进行比较</strong>。为了与原始RPNs[29]进行公平比较，我们使用$C_4$(与[16]相同)或$C_5$的单尺度映射运行了两个基线（表1（a，b）），都使用与我们相同的超参数，包括使用5种尺度锚点$\lbrace 32^2, 64^2, 128^2, 256^2, 512^2 \rbrace$。表1（b）显示没有优于（a），这表明单个更高级别的特征映射是不够的，因为存在在较粗分辨率和较强语义之间的权衡。</p>
<p>Placing FPN in RPN improves AR$^{1k}$ to 56.3 (Table 1 (c)), which is <strong>8.0</strong> points increase over the single-scale RPN baseline (Table 1 (a)). In addition, the performance on small objects (AR$^{1k}_s$) is boosted by a large margin of 12.9 points. Our pyramid representation greatly improves RPN’s robustness to object scale variation.</p>
<p>将FPN放在RPN中可将AR$^{1k}$提高到56.3（表1（c）），这比单尺度RPN基线（表1（a））增加了<strong>8.0</strong>个点。此外，在小型目标（AR$^{1k}_s$）上的性能也大幅上涨了12.9个点。我们的金字塔表示大大提高了RPN对目标尺度变化的鲁棒性。</p>
<p><strong>How important is top-down enrichment?</strong> Table 1(d) shows the results of our feature pyramid without the top-down pathway. With this modification, the 1×1 lateral connections followed by 3×3 convolutions are attached to the bottom-up pyramid. This architecture simulates the effect of reusing the pyramidal feature hierarchy (Fig. 1(b)).</p>
<p><strong>自上而下的改进有多重要？</strong>表1（d）显示了没有自上而下路径的特征金字塔的结果。通过这种修改，将1×1横向连接和后面的3×3卷积添加到自下而上的金字塔中。该架构模拟了重用金字塔特征层次结构的效果（图1（b））。</p>
<p>The results in Table 1(d) are just on par with the RPN baseline and lag far behind ours. We conjecture that this is because there are large semantic gaps between different levels on the bottom-up pyramid (Fig. 1(b)), especially for very deep ResNets. We have also evaluated a variant of Table 1(d) without sharing the parameters of the heads, but observed similarly degraded performance. This issue cannot be simply remedied by level-specific heads.</p>
<p>表1（d）中的结果与RPN基线相当，并且远远落后于我们的结果。我们推测这是因为自下而上的金字塔（图1（b））的不同层次之间存在较大的语义差距，尤其是对于非常深的ResNets。 我们还评估了表1（d）的一个变体，但没有分享磁头的参数，但观察到类似的性能下降。这个问题不能简单地由特定级别的负责人来解决。</p>
<p><strong>How important are lateral connections?</strong> Table 1(e) shows the ablation results of a top-down feature pyramid without the 1×1 lateral connections. This top-down pyramid has strong semantic features and fine resolutions. But we argue that the locations of these features are not precise, because these maps have been downsampled and upsampled several times. More precise locations of features can be directly passed from the finer levels of the bottom-up maps via the lateral connections to the top-down maps. As a results, FPN has an AR$^1k$ score 10 points higher than Table 1(e).</p>
<p><strong>横向连接有多重要？</strong>表1（e）显示了没有1×1横向连接的自顶向下特征金字塔的消融结果。这个自顶向下的金字塔具有强大的语义特征和良好的分辨率。但是我们认为这些特征的位置并不精确，因为这些映射已经进行了多次下采样和上采样。更精确的特征位置可以通过横向连接直接从自下而上映射的更精细层级传递到自上而下的映射。因此，FPN的AR$^1k$的得分比表1（e）高10个点。</p>
<p><strong>How important are pyramid representations?</strong> Instead of resorting to pyramid representations, one can attach the head to the highest-resolution, strongly semantic feature maps of $P_2$ (i.e., the finest level in our pyramids). Similar to the single-scale baselines, we assign all anchors to the $P_2$ feature map. This variant (Table 1(f)) is better than the baseline but inferior to our approach. RPN is a sliding window detector with a fixed window size, so scanning over pyramid levels can increase its robustness to scale variance.</p>
<p><strong>金字塔表示有多重要？</strong>可以将头部附加到$P_2$的最高分辨率的强语义特征映射上（即我们金字塔中的最好层级），而不采用金字塔表示。与单尺度基线类似，我们将所有锚点分配给$P_2$特征映射。这个变体（表1（f））比基线要好，但不如我们的方法。RPN是一个具有固定窗口大小的滑动窗口检测器，因此在金字塔层级上扫描可以增加其对尺度变化的鲁棒性。</p>
<p>In addition, we note that using $P_2$ alone leads to more anchors (750k, Table 1(f)) caused by its large spatial resolution. This result suggests that a larger number of anchors is not sufficient in itself to improve accuracy.</p>
<p>另外，我们注意到由于$P_2$较大的空间分辨率，单独使用$P_2$会导致更多的锚点（750k，表1（f））。这个结果表明，大量的锚点本身并不足以提高准确率。</p>
<h3 id="5-2-Object-Detection-with-Fast-Faster-R-CNN"><a href="#5-2-Object-Detection-with-Fast-Faster-R-CNN" class="headerlink" title="5.2. Object Detection with Fast/Faster R-CNN"></a>5.2. Object Detection with Fast/Faster R-CNN</h3><p>Next we investigate FPN for region-based (non-sliding window) detectors. We evaluate object detection by the COCO-style Average Precision (AP) and PASCAL-style AP (at a single IoU threshold of 0.5). We also report COCO AP on objects of small, medium, and large sizes (namely, AP$_s$, AP$_m$, and AP$_l$) following the definitions in [21].</p>
<h3 id="5-2-使用Fast-Faster-R-CNN的目标检测"><a href="#5-2-使用Fast-Faster-R-CNN的目标检测" class="headerlink" title="5.2. 使用Fast/Faster R-CNN的目标检测"></a>5.2. 使用Fast/Faster R-CNN的目标检测</h3><p>接下来我们研究基于区域（非滑动窗口）检测器的FPN。我们通过COCO类型的平均精度（AP）和PASCAL类型的AP（单个IoU阈值为0.5）来评估目标检测。我们还按照[21]中的定义报告了在小尺寸，中尺寸和大尺寸（即AP$_s$，AP$_m$和AP$_l$）目标上的COCO AP。</p>
<p><strong>Implementation details.</strong> The input image is resized such that its shorter side has 800 pixels. Synchronized SGD is used to train the model on 8 GPUs. Each mini-batch involves 2 image per GPU and 512 RoIs per image. We use a weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for the first 60k mini-batches and 0.002 for the next 20k. We use 2000 RoIs per image for training and 1000 for testing. Training Fast R-CNN with FPN takes about 10 hours on the COCO dataset.</p>
<p><strong>实现细节</strong>。调整大小输入图像，使其较短边为800像素。同步SGD用于在8个GPU上训练模型。每个小批量数据包括每个GPU2张图像和每张图像上512个RoI。我们使用0.0001的权重衰减和0.9的动量。前60k次小批量数据的学习率为0.02，而接下来的20k次迭代学习率为0.002。我们每张图像使用2000个RoIs进行训练，1000个RoI进行测试。使用FPN在COCO数据集上训练Fast R-CNN需要约10小时。</p>
<h4 id="5-2-1-Fast-R-CNN-on-fixed-proposals"><a href="#5-2-1-Fast-R-CNN-on-fixed-proposals" class="headerlink" title="5.2.1 Fast R-CNN (on fixed proposals)"></a>5.2.1 Fast R-CNN (on fixed proposals)</h4><p>To better investigate FPN’s effects on the region-based detector alone, we conduct ablations of Fast R-CNN on <em>a fixed set of proposals</em>. We choose to freeze the proposals as computed by RPN on FPN (Table 1(c)), because it has good performance on small objects that are to be recognized by the detector. For simplicity we do not share features between Fast R-CNN and RPN, except when specified.</p>
<h4 id="5-2-1-Fast-R-CNN-固定提议"><a href="#5-2-1-Fast-R-CNN-固定提议" class="headerlink" title="5.2.1 Fast R-CNN(固定提议)"></a>5.2.1 Fast R-CNN(固定提议)</h4><p>为了更好地调查FPN对仅基于区域的检测器的影响，我们在<em>一组固定的提议</em>上进行Fast R-CNN的消融。我们选择冻结RPN在FPN上计算的提议（表1（c）），因为它在能被检测器识别的小目标上具有良好的性能。为了简单起见，我们不在Fast R-CNN和RPN之间共享特征，除非指定。</p>
<p>As a ResNet-based Fast R-CNN baseline, following [16], we adopt RoI pooling with an output size of 14×14 and attach all conv5 layers as the hidden layers of the head. This gives an AP of 31.9 in Table 2(a). Table 2(b) is a baseline exploiting an MLP head with 2 hidden fc layers, similar to the head in our architecture. It gets an AP of 28.8, indicating that the 2-fc head does not give us any orthogonal advantage over the baseline in Table 2(a).</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e965fe8bceddc877.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>Table 2. Object detection results using <strong>Fast R-CNN</strong> [11] on a fixed set of proposals (RPN, {$P_k$}, Table 1(c)), evaluated on the COCO <code>minival</code> set. Models are trained on the <code>trainval35k</code> set. All results are based on ResNet-50 and share the same hyper-parameters.</p>
<p>作为基于ResNet的Fast R-CNN基线，遵循[16]，我们采用输出尺寸为14×14的RoI池化，并将所有conv5层作为头部的隐藏层。这得到了31.9的AP，如表2（a）。表2（b）是利用MLP头部的基线，其具有2个隐藏的fc层，类似于我们的架构中的头部。它得到了28.8的AP，表明2-fc头部没有给我们带来任何超过表2（a）中基线的正交优势。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e965fe8bceddc877.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>表2。使用<strong>Fast R-CNN</strong>[11]在一组固定提议（RPN，{$P_k$}，表1（c））上的目标检测结果，在COCO的<code>minival</code>数据集上进行评估。模型在<code>trainval35k</code>数据集上训练。所有结果都基于ResNet-50并共享相同的超参数。</p>
<p>Table 2(c) shows the results of our FPN in Fast R-CNN. Comparing with the baseline in Table 2(a), our method improves AP by 2.0 points and small object AP by 2.1 points. Comparing with the baseline that also adopts a 2$fc$ head (Table 2(b)), our method improves AP by 5.1 points. These comparisons indicate that our feature pyramid is superior to single-scale features for a region-based object detector.</p>
<p>表2（c）显示了Fast R-CNN中我们的FPN结果。与表2（a）中的基线相比，我们的方法将AP提高了2.0个点，小型目标AP提高了2.1个点。与也采用2$fc$头部的基线相比（表2（b）），我们的方法将AP提高了5.1个点。这些比较表明，对于基于区域的目标检测器，我们的特征金字塔优于单尺度特征。</p>
<p>Table 2(d) and (e) show that removing top-down connections or removing lateral connections leads to inferior results, similar to what we have observed in the above sub-section for RPN. It is noteworthy that removing top-down connections (Table 2(d)) significantly degrades the accuracy, suggesting that Fast R-CNN suffers from using the low-level features at the high-resolution maps.</p>
<p>表2（d）和（e）表明，去除自上而下的连接或去除横向连接会导致较差的结果，类似于我们在上面的RPN小节中观察到的结果。值得注意的是，去除自上而下的连接（表2（d））显著降低了准确性，表明Fast R-CNN在高分辨率映射中使用了低级特征。</p>
<p>In Table 2(f), we adopt Fast R-CNN on the single finest scale feature map of $P_2$. Its result (33.4 AP) is marginally worse than that of using all pyramid levels (33.9 AP, Table 2(c)). We argue that this is because RoI pooling is a warping-like operation, which is less sensitive to the region’s scales. Despite the good accuracy of this variant, it is based on the RPN proposals of {$P_k$} and has thus already benefited from the pyramid representation.</p>
<p>在表2（f）中，我们在$P_2$的单个最好的尺度特征映射上采用了Fast R-CNN。其结果（33.4 AP）略低于使用所有金字塔等级（33.9 AP，表2（c））的结果。我们认为这是因为RoI池化是一种扭曲式的操作，对区域尺度较不敏感。尽管这个变体具有很好的准确性，但它是基于{$P_k$}的RPN提议的，因此已经从金字塔表示中受益。</p>
<h4 id="5-2-2-Faster-R-CNN-on-consistent-proposals"><a href="#5-2-2-Faster-R-CNN-on-consistent-proposals" class="headerlink" title="5.2.2 Faster R-CNN (on consistent proposals)"></a>5.2.2 Faster R-CNN (on consistent proposals)</h4><p>In the above we used a fixed set of proposals to investigate the detectors. But in a Faster R-CNN system [29], the RPN and Fast R-CNN must use <em>the same network backbone</em> in order to make feature sharing possible. Table 3 shows the comparisons between our method and two baselines, all using consistent backbone architectures for RPN and Fast R-CNN. Table 3(a) shows our reproduction of the baseline Faster R-CNN system as described in [16]. Under controlled settings, our FPN (Table 3(c)) is better than this strong baseline by 2.3 points AP and 3.8 points AP@0.5.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-af1a1bee7762064e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 3"></p>
<p>Table 3. Object detection results using Faster R-CNN [29] evaluated on the COCO <code>minival</code> set. <em>The backbone network for RPN are consistent with Fast R-CNN</em>. Models are trained on the <code>trainval35k</code> set and use ResNet-50. $^†$Provided by authors of [16].</p>
<h4 id="5-2-2-Faster-R-CNN-一致提议"><a href="#5-2-2-Faster-R-CNN-一致提议" class="headerlink" title="5.2.2 Faster R-CNN(一致提议)"></a>5.2.2 Faster R-CNN(一致提议)</h4><p>在上面我们使用了一组固定的提议来研究检测器。但是在Faster R-CNN系统中[29]，RPN和Fast R-CNN必须使用<em>相同的骨干网络</em>来实现特征共享。表3显示了我们的方法和两个基线之间的比较，所有这些RPN和Fast R-CNN都使用一致的骨干架构。表3（a）显示了我们再现[16]中描述的Faster R-CNN系统的基线。在受控的环境下，我们的FPN（表3（c））比这个强劲的基线要好2.3个点的AP和3.8个点的AP@0.5。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-af1a1bee7762064e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 3"></p>
<p>表3。使用Faster R-CNN[29]在COCO<code>minival</code>数据集上评估的目标检测结果。<em>RPN与Fast R-CNN的骨干网络是一致的</em>。模型在<code>trainval35k</code>数据集上训练并使用ResNet-50。$^†$由[16]的作者提供。</p>
<p>Note that Table 3(a) and (b) are baselines that are much stronger than the baseline provided by He et al. [16] in Table 3(<em>). We find the following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in [11, 16]; (ii) We train with 512 RoIs per image which accelerate convergence, in contrast to 64 RoIs in [11, 16]; (iii) We use 5 scale anchors instead of 4 in [16] (adding $32^2$); (iv) At test time we use 1000 proposals per image instead of 300 in [16]. So comparing with He et al.’s ResNet-50 Faster R-CNN baseline in Table 3(</em>), our method improves AP by 7.6 points and AP@0.5 by 9.6 points.</p>
<p>请注意，表3（a）和（b）的基线比He等人[16]在表3（<em>）中提供的基线强大得多。我们发现以下实现有助于缩小差距：（i）我们使用800像素的图像尺度，而不是[11，16]中的600像素；（ii）与[11，16]中的64个ROI相比，我们训练时每张图像有512个ROIs，可以加速收敛；（iii）我们使用5个尺度的锚点，而不是[16]中的4个（添加$32^2$）；（iv）在测试时，我们每张图像使用1000个提议，而不是[16]中的300个。因此，与表3（</em>）中的He等人的ResNet-50 Faster R-CNN基线相比，我们的方法将AP提高了7.6点个并且将AP@0.5提高了9.6个点。</p>
<p><strong>Sharing features.</strong> In the above, for simplicity we do not share the features between RPN and Fast R-CNN. In Table 5, we evaluate sharing features following the 4-step training described in [29]. Similar to [29], we find that sharing features improves accuracy by a small margin. Feature sharing also reduces the testing time.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-af0ba17be104a204.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 5"></p>
<p>Table 5. More object detection results using Faster R-CNN and our FPNs, evaluated on <code>minival</code>. Sharing features increases train time by 1.5× (using 4-step training [29]), but reduces test time.</p>
<p><strong>共享特征</strong>。在上面，为了简单起见，我们不共享RPN和Fast R-CNN之间的特征。在表5中，我们按照[29]中描述的4步训练评估了共享特征。与[29]类似，我们发现共享特征提高了一点准确率。特征共享也缩短了测试时间。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-af0ba17be104a204.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 5"></p>
<p>表5。使用Faster R-CNN和我们的FPN在<code>minival</code>上的更多目标检测结果。共享特征将训练时间增加了1.5倍（使用4步训练[29]），但缩短了测试时间。</p>
<p><strong>Running time.</strong> With feature sharing, our FPN-based Faster R-CNN system has inference time of 0.148 seconds per image on a single NVIDIA M40 GPU for ResNet-50, and 0.172 seconds for ResNet-101. As a comparison, the single-scale ResNet-50 baseline in Table 3(a) runs at 0.32 seconds. Our method introduces small extra cost by the extra layers in the FPN, but has a lighter weight head. Overall our system is faster than the ResNet-based Faster R-CNN counterpart. We believe the efficiency and simplicity of our method will benefit future research and applications.</p>
<p><strong>运行时间</strong>。通过特征共享，我们的基于FPN的Faster R-CNN系统使用ResNet-50在单个NVIDIA M40 GPU上每张图像的推断时间为0.148秒，使用ResNet-101的时间为0.172秒。作为比较，表3（a）中的单尺度ResNet-50基线运行时间为0.32秒。我们的方法通过FPN中的额外层引入了较小的额外成本，但具有更轻的头部。总体而言，我们的系统比对应的基于ResNet的Faster R-CNN更快。我们相信我们方法的高效性和简洁性将有利于未来的研究和应用。</p>
<h4 id="5-2-3-Comparing-with-COCO-Competition-Winners"><a href="#5-2-3-Comparing-with-COCO-Competition-Winners" class="headerlink" title="5.2.3 Comparing with COCO Competition Winners"></a>5.2.3 Comparing with COCO Competition Winners</h4><p>We find that our ResNet-101 model in Table 5 is not sufficiently trained with the default learning rate schedule. So we increase the number of mini-batches by 2× at each learning rate when training the Fast R-CNN step. This increases AP on <code>minival</code> to 35.6, without sharing features. This model is the one we submitted to the COCO detection leaderboard, shown in Table 4. We have not evaluated its feature-sharing version due to limited time, which should be slightly better as implied by Table 5.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-c972eb27718af689.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 4"></p>
<p>Table 4. Comparisons of <strong>single-model</strong> results on the COCO detection benchmark. Some results were not available on the <code>test-std</code> set, so we also include the <code>test-dev</code> results (and for Multipath [40] on <code>minival</code>). $^†$: <a href="http://image-net.org/challenges/ talks/2016/GRMI-COCO-slidedeck.pdf" target="_blank" rel="external">http://image-net.org/challenges/ talks/2016/GRMI-COCO-slidedeck.pdf</a>. $^‡$: <a href="http://mscoco.org/dataset/#detections-leaderboard" target="_blank" rel="external">http://mscoco.org/dataset/#detections-leaderboard</a>. $^§$: This entry of AttractioNet [10] adopts VGG-16 for proposals and Wide ResNet [39] for object detection, so is not strictly a single-model result.</p>
<h4 id="5-2-3-与COCO竞赛获胜者的比较"><a href="#5-2-3-与COCO竞赛获胜者的比较" class="headerlink" title="5.2.3 与COCO竞赛获胜者的比较"></a>5.2.3 与COCO竞赛获胜者的比较</h4><p>我们发现表5中我们的ResNet-101模型在默认学习速率的情况下没有进行足够的训练。因此，在训练Fast R-CNN步骤时，我们将每个学习速率的小批量数据的数量增加了2倍。这将<code>minival</code>上的AP增加到了35.6，没有共享特征。该模型是我们提交给COCO检测排行榜的模型，如表4所示。由于时间有限，我们尚未评估其特征共享版本，这应该稍微好一些，如表5所示。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-c972eb27718af689.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 4"></p>
<p>表4。在COCO检测基线上<strong>单模型</strong>结果的比较。一些在<code>test-std</code>数据集上的结果是不可获得的，因此我们也包括了在<code>test-dev</code>上的结果（和Multipath[40]在<code>minival</code>上的结果）。$^†$：<a href="http://image-net.org/challenges/ talks/2016/GRMI-COCO-slidedeck.pdf" target="_blank" rel="external">http://image-net.org/challenges/ talks/2016/GRMI-COCO-slidedeck.pdf</a>。$^‡$：<a href="http://mscoco.org/dataset/#detections-leaderboard" target="_blank" rel="external">http://mscoco.org/dataset/#detections-leaderboard</a>。$^§$：AttractioNet[10]的输入采用VGG-16进行目标提议，用Wide ResNet[39]进行目标检测，因此它不是严格意义上的单模型。</p>
<p>Table 4 compares our method with the single-model results of the COCO competition winners, including the 2016 winner G-RMI and the 2015 winner Faster R-CNN+++. Without adding bells and whistles, our single-model entry has surpassed these strong, heavily engineered competitors. On the <code>test-dev</code> set, our method increases over the existing best results by 0.5 points of AP (36.2 vs. 35.7) and 3.4 points of AP@0.5 (59.1 vs. 55.7). It is worth noting that our method does not rely on image pyramids and only uses a single input image scale, but still has outstanding AP on small-scale objects. This could only be achieved by high-resolution image inputs with previous methods.</p>
<p>表4将我们方法的单模型结果与COCO竞赛获胜者的结果进行了比较，其中包括2016年冠军G-RMI和2015年冠军Faster R-CNN+++。没有添加额外的东西，我们的单模型提交就已经超越了这些强大的，经过严格设计的竞争对手。在<code>test-dev</code>数据集中，我们的方法在现有最佳结果上增加了0.5个点的AP（36.2 vs.35.7）和3.4个点的AP@0.5（59.1 vs. 55.7）。值得注意的是，我们的方法不依赖图像金字塔，只使用单个输入图像尺度，但在小型目标上仍然具有出色的AP。这只能通过使用前面方法的高分辨率图像输入来实现。</p>
<p>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative mining [35], context modeling [16], stronger data augmentation [22], etc. These improvements are complementary to FPNs and should boost accuracy further.</p>
<p>此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</p>
<p>Recently, FPN has enabled new top results in all tracks of the COCO competition, including detection, instance segmentation, and keypoint estimation. See [14] for details.</p>
<p>最近，FPN在COCO竞赛的所有方面都取得了新的最佳结果，包括检测，实例分割和关键点估计。详情请参阅[14]。</p>
<h2 id="6-Extensions-Segmentation-Proposals"><a href="#6-Extensions-Segmentation-Proposals" class="headerlink" title="6. Extensions: Segmentation Proposals"></a>6. Extensions: Segmentation Proposals</h2><p>Our method is a generic pyramid representation and can be used in applications other than object detection. In this section we use FPNs to generate segmentation proposals, following the DeepMask/SharpMask framework [27, 28].</p>
<h2 id="6-扩展：分割提议"><a href="#6-扩展：分割提议" class="headerlink" title="6. 扩展：分割提议"></a>6. 扩展：分割提议</h2><p>我们的方法是一种通用金字塔表示，可用于除目标检测之外的其他应用。在本节中，我们使用FPN生成分割建议，遵循DeepMask/SharpMask框架[27，28]。</p>
<p>DeepMask/SharpMask were trained on image crops for predicting instance segments and object/non-object scores. At inference time, these models are run convolutionally to generate dense proposals in an image. To generate segments at multiple scales, image pyramids are necessary [27, 28].</p>
<p>DeepMask/SharpMask在裁剪图像上进行训练，可以预测实例块和目标/非目标分数。在推断时，这些模型是卷积运行的，以在图像中生成密集的提议。为了在多个尺度上生成分割块，图像金字塔是必要的[27，28]。</p>
<p>It is easy to adapt FPN to generate mask proposals. We use a fully convolutional setup for both training and inference. We construct our feature pyramid as in Sec. 5.1 and set $d=128$. On top of each level of the feature pyramid, we apply a small 5×5 MLP to predict 14×14 masks and object scores in a fully convolutional fashion, see Fig. 4. Additionally, motivated by the use of 2 scales per octave in the image pyramid of [27, 28], we use a second MLP of input size 7×7 to handle half octaves. The two MLPs play a similar role as anchors in RPN. The architecture is trained end-to-end; full implementation details are given in the appendix.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a3c3535037db1b70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>Figure 4. FPN for object segment proposals. The feature pyramid is constructed with identical structure as for object detection. We apply a small MLP on 5x5 windows to generate dense object segments with output dimension of 14x14. Shown in orange are the size of the image regions the mask corresponds to for each pyramid level (levels $P_{3-5} are shown here). Both the corresponding image region size (light orange) and canonical object size (dark orange) are shown. Half octaves are handled by an MLP on 7x7 windows ($7 \approx 5 \sqrt 2$), not shown here. Details are in the appendix.</p>
<p>改编FPN生成掩码提议很容易。我们对训练和推断都使用全卷积设置。我们在5.1小节中构造我们的特征金字塔并设置$d=128$。在特征金字塔的每个层级上，我们应用一个小的5×5MLP以全卷积方式预测14×14掩码和目标分数，参见图4。此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。这两个MLP在RPN中扮演着类似于锚点的角色。该架构是端到端训练的，完整的实现细节在附录中给出。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a3c3535037db1b70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>图4。目标分割提议的FPN。特征金字塔的构造结构与目标检测相同。我们在5x5窗口上应用一个小的MLP来生成输出尺寸为14x14的密集目标块。以橙色显示的掩码是每个金字塔层级所对应的图像区域的大小（此处显示的是层级$P_{3-5}）。显示了相应的图像区域大小（浅橙色）和典型目标大小（深橙色）。半个组由MLP在7x7窗口（$ 7 \ approx 5 \ sqrt 2 $）处理，此处未展示。详情见附录。</p>
<h3 id="6-1-Segmentation-Proposal-Results"><a href="#6-1-Segmentation-Proposal-Results" class="headerlink" title="6.1. Segmentation Proposal Results"></a>6.1. Segmentation Proposal Results</h3><p>Results are shown in Table 6. We report segment AR and segment AR on small, medium, and large objects, always for 1000 proposals. Our baseline FPN model with a single 5×5 MLP achieves an AR of 43.4. Switching to a slightly larger 7×7 MLP leaves accuracy largely unchanged. Using both MLPs together increases accuracy to 45.7 AR. Increasing mask output size from 14×14 to 28×28 increases AR another point (larger sizes begin to degrade accuracy). Finally, doubling the training iterations increases AR to 48.1.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-71b1cb0a1885c7ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 6"></p>
<p>Table 6. Instance segmentation proposals evaluated on the first 5k COCO <code>val</code> images. All models are trained on the <code>train</code> set. DeepMask, SharpMask, and FPN use ResNet-50 while Instance-FCN uses VGG-16. DeepMask and SharpMask performance is computed with models available from <a href="https://github. com/facebookresearch/deepmask" target="_blank" rel="external">https://github. com/facebookresearch/deepmask</a> (both are the ‘zoom’ variants). $^†$Runtimes are measured on an NVIDIA M40 GPU, except the InstanceFCN timing which is based on the slower K40.</p>
<h3 id="6-1-分割提议结果"><a href="#6-1-分割提议结果" class="headerlink" title="6.1. 分割提议结果"></a>6.1. 分割提议结果</h3><p>结果如表6所示。我们报告了分割AR和在小型，中型和大型目标上的分割AR，都是对于1000个提议而言的。我们的具有单个5×5MLP的基线FPN模型达到了43.4的AR。切换到稍大的7×7MLP，精度基本保持不变。同时使用两个MLP将精度提高到了45.7的AR。将掩码输出尺寸从14×14增加到28×28会增加AR另一个点（更大的尺寸开始降低准确度）。最后，加倍训练迭代将AR增加到48.1。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-71b1cb0a1885c7ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 6"></p>
<p>表6。在前5k张COCO<code>val</code>图像上评估的实例分割提议。所有模型都是在<code>train</code>数据集上训练的。DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。DeepMask和SharpMask性能计算的模型是从<a href="https://github. com/facebookresearch/deepmask" target="_blank" rel="external">https://github. com/facebookresearch/deepmask</a>上获得的（都是‘zoom’变体）。$^†$运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</p>
<p>We also report comparisons to DeepMask [27], Sharp-Mask [28], and InstanceFCN [4], the previous state of the art methods in mask proposal generation. We outperform the accuracy of these approaches by over 8.3 points AR. In particular, we nearly double the accuracy on small objects.</p>
<p>我们还报告了与DeepMask[27]，Sharp-Mask[28]和InstanceFCN[4]的比较，这是以前的掩模提议生成中的先进方法。我们的准确度超过这些方法8.3个点的AR。尤其是我们几乎将小目标的精度提高了一倍。</p>
<p>Existing mask proposal methods [27, 28, 4] are based on densely sampled image pyramids (e.g., scaled by $2^{\lbrace −2:0.5:1 \rbrace}$ in [27, 28]), making them computationally expensive. Our approach, based on FPNs, is substantially faster (our models run at 6 to 7 FPS). These results demonstrate that our model is a generic feature extractor and can replace image pyramids for other multi-scale detection problems.</p>
<p>现有的掩码提议方法[27，28，4]是基于密集采样的图像金字塔的（例如，[27，28]中的缩放为$2^{\lbrace −2:0.5:1 \rbrace}$），使得它们是计算昂贵的。我们的方法基于FPN，速度明显加快（我们的模型运行速度为6至7FPS）。这些结果表明，我们的模型是一个通用的特征提取器，可以替代图像金字塔以用于其他多尺度检测问题。</p>
<h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>We have presented a clean and simple framework for building feature pyramids inside ConvNets. Our method shows significant improvements over several strong baselines and competition winners. Thus, it provides a practical solution for research and applications of feature pyramids, without the need of computing image pyramids. Finally, our study suggests that despite the strong representational power of deep ConvNets and their implicit robustness to scale variation, it is still critical to explicitly address multi-scale problems using pyramid representations.</p>
<h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7. 结论"></a>7. 结论</h2><p>我们提出了一个干净而简单的框架，用于在ConvNets内部构建特征金字塔。我们的方法比几个强大的基线和竞赛获胜者显示出了显著的改进。因此，它为特征金字塔的研究和应用提供了一个实用的解决方案，而不需要计算图像金字塔。最后，我们的研究表明，尽管深层ConvNets具有强大的表示能力以及它们对尺度变化的隐式鲁棒性，但使用金字塔表示对于明确地解决多尺度问题仍然至关重要。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 1984.</p>
<p>[2] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016.</p>
<p>[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection. In ECCV, 2016.</p>
<p>[4] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive fully convolutional networks. In ECCV, 2016.</p>
<p>[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.</p>
<p>[6] P. Dollar, R. Appel, S. Belongie, and P. Perona. Fast feature pyramids for object detection. TPAMI, 2014.</p>
<p>[7] P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 2010.</p>
<p>[8] G.GhiasiandC.C.Fowlkes.Laplacianpyramidreconstruction and refinement for semantic segmentation. In ECCV, 2016.</p>
<p>[9] S. Gidaris and N. Komodakis. Object detection via a multi-region &amp; semantic segmentation-aware CNN model. In ICCV, 2015.</p>
<p>[10] S. Gidaris and N. Komodakis. Attend refine repeat: Active box proposal generation via in-out localization. In BMVC, 2016.</p>
<p>[11] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.</p>
<p>[13] B.Hariharan,P.Arbelaez,R.Girshick,andJ.Malik.Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015.</p>
<p>[14] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn. arXiv:1703.06870, 2017.</p>
<p>[15] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.</p>
<p>[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>[17] S. Honari, J. Yosinski, P. Vincent, and C. Pal. Recombinator networks: Learning coarse-to-fine feature aggregation. In CVPR, 2016.</p>
<p>[18] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards accurate region proposal generation and joint object detection. In CVPR, 2016.</p>
<p>[19] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.</p>
<p>[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[22] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot multibox detector. In ECCV, 2016.</p>
<p>[23] W. Liu, A. Rabinovich, and A. C. Berg. ParseNet: Looking wider to see better. In ICLR workshop, 2016.</p>
<p>[24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[25] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.</p>
<p>[26] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In ECCV, 2016.</p>
<p>[27] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to segment object candidates. In NIPS, 2015.</p>
<p>[28] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár. Learning to refine object segments. In ECCV, 2016.</p>
<p>[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. PAMI, 2016.</p>
<p>[31] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image segmentation. In MIC- CAI, 2015.</p>
<p>[32] H. Rowley, S. Baluja, and T. Kanade. Human face detection in visual scenes. Technical Report CMU-CS-95-158R, Carnegie Mellon University, 1995.</p>
<p>[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.</p>
<p>[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[35] A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object detectors with online hard example mining. In CVPR, 2016.</p>
<p>[36] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[37] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.</p>
<p>[38] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE Proc. on Vision, Image, and Signal Processing, 1994.</p>
<p>[39] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC, 2016.</p>
<p>[40] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chintala, and P. Dollár. A multipath network for object detection. In BMVC, 2016. 10</p>
]]></content>
    
    <summary type="html">
    
      Feature Pyramid Networks for Object Detection论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Feature Pyramid Networks for Object Detection论文翻译——中文版</title>
    <link href="http://noahsnail.com/2018/03/20/2018-03-20-Feature%20Pyramid%20Networks%20for%20Object%20Detection%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2018/03/20/2018-03-20-Feature Pyramid Networks for Object Detection论文翻译——中文版/</id>
    <published>2018-03-20T10:30:15.000Z</published>
    <updated>2018-05-09T13:07:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Feature-Pyramid-Networks-for-Object-Detection"><a href="#Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="Feature Pyramid Networks for Object Detection"></a>Feature Pyramid Networks for Object Detection</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>特征金字塔是识别系统中用于检测不同尺度目标的基本组件。但最近的深度学习目标检测器已经避免了金字塔表示，部分原因是它们是计算和内存密集型的。在本文中，我们利用深度卷积网络内在的多尺度、金字塔分级来构造具有很少额外成本的特征金字塔。开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。此外，我们的方法可以在GPU上以6FPS运行，因此是多尺度目标检测的实用和准确的解决方案。代码将公开发布。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>识别不同尺度的目标是计算机视觉中的一个基本挑战。建立在图像金字塔之上的特征金字塔（我们简称为特征化图像金字塔）构成了标准解决方案的基础[1]（图1（a））。这些金字塔是尺度不变的，因为目标的尺度变化是通过在金字塔中移动它的层级来抵消的。直观地说，该属性使模型能够通过在位置和金字塔等级上扫描模型来检测大范围尺度内的目标。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-79414c80444765b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1。（a）使用图像金字塔构建特征金字塔。每个图像尺度上的特征都是独立计算的，速度很慢。（b）最近的检测系统选择只使用单一尺度特征进行更快的检测。（c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。（d）我们提出的特征金字塔网络（FPN）与（b）和（c）类似，但更准确。在该图中，特征映射用蓝色轮廓表示，较粗的轮廓表示语义上较强的特征。</p>
<p>特征化图像金字塔在手工设计的时代被大量使用[5，25]。它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave参考SIFT特征）。对于识别任务，工程特征大部分已经被深度卷积网络（ConvNets）[19，20]计算的特征所取代。除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。但即使有这种鲁棒性，金字塔仍然需要得到最准确的结果。在ImageNet[33]和COCO[21]检测挑战中，最近的所有排名靠前的输入都使用了针对特征化图像金字塔的多尺度测试（例如[16，35]）。对图像金字塔的每个层次进行特征化的主要优势在于它产生了多尺度的特征表示，其中所有层次上在语义上都很强，包括高分辨率层。</p>
<p>尽管如此，特征化图像金字塔的每个层次都具有明显的局限性。推断时间显著增加（例如，四倍[11]），使得这种方法在实际应用中不切实际。此外，在图像金字塔上端对端地训练深度网络在内存方面是不可行的，所以如果被采用，图像金字塔仅在测试时被使用[15，11，16，35]，这造成了训练/测试时推断的不一致性。出于这些原因，Fast和Faster R-CNN[11，29]选择在默认设置下不使用特征化图像金字塔。</p>
<p>但是，图像金字塔并不是计算多尺度特征表示的唯一方法。深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。这种网内特征层级产生不同空间分辨率的特征映射，但引入了由不同深度引起的较大的语义差异。高分辨率映射具有损害其目标识别表示能力的低级特征。</p>
<p>单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。理想情况下，SSD风格的金字塔将重用正向传递中从不同层中计算的多尺度特征映射，因此是零成本的。但为了避免使用低级特征，SSD放弃重用已经计算好的图层，而从网络中的最高层开始构建金字塔（例如，VGG网络的conv4_3[36]），然后添加几个新层。因此它错过了重用特征层级的更高分辨率映射的机会。我们证明这些对于检测小目标很重要。</p>
<p>本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。为了实现这个目标，我们依赖于一种结构，它将低分辨率，具有高分辨率的强大语义特征，语义上的弱特征通过自顶向下的路径和横向连接相结合（图1（d））。其结果是一个特征金字塔，在所有级别都具有丰富的语义，并且可以从单个输入图像尺度上进行快速构建。换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</p>
<p>最近的研究[28，17，8，26]中流行采用自顶向下和跳跃连接的类似架构。他们的目标是生成具有高分辨率的单个高级特征映射，并在其上进行预测（图2顶部）。相反，我们的方法利用这个架构作为特征金字塔，其中预测（例如目标检测）在每个级别上独立进行（图2底部）。我们的模型反映了一个特征化的图像金字塔，这在这些研究中还没有探索过。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e1ee021eac1b88be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2。顶部：带有跳跃连接的自顶向下的架构，在最好的级别上进行预测（例如，[28]）。底部：我们的模型具有类似的结构，但将其用作特征金字塔，并在各个层级上独立进行预测。</p>
<p>我们评估了我们称为特征金字塔网络（FPN）的方法，其在各种系统中用于检测和分割[11，29，27]。没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。我们的方法也很容易扩展掩模提议，改进实例分隔AR，加速严重依赖图像金字塔的最先进方法。</p>
<p>另外，我们的金字塔结构可以通过所有尺度进行端对端培训，并且在训练/测试时一致地使用，这在使用图像金字塔时是内存不可行的。因此，FPN能够比所有现有的最先进方法获得更高的准确度。此外，这种改进是在不增加单尺度基准测试时间的情况下实现的。我们相信这些进展将有助于未来的研究和应用。我们的代码将公开发布。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>手工设计特征和早期神经网络</strong>。SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。HOG特征[5]，以及后来的SIFT特征，都是在整个图像金字塔上密集计算的。这些HOG和SIFT金字塔已在许多工作中得到了应用，用于图像分类，目标检测，人体姿势估计等。这对快速计算特征化图像金字塔也很有意义。Dollar等人[6]通过先计算一个稀疏采样（尺度）金字塔，然后插入缺失的层级，从而演示了快速金字塔计算。在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</p>
<p><strong>Deep ConvNet目标检测器</strong>。随着现代深度卷积网络[19]的发展，像OverFeat[34]和R-CNN[12]这样的目标检测器在精度上显示出了显著的提高。OverFeat采用了一种类似于早期神经网络人脸检测器的策略，通过在图像金字塔上应用ConvNet作为滑动窗口检测器。R-CNN采用了基于区域提议的策略[37]，其中每个提议在用ConvNet进行分类之前都进行了尺度归一化。SPPnet[15]表明，这种基于区域的检测器可以更有效地应用于在单个图像尺度上提取的特征映射。最近更准确的检测方法，如Fast R-CNN[11]和Faster R-CNN[29]提倡使用从单一尺度计算出的特征，因为它提供了精确度和速度之间的良好折衷。然而，多尺度检测性能仍然更好，特别是对于小型目标。</p>
<p><strong>使用多层的方法</strong>。一些最近的方法通过使用ConvNet中的不同层来改进检测和分割。FCN[24]将多个尺度上的每个类别的部分分数相加以计算语义分割。Hypercolumns[13]使用类似的方法进行目标实例分割。在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。SSD[22]和MS-CNN[3]可预测特征层级中多个层的目标，而不需要组合特征或分数。</p>
<p>最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。尽管这些方法采用的是金字塔形状的架构，但它们不同于特征化的图像金字塔[5，7，34]，其中所有层次上的预测都是独立进行的，参见图2。事实上，对于图2（顶部）中的金字塔结构，图像金字塔仍然需要跨多个尺度上识别目标[28]。</p>
<h2 id="3-特征金字塔网络"><a href="#3-特征金字塔网络" class="headerlink" title="3. 特征金字塔网络"></a>3. 特征金字塔网络</h2><p>我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。在第6节中我们还将FPN泛化到实例细分提议。</p>
<p>我们的方法以任意大小的单尺度图像作为输入，并以全卷积的方式输出多层适当大小的特征映射。这个过程独立于主卷积体系结构（例如[19，36，16]），在本文中，我们呈现了使用ResNets[16]的结果。如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</p>
<p><strong>自下而上的路径</strong>。自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。通常有许多层产生相同大小的输出映射，并且我们认为这些层位于相同的网络阶段。对于我们的特征金字塔，我们为每个阶段定义一个金字塔层。我们选择每个阶段的最后一层的输出作为我们的特征映射参考集，我们将丰富它来创建我们的金字塔。这种选择是自然的，因为每个阶段的最深层应具有最强大的特征。</p>
<p>具体而言，对于ResNets[16]，我们使用每个阶段的最后一个残差块输出的特征激活。对于conv2，conv3，conv4和conv5输出，我们将这些最后残差块的输出表示为$\lbrace C_2 , C_3 , C_4 , C_5 \rbrace$，并注意相对于输入图像它们的步长为{4，8，16，32}个像素。由于其庞大的内存占用，我们不会将conv1纳入金字塔。</p>
<p><strong>自顶向下的路径和横向连接</strong>。自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。这些特征随后通过来自自下而上路径上的特征经由横向连接进行增强。每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</p>
<p>图3显示了建造我们的自顶向下特征映射的构建块。使用较粗糙分辨率的特征映射，我们将空间分辨率上采样为2倍（为了简单起见，使用最近邻上采样）。然后通过按元素相加，将上采样映射与相应的自下而上映射（其经过1×1卷积层来减少通道维度）合并。迭代这个过程，直到生成最佳分辨率映射。为了开始迭代，我们只需在$C_5$上添加一个1×1卷积层来生成最粗糙分辨率映射。最后，我们在每个合并的映射上添加一个3×3卷积来生成最终的特征映射，这是为了减少上采样的混叠效应。这个最终的特征映射集称为$\lbrace P_2 , P_3 , P_4 , P_5 \rbrace$，对应于$\lbrace C_2, C_3, C_4, C_5 \rbrace$，分别具有相同的空间大小。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a51d06a2a94bfec4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3。构建模块说明了横向连接和自顶向下路径，通过加法合并。</p>
<p>由于金字塔的所有层都像传统的特征图像金字塔一样使用共享分类器/回归器，因此我们在所有特征映射中固定特征维度（通道数记为$d$）。我们在本文中设置$d=256$，因此所有额外的卷积层都有256个通道的输出。在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</p>
<p>简洁性是我们设计的核心，我们发现我们的模型对许多设计选择都很鲁棒。我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。设计更好的连接模块并不是本文的重点，所以我们选择上述的简单设计。</p>
<h2 id="4-应用"><a href="#4-应用" class="headerlink" title="4. 应用"></a>4. 应用</h2><p>我们的方法是在深度ConvNets内部构建特征金字塔的通用解决方案。在下面，我们采用我们的方法在RPN[29]中进行边界框提议生成，并在Fast R-CNN[11]中进行目标检测。为了证明我们方法的简洁性和有效性，我们对[29，11]的原始系统进行最小修改，使其适应我们的特征金字塔。</p>
<h3 id="4-1-RPN的特征金字塔网络"><a href="#4-1-RPN的特征金字塔网络" class="headerlink" title="4.1. RPN的特征金字塔网络"></a>4.1. RPN的特征金字塔网络</h3><p>RPN[29]是一个滑动窗口类不可知的目标检测器。在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。这是通过一个3×3的卷积层实现的，后面跟着两个用于分类和回归的1×1兄弟卷积，我们称之为网络头部。目标/非目标标准和边界框回归目标的定义是关于一组称为锚点的参考框的[29]。这些锚点具有多个预定义的尺度和长宽比，以覆盖不同形状的目标。</p>
<p>我们通过用我们的FPN替换单尺度特征映射来适应RPN。我们在我们的特征金字塔的每个层级上附加一个相同设计的头部（3x3 conv和两个1x1兄弟convs）。由于头部在所有金字塔等级上的所有位置密集滑动，所以不需要在特定层级上具有多尺度锚点。相反，我们为每个层级分配单尺度的锚点。在形式上，我们定义锚点$\lbrace P_2, P_3, P_4, P_5, P_6 \rbrace$分别具有$\lbrace 32^2 , 64^2 , 128^2 , 256^2 , 512^2 \rbrace$个像素的面积。正如在[29]中，我们在每个层级上也使用了多个长宽比$\lbrace 1:2, 1:1, 2:1 \rbrace$的锚点。所以在金字塔上总共有十五个锚点。</p>
<p>如[29]，我们根据锚点和实际边界框的交并比（IoU）比例将训练标签分配给锚点。形式上，如果一个锚点对于一个给定的实际边界框具有最高的IoU或者与任何实际边界框的IoU超过0.7，则给其分配一个正标签，如果其与所有实际边界框的IoU都低于0.3，则为其分配一个负标签。请注意，实际边界框的尺度并未明确用于将它们分配到金字塔的层级；相反，实际边界框与已经分配给金字塔等级的锚点相关联。因此，除了[29]中的内容外，我们不引入额外的规则。</p>
<p>我们注意到头部的参数在所有特征金字塔层级上共享；我们也评估了替代方案，没有共享参数并且观察到相似的准确性。共享参数的良好性能表明我们的金字塔的所有层级共享相似的语义级别。这个优点类似于使用特征图像金字塔的优点，其中可以将常见头部分类器应用于在任何图像尺度下计算的特征。</p>
<p>通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。我们在实验中详细说明实施细节。</p>
<h3 id="4-2-Fast-R-CNN的特征金字塔网络"><a href="#4-2-Fast-R-CNN的特征金字塔网络" class="headerlink" title="4.2. Fast R-CNN的特征金字塔网络"></a>4.2. Fast R-CNN的特征金字塔网络</h3><p>Fast R-CNN[11]是一个基于区域的目标检测器，利用感兴趣区域（RoI）池化来提取特征。Fast R-CNN通常在单尺度特征映射上执行。要将其与我们的FPN一起使用，我们需要为金字塔等级分配不同尺度的RoI。</p>
<p>我们将我们的特征金字塔看作是从图像金字塔生成的。因此，当它们在图像金字塔上运行时，我们可以适应基于区域的检测器的分配策略[15，11]。在形式上，我们通过以下公式将宽度为$w$和高度为$h$（在网络上的输入图像上）的RoI分配到特征金字塔的级别$P_k$上：$$k=\lfloor k_0+\log_2(\sqrt{wh}/224) \rfloor. \tag{1}$$ 这里$224$是规范的ImageNet预训练大小，而$k_0$是大小为$w \times h=224^2$的RoI应该映射到的目标级别。类似于基于ResNet的Faster R-CNN系统[16]使用$C_4$作为单尺度特征映射，我们将$k_0$设置为4。直觉上，方程（1）意味着如果RoI的尺寸变小了（比如224的1/2），它应该被映射到一个更精细的分辨率级别（比如$k=3$）。</p>
<p>我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。再次，预测器头部都共享参数，不管他们在什么层级。在[16]中，ResNet的conv5层（9层深的子网络）被用作conv4特征之上的头部，但我们的方法已经利用了conv5来构建特征金字塔。因此，与[16]不同，我们只是采用RoI池化提取7×7特征，并在最终的分类层和边界框回归层之前附加两个隐藏单元为1024维的全连接（$fc$）层（每层后都接ReLU层）。这些层是随机初始化的，因为ResNets中没有预先训练好的$fc$层。请注意，与标准的conv5头部相比，我们的2-$fc$ MLP头部更轻更快。</p>
<p>基于这些改编，我们可以在特征金字塔之上训练和测试Fast R-CNN。实现细节在实验部分给出。</p>
<h2 id="5-目标检测实验"><a href="#5-目标检测实验" class="headerlink" title="5. 目标检测实验"></a>5. 目标检测实验</h2><p>我们在80类的COCO检测数据集[21]上进行实验。我们训练使用80k张训练图像和35k大小的验证图像子集（<code>trainval35k</code>[2]）的联合，并报告了在5k大小的验证图像子集（<code>minival</code>）上的消融实验。我们还报告了在没有公开标签的标准测试集（<code>test-std</code>）[21]上的最终结果。</p>
<p>正如通常的做法[12]，所有的网络骨干都是在ImageNet1k分类集[33]上预先训练好的，然后在检测数据集上进行微调。我们使用公开可用的预训练的ResNet-50和ResNet-101模型。我们的代码是使用Caffe2重新实现<code>py-faster-rcnn</code>。</p>
<h3 id="5-1-区域提议与RPN"><a href="#5-1-区域提议与RPN" class="headerlink" title="5.1. 区域提议与RPN"></a>5.1. 区域提议与RPN</h3><p>根据[21]中的定义，我们评估了COCO类型的平均召回率（AR）和在小型，中型和大型目标(AR$_s$, AR$_m$, and AR$_l$)上的AR。我们报告了每张图像使用100个提议和1000个提议的结果(AR$^{100}$ and AR$^{1k}$)。</p>
<p><strong>实施细节</strong>。表1中的所有架构都是端对端训练。输入图像的大小调整为其较短边有800像素。我们采用8个GPU进行同步SGD训练。小批量数据包括每个GPU上2张图像和每张图像上256个锚点。我们使用0.0001的权重衰减和0.9的动量。前30k次小批量数据的学习率为0.02，而下一个10k次的学习率为0.002。对于所有的RPN实验（包括基准数据集），我们都包含了图像外部的锚盒来进行训练，这不同于[29]中的忽略这些锚盒。其它实现细节如[29]中所述。使用具有FPN的RPN在8个GPU上训练COCO数据集需要约8小时。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e14d05236d572fe7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<p>表1。使用RPN[29]的边界框提议结果，在COCO的<code>minival</code>数据集上进行评估。所有模型都是通过<code>trainval35k</code>训练的。列“lateral”和“top-down”分别表示横向连接和自上而下连接的存在。列“feature”表示附着头部的特征映射。所有结果都是基于ResNet-50的并共享相同的超参数。</p>
<h4 id="5-1-1-消融实验"><a href="#5-1-1-消融实验" class="headerlink" title="5.1.1 消融实验"></a>5.1.1 消融实验</h4><p><strong>与基线进行比较</strong>。为了与原始RPNs[29]进行公平比较，我们使用$C_4$(与[16]相同)或$C$_5$的单尺度映射运行了两个基线（表1（a，b）），都使用与我们相同的超参数，包括使用5种尺度锚点$\lbrace 32^2, 64^2, 128^2, 256^2, 512^2\rbrace$。表1（b）显示没有优于（a），这表明单个更高级别的特征映射是不够的，因为存在在较粗分辨率和较强语义之间的权衡。</p>
<p>将FPN放在RPN中可将AR$^{1k}$提高到56.3（表1（c）），这比单尺度RPN基线（表1（a））增加了<strong>8.0</strong>个点。此外，在小型目标（AR$^{1k}_s$）上的性能也大幅上涨了12.9个点。我们的金字塔表示大大提高了RPN对目标尺度变化的鲁棒性。</p>
<p><strong>自上而下改进的重要性如何？</strong>表1（d）显示了没有自上而下路径的特征金字塔的结果。通过这种修改，将1×1横向连接和后面的3×3卷积添加到自下而上的金字塔中。该架构模拟了重用金字塔特征层次结构的效果（图1（b））。</p>
<p><strong>横向连接有多重要？</strong>表1（e）显示了没有1×1横向连接的自顶向下特征金字塔的消融结果。这个自顶向下的金字塔具有强大的语义特征和良好的分辨率。但是我们认为这些特征的位置并不精确，因为这些映射已经进行了多次下采样和上采样。更精确的特征位置可以通过横向连接直接从自下而上映射的更精细层级传递到自上而下的映射。因此，FPN的AR$^1k$的得分比表1（e）高10个点。</p>
<p><strong>金字塔表示有多重要？</strong>可以将头部附加到$P_2$的最高分辨率的强语义特征映射上（即我们金字塔中的最好层级），而不采用金字塔表示。与单尺度基线类似，我们将所有锚点分配给$P_2$特征映射。这个变体（表1（f））比基线要好，但不如我们的方法。RPN是一个具有固定窗口大小的滑动窗口检测器，因此在金字塔层级上扫描可以增加其对尺度变化的鲁棒性。</p>
<p>另外，我们注意到由于$P_2$较大的空间分辨率，单独使用$P_2$会导致更多的锚点（750k，表1（f））。这个结果表明，大量的锚点本身并不足以提高准确率。</p>
<h3 id="5-2-使用Fast-Faster-R-CNN的目标检测"><a href="#5-2-使用Fast-Faster-R-CNN的目标检测" class="headerlink" title="5.2. 使用Fast/Faster R-CNN的目标检测"></a>5.2. 使用Fast/Faster R-CNN的目标检测</h3><p>接下来我们研究基于区域（非滑动窗口）检测器的FPN。我们通过COCO类型的平均精度（AP）和PASCAL类型的AP（单个IoU阈值为0.5）来评估目标检测。我们还按照[21]中的定义报告了在小尺寸，中尺寸和大尺寸（即AP$_s$，AP$_m$和AP$_l$）目标上的COCO AP。</p>
<p><strong>实现细节</strong>。调整大小输入图像，使其较短边为800像素。同步SGD用于在8个GPU上训练模型。每个小批量数据包括每个GPU2张图像和每张图像上512个RoI。我们使用0.0001的权重衰减和0.9的动量。前60k次小批量数据的学习率为0.02，而接下来的20k次迭代学习率为0.002。我们每张图像使用2000个RoIs进行训练，1000个RoI进行测试。使用FPN在COCO数据集上训练Fast R-CNN需要约10小时。</p>
<h4 id="5-2-1-Fast-R-CNN-固定提议"><a href="#5-2-1-Fast-R-CNN-固定提议" class="headerlink" title="5.2.1 Fast R-CNN(固定提议)"></a>5.2.1 Fast R-CNN(固定提议)</h4><p>为了更好地调查FPN对仅基于区域的检测器的影响，我们在<em>一组固定的提议</em>上进行Fast R-CNN的消融。我们选择冻结RPN在FPN上计算的提议（表1（c）），因为它在能被检测器识别的小目标上具有良好的性能。为了简单起见，我们不在Fast R-CNN和RPN之间共享特征，除非指定。</p>
<p>作为基于ResNet的Fast R-CNN基线，遵循[16]，我们采用输出尺寸为14×14的RoI池化，并将所有conv5层作为头部的隐藏层。这得到了31.9的AP，如表2（a）。表2（b）是利用MLP头部的基线，其具有2个隐藏的fc层，类似于我们的架构中的头部。它得到了28.8的AP，表明2-fc头部没有给我们带来任何超过表2（a）中基线的正交优势。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-e965fe8bceddc877.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>表2。使用<strong>Fast R-CNN</strong>[11]在一组固定提议（RPN，{$P_k$}，表1（c））上的目标检测结果，在COCO的<code>minival</code>数据集上进行评估。模型在<code>trainval35k</code>数据集上训练。所有结果都基于ResNet-50并共享相同的超参数。</p>
<p>表2（c）显示了Fast R-CNN中我们的FPN结果。与表2（a）中的基线相比，我们的方法将AP提高了2.0个点，小型目标AP提高了2.1个点。与也采用2$fc$头部的基线相比（表2（b）），我们的方法将AP提高了5.1个点。这些比较表明，对于基于区域的目标检测器，我们的特征金字塔优于单尺度特征。</p>
<p>表2（d）和（e）表明，去除自上而下的连接或去除横向连接会导致较差的结果，类似于我们在上面的RPN小节中观察到的结果。值得注意的是，去除自上而下的连接（表2（d））显著降低了准确性，表明Fast R-CNN在高分辨率映射中使用了低级特征。</p>
<p>在表2（f）中，我们在$P_2$的单个最好的尺度特征映射上采用了Fast R-CNN。其结果（33.4 AP）略低于使用所有金字塔等级（33.9 AP，表2（c））的结果。我们认为这是因为RoI池化是一种扭曲式的操作，对区域尺度较不敏感。尽管这个变体具有很好的准确性，但它是基于{$P_k$}的RPN提议的，因此已经从金字塔表示中受益。</p>
<h4 id="5-2-2-Faster-R-CNN-一致提议"><a href="#5-2-2-Faster-R-CNN-一致提议" class="headerlink" title="5.2.2 Faster R-CNN(一致提议)"></a>5.2.2 Faster R-CNN(一致提议)</h4><p>在上面我们使用了一组固定的提议来研究检测器。但是在Faster R-CNN系统中[29]，RPN和Fast R-CNN必须使用<em>相同的骨干网络</em>来实现特征共享。表3显示了我们的方法和两个基线之间的比较，所有这些RPN和Fast R-CNN都使用一致的骨干架构。表3（a）显示了我们再现[16]中描述的Faster R-CNN系统的基线。在受控的环境下，我们的FPN（表3（c））比这个强劲的基线要好2.3个点的AP和3.8个点的AP@0.5。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-af1a1bee7762064e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 3"></p>
<p>表3。使用Faster R-CNN[29]在COCO<code>minival</code>数据集上评估的目标检测结果。<em>RPN与Fast R-CNN的骨干网络是一致的</em>。模型在<code>trainval35k</code>数据集上训练并使用ResNet-50。$^†$由[16]的作者提供。</p>
<p>请注意，表3（a）和（b）的基线比He等人[16]在表3（<em>）中提供的基线强大得多。我们发现以下实现有助于缩小差距：（i）我们使用800像素的图像尺度，而不是[11，16]中的600像素；（ii）与[11，16]中的64个ROI相比，我们训练时每张图像有512个ROIs，可以加速收敛；（iii）我们使用5个尺度的锚点，而不是[16]中的4个（添加$32^2$）；（iv）在测试时，我们每张图像使用1000个提议，而不是[16]中的300个。因此，与表3（</em>）中的He等人的ResNet-50 Faster R-CNN基线相比，我们的方法将AP提高了7.6点个并且将AP@0.5提高了9.6个点。</p>
<p><strong>共享特征</strong>。在上面，为了简单起见，我们不共享RPN和Fast R-CNN之间的特征。在表5中，我们按照[29]中描述的4步训练评估了共享特征。与[29]类似，我们发现共享特征提高了一点准确率。特征共享也缩短了测试时间。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-af0ba17be104a204.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 5"></p>
<p>表5。使用Faster R-CNN和我们的FPN在<code>minival</code>上的更多目标检测结果。共享特征将训练时间增加了1.5倍（使用4步训练[29]），但缩短了测试时间。</p>
<p><strong>运行时间</strong>。通过特征共享，我们的基于FPN的Faster R-CNN系统使用ResNet-50在单个NVIDIA M40 GPU上每张图像的推断时间为0.148秒，使用ResNet-101的时间为0.172秒。作为比较，表3（a）中的单尺度ResNet-50基线运行时间为0.32秒。我们的方法通过FPN中的额外层引入了较小的额外成本，但具有更轻的头部。总体而言，我们的系统比对应的基于ResNet的Faster R-CNN更快。我们相信我们方法的高效性和简洁性将有利于未来的研究和应用。</p>
<h4 id="5-2-3-与COCO竞赛获胜者的比较"><a href="#5-2-3-与COCO竞赛获胜者的比较" class="headerlink" title="5.2.3 与COCO竞赛获胜者的比较"></a>5.2.3 与COCO竞赛获胜者的比较</h4><p>我们发现表5中我们的ResNet-101模型在默认学习速率的情况下没有进行足够的训练。因此，在训练Fast R-CNN步骤时，我们将每个学习速率的小批量数据的数量增加了2倍。这将<code>minival</code>上的AP增加到了35.6，没有共享特征。该模型是我们提交给COCO检测排行榜的模型，如表4所示。由于时间有限，我们尚未评估其特征共享版本，这应该稍微好一些，如表5所示。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-c972eb27718af689.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 4"></p>
<p>表4。在COCO检测基线上<strong>单模型</strong>结果的比较。一些在<code>test-std</code>数据集上的结果是不可获得的，因此我们也包括了在<code>test-dev</code>上的结果（和Multipath[40]在<code>minival</code>上的结果）。$^†$：<a href="http://image-net.org/challenges/ talks/2016/GRMI-COCO-slidedeck.pdf" target="_blank" rel="external">http://image-net.org/challenges/ talks/2016/GRMI-COCO-slidedeck.pdf</a>。$^‡$：<a href="http://mscoco.org/dataset/#detections-leaderboard" target="_blank" rel="external">http://mscoco.org/dataset/#detections-leaderboard</a>。$^§$：AttractioNet[10]的输入采用VGG-16进行目标提议，用Wide ResNet[39]进行目标检测，因此它不是严格意义上的单模型。</p>
<p>表4将我们方法的单模型结果与COCO竞赛获胜者的结果进行了比较，其中包括2016年冠军G-RMI和2015年冠军Faster R-CNN+++。没有添加额外的东西，我们的单模型提交就已经超越了这些强大的，经过严格设计的竞争对手。在<code>test-dev</code>数据集中，我们的方法在现有最佳结果上增加了0.5个点的AP（36.2 vs.35.7）和3.4个点的AP@0.5（59.1 vs. 55.7）。值得注意的是，我们的方法不依赖图像金字塔，只使用单个输入图像尺度，但在小型目标上仍然具有出色的AP。这只能通过使用前面方法的高分辨率图像输入来实现。</p>
<p>此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</p>
<p>最近，FPN在COCO竞赛的所有方面都取得了新的最佳结果，包括检测，实例分割和关键点估计。详情请参阅[14]。</p>
<h2 id="6-扩展：分割提议"><a href="#6-扩展：分割提议" class="headerlink" title="6. 扩展：分割提议"></a>6. 扩展：分割提议</h2><p>我们的方法是一种通用金字塔表示，可用于除目标检测之外的其他应用。在本节中，我们使用FPN生成分割建议，遵循DeepMask/SharpMask框架[27，28]。</p>
<p>DeepMask/SharpMask在裁剪图像上进行训练，可以预测实例块和目标/非目标分数。在推断时，这些模型是卷积运行的，以在图像中生成密集的提议。为了在多个尺度上生成分割块，图像金字塔是必要的[27，28]。</p>
<p>改编FPN生成掩码提议很容易。我们对训练和推断都使用全卷积设置。我们在5.1小节中构造我们的特征金字塔并设置$d=128$。在特征金字塔的每个层级上，我们应用一个小的5×5MLP以全卷积方式预测14×14掩码和目标分数，参见图4。此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。这两个MLP在RPN中扮演着类似于锚点的角色。该架构是端到端训练的，完整的实现细节在附录中给出。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-a3c3535037db1b70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>图4。目标分割提议的FPN。特征金字塔的构造结构与目标检测相同。我们在5x5窗口上应用一个小的MLP来生成输出尺寸为14x14的密集目标块。以橙色显示的掩码是每个金字塔层级所对应的图像区域的大小（此处显示的是层级$P_{3-5}）。显示了相应的图像区域大小（浅橙色）和典型目标大小（深橙色）。半个组由MLP在7x7窗口（$ 7 \ approx 5 \ sqrt 2 $）处理，此处未展示。详情见附录。</p>
<h3 id="6-1-分割提议结果"><a href="#6-1-分割提议结果" class="headerlink" title="6.1. 分割提议结果"></a>6.1. 分割提议结果</h3><p>结果如表6所示。我们报告了分割AR和在小型，中型和大型目标上的分割AR，都是对于1000个提议而言的。我们的具有单个5×5MLP的基线FPN模型达到了43.4的AR。切换到稍大的7×7MLP，精度基本保持不变。同时使用两个MLP将精度提高到了45.7的AR。将掩码输出尺寸从14×14增加到28×28会增加AR另一个点（更大的尺寸开始降低准确度）。最后，加倍训练迭代将AR增加到48.1。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/3232548-71b1cb0a1885c7ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 6"></p>
<p>表6。在前5k张COCO<code>val</code>图像上评估的实例分割提议。所有模型都是在<code>train</code>数据集上训练的。DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。DeepMask和SharpMask性能计算的模型是从<a href="https://github. com/facebookresearch/deepmask" target="_blank" rel="external">https://github. com/facebookresearch/deepmask</a>上获得的（都是‘zoom’变体）。$^†$运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</p>
<p>我们还报告了与DeepMask[27]，Sharp-Mask[28]和InstanceFCN[4]的比较，这是以前的掩模提议生成中的先进方法。我们的准确度超过这些方法8.3个点的AR。尤其是我们几乎将小目标的精度提高了一倍。</p>
<p>现有的掩码提议方法[27，28，4]是基于密集采样的图像金字塔的（例如，[27，28]中的缩放为$2^{\lbrace −2:0.5:1 \rbrace}$），使得它们是计算昂贵的。我们的方法基于FPN，速度明显加快（我们的模型运行速度为6至7FPS）。这些结果表明，我们的模型是一个通用的特征提取器，可以替代图像金字塔以用于其他多尺度检测问题。</p>
<h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7. 结论"></a>7. 结论</h2><p>我们提出了一个干净而简单的框架，用于在ConvNets内部构建特征金字塔。我们的方法比几个强大的基线和竞赛获胜者显示出了显著的改进。因此，它为特征金字塔的研究和应用提供了一个实用的解决方案，而不需要计算图像金字塔。最后，我们的研究表明，尽管深层ConvNets具有强大的表示能力以及它们对尺度变化的隐式鲁棒性，但使用金字塔表示对于明确地解决多尺度问题仍然至关重要。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 1984.</p>
<p>[2] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016.</p>
<p>[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection. In ECCV, 2016.</p>
<p>[4] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive fully convolutional networks. In ECCV, 2016.</p>
<p>[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.</p>
<p>[6] P. Dollar, R. Appel, S. Belongie, and P. Perona. Fast feature pyramids for object detection. TPAMI, 2014.</p>
<p>[7] P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 2010.</p>
<p>[8] G.GhiasiandC.C.Fowlkes.Laplacianpyramidreconstruction and refinement for semantic segmentation. In ECCV, 2016.</p>
<p>[9] S. Gidaris and N. Komodakis. Object detection via a multi-region &amp; semantic segmentation-aware CNN model. In ICCV, 2015.</p>
<p>[10] S. Gidaris and N. Komodakis. Attend refine repeat: Active box proposal generation via in-out localization. In BMVC, 2016.</p>
<p>[11] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.</p>
<p>[13] B.Hariharan,P.Arbelaez,R.Girshick,andJ.Malik.Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015.</p>
<p>[14] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn. arXiv:1703.06870, 2017.</p>
<p>[15] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.</p>
<p>[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>[17] S. Honari, J. Yosinski, P. Vincent, and C. Pal. Recombinator networks: Learning coarse-to-fine feature aggregation. In CVPR, 2016.</p>
<p>[18] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards accurate region proposal generation and joint object detection. In CVPR, 2016.</p>
<p>[19] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.</p>
<p>[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[22] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot multibox detector. In ECCV, 2016.</p>
<p>[23] W. Liu, A. Rabinovich, and A. C. Berg. ParseNet: Looking wider to see better. In ICLR workshop, 2016.</p>
<p>[24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[25] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.</p>
<p>[26] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In ECCV, 2016.</p>
<p>[27] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to segment object candidates. In NIPS, 2015.</p>
<p>[28] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár. Learning to refine object segments. In ECCV, 2016.</p>
<p>[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. PAMI, 2016.</p>
<p>[31] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image segmentation. In MIC- CAI, 2015.</p>
<p>[32] H. Rowley, S. Baluja, and T. Kanade. Human face detection in visual scenes. Technical Report CMU-CS-95-158R, Carnegie Mellon University, 1995.</p>
<p>[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.</p>
<p>[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[35] A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object detectors with online hard example mining. In CVPR, 2016.</p>
<p>[36] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[37] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.</p>
<p>[38] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE Proc. on Vision, Image, and Signal Processing, 1994.</p>
<p>[39] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC, 2016.</p>
<p>[40] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chintala, and P. Dollár. A multipath network for object detection. In BMVC, 2016. 10</p>
]]></content>
    
    <summary type="html">
    
      Feature Pyramid Networks for Object Detection论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的优化方法</title>
    <link href="http://noahsnail.com/2018/03/19/2018-03-19-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>http://noahsnail.com/2018/03/19/2018-03-19-深度学习中的优化方法/</id>
    <published>2018-03-19T03:33:03.000Z</published>
    <updated>2018-03-20T02:01:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-优化算法"><a href="#1-优化算法" class="headerlink" title="1. 优化算法"></a>1. 优化算法</h2><h2 id="2-SGD"><a href="#2-SGD" class="headerlink" title="2. SGD"></a>2. SGD</h2><h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3. Momentum"></a>3. Momentum</h2><h2 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4. Adam"></a>4. Adam</h2><h2 id="5-Adamgrad"><a href="#5-Adamgrad" class="headerlink" title="5. Adamgrad"></a>5. Adamgrad</h2>]]></content>
    
    <summary type="html">
    
      深度学习中的优化方法
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(三)——丢弃法(gluon)</title>
    <link href="http://noahsnail.com/2018/03/15/2018-03-15-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%89)%E2%80%94%E2%80%94%E4%B8%A2%E5%BC%83%E6%B3%95(gluon)/"/>
    <id>http://noahsnail.com/2018/03/15/2018-03-15-动手学深度学习(三)——丢弃法(gluon)/</id>
    <published>2018-03-15T10:53:32.000Z</published>
    <updated>2018-03-15T10:55:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="定义模型并添加丢弃层"><a href="#定义模型并添加丢弃层" class="headerlink" title="定义模型并添加丢弃层"></a>定义模型并添加丢弃层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义模型</span></div><div class="line">net = nn.Sequential()</div><div class="line"><span class="comment"># 丢弃概率</span></div><div class="line">drop_prob1 = <span class="number">0.2</span></div><div class="line">drop_prob2 = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 添加层</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    <span class="comment"># 将输入数据展开</span></div><div class="line">    net.add(nn.Flatten())</div><div class="line">    <span class="comment"># 第一个全连接层</span></div><div class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</div><div class="line">    <span class="comment"># 添加丢弃层</span></div><div class="line">    net.add(nn.Dropout(drop_prob1))</div><div class="line">    <span class="comment"># 第二个全连接层</span></div><div class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</div><div class="line">    <span class="comment"># 添加丢弃层</span></div><div class="line">    net.add(nn.Dropout(drop_prob2))</div><div class="line">    <span class="comment"># 定义输出层</span></div><div class="line">    net.add(nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 初始化模型参数</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="读取数据并训练"><a href="#读取数据并训练" class="headerlink" title="读取数据并训练"></a>读取数据并训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 加载数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div><div class="line"></div><div class="line"><span class="comment"># 优化</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 梯度反向传播</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 更新梯度</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 记录训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 记录训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 计算测试准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.817475, Train acc 0.697349, Test acc 0.778145
Epoch 1. Loss: 0.515098, Train acc 0.810831, Test acc 0.847456
Epoch 2. Loss: 0.458402, Train acc 0.833450, Test acc 0.823918
Epoch 3. Loss: 0.419452, Train acc 0.846554, Test acc 0.862079
Epoch 4. Loss: 0.396483, Train acc 0.854067, Test acc 0.874499
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(三)——丢弃法(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(三)——丢弃法(从零开始)</title>
    <link href="http://noahsnail.com/2018/03/14/2018-03-14-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%89)%E2%80%94%E2%80%94%E4%B8%A2%E5%BC%83%E6%B3%95(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/03/14/2018-03-14-动手学深度学习(三)——丢弃法(从零开始)/</id>
    <published>2018-03-14T11:02:04.000Z</published>
    <updated>2018-03-15T10:43:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="丢弃法的概念"><a href="#丢弃法的概念" class="headerlink" title="丢弃法的概念"></a>丢弃法的概念</h2><p>在现代神经网络中，我们所指的丢弃法，通常是对输入层或者隐含层做以下操作：</p>
<ul>
<li>随机选择一部分该层的输出作为丢弃元素；</li>
<li>把丢弃元素乘以0；</li>
<li>把非丢弃元素拉伸。</li>
</ul>
<h2 id="丢弃法的实现"><a href="#丢弃法的实现" class="headerlink" title="丢弃法的实现"></a>丢弃法的实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy, SGD</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 实现dropout</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_probability)</span>:</span></div><div class="line">    <span class="comment"># 计算保留数据的比例</span></div><div class="line">    keep_probability = <span class="number">1</span> - drop_probability</div><div class="line">    <span class="comment"># 确保drop_probability的输入合法</span></div><div class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= keep_probability &lt;= <span class="number">1</span></div><div class="line">    <span class="comment"># 丢弃所有元素</span></div><div class="line">    <span class="keyword">if</span> keep_probability == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span> X.zeros_like()</div><div class="line">    <span class="comment"># 随机生成一个相同纬度的矩阵, 根据随机值和keep_probability的对比确定是否丢弃该元素</span></div><div class="line">    mask = nd.random.uniform(<span class="number">0</span>, <span class="number">1.0</span>, X.shape, ctx=X.context) &lt; keep_probability</div><div class="line">    <span class="comment"># 保证 E[dropout(X)] == X, 对剩下的数据进行缩放</span></div><div class="line">    scale = <span class="number">1</span> / keep_probability</div><div class="line">    <span class="keyword">return</span> mask * X * scale</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试dropout</span></div><div class="line">A = nd.arange(<span class="number">20</span>).reshape((<span class="number">5</span>,<span class="number">4</span>))</div><div class="line">dropout(A, <span class="number">0.0</span>)</div></pre></td></tr></table></figure>
<pre><code>[[  0.   1.   2.   3.]
 [  4.   5.   6.   7.]
 [  8.   9.  10.  11.]
 [ 12.  13.  14.  15.]
 [ 16.  17.  18.  19.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dropout(A, <span class="number">1.0</span>)</div></pre></td></tr></table></figure>
<pre><code>[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dropout(A, <span class="number">0.5</span>)</div></pre></td></tr></table></figure>
<pre><code>[[  0.   2.   4.   0.]
 [  8.   0.  12.   0.]
 [ 16.  18.   0.   0.]
 [  0.   0.   0.  30.]
 [  0.  34.  36.   0.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><h2 id="丢弃法的本质"><a href="#丢弃法的本质" class="headerlink" title="丢弃法的本质"></a>丢弃法的本质</h2><p>一般来说，在集成学习里，我们可以对训练数据集有放回地采样若干次并分别训练若干个不同的分类器；测试时，把这些分类器的结果集成一下作为最终分类结果。事实上，丢弃法在模拟集成学习。丢弃法实质上是对每一个这样的数据集分别训练一个原神经网络子集的分类器。与一般的集成学习不同，这里每个原神经网络子集的分类器用的是同一套参数。因此丢弃法只是在模拟集成学习。使用丢弃法的神经网络实质上是对输入层和隐含层的参数做了正则化：学到的参数使得原神经网络不同子集在训练数据上都尽可能表现良好。</p>
<h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 加载数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="含两个隐藏层的多层感知机"><a href="#含两个隐藏层的多层感知机" class="headerlink" title="含两个隐藏层的多层感知机"></a>含两个隐藏层的多层感知机</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型输入大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"></div><div class="line"><span class="comment"># 模型输出大小</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 第一个隐藏层节点数量</span></div><div class="line">num_hidden1 = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 第二个隐藏层节点数量</span></div><div class="line">num_hidden2 = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 随机数据时的标准差</span></div><div class="line">weight_scale = <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 第一个隐藏层权重</span></div><div class="line">W1 = nd.random_normal(shape=(num_inputs, num_hidden1), scale=weight_scale)</div><div class="line"><span class="comment"># 第一个隐藏层偏置</span></div><div class="line">b1 = nd.zeros(num_hidden1)</div><div class="line"></div><div class="line"><span class="comment"># 第二个隐藏层权重</span></div><div class="line">W2 = nd.random_normal(shape=(num_hidden1, num_hidden2), scale=weight_scale)</div><div class="line"><span class="comment"># 第二个隐藏层偏置</span></div><div class="line">b2 = nd.zeros(num_hidden2)</div><div class="line"></div><div class="line"><span class="comment"># 输出层权重</span></div><div class="line">W3 = nd.random_normal(shape=(num_hidden2, num_outputs), scale=weight_scale)</div><div class="line"><span class="comment"># 输出层偏置</span></div><div class="line">b3 = nd.zeros(num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W1, b1, W2, b2, W3, b3]</div><div class="line"></div><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="定义包含丢弃层的模型"><a href="#定义包含丢弃层的模型" class="headerlink" title="定义包含丢弃层的模型"></a>定义包含丢弃层的模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 第一个隐藏层的丢弃概率</span></div><div class="line">drop_prob1 = <span class="number">0.2</span></div><div class="line"><span class="comment"># 第二个隐藏层的丢弃概率</span></div><div class="line">drop_prob2 = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</div><div class="line">    <span class="comment"># 第一层全连接</span></div><div class="line">    h1 = nd.relu(nd.dot(X, W1) + b1)</div><div class="line">    <span class="comment"># 在第一层全连接后添加丢弃层</span></div><div class="line">    h1 = dropout(h1, drop_prob1)</div><div class="line">    <span class="comment"># 第二层全连接</span></div><div class="line">    h2 = nd.relu(nd.dot(h1, W2) + b2)</div><div class="line">    <span class="comment"># 在第二层全连接后添加丢弃层</span></div><div class="line">    h2 = dropout(h2, drop_prob2)</div><div class="line">    <span class="comment"># 返回输出</span></div><div class="line">    <span class="keyword">return</span> nd.dot(h2, W3) + b3</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># 定义学习率</span></div><div class="line">learning_rate = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 梯度反向传播</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># SGD更新梯度</span></div><div class="line">        SGD(params, learning_rate / batch_size)</div><div class="line">        <span class="comment"># 记录训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 记录训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 计算测试准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 1.221062, Train acc 0.528746, Test acc 0.754006
Epoch 1. Loss: 0.598503, Train acc 0.774890, Test acc 0.813101
Epoch 2. Loss: 0.499490, Train acc 0.818493, Test acc 0.840244
Epoch 3. Loss: 0.457343, Train acc 0.832699, Test acc 0.835036
Epoch 4. Loss: 0.426575, Train acc 0.846070, Test acc 0.849159
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(三)——丢弃法(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——正则化(gluon)</title>
    <link href="http://noahsnail.com/2018/03/09/2018-03-09-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96(gluon)/"/>
    <id>http://noahsnail.com/2018/03/09/2018-03-09-动手学深度学习(二)——正则化(gluon)/</id>
    <published>2018-03-09T10:07:25.000Z</published>
    <updated>2018-03-09T10:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="高维线性回归数据集"><a href="#高维线性回归数据集" class="headerlink" title="高维线性回归数据集"></a>高维线性回归数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">random.seed(<span class="number">2</span>)</div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># 输入数据特征维度</span></div><div class="line">num_inputs = <span class="number">200</span></div><div class="line"></div><div class="line"><span class="comment"># 实际权重</span></div><div class="line">true_w = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 实际偏置</span></div><div class="line">true_b = <span class="number">0.05</span></div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">X = nd.random_normal(shape=(num_train + num_test, num_inputs))</div><div class="line">y = nd.dot(X, true_w) + true_b</div><div class="line"></div><div class="line"><span class="comment"># 添加随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据和测试数据</span></div><div class="line">X_train, X_test = X[:num_train, :], X[num_train:, :]</div><div class="line">y_train, y_test = y[:num_train], y[num_train:]</div></pre></td></tr></table></figure>
<h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># 创建数据集</span></div><div class="line">dataset_train = gluon.data.ArrayDataset(X_train, y_train)</div><div class="line"></div><div class="line"><span class="comment"># 读取数据</span></div><div class="line">data_iter = gluon.data.DataLoader(dataset_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 损失函数</span></div><div class="line">square_loss = gluon.loss.L2Loss()</div><div class="line"></div><div class="line"><span class="comment"># 测试</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(net, X, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> square_loss(net(X), y).mean().asscalar()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(weight_decay)</span>:</span></div><div class="line">    <span class="comment"># 定义训练的迭代周期</span></div><div class="line">    epochs = <span class="number">10</span></div><div class="line">    <span class="comment"># 定义学习率</span></div><div class="line">    learning_rate = <span class="number">0.005</span></div><div class="line">    <span class="comment"># 定义网络</span></div><div class="line">    net = gluon.nn.Sequential()</div><div class="line">    <span class="keyword">with</span> net.name_scope():</div><div class="line">        net.add(gluon.nn.Dense(<span class="number">1</span>))</div><div class="line">    <span class="comment">#net.collect_params().initialize(mx.init.Normal(sigma=1))</span></div><div class="line">    <span class="comment"># 初始化网络参数</span></div><div class="line">    net.initialize(mx.init.Normal(sigma=<span class="number">1</span>))</div><div class="line">    <span class="comment"># SGD训练, 使用权重衰减代替L2正则化</span></div><div class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: learning_rate, <span class="string">'wd'</span>: weight_decay&#125;)</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">            <span class="comment"># 记录梯度</span></div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                <span class="comment"># 计算预测值</span></div><div class="line">                output = net(data)</div><div class="line">                <span class="comment"># 计算损失</span></div><div class="line">                loss = square_loss(output, label)</div><div class="line">            <span class="comment"># 反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新权重</span></div><div class="line">            trainer.step(batch_size)</div><div class="line">            <span class="comment"># 训练损失</span></div><div class="line">            train_loss.append(test(net, X_train, y_train))</div><div class="line">            <span class="comment"># 测试损失</span></div><div class="line">            test_loss.append(test(net, X_test, y_test))</div><div class="line">    <span class="comment"># 绘制图像</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>,<span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> (<span class="string">'learned w[:10]:'</span>, net[<span class="number">0</span>].weight.data()[:,:<span class="number">10</span>], <span class="string">'\nlearned b:'</span>, net[<span class="number">0</span>].bias.data())</div></pre></td></tr></table></figure>
<h2 id="训练模型并观察过拟合"><a href="#训练模型并观察过拟合" class="headerlink" title="训练模型并观察过拟合"></a>训练模型并观察过拟合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-bd038b77c2ef488a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Overfitting"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 1.04817235 -0.02568591  0.86764944  0.29322273  0.01006198 -0.56152564
    0.38436413 -0.3084037  -2.32450151  0.03733355]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;\nlearned b:&apos;, 
 [ 0.79914868]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="使用Gluon的正则化"><a href="#使用Gluon的正则化" class="headerlink" title="使用Gluon的正则化"></a>使用Gluon的正则化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">5</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8d88c219f34f10ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Normal"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 0.00107634 -0.00052574  0.00450234 -0.00110544 -0.00683913 -0.00181657
   -0.00530634  0.00512847 -0.00742552 -0.00058494]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;\nlearned b:&apos;, 
 [ 0.00449433]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="可用权重衰减代替L2正则化的原因"><a href="#可用权重衰减代替L2正则化的原因" class="headerlink" title="可用权重衰减代替L2正则化的原因"></a>可用权重衰减代替L2正则化的原因</h2><p><img src="http://upload-images.jianshu.io/upload_images/3232548-c9da036502d1949d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="推导"></p>
<p>注：图片来自Gluon社区。</p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——正则化(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——正则化(从零开始)</title>
    <link href="http://noahsnail.com/2018/03/08/2018-03-08-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/03/08/2018-03-08-动手学深度学习(二)——正则化(从零开始)/</id>
    <published>2018-03-08T11:09:49.000Z</published>
    <updated>2018-03-08T11:12:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="高维线性回归"><a href="#高维线性回归" class="headerlink" title="高维线性回归"></a>高维线性回归</h2><p>使用线性函数$y = 0.05 + \sum_{i = 1}^p 0.01x_i +  \text{noise}$生成数据样本，噪音服从均值0和标准差为0.01的正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">random.seed(<span class="number">2</span>)</div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div></pre></td></tr></table></figure>
<h2 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># 输入数据特征维度</span></div><div class="line">num_inputs = <span class="number">200</span></div><div class="line"></div><div class="line"><span class="comment"># 实际权重</span></div><div class="line">true_w = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 实际偏置</span></div><div class="line">true_b = <span class="number">0.05</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成数据</span></div><div class="line">X = nd.random_normal(shape=(num_train + num_test, num_inputs))</div><div class="line">y = nd.dot(X, true_w) + true_b</div><div class="line"></div><div class="line"><span class="comment"># 添加随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据和测试数据</span></div><div class="line">X_train, X_test = X[:num_train, :], X[num_train:, :]</div><div class="line">y_train, y_test = y[:num_train], y[num_train:]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># 通过yield进行数据读取</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(num_examples)</span>:</span></div><div class="line">    <span class="comment"># 产生样本的索引</span></div><div class="line">    idx = list(range(num_examples))</div><div class="line">    <span class="comment"># 将索引随机打乱</span></div><div class="line">    random.shuffle(idx)</div><div class="line">    <span class="comment"># 迭代一个epoch</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</div><div class="line">        <span class="comment"># 依次取出样本的索引, 这种实现方式在num_examples/batch_size不能整除时也适用</span></div><div class="line">        j = nd.array(idx[i:min((i + batch_size), num_examples)])</div><div class="line">        <span class="comment"># 根据提供的索引取元素</span></div><div class="line">        <span class="keyword">yield</span> nd.take(X, j), nd.take(y, j)</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 随机初始化权重w</span></div><div class="line">    w = nd.random_normal(shape=(num_inputs, <span class="number">1</span>))</div><div class="line">    <span class="comment"># 偏置b初始化为0</span></div><div class="line">    b = nd.zeros((<span class="number">1</span>,))</div><div class="line">    <span class="comment"># w, b放入list里</span></div><div class="line">    params = [w, b]</div><div class="line"></div><div class="line">    <span class="comment"># 需要计算反向传播, 添加自动求导</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        param.attach_grad()</div><div class="line">    <span class="keyword">return</span> params</div></pre></td></tr></table></figure>
<h2 id="L-2-范数正则化"><a href="#L-2-范数正则化" class="headerlink" title="$L_2$ 范数正则化"></a>$L_2$ 范数正则化</h2><p>在训练时最小化函数为：$\text{loss} + \lambda \sum_{p \in \textrm{params}}|p|_2^2。$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># L2范数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2_penalty</span><span class="params">(w, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> ((w**<span class="number">2</span>).sum() + b ** <span class="number">2</span>) / <span class="number">2</span></div></pre></td></tr></table></figure>
<h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, w, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</div><div class="line"></div><div class="line"><span class="comment"># 损失函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span><span class="params">(yhat, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> (yhat - y.reshape(yhat.shape)) ** <span class="number">2</span> / <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># 梯度下降</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        param[:] = param - lr * param.grad / batch_size</div><div class="line"></div><div class="line"><span class="comment"># 测试</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(net, params, X, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> square_loss(net(X, *params), y).mean().asscalar()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(_lambda)</span>:</span></div><div class="line">    <span class="comment"># 定义训练的迭代周期</span></div><div class="line">    epochs = <span class="number">10</span></div><div class="line">    <span class="comment"># 定义学习率</span></div><div class="line">    learning_rate = <span class="number">0.005</span></div><div class="line">    <span class="comment"># 初始化参数</span></div><div class="line">    w, b = params = init_params()</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter(num_train):</div><div class="line">            <span class="comment"># 记录梯度</span></div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                 <span class="comment"># 计算预测值</span></div><div class="line">                output = net(data, *params)</div><div class="line">                <span class="comment"># 计算loss</span></div><div class="line">                loss = square_loss(output, label) + _lambda * L2_penalty(*params)</div><div class="line">            <span class="comment"># 反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新梯度</span></div><div class="line">            sgd(params, learning_rate, batch_size)</div><div class="line">            <span class="comment"># 训练损失</span></div><div class="line">            train_loss.append(test(net, params, X_train, y_train))</div><div class="line">            <span class="comment"># 测试损失</span></div><div class="line">            test_loss.append(test(net, params, X_test, y_test))</div><div class="line">            </div><div class="line">    <span class="comment"># 绘制损失图像</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> <span class="string">'learned w[:10]:'</span>, w[:<span class="number">10</span>].T, <span class="string">'learned b:'</span>, b</div></pre></td></tr></table></figure>
<h1 id="观察过拟合"><a href="#观察过拟合" class="headerlink" title="观察过拟合"></a>观察过拟合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-bd038b77c2ef488a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Overfitting"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 1.04817176 -0.02568593  0.86764956  0.29322267  0.01006179 -0.56152576
    0.38436398 -0.30840367 -2.32450151  0.03733355]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;learned b:&apos;, 
 [ 0.79914856]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="使用正则化"><a href="#使用正则化" class="headerlink" title="使用正则化"></a>使用正则化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">5</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8d88c219f34f10ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Normal"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 0.00107633 -0.00052574  0.00450233 -0.00110545 -0.0068391  -0.00181657
   -0.00530632  0.00512845 -0.00742549 -0.00058495]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;learned b:&apos;, 
 [ 0.00449432]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——正则化(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——欠拟合和过拟合</title>
    <link href="http://noahsnail.com/2018/03/01/2018-03-01-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>http://noahsnail.com/2018/03/01/2018-03-01-动手学深度学习(二)——欠拟合和过拟合/</id>
    <published>2018-03-01T10:49:47.000Z</published>
    <updated>2018-03-08T11:09:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h1 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h1><h3 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h3><p>机器学习模型在训练数据集上表现出的误差叫做训练误差，在任意一个测试数据样本上表现出的误差的期望值叫做泛化误差。</p>
<p>统计学习理论的一个假设是：训练数据集和测试数据集里的每一个数据样本都是从同一个概率分布中相互独立地生成出的（独立同分布假设）。</p>
<p>一个重要结论是：训练误差的降低不一定意味着泛化误差的降低。机器学习既需要降低训练误差，又需要降低泛化误差。</p>
<h3 id="欠拟合和过拟合-1"><a href="#欠拟合和过拟合-1" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h3><ul>
<li>欠拟合：机器学习模型无法得到较低训练误差。</li>
<li>过拟合：机器学习模型的训练误差远小于其在测试数据集上的误差。</li>
</ul>
<h3 id="模型的选择"><a href="#模型的选择" class="headerlink" title="模型的选择"></a>模型的选择</h3><p>模型拟合能力和误差之间的关系如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f50a8a6737e3927b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="error_model_complexity.png"></p>
<h3 id="训练数据集的大小"><a href="#训练数据集的大小" class="headerlink" title="训练数据集的大小"></a>训练数据集的大小</h3><p>一般来说，如果训练数据集过小，特别是比模型参数数量更小时，过拟合更容易发生。除此之外，泛化误差不会随训练数据集里样本数量增加而增大。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-7e957140a2d8242c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="model_vs_data.png"></p>
<h3 id="多项式拟合"><a href="#多项式拟合" class="headerlink" title="多项式拟合"></a>多项式拟合</h3><p>给定一个<strong>标量</strong>数据点集合<code>x</code>和对应的标量目标值<code>y</code>，多项式拟合的目标是找一个K阶多项式，其由向量<code>w</code>和位移<code>b</code>组成，来最好地近似每个样本<code>x</code>和<code>y</code>。用数学符号来表示就是我们将学<code>w</code>和<code>b</code>来预测</p>
<p>$$\hat{y} = b + \sum_{k=1}^K x^k w_k$$</p>
<p>并以平方误差为损失函数，一阶多项式拟合又叫线性拟合。</p>
<h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><p>使用二阶多项式来生成每一个数据样本，$y=1.2x−3.4x^2+5.6x^3+5.0+noise$，噪音服从均值0和标准差为0.1的正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">100</span></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"><span class="comment"># 多项式权重</span></div><div class="line">true_w = [<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>]</div><div class="line"><span class="comment"># 多项式偏置</span></div><div class="line">true_b = <span class="number">5.0</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成随机数据x</span></div><div class="line">x = nd.random.normal(shape=(num_train + num_test, <span class="number">1</span>))</div><div class="line"><span class="comment"># 计算x的多项式值</span></div><div class="line">X = nd.concat(x, nd.power(x, <span class="number">2</span>), nd.power(x, <span class="number">3</span>))</div><div class="line"><span class="comment"># 计算y</span></div><div class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_w[<span class="number">2</span>] * X[:, <span class="number">2</span>] + true_b</div><div class="line"><span class="comment"># 查看数据</span></div><div class="line">(<span class="string">'x:'</span>, x[:<span class="number">5</span>], <span class="string">'X:'</span>, X[:<span class="number">5</span>], <span class="string">'y:'</span>, y[:<span class="number">5</span>])</div></pre></td></tr></table></figure>
<pre><code>(200L,)
</code></pre><h2 id="定义训练和测试步骤"><a href="#定义训练和测试步骤" class="headerlink" title="定义训练和测试步骤"></a>定义训练和测试步骤</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义训练过程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X_train, X_test, y_train, y_test)</span>:</span></div><div class="line">    <span class="comment"># 定义线性回归模型</span></div><div class="line">    net = gluon.nn.Sequential()</div><div class="line">    <span class="keyword">with</span> net.name_scope():</div><div class="line">        net.add(gluon.nn.Dense(<span class="number">1</span>))</div><div class="line">    <span class="comment"># 权重初始化</span></div><div class="line">    net.initialize()</div><div class="line">    <span class="comment"># 学习率</span></div><div class="line">    learning_rate = <span class="number">0.01</span></div><div class="line">    <span class="comment"># 迭代周期</span></div><div class="line">    epochs = <span class="number">100</span></div><div class="line">    <span class="comment"># 训练的批数据大小</span></div><div class="line">    batch_size = min(<span class="number">10</span>, y_train.shape[<span class="number">0</span>])</div><div class="line">    <span class="comment"># 创建训练数据集</span></div><div class="line">    dataset_train = gluon.data.ArrayDataset(X_train, y_train)</div><div class="line">    <span class="comment"># 读取数据</span></div><div class="line">    data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 训练方法SGD</span></div><div class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: learning_rate&#125;)</div><div class="line">    <span class="comment"># 定义损失函数</span></div><div class="line">    square_loss = gluon.loss.L2Loss()</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="comment"># 进行训练</span></div><div class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter_train:</div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                <span class="comment"># 进行预测</span></div><div class="line">                output = net(data)</div><div class="line">                <span class="comment"># 计算预测值与实际值之间的损失</span></div><div class="line">                loss = square_loss(output, label)</div><div class="line">            <span class="comment"># 损失进行反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新权重</span></div><div class="line">            trainer.step(batch_size)</div><div class="line">        <span class="comment"># 保存训练损失</span></div><div class="line">        train_loss.append(square_loss(net(X_train), y_train).mean().asscalar())</div><div class="line">        <span class="comment"># 保存测试损失</span></div><div class="line">        test_loss.append(square_loss(net(X_test), y_test).mean().asscalar())</div><div class="line">    <span class="comment"># 绘制损失</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>,<span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> (<span class="string">'learned weight'</span>, net[<span class="number">0</span>].weight.data(), <span class="string">'learned bias'</span>, net[<span class="number">0</span>].bias.data())</div></pre></td></tr></table></figure>
<h2 id="三阶多项式拟合（正常）"><a href="#三阶多项式拟合（正常）" class="headerlink" title="三阶多项式拟合（正常）"></a>三阶多项式拟合（正常）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(X[:num_train, :], X[num_train:, :], y[:num_train], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-4500dba51617c3ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 1.22117233 -3.39606118  5.59531116]]
 &lt;NDArray 1x3 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [ 4.98550272]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="线性拟合（欠拟合）"><a href="#线性拟合（欠拟合）" class="headerlink" title="线性拟合（欠拟合）"></a>线性拟合（欠拟合）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(x[:num_train, :], x[num_train:, :], y[:num_train], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cac456292c149670.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 19.74101448]]
 &lt;NDArray 1x1 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [-0.23861444]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="训练量不足（过拟合）"><a href="#训练量不足（过拟合）" class="headerlink" title="训练量不足（过拟合）"></a>训练量不足（过拟合）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(X[<span class="number">0</span>:<span class="number">2</span>, :], X[num_train:, :], y[<span class="number">0</span>:<span class="number">2</span>], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cc94f0d732ebd573.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 3.10832024 -0.740421    4.85165691]]
 &lt;NDArray 1x3 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [ 0.29450524]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>训练误差的降低并不一定意味着泛化误差的降低。</li>
<li>欠拟合和过拟合都是需要尽量避免的。我们要注意模型的选择和训练量的大小。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——欠拟合和过拟合
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——多层感知机(gluon)</title>
    <link href="http://noahsnail.com/2018/02/28/2018-02-28-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(gluon)/"/>
    <id>http://noahsnail.com/2018/02/28/2018-02-28-动手学深度学习(二)——多层感知机(gluon)/</id>
    <published>2018-02-28T08:42:14.000Z</published>
    <updated>2018-02-28T08:42:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 按顺序堆叠网络层</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># name_scope作用, 方便管理参数名称</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    net.add(gluon.nn.Flatten())</div><div class="line">    net.add(gluon.nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</div><div class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 参数初始化</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练器和优化方法</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.819048, Train acc 0.728666, Test acc 0.768530
Epoch 1. Loss: 0.550646, Train acc 0.808644, Test acc 0.823618
Epoch 2. Loss: 0.488554, Train acc 0.829210, Test acc 0.845553
Epoch 3. Loss: 0.457407, Train acc 0.839493, Test acc 0.842448
Epoch 4. Loss: 0.438059, Train acc 0.845486, Test acc 0.852063
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——多层感知机(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
