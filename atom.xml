<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="noahsnail.com/"/>
  <updated>2017-04-23T13:07:44.000Z</updated>
  <id>noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>numpy的基本用法(一)——基本运算</title>
    <link href="noahsnail.com/2017/04/23/2017-4-23-numpy%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/"/>
    <id>noahsnail.com/2017/04/23/2017-4-23-numpy的基本用法(一)——基本运算/</id>
    <published>2017-04-23T12:45:49.000Z</published>
    <updated>2017-04-23T13:07:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于numpy的一些基本运算的用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Test 1</div><div class="line"># 定义矩阵</div><div class="line">arr = np.array([[1, 2, 3],</div><div class="line">                [4, 5, 6]])</div><div class="line">print arr</div><div class="line"></div><div class="line"># Test 1 Result</div><div class="line">[[1 2 3]</div><div class="line"> [4 5 6]]</div><div class="line"></div><div class="line"># Test 2</div><div class="line"># 矩阵的维度</div><div class="line">print &apos;number of dim: &apos;, arr.ndim</div><div class="line"># 矩阵的shape,即每一维度上的元素个数</div><div class="line">print &apos;shape: &apos;, arr.shape</div><div class="line"># 矩阵的元素总数</div><div class="line">print &apos;size: &apos;, arr.size</div><div class="line"># 矩阵的元素类型</div><div class="line">print &apos;dtype: &apos;, arr.dtype</div><div class="line"></div><div class="line"># Test 2 Result</div><div class="line">number of dim:  2</div><div class="line">shape:  (2, 3)</div><div class="line">size:  6</div><div class="line">dtype:  int64</div><div class="line"></div><div class="line"># Test 3</div><div class="line"># 定义矩阵及矩阵的元素类型——int32, int64, float32, float64</div><div class="line">a = np.array([1, 2, 3], dtype = np.int32)</div><div class="line">print a</div><div class="line">print a.ndim</div><div class="line">print a.shape</div><div class="line">print a.size</div><div class="line">print a.dtype</div><div class="line"></div><div class="line"></div><div class="line"># Test 3 Result</div><div class="line">[1 2 3]</div><div class="line">1</div><div class="line">(3,)</div><div class="line">3</div><div class="line">int32</div><div class="line"></div><div class="line"># Test 4</div><div class="line"># 定义零矩阵</div><div class="line">z = np.zeros((3, 4), dtype = np.int16)</div><div class="line">print z</div><div class="line">print z.dtype</div><div class="line"></div><div class="line"># 定义空矩阵</div><div class="line">n = np.empty((3, 4))</div><div class="line">print n</div><div class="line"></div><div class="line"># Test 4 Result</div><div class="line">[[0 0 0 0]</div><div class="line"> [0 0 0 0]</div><div class="line"> [0 0 0 0]]</div><div class="line">int16</div><div class="line"></div><div class="line">[[ 0.  0.  0.  0.]</div><div class="line"> [ 0.  0.  0.  0.]</div><div class="line"> [ 0.  0.  0.  0.]]</div><div class="line"></div><div class="line"># Test 5</div><div class="line"># 定义向量, 10-20之间, 元素间隔为2, 左闭右开</div><div class="line">a = np.arange(10, 20, 2)</div><div class="line">print a</div><div class="line"></div><div class="line"># 定义向量并转为矩阵</div><div class="line">b = np.arange(12).reshape((3, 4))</div><div class="line">print b</div><div class="line"></div><div class="line"># 定义向量, 类型是线性间隔</div><div class="line">a = np.linspace(1, 10, 6).reshape((2, 3))</div><div class="line">print a</div><div class="line"></div><div class="line"># Test 5 Result</div><div class="line">[10 12 14 16 18]</div><div class="line"></div><div class="line">[[ 0  1  2  3]</div><div class="line"> [ 4  5  6  7]</div><div class="line"> [ 8  9 10 11]]</div><div class="line"></div><div class="line">[[  1.    2.8   4.6]</div><div class="line"> [  6.4   8.2  10. ]]</div><div class="line"></div><div class="line"># Test 6</div><div class="line"># 矩阵的加、减、点乘、平方</div><div class="line">a = np.array([10, 20, 30, 40])</div><div class="line">b = np.arange(4)</div><div class="line">c = a - b</div><div class="line">d = a + b</div><div class="line">print a, b</div><div class="line">print c, d</div><div class="line">e = a * b</div><div class="line">print e</div><div class="line">f = e ** 2</div><div class="line">print f</div><div class="line"></div><div class="line"># Test 6 Result</div><div class="line">[10 20 30 40] [0 1 2 3]</div><div class="line">[10 19 28 37] [10 21 32 43]</div><div class="line">[  0  20  60 120]</div><div class="line">[    0   400  3600 14400]</div><div class="line"></div><div class="line"># Test 7</div><div class="line"># 矩阵的三角运算——sin, cos, tan</div><div class="line">sin = 10 * np.sin(a)</div><div class="line">print sin</div><div class="line"></div><div class="line"># 矩阵的判断</div><div class="line">print b &lt; 3</div><div class="line">print b == 3</div><div class="line"></div><div class="line"># Test 7 Result</div><div class="line">[-5.44021111  9.12945251 -9.88031624  7.4511316 ]</div><div class="line"></div><div class="line">[ True  True  True False]</div><div class="line">[False False False  True]</div><div class="line"></div><div class="line"># Test 8</div><div class="line"># 矩阵的点乘及乘法</div><div class="line">a = [ [1, 1], [0, 1]]</div><div class="line">b = np.arange(4).reshape((2, 2))</div><div class="line">c = a * b</div><div class="line">d = np.dot(a, b)</div><div class="line">print c</div><div class="line">print d</div><div class="line"></div><div class="line"># Test 8 Result</div><div class="line">[[0 1]</div><div class="line"> [0 3]]</div><div class="line">[[2 4]</div><div class="line"> [2 3]]</div><div class="line"></div><div class="line"># Test 9</div><div class="line"># np.random返回随机的浮点数，在半开区间 [0.0, 1.0)</div><div class="line"># 定义随机矩阵</div><div class="line">a = np.random.random((2, 4))</div><div class="line">print a</div><div class="line"></div><div class="line"># Test 9 Result</div><div class="line">[[ 0.93213483  0.58102186  0.98259187  0.27387014]</div><div class="line"> [ 0.43796835  0.98195976  0.29343791  0.94752226]]</div><div class="line"></div><div class="line"># Test 10</div><div class="line"># 矩阵的求和, 最小值, 最大值</div><div class="line">print np.sum(a)</div><div class="line">print np.min(a)</div><div class="line">print np.max(a)</div><div class="line"></div><div class="line"># 矩阵某一维度的求和, 最小值, 最大值, 0是列, 1是行</div><div class="line">print np.sum(a, axis = 1)</div><div class="line">print np.max(a, axis = 1)</div><div class="line">print np.min(a, axis = 0)</div><div class="line"></div><div class="line"># Test 10 Result</div><div class="line">5.43050697485</div><div class="line">0.273870140282</div><div class="line">0.982591870104</div><div class="line"></div><div class="line">[ 2.7696187   2.66088828]</div><div class="line">[ 0.98259187  0.98195976]</div><div class="line">[ 0.43796835  0.58102186  0.29343791  0.27387014]</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      numpy的基本用法(一)——基本运算
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(十)——保存神经网络参数和加载神经网络参数</title>
    <link href="noahsnail.com/2017/04/20/2017-4-20-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%8D%81)%E2%80%94%E2%80%94%E4%BF%9D%E5%AD%98%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%92%8C%E5%8A%A0%E8%BD%BD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0/"/>
    <id>noahsnail.com/2017/04/20/2017-4-20-tensorflow的基本用法(十)——保存神经网络参数和加载神经网络参数/</id>
    <published>2017-04-20T11:45:30.000Z</published>
    <updated>2017-04-20T11:59:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是使用tensorfl保存神经网络参数和加载神经网络参数，不包括神经网络框架。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 保存神经网络参数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_para</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># 定义权重参数</span></div><div class="line">	W = tf.Variable([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype = tf.float32, name = <span class="string">'weights'</span>)</div><div class="line">	<span class="comment"># 定义偏置参数</span></div><div class="line">	b = tf.Variable([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], dtype = tf.float32, name = <span class="string">'biases'</span>)</div><div class="line">	<span class="comment"># 参数初始化</span></div><div class="line">	init = tf.global_variables_initializer()</div><div class="line">	<span class="comment"># 定义保存参数的saver</span></div><div class="line">	saver = tf.train.Saver()</div><div class="line"></div><div class="line">	<span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">		sess.run(init)</div><div class="line">		<span class="comment"># 保存session中的数据</span></div><div class="line">		save_path = saver.save(sess, <span class="string">'my_net/save_net.ckpt'</span>)</div><div class="line">		<span class="comment"># 输出保存路径</span></div><div class="line">		<span class="keyword">print</span> <span class="string">'Save to path: '</span>, save_path</div><div class="line"></div><div class="line"><span class="comment"># 恢复神经网络参数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_para</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># 定义权重参数</span></div><div class="line">	W = tf.Variable(np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>)), dtype = tf.float32, name = <span class="string">'weights'</span>)</div><div class="line">	<span class="comment"># 定义偏置参数</span></div><div class="line">	b = tf.Variable(np.arange(<span class="number">3</span>).reshape((<span class="number">1</span>, <span class="number">3</span>)), dtype = tf.float32, name = <span class="string">'biases'</span>)</div><div class="line">	<span class="comment"># 定义提取参数的saver</span></div><div class="line">	saver = tf.train.Saver()</div><div class="line"></div><div class="line">	<span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">		<span class="comment"># 加载文件中的参数数据，会根据name加载数据并保存到变量W和b中</span></div><div class="line">		save_path = saver.restore(sess, <span class="string">'my_net/save_net.ckpt'</span>)</div><div class="line">		<span class="comment"># 输出保存路径</span></div><div class="line">		<span class="keyword">print</span> <span class="string">'Weights: '</span>, sess.run(W)</div><div class="line">		<span class="keyword">print</span> <span class="string">'biases:  '</span>, sess.run(b)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># save_para()</span></div><div class="line">restore_para()</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># save</div><div class="line">Save to path:  my_net/save_net.ckpt</div><div class="line"></div><div class="line"></div><div class="line"># restore</div><div class="line">Weights:  [[ 1.  2.  3.]</div><div class="line"> [ 4.  5.  6.]]</div><div class="line">biases:   [[ 1.  2.  3.]]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(十)——保存神经网络参数和加载神经网络参数
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(九)——定义卷积神经网络训练MNIST</title>
    <link href="noahsnail.com/2017/04/19/2017-4-19-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B9%9D)%E2%80%94%E2%80%94%E5%AE%9A%E4%B9%89%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83MNIST/"/>
    <id>noahsnail.com/2017/04/19/2017-4-19-tensorflow的基本用法(九)——定义卷积神经网络训练MNIST/</id>
    <published>2017-04-19T14:40:40.000Z</published>
    <updated>2017-04-19T14:43:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是使用tensorflow定义卷积神经网络来训练MNIST数据集。定义的神经网络结构为两个卷积层+两个连接层，每个卷积层包括卷积层、ReLU层和Pooling层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"></div><div class="line"><span class="comment"># 定义神经网络模型的评估部分</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(test_xs, test_ys)</span>:</span></div><div class="line">	<span class="comment"># 使用全局变量prediction</span></div><div class="line">    <span class="keyword">global</span> prediction</div><div class="line">    <span class="comment"># 获得预测值y_pre</span></div><div class="line">    y_pre = sess.run(prediction, feed_dict = &#123; xs: test_xs, keep_prob: <span class="number">1</span>&#125;)</div><div class="line">    <span class="comment"># 判断预测值y和真实值y_中最大数的索引是否一致，y_pre的值为1-10概率</span></div><div class="line">    correct_prediction = tf.equal(tf.argmax(y_pre, <span class="number">1</span>), tf.argmax(test_ys, <span class="number">1</span>))</div><div class="line">    <span class="comment"># 定义准确率的计算</span></div><div class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">    <span class="comment"># 计算准确率</span></div><div class="line">    result = sess.run(accuracy)</div><div class="line">    <span class="keyword">return</span> result</div><div class="line"></div><div class="line"><span class="comment"># 下载mnist数据</span></div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 权重参数初始化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.truncated_normal(shape, stddev = <span class="number">0.1</span>)</div><div class="line">	<span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="comment"># 偏置参数初始化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">	initial = tf.constant(<span class="number">0.1</span>, shape = shape)</div><div class="line">	<span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="comment"># 定义卷积层</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">	<span class="comment"># stride的四个参数：[batch, height, width, channels], [batch_size, image_rows, image_cols, number_of_colors]</span></div><div class="line">	<span class="comment"># height, width就是图像的高度和宽度，batch和channels在卷积层中通常设为1</span></div><div class="line">	<span class="keyword">return</span> tf.nn.conv2d(x, W, strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">	<span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 输入输出数据的placeholder</span></div><div class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line"><span class="comment"># dropout的比例</span></div><div class="line">keep_prob = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line"><span class="comment"># 对数据进行重新排列，形成图像</span></div><div class="line">x_image = tf.reshape(xs, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="keyword">print</span> x_image.shape</div><div class="line"></div><div class="line"><span class="comment"># 卷积层一</span></div><div class="line"><span class="comment"># patch为5*5，in_size为1，即图像的厚度，如果是彩色，则为3，32是out_size，输出的大小</span></div><div class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</div><div class="line">b_conv1 = bias_variable([<span class="number">32</span>])</div><div class="line"><span class="comment"># ReLU操作，输出大小为28*28*32</span></div><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line"><span class="comment"># Pooling操作，输出大小为14*14*32</span></div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div><div class="line"></div><div class="line"><span class="comment"># 卷积层二</span></div><div class="line"><span class="comment"># patch为5*5，in_size为32，即图像的厚度，64是out_size，输出的大小</span></div><div class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line">b_conv2 = bias_variable([<span class="number">64</span>])</div><div class="line"><span class="comment"># ReLU操作，输出大小为14*14*64</span></div><div class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line"><span class="comment"># Pooling操作，输出大小为7*7*64</span></div><div class="line">h_pool2 = max_pool_2x2(h_conv2)</div><div class="line"></div><div class="line"><span class="comment"># 全连接层一</span></div><div class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</div><div class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</div><div class="line"><span class="comment"># 输入数据变换</span></div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</div><div class="line"><span class="comment"># 进行全连接操作</span></div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div><div class="line"><span class="comment"># 防止过拟合，dropout</span></div><div class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 全连接层二</span></div><div class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</div><div class="line">b_fc2 = bias_variable([<span class="number">10</span>])</div><div class="line"><span class="comment"># 预测</span></div><div class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</div><div class="line"></div><div class="line"><span class="comment"># 计算loss</span></div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[<span class="number">1</span>])) </div><div class="line"><span class="comment"># 神经网络训练</span></div><div class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(cross_entropy)</div><div class="line"></div><div class="line"><span class="comment"># 定义Session</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># 根据tensorflow版本选择初始化函数</span></div><div class="line"><span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</div><div class="line">    init = tf.initialize_all_variables()</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    init = tf.global_variables_initializer()</div><div class="line"><span class="comment"># 执行初始化</span></div><div class="line">sess.run(init)</div><div class="line"></div><div class="line"><span class="comment"># 进行训练迭代</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    <span class="comment"># 取出mnist数据集中的100个数据</span></div><div class="line">	batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">	<span class="comment"># 执行训练过程并传入真实数据</span></div><div class="line">	sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys, keep_prob: <span class="number">0.5</span>&#125;)</div><div class="line">	<span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">		<span class="keyword">print</span> compute_accuracy(mnist.test.images, mnist.test.labels)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">$ python practice4.py</div><div class="line">Extracting MNIST_data/train-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/train-labels-idx1-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</div><div class="line">0.0823</div><div class="line">0.875</div><div class="line">0.9243</div><div class="line">0.9427</div><div class="line">0.9502</div><div class="line">0.9573</div><div class="line">0.9595</div><div class="line">0.9623</div><div class="line">0.963</div><div class="line">0.9687</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(九)——定义卷积神经网络训练MNIST
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(八)——dropout的作用</title>
    <link href="noahsnail.com/2017/04/17/2017-4-17-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AB)%E2%80%94%E2%80%94dropout%E7%9A%84%E4%BD%9C%E7%94%A8/"/>
    <id>noahsnail.com/2017/04/17/2017-4-17-tensorflow的基本用法(八)——dropout的作用/</id>
    <published>2017-04-17T13:45:48.000Z</published>
    <updated>2017-04-17T13:51:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是介绍tensorflow中dropout的作用，dropout主要是用来防止过拟合，即提供模型的泛化能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</div><div class="line"></div><div class="line"><span class="comment"># 加载数据 </span></div><div class="line">digits = load_digits()</div><div class="line"><span class="comment"># 输入数据</span></div><div class="line">X = digits.data</div><div class="line"><span class="comment"># 输出数据</span></div><div class="line">y = digits.target</div><div class="line"><span class="comment"># 标签变换</span></div><div class="line">y = LabelBinarizer().fit_transform(y)</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.3</span>)</div><div class="line"></div><div class="line"><span class="comment"># 创建一个神经网络层</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(input, in_size, out_size, layer_name, activation_function = None)</span>:</span></div><div class="line">	<span class="string">"""</span></div><div class="line">	:param input:</div><div class="line">		神经网络层的输入</div><div class="line">	:param in_zize:</div><div class="line">		输入数据的大小</div><div class="line">	:param out_size:</div><div class="line">		输出数据的大小</div><div class="line">	:param layer_name</div><div class="line">		神经网络层的名字</div><div class="line">	:param activation_function:</div><div class="line">		神经网络激活函数，默认没有</div><div class="line">	"""</div><div class="line">	<span class="comment"># 定义神经网络的初始化权重</span></div><div class="line">	Weights = tf.Variable(tf.random_normal([in_size, out_size]))</div><div class="line">	<span class="comment"># 定义神经网络的偏置</span></div><div class="line">	biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</div><div class="line">	<span class="comment"># 计算w*x+b</span></div><div class="line">	W_mul_x_plus_b = tf.matmul(input, Weights) + biases</div><div class="line">	<span class="comment"># 进行dropout，可以注释和不注释来对比dropout的效果</span></div><div class="line"><span class="comment">#	W_mul_x_plus_b = tf.nn.dropout(W_mul_x_plus_b, keep_prob)</span></div><div class="line">	<span class="comment"># 根据是否有激活函数进行处理</span></div><div class="line">	<span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">		output = W_mul_x_plus_b</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		output = activation_function(W_mul_x_plus_b)</div><div class="line">	<span class="comment"># 查看权重变化</span></div><div class="line">	tf.summary.histogram(layer_name + <span class="string">'/output'</span>, output)</div><div class="line">	<span class="keyword">return</span> output</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义dropout的placeholder</span></div><div class="line">keep_prob = tf.placeholder(tf.float32)</div><div class="line"><span class="comment"># 输入数据64个特征</span></div><div class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">64</span>])  <span class="comment"># 8x8</span></div><div class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line"></div><div class="line"><span class="comment"># 添加隐藏层和输出层</span></div><div class="line">l1 = add_layer(xs, <span class="number">64</span>, <span class="number">50</span>, <span class="string">'l1'</span>, activation_function=tf.nn.tanh)</div><div class="line">prediction = add_layer(l1, <span class="number">50</span>, <span class="number">10</span>, <span class="string">'l2'</span>, activation_function=tf.nn.softmax)</div><div class="line"></div><div class="line"><span class="comment"># 计算loss</span></div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[<span class="number">1</span>]))</div><div class="line"><span class="comment"># 存储loss</span></div><div class="line">tf.summary.scalar(<span class="string">'loss'</span>, cross_entropy)</div><div class="line"><span class="comment"># 神经网络训练</span></div><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</div><div class="line"><span class="comment"># 定义Session</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># 收集所有的数据</span></div><div class="line">merged = tf.summary.merge_all()</div><div class="line"><span class="comment"># 将数据写入到tensorboard中</span></div><div class="line">train_writer = tf.summary.FileWriter(<span class="string">"logs/train"</span>, sess.graph)</div><div class="line">test_writer = tf.summary.FileWriter(<span class="string">"logs/test"</span>, sess.graph)</div><div class="line"></div><div class="line"><span class="comment"># 根据tensorflow版本选择初始化函数</span></div><div class="line"><span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</div><div class="line">    init = tf.initialize_all_variables()</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    init = tf.global_variables_initializer()</div><div class="line"><span class="comment"># 执行初始化</span></div><div class="line">sess.run(init)</div><div class="line"><span class="comment"># 进行训练迭代</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># 执行训练，dropout为1-0.5=0.5</span></div><div class="line">    sess.run(train_step, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: <span class="number">0.5</span>&#125;)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</div><div class="line">        <span class="comment"># 记录损失</span></div><div class="line">        train_result = sess.run(merged, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: <span class="number">1</span>&#125;)</div><div class="line">        test_result = sess.run(merged, feed_dict=&#123;xs: X_test, ys: y_test, keep_prob: <span class="number">1</span>&#125;)</div><div class="line">        train_writer.add_summary(train_result, i)</div><div class="line">        test_writer.add_summary(test_result, i)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<ul>
<li>没有dropout</li>
</ul>
<p><img src="http://ocs628urt.bkt.clouddn.com/no_dropout.png" alt="no_dropout"></p>
<p>测试误差与训练误差的损失差的较大，说明模型更拟合训练数据。</p>
<ul>
<li>有dropout</li>
</ul>
<p><img src="http://ocs628urt.bkt.clouddn.com/dropout.png" alt="dropout"></p>
<p>测试误差与训练误差相差不大，说明模型泛化能力较好。</p>
]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(八)——dropout的作用
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(七)——使用MNIST训练神经网络</title>
    <link href="noahsnail.com/2017/04/15/2017-4-15-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%83)%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8MNIST%E8%AE%AD%E7%BB%83/"/>
    <id>noahsnail.com/2017/04/15/2017-4-15-tensorflow的基本用法(七)——使用MNIST训练/</id>
    <published>2017-04-15T15:32:42.000Z</published>
    <updated>2017-04-15T16:23:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是使用tensorflow和mnist数据集来训练神经网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"></div><div class="line"><span class="comment"># 下载mnist数据</span></div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义神经网络模型的评估部分</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(W, b)</span>:</span></div><div class="line">	<span class="comment"># 定义测试数据的placeholder</span></div><div class="line">	x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">	<span class="comment"># 定义测试数据的真实标签的placeholder</span></div><div class="line">	y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line">	<span class="comment"># 定义预测值</span></div><div class="line">	y = tf.nn.softmax(tf.matmul(x, W) + b)</div><div class="line">	<span class="comment"># 判断预测值y和真实值y_中最大数的索引是否一致，y的值为1-10概率</span></div><div class="line">	correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</div><div class="line">	<span class="comment"># 计算准确率</span></div><div class="line">	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">	<span class="comment"># 输入测试数据，执行准确率的计算并返回</span></div><div class="line">	<span class="keyword">return</span> sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</div><div class="line"></div><div class="line"><span class="comment"># 定义神经网络模型的训练部分</span></div><div class="line"><span class="comment"># 下面定义的神经网络只有一层W*x+b</span></div><div class="line"><span class="comment"># 定义输入数据placeholder，不定义输入样本的数目——None，但定义每个样本的大小为784</span></div><div class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line"><span class="comment"># 定义神经网络层的权重参数</span></div><div class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</div><div class="line"><span class="comment"># 定义神经网络层的偏置参数</span></div><div class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</div><div class="line"><span class="comment"># 定义一层神经网络运算，激活函数为softmax</span></div><div class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</div><div class="line"><span class="comment"># 定义训练数据真实标签的placeholder</span></div><div class="line">y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</div><div class="line"><span class="comment"># 定义损失函数，损失函数为交叉熵，reduction_indices表示沿着tensor的哪个纬度来求和</span></div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[<span class="number">1</span>]))</div><div class="line"><span class="comment"># 定义神经网络的训练步骤，使用的是梯度下降法，学习率为0.5</span></div><div class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</div><div class="line"><span class="comment"># 初始化所有变量</span></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"><span class="comment"># 定义Session</span></div><div class="line">sess = tf.Session()</div><div class="line"><span class="comment"># 执行变量的初始化</span></div><div class="line">sess.run(init)</div><div class="line"><span class="comment"># 迭代进行训练</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">	<span class="comment"># 取出mnist数据集中的100个数据</span></div><div class="line">	batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">	<span class="comment"># 执行训练过程并传入真实数据x, y_</span></div><div class="line">	sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</div><div class="line">	<span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">		<span class="keyword">print</span> compute_accuracy(W, b)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Extracting MNIST_data/train-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/train-labels-idx1-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</div><div class="line">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</div><div class="line">0.4075</div><div class="line">0.8948</div><div class="line">0.9031</div><div class="line">0.9074</div><div class="line">0.9037</div><div class="line">0.9125</div><div class="line">0.9158</div><div class="line">0.912</div><div class="line">0.9181</div><div class="line">0.9169</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(七)——使用MNIST训练神经网络
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(六)——神经网络可视化</title>
    <link href="noahsnail.com/2017/04/14/2017-4-14-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AD)%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>noahsnail.com/2017/04/14/2017-4-14-tensorflow的基本用法(六)——神经网络可视化/</id>
    <published>2017-04-14T14:40:35.000Z</published>
    <updated>2017-04-14T14:54:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是对tensorflow的神经网络训练过程以及神经网络结构进行可视化工作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 创建一个神经网络层</div><div class="line">def add_layer(input, in_size, out_size, activation_function = None):</div><div class="line">	&quot;&quot;&quot;</div><div class="line">	:param input:</div><div class="line">		神经网络层的输入</div><div class="line">	:param in_zize:</div><div class="line">		输入数据的大小</div><div class="line">	:param out_size:</div><div class="line">		输出数据的大小</div><div class="line">	:param activation_function:</div><div class="line">		神经网络激活函数，默认没有</div><div class="line">	&quot;&quot;&quot;</div><div class="line">	with tf.name_scope(&apos;layer&apos;):</div><div class="line">		with tf.name_scope(&apos;weights&apos;):</div><div class="line">			# 定义神经网络的初始化权重</div><div class="line">			Weights = tf.Variable(tf.random_normal([in_size, out_size]))</div><div class="line">		with tf.name_scope(&apos;biases&apos;):</div><div class="line">			# 定义神经网络的偏置</div><div class="line">			biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)</div><div class="line">		with tf.name_scope(&apos;W_mul_x_plus_b&apos;):</div><div class="line">			# 计算w*x+b</div><div class="line">			W_mul_x_plus_b = tf.matmul(input, Weights) + biases</div><div class="line">		# 根据是否有激活函数进行处理</div><div class="line">		if activation_function is None:</div><div class="line">			output = W_mul_x_plus_b</div><div class="line">		else:</div><div class="line">			output = activation_function(W_mul_x_plus_b)</div><div class="line"></div><div class="line">		return output</div><div class="line"></div><div class="line"># 创建一个具有输入层、隐藏层、输出层的三层神经网络，神经元个数分别为1,10,1</div><div class="line"># 创建只有一个特征的输入数据，数据数目为300，输入层</div><div class="line">x_data = np.linspace(-1, 1, 300)[:, np.newaxis]</div><div class="line"># 创建数据中的噪声</div><div class="line">noise = np.random.normal(0, 0.05, x_data.shape)</div><div class="line"># 创建输入数据对应的输出</div><div class="line">y_data = np.square(x_data) + 1 + noise</div><div class="line"></div><div class="line">with tf.name_scope(&apos;input&apos;):</div><div class="line">	# 定义输入数据，None是样本数目，表示多少输入数据都行，1是输入数据的特征数目</div><div class="line">	xs = tf.placeholder(tf.float32, [None, 1], name = &apos;x_input&apos;)</div><div class="line">	# 定义输出数据，与xs同理</div><div class="line">	ys = tf.placeholder(tf.float32, [None, 1], name = &apos;y_input&apos;)</div><div class="line"></div><div class="line"># 定义一个隐藏层</div><div class="line">hidden_layer = add_layer(xs, 1, 10, activation_function = tf.nn.relu)</div><div class="line"># 定义输出层</div><div class="line">prediction = add_layer(hidden_layer, 10, 1, activation_function = None)</div><div class="line"></div><div class="line"># 求解神经网络参数</div><div class="line"></div><div class="line"># 定义损失函数</div><div class="line">with tf.name_scope(&apos;loss&apos;):</div><div class="line">	loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices = [1]))</div><div class="line"># 定义训练过程</div><div class="line">with tf.name_scope(&apos;train&apos;):</div><div class="line">	train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</div><div class="line"># 变量初始化</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"># 定义Session</div><div class="line">sess = tf.Session()</div><div class="line"># 将网络结构图写到文件中</div><div class="line">writer = tf.summary.FileWriter(&apos;logs/&apos;, sess.graph)</div><div class="line"># 执行初始化工作</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line"># 绘制求解的曲线</div><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(1, 1, 1)</div><div class="line">ax.scatter(x_data, y_data)</div><div class="line">plt.ion()</div><div class="line">plt.show()</div><div class="line"></div><div class="line"># 进行训练</div><div class="line">for i in range(1000):</div><div class="line">	# 执行训练，并传入数据</div><div class="line">	sess.run(train_step, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line">	if i % 100 == 0:</div><div class="line">		try:</div><div class="line">			ax.lines.remove(lines[0])</div><div class="line">		except Exception:</div><div class="line">			pass</div><div class="line"></div><div class="line">		# print sess.run(loss, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line">		# 计算预测值</div><div class="line">		prediction_value = sess.run(prediction, feed_dict = &#123;xs: x_data&#125;)</div><div class="line">		绘制预测值</div><div class="line">		lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw = 5)</div><div class="line">		plt.pause(0.1)</div><div class="line"># 关闭Session</div><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/visualization.png" alt="优化"></p>
<p>在网络结果代码中添加<code>tf.name_scope(&#39;name&#39;)</code>并将网络结构图写入文件后，可用tensorboard命令查看神经网络的结果图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorboard --logdir=log/</div></pre></td></tr></table></figure>
<p>结果如图：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/neural_network.png" alt="神经网络结构图"></p>
]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(六)——神经网络可视化
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(五)——创建神经网络并训练</title>
    <link href="noahsnail.com/2017/04/12/2017-4-12-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%94)%E2%80%94%E2%80%94%E5%88%9B%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82/"/>
    <id>noahsnail.com/2017/04/12/2017-4-12-tensorflow的基本用法(五)——创建神经网络层/</id>
    <published>2017-04-12T14:09:21.000Z</published>
    <updated>2017-04-12T15:12:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是介绍利用tensorflow创建一个简单的神经网络并进行训练。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 创建一个神经网络层</div><div class="line">def add_layer(input, in_size, out_size, activation_function = None):</div><div class="line">	&quot;&quot;&quot;</div><div class="line">	:param input:</div><div class="line">		神经网络层的输入</div><div class="line">	:param in_zize:</div><div class="line">		输入数据的大小</div><div class="line">	:param out_size:</div><div class="line">		输出数据的大小</div><div class="line">	:param activation_function:</div><div class="line">		神经网络激活函数，默认没有</div><div class="line">	&quot;&quot;&quot;</div><div class="line"></div><div class="line">	# 定义神经网络的初始化权重</div><div class="line">	Weights = tf.Variable(tf.random_normal([in_size, out_size]))</div><div class="line">	# 定义神经网络的偏置</div><div class="line">	biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)</div><div class="line">	# 计算w*x+b</div><div class="line">	W_mul_x_plus_b = tf.matmul(input, Weights) + biases</div><div class="line">	# 根据是否有激活函数进行处理</div><div class="line">	if activation_function is None:</div><div class="line">		output = W_mul_x_plus_b</div><div class="line">	else:</div><div class="line">		output = activation_function(W_mul_x_plus_b)</div><div class="line"></div><div class="line">	return output</div><div class="line"></div><div class="line"># 创建一个具有输入层、隐藏层、输出层的三层神经网络，神经元个数分别为1,10,1</div><div class="line"># 创建只有一个特征的输入数据，数据数目为300，输入层</div><div class="line">x_data = np.linspace(-1, 1, 300)[:, np.newaxis]</div><div class="line"># 创建数据中的噪声</div><div class="line">noise = np.random.normal(0, 0.05, x_data.shape)</div><div class="line"># 创建输入数据对应的输出</div><div class="line">y_data = np.square(x_data) + 1 + noise</div><div class="line"></div><div class="line"># 定义输入数据，None是样本数目，表示多少输入数据都行，1是输入数据的特征数目</div><div class="line">xs = tf.placeholder(tf.float32, [None, 1])</div><div class="line"># 定义输出数据，与xs同理</div><div class="line">ys = tf.placeholder(tf.float32, [None, 1])</div><div class="line"></div><div class="line"># 定义一个隐藏层</div><div class="line">hidden_layer = add_layer(xs, 1, 10, activation_function = tf.nn.relu)</div><div class="line"># 定义输出层</div><div class="line">prediction = add_layer(hidden_layer, 10, 1, activation_function = None)</div><div class="line"></div><div class="line"># 求解神经网络参数</div><div class="line"># 定义损失函数</div><div class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices = [1]))</div><div class="line"># 定义训练过程</div><div class="line">train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</div><div class="line"># 变量初始化</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"># 定义Session</div><div class="line">sess = tf.Session()</div><div class="line"># 执行初始化工作</div><div class="line">sess.run(init)</div><div class="line"># 进行训练</div><div class="line">for i in range(1000):</div><div class="line">	# 执行训练，并传入数据</div><div class="line">	sess.run(train_step, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line">	if i % 100 == 0:</div><div class="line">		print sess.run(loss, feed_dict = &#123;xs: x_data, ys: y_data&#125;)</div><div class="line"># 关闭Session</div><div class="line">sess.close()</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">1.06731</div><div class="line">0.0111914</div><div class="line">0.00651229</div><div class="line">0.00530187</div><div class="line">0.00472237</div><div class="line">0.00429948</div><div class="line">0.00399815</div><div class="line">0.00377548</div><div class="line">0.00359714</div><div class="line">0.00345819</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(五)——创建神经网络并训练
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(四)——placeholder</title>
    <link href="noahsnail.com/2017/04/11/2017-4-11-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%9B%9B)/"/>
    <id>noahsnail.com/2017/04/11/2017-4-11-tensorflow的基本用法(四)/</id>
    <published>2017-04-11T14:26:39.000Z</published>
    <updated>2017-04-12T15:08:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是介绍tensorflow中的placeholder及用法。placeholder，中文意思是占位符，在tensorflow中类似于函数参数，运行时必须传入值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 定义placeholder</div><div class="line">input1 = tf.placeholder(tf.float32)</div><div class="line">input2 = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line"># 定义乘法运算</div><div class="line">output = tf.multiply(input1, input2)</div><div class="line"></div><div class="line"># 通过session执行乘法运行</div><div class="line">with tf.Session() as sess:</div><div class="line">	# 执行时要传入placeholder的值</div><div class="line">	print sess.run(output, feed_dict = &#123;input1:[7.], input2: [2.]&#125;)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[ 14.]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(四)——placeholder
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(三)——Variable</title>
    <link href="noahsnail.com/2017/04/10/2017-4-10-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%89)/"/>
    <id>noahsnail.com/2017/04/10/2017-4-10-tensorflow的基本用法(三)/</id>
    <published>2017-04-10T10:58:57.000Z</published>
    <updated>2017-04-11T14:34:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是介绍tensorflow中的变量Variable及用法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 定义变量</div><div class="line">state = tf.Variable(0, name = &apos;counter&apos;)</div><div class="line">print(state.name)</div><div class="line"></div><div class="line"># 定义常量</div><div class="line">one = tf.constant(1)</div><div class="line"># 定义新的变量，值等于state + one</div><div class="line">new_value = tf.add(state, one)</div><div class="line"># 定义update操作，将new_value赋值给state</div><div class="line">update = tf.assign(state, new_value)</div><div class="line"></div><div class="line"># 定义的变量激活，如果定义了Variable，必须有</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"> </div><div class="line">with tf.Session() as sess:</div><div class="line">	sess.run(init)</div><div class="line">	for _ in range(3):</div><div class="line">		# 执行update操作，update操作包括add,assign两部分</div><div class="line">		sess.run(update)</div><div class="line">		# 输出state结果，必须放到sess.run()中</div><div class="line">		print sess.run(state)</div></pre></td></tr></table></figure>
<p>执行结果如下图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">counter:0</div><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(三)——Variable
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法(二)——感知机</title>
    <link href="noahsnail.com/2017/04/10/2017-4-10-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95(%E4%BA%8C)%E2%80%94%E2%80%94%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>noahsnail.com/2017/04/10/2017-4-10-统计学习方法(二)——感知机/</id>
    <published>2017-04-10T09:27:03.000Z</published>
    <updated>2017-04-18T11:09:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="第2章-感知机"><a href="#第2章-感知机" class="headerlink" title="第2章 感知机"></a>第2章 感知机</h1><p>感知机（perceptron）是二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1值。感知机将实例分为正负两类的分离超平面，属于判别模型。感知机旨在求出将训练数据进行线性可分的分离超平面，根据损失函数用梯度下降法对损失函数进行极小化，求的感知机模型。</p>
<h2 id="2-1-感知机模型"><a href="#2-1-感知机模型" class="headerlink" title="2.1 感知机模型"></a>2.1 感知机模型</h2><p><strong>感知机定义</strong></p>
<p>假设输入空间是$x \in R^n$，输出空间是$y=\lbrace +1,-1 \rbrace$，输入$x \in X$是实例的特征向量，是输入空间的点，输出$y \in Y$表示实例的类别，输入输出之间的映射函数为：</p>
<p>$$f(x)=sign(w * x + b)$$</p>
<p>f(x)称为感知机。其中，w和b为感知机的模型参数，$w \in R^n$叫做权值(weight)或权值向量(weight vector)，$b \in R$叫做偏置（bias）。$w*x$表示w与x的内积。sign是符号函数，即</p>
<p>$$sign(x)=<br>\begin{cases}<br>+1，x \geq 0 \\<br>-1，x \lt 0<br>\end{cases}$$</p>
<p>感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合$\lbrace f|f(x) = w * x + b\rbrace$。感知机的几何解释为：线性方程</p>
<p>$$w * x + b = 0$$</p>
<p>对应于特征空间$R^n$中的一个超平面S，其中w是超平面的法向量，b是超平面的截距，这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正负两类，因此超平面S称为分离超平面（separating hyperplane），如下图所示：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/perceptron.png" alt="感知机模型"></p>
<h2 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h2><h3 id="2-2-1-数据集的线性可分性"><a href="#2-2-1-数据集的线性可分性" class="headerlink" title="2.2.1 数据集的线性可分性"></a>2.2.1 数据集的线性可分性</h3><p><strong>线性可分性</strong></p>
<p>给定一个数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i \in X = R^n$，$y \in Y=\lbrace +1,-1 \rbrace$，$i=1,2,…,N$，如果存在超平面S</p>
<p>$$w * x + b = 0$$</p>
<p>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例i，有$w<em>_i+b&gt;0$，对所有$y_i=-1$的实例i，有$w</em>_i+b&lt;0$，则称数据集T为线性可分数据集（linear separable data set）；否则，称数据集T线性不可分。</p>
<h3 id="2-2-2-感知机学习策略"><a href="#2-2-2-感知机学习策略" class="headerlink" title="2.2.2 感知机学习策略"></a>2.2.2 感知机学习策略</h3><p>假定训练数据集是线性可分的，感知机的学习目标是求得一个能将训练集正负实例点完全正确分开的分离超平面。为了找出超平面，即确定模型参数w，b，需要确定学习策略，即定义损失函数并将损失函数最小化。损失函数的一个自然选择是分类点的错误数。但这样的损失函数不是连续可导函数，不易优化。因此感知机采用误分类点到超平面S的总距离作为损失函数。输入空间中任意一点$x_0$到超平面S的距离为：</p>
<p>$$\frac {1} {||w||} |w * x_0 + b|$$</p>
<p>$||w||$为w的$L_2$范数。对于误分类的数据$(x_i,y_i)$，$-y_i(w * x + b) \lt 0$始终成立。因此，误分类点$x_i$到超平面S的距离为：</p>
<p>$$-\frac {1} {||w||} y_i(w * x_i + b)$$</p>
<p>假设误分类点集合为M，M到超平面S的总距离为</p>
<p>$$-\frac {1} {||w||} \sum_{x_i \in M} y_i(w * x_i + b)$$</p>
<p>去掉$||w||$，就得到了感知机的损失函数。感知机${sign}(w * x + b)$的损失函数为</p>
<p>$$L(w,b)=-\sum_{x_i \in M} y_i(w * x_i + b)$$</p>
<p>损失函数L(w,b)是非负的，如果没有误分类点，损失函数为0。误分类点离超平面越近，损失函数值越小。</p>
<h2 id="2-3-感知机学习算法"><a href="#2-3-感知机学习算法" class="headerlink" title="2.3 感知机学习算法"></a>2.3 感知机学习算法</h2><p>感知机学习问题转化为求解损失函数L(w,b)的问题，最优化的方法是随机梯度下降法。</p>
<h3 id="2-3-1-感知机学习算法的原始形式"><a href="#2-3-1-感知机学习算法的原始形式" class="headerlink" title="2.3.1 感知机学习算法的原始形式"></a>2.3.1 感知机学习算法的原始形式</h3><p>感知机是对以下最优化问题的算法。给定一个训练数据集$T = {(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i \in X = R^n$，$y \in Y=\lbrace +1,-1 \rbrace$，$i=1,2,…,N$，求参数w，b，使其为以下损失函数极小化问题的解</p>
<p>$${min}_{w,b} L(w,b) = -\sum_{x_i \in M} y_i(w * x_i + b)$$</p>
<p>其中M为误分类点的集合。感知机学习算法是通过误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent）。首先，选取任意一个超平面$w_0，b_0$，然后用梯度下降法不断的极小化目标函数。极小化过程不是一次使M中的所有误分类点的梯度下降，而是一次随机选择一个误分类点使其梯度下降。</p>
<p>假设误分类点集合M是固定的，那么损失函数的L(w,b)的梯度由<br>$$\nabla_wL(w,b)=-\sum_{x_i \in M} y_ix_i$$</p>
<p>$$\nabla_bL(w,b)=-\sum_{x_i \in M} y_i$$</p>
<p>给出。随机选取一个误分类点(x_i,y_i)，对w，b进行更新：</p>
<p>$$w \longleftarrow w + \eta y_ix_i$$<br>$$b \longleftarrow b + \eta y_i$$</p>
<p>式子中$\eta(0 &lt; \eta \le 1)$是步长，在统计学习中又称为学习率（learning rate）。通过迭代可以期待损失函数L(w,b)不断减小，直到为0。</p>
<p>感知机学习算法的流程如下：</p>
<p>输入：训练数据集$T={(x_1,y_1)，(x_2,y_2)，…，(x_n,y_n)}$，其中$x_i \in X = R^n$，$y \in Y=\lbrace +1,-1 \rbrace$，$i=1,2,…,N$，学习率$\eta(0&lt;\eta \le 1)$；输出w，b；感知机学习模型$f(x) = {sign}(w * x + b)$。</p>
<p>(1) 选取初值$w_0，b_0$<br>(2) 在训练集中选取数据$(x_i,y_i)$<br>(3) 如果$y_i(w * x_i + b) \le 0$</p>
<p>$$w \longleftarrow w + \eta y_ix_i$$</p>
<p>$$b \longleftarrow b + \eta y_i$$<br>(4) 转至(2)，知道训练集中没有误分类点。</p>
<p><strong>解释：</strong>当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w，b的值，使分离超平面向该误分类点的一侧移动，以减少改误分类点与超平面间的距离，直至超平面越过该分类点使其被正确分类。</p>
<h3 id="2-3-2-算法的收敛性"><a href="#2-3-2-算法的收敛性" class="headerlink" title="2.3.2 算法的收敛性"></a>2.3.2 算法的收敛性</h3><p>对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。</p>
<p>证明略。</p>
<h3 id="2-3-3-感知机学习算法的对偶形式"><a href="#2-3-3-感知机学习算法的对偶形式" class="headerlink" title="2.3.3 感知机学习算法的对偶形式"></a>2.3.3 感知机学习算法的对偶形式</h3>]]></content>
    
    <summary type="html">
    
      统计学习方法(二)——感知机
    
    </summary>
    
      <category term="机器学习" scheme="noahsnail.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="统计学习方法" scheme="noahsnail.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Mac安装Python版OpenCV</title>
    <link href="noahsnail.com/2017/04/10/2017-4-10-Mac%E5%AE%89%E8%A3%85Python%E7%89%88OpenCV/"/>
    <id>noahsnail.com/2017/04/10/2017-4-10-Mac安装Python版OpenCV/</id>
    <published>2017-04-10T08:10:24.000Z</published>
    <updated>2017-04-10T08:53:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-brew安装opencv"><a href="#1-brew安装opencv" class="headerlink" title="1. brew安装opencv"></a>1. brew安装opencv</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">brew install homebrew/science/opencv</div><div class="line"></div><div class="line">brew install homebrew/science/opencv3</div></pre></td></tr></table></figure>
<h2 id="2-把opencv添加到python中"><a href="#2-把opencv添加到python中" class="headerlink" title="2. 把opencv添加到python中"></a>2. 把opencv添加到python中</h2><p>执行下面的命令，或直接将<code>/usr/local/Cellar/opencv3/3.1.0_4/lib/python2.7/site-packages/</code>下的cv2.so拷贝到<code>/Users/ltc/anaconda/lib/python2.7/site-packages/</code>目录中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo /usr/local/Cellar/opencv3/3.1.0_4/lib/python2.7/site-packages/ &gt;&gt; /Users/ltc/anaconda/lib/python2.7/site-packages/opencv3.pth</div></pre></td></tr></table></figure>
<h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3. 测试"></a>3. 测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">$ import cv2</div></pre></td></tr></table></figure>
<p>不报错，安装成功。</p>
]]></content>
    
    <summary type="html">
    
      Mac安装Python版OpenCV
    
    </summary>
    
      <category term="OpenCV" scheme="noahsnail.com/categories/OpenCV/"/>
    
    
      <category term="OpenCV" scheme="noahsnail.com/tags/OpenCV/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode402——Remove K Digits</title>
    <link href="noahsnail.com/2017/04/09/2017-4-9-Leetcode402%E2%80%94%E2%80%94%20Remove%20K%20Digits/"/>
    <id>noahsnail.com/2017/04/09/2017-4-9-Leetcode402—— Remove K Digits/</id>
    <published>2017-04-09T06:09:34.000Z</published>
    <updated>2017-04-09T10:16:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>Given a non-negative integer num represented as a string, remove k digits from the number so that the new number is the smallest possible.</p>
<p>Note:<br>The length of num is less than 10002 and will be ≥ k.<br>The given num does not contain any leading zero.<br>Example 1:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = &quot;1432219&quot;, k = 3</div><div class="line">Output: &quot;1219&quot;</div><div class="line">Explanation: Remove the three digits 4, 3, and 2 to form the new number 1219 which is the smallest.</div></pre></td></tr></table></figure>
<p>Example 2:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = &quot;10200&quot;, k = 1</div><div class="line">Output: &quot;200&quot;</div><div class="line">Explanation: Remove the leading 1 and the number is 200. Note that the output must not contain leading zeroes.</div></pre></td></tr></table></figure>
<p>Example 3:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Input: num = &quot;10&quot;, k = 2</div><div class="line">Output: &quot;0&quot;</div><div class="line">Explanation: Remove all the digits from the number and it is left with nothing which is 0.</div></pre></td></tr></table></figure>
<h2 id="2-求解"><a href="#2-求解" class="headerlink" title="2. 求解"></a>2. 求解</h2><p>每移除一个字符，找出移除一个字符串后得到的字符串中最小的那个，作为下一次移除字符的输入，这样每次移除字符后得到子串都是最小子串。这里必须要明确每次移除字符的最优解必定是下一次移除字符最优解的输入，即f(n)的最优解必定是求解f(n-1)的最优解的一部分。问题的最优解解中包含了子问题的最优解。</p>
<p><strong>方法一：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">    public String removeKdigits(String num, int k) &#123;</div><div class="line">        String min = remove(num, k);</div><div class="line">        if(min.equals(&quot;&quot;)) &#123;</div><div class="line">            return &quot;0&quot;;</div><div class="line">        &#125;</div><div class="line">        while(min.charAt(0) == &apos;0&apos;) &#123;</div><div class="line">            if(min.length() == 1) &#123;</div><div class="line">                break;</div><div class="line">            &#125;</div><div class="line">            min = min.substring(1, min.length());</div><div class="line">        &#125;</div><div class="line">        return min;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    public String remove(String num, int k) &#123;</div><div class="line">        if(k == 0) &#123;</div><div class="line">            return num;</div><div class="line">        &#125;</div><div class="line">        int n = num.length();</div><div class="line">        String minString = num.substring(0, num.length() - 1);</div><div class="line">        for(int i = 0; i &lt; n; i++) &#123;</div><div class="line">            String temp = num.substring(0, i) + num.substring(i + 1, n);</div><div class="line">            if(temp.compareTo(minString) &lt; 0) &#123;</div><div class="line">                minString = temp;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        return remove(minString, k - 1);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Leetcode超时。</p>
<p><strong>方法二</strong></p>
<p>要想数字变小，应该从前往后删除字符，因为前面的字符是数字的高位，在删除一个字符的情况下，删除数字的位置会被它的后一位替代，因此应该删除当前数字大于后一位数字的字符。如果前面没有找到符合条件的数字，则删除最后一位数字。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">       public String removeKdigits(String num, int k) &#123;</div><div class="line">    	//用StringBuilder是因为StringBuilder有删除指定字符的功能</div><div class="line">        StringBuilder min = new StringBuilder(num);</div><div class="line">        for(int m = 0; m &lt; k; m++) &#123;</div><div class="line">            int index = 0;</div><div class="line">            int n = min.length();</div><div class="line">            for(int i = 0; i &lt; n; i++) &#123;</div><div class="line"></div><div class="line">                if((i == n -1) || min.charAt(i) &gt; min.charAt(i + 1)) &#123;</div><div class="line">                    index = i;</div><div class="line">                    break;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            min = min.deleteCharAt(index);</div><div class="line">        &#125;</div><div class="line">        //判断字符串为空的情况</div><div class="line">        if(min.length() == 0) &#123;</div><div class="line">            return &quot;0&quot;;</div><div class="line">        &#125;</div><div class="line">        //去掉字符串前面的“0”</div><div class="line">        while(min.charAt(0) == &apos;0&apos;) &#123;</div><div class="line">            if(min.length() == 1) &#123;</div><div class="line">                break;</div><div class="line">            &#125;</div><div class="line">            min = min.deleteCharAt(0);</div><div class="line">        &#125;</div><div class="line">        return min.toString();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Leetcode402—— Remove K Digits
    
    </summary>
    
      <category term="基础算法" scheme="noahsnail.com/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="noahsnail.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(二)——Session</title>
    <link href="noahsnail.com/2017/04/08/2017-4-8-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%8C)/"/>
    <id>noahsnail.com/2017/04/08/2017-4-8-tensorflow的基本用法(二)/</id>
    <published>2017-04-08T15:37:15.000Z</published>
    <updated>2017-04-11T14:35:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是使用tensorflow进行矩阵的乘法运算。代码中介绍了两种不同的使用session的方式。Demo源码及解释如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 定义两个矩阵</div><div class="line">matrix1 = tf.constant([[3, 3]])</div><div class="line">matrix2 = tf.constant([[2], [2]])</div><div class="line"></div><div class="line"># 定义矩阵乘法</div><div class="line">product = tf.matmul(matrix1, matrix2)</div><div class="line"></div><div class="line"># 运行矩阵乘法，session用法一</div><div class="line">sess = tf.Session()</div><div class="line">result = sess.run(product)</div><div class="line">print &apos;Session用法一&apos;</div><div class="line">print result</div><div class="line">sess.close()</div><div class="line"></div><div class="line">## session用法二，不用考虑close，会自动关闭</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">	result = sess.run(product)</div><div class="line">	print &apos;Session用法二&apos;</div><div class="line">	print result</div></pre></td></tr></table></figure>
<p>执行结果如下图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Session用法一</div><div class="line">[[12]]</div><div class="line">Session用法二</div><div class="line">[[12]]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(二)——Session
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的基本用法(一)</title>
    <link href="noahsnail.com/2017/04/08/2017-4-8-tensorflow%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/"/>
    <id>noahsnail.com/2017/04/08/2017-4-8-tensorflow的基本用法/</id>
    <published>2017-04-08T15:07:03.000Z</published>
    <updated>2017-04-10T11:10:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是以求解线性回归的参数为例，讲解一下tensorflow的用法。下面的代码中我们自己构建了一个线性回归模型<code>y = 0.1 * x + 0.3</code>，然后我们使用tensorflow来进行了求解。Demo源码及解释如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 随机生成100个浮点数</div><div class="line">x_data = np.random.rand(100).astype(np.float32)</div><div class="line"># y=0.1*x+0.3，这样就构造了训练数据(x_data,y_data)</div><div class="line">y_data = x_data * 0.1 + 0.3</div><div class="line"></div><div class="line"># 定义线性回归的权重参数</div><div class="line">Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))</div><div class="line"># 定义线性回归的偏置参数</div><div class="line">Biases = tf.Variable(tf.zeros([1]))</div><div class="line"></div><div class="line"># 构建线性回归模型</div><div class="line">y = Weights * x_data + Biases</div><div class="line"># 计算线性回归的损失函数</div><div class="line">loss = tf.reduce_mean(tf.square(y - y_data))</div><div class="line"></div><div class="line"># 定义线性回归的求解方法，梯度下降法，学习率为0.5</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(0.5)</div><div class="line"># 使用tensorflow求解</div><div class="line">train = optimizer.minimize(loss)</div><div class="line"># 初始化tensorflow的所有变量</div><div class="line">init = tf.global_variables_initializer()</div><div class="line"># 定义tensorflow的session</div><div class="line">sess = tf.Session()</div><div class="line"># 将初始化数据放入到session中，执行时会用到</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line"># 迭代201次求解线性回归参数</div><div class="line">for step in range(201):</div><div class="line">	# session执行训练</div><div class="line">	sess.run(train)</div><div class="line">	if step % 20 == 0:</div><div class="line">		# 每迭代二十次输出一次结果</div><div class="line">		print step, sess.run(Weights), sess.run(Biases)</div></pre></td></tr></table></figure>
<p>执行结果如下图：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">0 [ 0.46898228] [ 0.12989661]</div><div class="line">20 [ 0.18490312] [ 0.25323763]</div><div class="line">40 [ 0.1203066] [ 0.28881568]</div><div class="line">60 [ 0.10485679] [ 0.29732502]</div><div class="line">80 [ 0.10116163] [ 0.29936022]</div><div class="line">100 [ 0.10027781] [ 0.29984701]</div><div class="line">120 [ 0.10006645] [ 0.29996341]</div><div class="line">140 [ 0.10001589] [ 0.29999125]</div><div class="line">160 [ 0.10000382] [ 0.29999793]</div><div class="line">180 [ 0.10000091] [ 0.29999951]</div><div class="line">200 [ 0.10000024] [ 0.29999989]</div></pre></td></tr></table></figure>
<p>从结果中可以看出，求解的结果还是很准确的。</p>
]]></content>
    
    <summary type="html">
    
      tensorflow的基本用法(一)
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode78——Subsets</title>
    <link href="noahsnail.com/2017/04/08/2017-4-8-Leetcode78%E2%80%94%E2%80%94Subsets/"/>
    <id>noahsnail.com/2017/04/08/2017-4-8-Leetcode78——Subsets/</id>
    <published>2017-04-08T14:30:15.000Z</published>
    <updated>2017-04-08T15:00:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>Given a set of distinct integers, nums, return all possible subsets.</p>
<p>Note: The solution set must not contain duplicate subsets.</p>
<p>For example,<br>If nums = <code>[1,2,3]</code>, a solution is:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[</div><div class="line">  [3],</div><div class="line">  [1],</div><div class="line">  [2],</div><div class="line">  [1,2,3],</div><div class="line">  [1,3],</div><div class="line">  [2,3],</div><div class="line">  [1,2],</div><div class="line">  []</div><div class="line">]</div></pre></td></tr></table></figure>
<h2 id="2-求解"><a href="#2-求解" class="headerlink" title="2. 求解"></a>2. 求解</h2><p><strong>递归法</strong></p>
<p>这道题类似于数组的组合问题，可以用递归法求解。N个数中每个数都分为要与不要两种情况，求解的过程如下图。递归的边界条件为N个数都遍历完了。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/subset.png" alt="递归过程"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">    public static List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;List&lt;Integer&gt;&gt;();</div><div class="line">    public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123;</div><div class="line">        result.clear();</div><div class="line">        result.add(new ArrayList&lt;Integer&gt;());</div><div class="line">        combination(nums, 0, new ArrayList&lt;Integer&gt;());</div><div class="line">        return result;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    public void combination(int[] nums, int index, List&lt;Integer&gt; list) &#123;</div><div class="line">        if(index == nums.length) &#123;</div><div class="line">            return;</div><div class="line">        &#125;</div><div class="line">        combination(nums, index + 1, new ArrayList&lt;Integer&gt;(list));</div><div class="line">        list.add(nums[index]);</div><div class="line">        result.add(list);</div><div class="line">        combination(nums, index + 1, new ArrayList&lt;Integer&gt;(list));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Leetcode78——Subsets
    
    </summary>
    
      <category term="基础算法" scheme="noahsnail.com/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="noahsnail.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode46——Permutations</title>
    <link href="noahsnail.com/2017/04/08/2017-4-8-Leetcode46%E2%80%94%E2%80%94Permutations/"/>
    <id>noahsnail.com/2017/04/08/2017-4-8-Leetcode46——Permutations/</id>
    <published>2017-04-08T12:42:53.000Z</published>
    <updated>2017-04-08T13:04:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>Given a collection of distinct numbers, return all possible permutations.</p>
<p>For example, <code>[1,2,3]</code> have the following permutations:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[</div><div class="line">  [1,2,3],</div><div class="line">  [1,3,2],</div><div class="line">  [2,1,3],</div><div class="line">  [2,3,1],</div><div class="line">  [3,1,2],</div><div class="line">  [3,2,1]</div><div class="line">]</div></pre></td></tr></table></figure>
<h2 id="2-求解"><a href="#2-求解" class="headerlink" title="2. 求解"></a>2. 求解</h2><p><strong>递归法</strong></p>
<p>这道题是求数组的全排列，首先可以用递归法，第一次往链表添加一个元素，有n中可能，然后将剩下的元素再添加一个元素到链表中，有n-1种可能，以此类推，直至链表中的元素大小等于数组大小。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">    public static List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;List&lt;Integer&gt;&gt;();</div><div class="line">    public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123;</div><div class="line">        result.clear();</div><div class="line">        permutation(nums, new ArrayList());</div><div class="line">        return result;</div><div class="line">    &#125;</div><div class="line">    public void permutation(int[] nums, List&lt;Integer&gt; list) &#123;</div><div class="line">        if(list.size() == nums.length) &#123;</div><div class="line">            result.add(list);</div><div class="line">            return;</div><div class="line">        &#125;</div><div class="line">        for(int i = 0; i &lt; nums.length; i++) &#123;</div><div class="line">            if(!list.contains(nums[i])) &#123;</div><div class="line">                List&lt;Integer&gt; subList = new ArrayList&lt;Integer&gt;(list);</div><div class="line">                subList.add(nums[i]);</div><div class="line">                permutation(nums, subList);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Leetcode46——Permutations
    
    </summary>
    
      <category term="基础算法" scheme="noahsnail.com/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="noahsnail.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>《异类》读书笔记</title>
    <link href="noahsnail.com/2017/04/08/2017-4-8-%E3%80%8A%E5%BC%82%E7%B1%BB%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>noahsnail.com/2017/04/08/2017-4-8-《异类》读书笔记/</id>
    <published>2017-04-08T07:10:29.000Z</published>
    <updated>2017-04-08T09:54:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>《异类》的作者是马尔科姆-格拉德威尔，《异类》主要探究许多人成功背后的原因到底是什么。《异类》分为两大部分，第一部分主要是讲述机遇对人的影响；第二部分主要是讲文化对人的影响。</p>
<h2 id="第1章-马太效应"><a href="#第1章-马太效应" class="headerlink" title="第1章 马太效应"></a>第1章 马太效应</h2><p>第一章主要是讲述了马太效应。通常我们认为，个体特征的卓越是一个人出类拔萃的根本原因，但作者认为不是这样的。作者以加拿大冰球队的选手为例，分析了这些选手的出生时间，发现大多数选手都是出生在前三个月，为什么出生在前三个月的人能成功优秀的冰球选手呢？这主要是因为加拿大冰球队按年龄分组的分界线是1月1日，同一年龄的选手，出生在1月份的比出生在12月份的选手发育的时间更长，球员之间在生理成熟度上表现出巨大的差异，因此在冰球选拔时具有更大的优势，而当出生在1月份的选手被选入冰球队后，能获得更优秀的辅导，进行的训练比没选拔上的12月份的人更多，优势变得更大，优势慢慢的积累，最终变成了出生在1月份的人变成了优秀的冰球选手，而同年出生的12月份的人慢慢的变成了普通人，两者差距越来越大。这种情况主要是3种制度共通作用的结果：筛选、分组和区别训练。美国的棒球队，英国足球队都有类似的现象。从年龄大几个月的优势在孩子的成长过程中不断积累，最终导致孩子成功或不成功，正是这种领先优势造就了天才们辉煌的成就。在社会学领域，成功就是“优势积累的结果”。天才并非开始就出众，一开始他只是比别人优秀一点点。</p>
<p><strong>备注：</strong>马太效应（Matthew Effect），指强者愈强、弱者愈弱的现象。</p>
<p><strong>想法：</strong>感觉这是不是与国内的读书的重点班，预科班很像啊。不让孩子输在起跑线上还是有些道理的。</p>
<h2 id="第2章-10000小时法则"><a href="#第2章-10000小时法则" class="headerlink" title="第2章 10000小时法则"></a>第2章 10000小时法则</h2><p>10000小时法则是讲当某个人在某个领域练习10000小时时，他就能成为这个领域的专家。作者以Sun公司创始人和比尔盖茨为例，他们从小就接触电脑，当他们成年时他们已经达到了10000小时的训练量，因此当机会来临时他们能够很好的抓住机会并取得成功。成功是天赋加上后天努力，天赋的作用其实很小，而后天努力的作用其实很大。练习并不是为了让你一次就能把事情做好，而是帮助你越做约好。</p>
<p><strong>想法：</strong>确实挺有道理，很多时候别人成功不是因为别人比你聪明，而是因为别人比你更努力，练习的更多。</p>
<h2 id="第3、4章-天才之忧"><a href="#第3、4章-天才之忧" class="headerlink" title="第3、4章 天才之忧"></a>第3、4章 天才之忧</h2><p>这两章主要讲述了一个智商接近200的叫兰根的人，虽然他的天赋非常的好，但他却并没有取得成功。爱因斯坦的智商才150左右，智商120以上的人就可以称为聪明人了。那他为什么不成功呢？作者举了其它的研究人员研究的例子，小时候具有同样高智商的人，长大后有的很成功，有的一般，有的混的很惨。他们小时候具有同样的智商优势，长大后的成就却差别很大。研究发现，比较成功的人小时候家庭条件通常都很好，而比较差的人家庭条件通常较差。这是为什么呢？原来，智商与成功只在一定程度上相互关联，除了智商之外，还有一种“实践智力”，即与他人沟通、交流、良好协作的能力。这种“实践智力”通常是家庭条件较差的人缺乏的。社交知识是门学问，是一系列可以习得的技能，而富人的孩子这种能力通常较好。这其实就是一种阶级优势。作者以奥本海默的例子与兰根作了对比，两个人有类似的境遇却有不同的成就，原因就在于家庭环境。你应该想到了，兰根的家庭条件确实很差。</p>
<p><strong>想法：</strong>“实践智力”我认为就是我们所说的情商，家庭环境对人的影响很大，事实确实如此。看看现在的社会，北京孩子跟西部山区的孩子确实差距很大。</p>
<h2 id="第5章-乔弗洛姆的启示"><a href="#第5章-乔弗洛姆的启示" class="headerlink" title="第5章 乔弗洛姆的启示"></a>第5章 乔弗洛姆的启示</h2><p>第五章讲述了乔弗洛姆，美国世达律师事务所总裁的故事，讲述了莫里斯简克洛的故事，他们时常身处逆境，这些不利因素却常常称为他们的机遇之源。</p>
<p><strong>想法：</strong>中国的古话时势造英雄能很好的解释这章。</p>
<h2 id="第6章-小镇哈伦"><a href="#第6章-小镇哈伦" class="headerlink" title="第6章 小镇哈伦"></a>第6章 小镇哈伦</h2><p>这章主要讲述了美国肯塔基州坎伯兰高原上的小镇哈伦里的两大家族，霍华德家族和特纳家族的恩怨情仇。同时美国还有许多其它的地方存在类似的世仇，这种世仇的模式被称为阿巴拉契亚模式。最后的调查发现民族的文化遗产才是这些世仇的诱因。</p>
<p><strong>想法：</strong>不同地方的人有不同的文化，不同的行事方式，这就是文化传承的力量。</p>
<h2 id="第7章-飞机失事的族裔理论"><a href="#第7章-飞机失事的族裔理论" class="headerlink" title="第7章 飞机失事的族裔理论"></a>第7章 飞机失事的族裔理论</h2><p>第七章讲述的是韩国的飞机在以前失事的概率很高，分析了一些空难的原因，当然还有其它国家的空难，最终研究发现这些空难主要是由文化原因导致的。其中提到了一个权利距离指数的概念，即下位者对上位者的服从程度，是否敢于与上位者发表不同的意见。这主要是因为飞机上的副驾驶及其他人要在机长有问题时及时指出，以降低可能产生问题的风险。美国典型的是低权利距离的国家，而韩国属于高权利距离的国家。</p>
<p><strong>想法：</strong>权利指数分析的确实没错，而权利指数形成的原因主要是由文化背景决定。</p>
<h2 id="第8章-稻田与数学"><a href="#第8章-稻田与数学" class="headerlink" title="第8章 稻田与数学"></a>第8章 稻田与数学</h2><p>这章讲了我国南方主要种植稻田，同时分析了我们读数字与英语读数字的区别，作者发现造成东西方数学差异的可能是得益于我们的族裔文化。同时比较了东西方种植时间的差异，我国种植水稻一年的劳作时间比西方要多很多，我们的文化是建立在勤劳基础上的文化。我们的发音学数学有优势，我们的文化使我们学习很勤劳，我们的数学成绩高当然也应该是理所当然的。努力工作是所有成功人士的共性，而稻田中产生的文明精华是，通过努力工作，在巨大的不确定性和贫穷中寻找人生真正的价值。成功就是坚持不懈，顽强不屈，就是别人花30秒放弃的事情你花22分钟去坚定思考的坚定信念。</p>
<p><strong>想法：</strong>成功机遇、努力、坚持都是必备的品质，当然还要有一点运气，而这些品质就存在于我们的族裔文化中。</p>
<h2 id="第9章-玛丽塔之幸"><a href="#第9章-玛丽塔之幸" class="headerlink" title="第9章 玛丽塔之幸"></a>第9章 玛丽塔之幸</h2><p>作者分析了美国的一个在KIPP学院上学的穷人家孩子玛丽塔的作息表，她的学习时间比普通美国孩子要多得多，当然也因此取得了更好的成绩，因此她的命运比她之前有了很大的改变。</p>
<p><strong>想法：</strong>穷人还是只有努力才能改变自己的命运。</p>
<h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p>作者讲述了他自己的家庭历史，通过一步步分析说明他母亲的是如何一点点成功的。成功人士是历史和环境的产物，是机遇与积累的结果。</p>
<p><strong>想法：</strong>现在就要努力，这样才能一点点改变你的命运，你孩子的命运，你后代的命运。</p>
<p><strong>总结：</strong>花了将近两个小时总结，最后自己也感觉不知所云，但推荐看一下《异类》。</p>
]]></content>
    
    <summary type="html">
    
      《异类》读书笔记
    
    </summary>
    
      <category term="社科人文" scheme="noahsnail.com/categories/%E7%A4%BE%E7%A7%91%E4%BA%BA%E6%96%87/"/>
    
    
      <category term="杂谈" scheme="noahsnail.com/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode198——House Robber</title>
    <link href="noahsnail.com/2017/04/08/2017-4-8-Leetcode198%E2%80%94%E2%80%94House%20Robber/"/>
    <id>noahsnail.com/2017/04/08/2017-4-8-Leetcode198——House Robber/</id>
    <published>2017-04-08T02:47:08.000Z</published>
    <updated>2017-04-08T12:47:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>ou are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security system connected and it will automatically contact the police if two adjacent houses were broken into on the same night.</p>
<p>Given a list of non-negative integers representing the amount of money of each house, determine the maximum amount of money you can rob tonight without alerting the police.</p>
<h2 id="2-求解"><a href="#2-求解" class="headerlink" title="2. 求解"></a>2. 求解</h2><p><strong>方法一</strong>递归法</p>
<p>递归法如下图所示，从第0家开始，偷与不偷有两种情况，如果偷了则从第2家开始重复此过程，如果没偷，从第1家开始重复此过程，直至结束。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt="递归过程"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">    </div><div class="line">    public int rob(int[] nums) &#123;</div><div class="line">        if(nums.length == 0) &#123;</div><div class="line">            return 0;</div><div class="line">        &#125;</div><div class="line">        return robot(0, nums);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    public int robot(int start, int[] nums) &#123;</div><div class="line">        if(start &gt;= nums.length) &#123;</div><div class="line">            return 0;</div><div class="line">        &#125;</div><div class="line">        int left = nums[start] + robot(start + 2, nums);</div><div class="line">        int right = 0 + robot(start + 1, nums);</div><div class="line">        int max = Math.max(left, right);</div><div class="line">        return max;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Leetcode运行超时，说明存在优化空间。</p>
<p><strong>方法二</strong>递归法+缓存</p>
<p>从图中可以看出，上述过程中会有重复的计算，例如从第二家开始偷的情况计算了两次，我们可以用一个HashMap将计算过的数据保存起来，去掉冗余计算。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">    public static Map&lt;Integer, Integer&gt; cache = new HashMap&lt;Integer, Integer&gt;();</div><div class="line">    </div><div class="line">    public int rob(int[] nums) &#123;</div><div class="line">        cache.clear();</div><div class="line">        if(nums.length == 0) &#123;</div><div class="line">            return 0;</div><div class="line">        &#125;</div><div class="line">        return robot(0, nums);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    public int robot(int start, int[] nums) &#123;</div><div class="line">        if(start &gt;= nums.length) &#123;</div><div class="line">            return 0;</div><div class="line">        &#125;</div><div class="line">        if(cache.containsKey(start)) &#123;</div><div class="line">            return cache.get(start);</div><div class="line">        &#125;</div><div class="line">        int left = nums[start] + robot(start + 2, nums);</div><div class="line">        int right = 0 + robot(start + 1, nums);</div><div class="line">        int max = Math.max(left, right);</div><div class="line">        cache.put(start, max);</div><div class="line">        return max;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>方法三：</strong>非递归</p>
<p>将递归改造成非递归。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">    public static Map&lt;Integer, Integer&gt; cache = new HashMap&lt;Integer, Integer&gt;();</div><div class="line">    </div><div class="line">    public int rob(int[] nums) &#123;</div><div class="line">        cache.clear();</div><div class="line">        if(nums.length == 0) &#123;</div><div class="line">            return 0;</div><div class="line">        &#125;</div><div class="line">        int n = nums.length;</div><div class="line">        cache.put(n-1, nums[n - 1]);</div><div class="line">        for(int i = n - 2; i &gt;=0; i--) &#123;</div><div class="line">            int a = nums[i] + (cache.containsKey(i + 2) ? cache.get(i + 2) : 0);</div><div class="line">            int b = cache.containsKey(i + 1) ? cache.get(i + 1) : 0;</div><div class="line">            int max = Math.max(a, b);</div><div class="line">            cache.put(i, max);</div><div class="line">        &#125;</div><div class="line">        return cache.get(0);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>方法四：</strong>HashMap变为数组</p>
<p>为了节省空间，可以将HashMap变为数组，代码也可以进一步优化，去掉三目运算符。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">public class Solution &#123;</div><div class="line">    public int rob(int[] nums) &#123;</div><div class="line">        if(nums.length == 0) &#123;</div><div class="line">            return 0;</div><div class="line">        &#125;</div><div class="line">        if(nums.length == 1) &#123;</div><div class="line">            return nums[0];</div><div class="line">        &#125;</div><div class="line">        if(nums.length == 2) &#123;</div><div class="line">            return Math.max(nums[0], nums[1]);</div><div class="line">        &#125;</div><div class="line">        int n = nums.length;</div><div class="line">        int[] cache = new int[n];</div><div class="line">        cache[n - 1] = nums[n - 1];</div><div class="line">        cache[n - 2] = Math.max(nums[n - 1], nums[n - 2]);</div><div class="line">        for(int i = n - 3; i &gt;=0; i--) &#123;</div><div class="line">            cache[i] = Math.max(nums[i] + cache[i + 2], cache[i + 1]);</div><div class="line">        &#125;</div><div class="line">        return cache[0];</div><div class="line">    &#125;   </div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Leetcode198——House Robber
    
    </summary>
    
      <category term="基础算法" scheme="noahsnail.com/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="noahsnail.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>ROC，AUC，Precision，Recall，F1的介绍与计算</title>
    <link href="noahsnail.com/2017/04/06/2017-4-6-ROC%E5%92%8CAUC%E6%9B%B2%E7%BA%BF%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E8%AE%A1%E7%AE%97/"/>
    <id>noahsnail.com/2017/04/06/2017-4-6-ROC和AUC曲线的介绍与计算/</id>
    <published>2017-04-06T03:40:00.000Z</published>
    <updated>2017-04-07T10:25:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><h3 id="1-1-ROC与AUC"><a href="#1-1-ROC与AUC" class="headerlink" title="1.1 ROC与AUC"></a>1.1 ROC与AUC</h3><p>ROC曲线和AUC常被用来评价一个二值分类器（binary classifier）的优劣，ROC曲线称为受试者工作特征曲线 （receiver operating characteristic curve，简称ROC曲线），又称为感受性曲线（sensitivity curve），AUC（Area Under Curve）是ROC曲线下的面积。在计算ROC曲线之前，首先要了解一些基本概念。在二元分类模型的预测结果有四种，以判断人是否有病为例：</p>
<ul>
<li>真阳性（TP）：诊断为有，实际上也有病。</li>
<li>伪阳性（FP）：诊断为有，实际却没有病。</li>
<li>真阴性（TN）：诊断为没有，实际上也没有病。</li>
<li>伪阴性（FN）：诊断为没有，实际却有病。</li>
</ul>
<p>其关系如下图所示：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/roc.png" alt="TP、FP、TN、FN"></p>
<p>ROC空间将伪阳性率（FPR）定义为X轴，真阳性率（TPR）定义为Y轴。TPR：在所有实际为阳性的样本中，被正确地判断为阳性之比率，$TPR=\frac {TP} {TP+FN}$ 。FPR：在所有实际为阴性的样本中，被错误地判断为阳性之比率，$FPR=\frac {FP} {FP+TN}$。</p>
<h3 id="1-2-Precision、Recall与F1"><a href="#1-2-Precision、Recall与F1" class="headerlink" title="1.2 Precision、Recall与F1"></a>1.2 Precision、Recall与F1</h3><p>对于二分类问题另一个常用的评价指标是精确率（precision）与召回率（recall）以及F1值。精确率表示在预测为阳性的样本中，真正有阳性的样本所占的比例。精确率的定义为$P=\frac {TP} {TP+FP}$。召回率表示所有真正呈阳性的样本中，预测为阳性所占的比例。召回率的定义为$R=\frac {TP} {TP+FN}$，F1值是精确率和召回率的调和均值，公式为$F1=\frac {2PR} {P+R}$。精确率和召回率都高时，F1值也会高。通常情况下，Precision与Recall是相互矛盾的。</p>
<h2 id="2-曲线介绍"><a href="#2-曲线介绍" class="headerlink" title="2. 曲线介绍"></a>2. 曲线介绍</h2><h3 id="2-1-ROC曲线"><a href="#2-1-ROC曲线" class="headerlink" title="2.1 ROC曲线"></a>2.1 ROC曲线</h3><p>ROC曲线坐标系如下图所示，虚线为随机猜测的概率，即猜对跟猜错的概率是一样的。理想情况下，我们是希望FPR为0，没有一个假阳性，TPR为1，即全为真阳性，此时所有样本都被正确分类，点位于左上角(0,1)位置处，没有一个分错的数据，这是最完美的情况，实际情况中基本不可能。如果点位于虚线下方，例如C点，说明分类错误的多，分类正确的少，此时不是我们想要的。如果点位于虚线上方，例如$C \prime$点，说明分类错误的少，分类正确的多，此时是我们想要的，因此我们希望ROC曲线尽可能的靠近左上角。对于一个特定的分类器和测试数据集，只能得到一个分类结果，即ROC曲线坐标系中的一点，那么如何得到一条ROC曲线呢？分类问题中我们经常会得到某个样本是正样本的概率，根据概率值与阈值的比较来判断某个样本是否是正样本。在不同的阈值下可以得到不同的TPR和FPR值，即可以得到一系列的点，将它们在图中绘制出来，并依次连接起来就得到了ROC曲线，阈值取值越多，ROC曲线越平滑。</p>
<p>AUC为ROC曲线下的面积，它的面积不会大于1，由于ROC曲线一般都处于直线y=x的上方，因此AUC的取值范围通常在(0.5，1)之间。由于ROC曲线不能很好的看出分类器模型的好坏，因此采用AUC值来进行分类器模型的评估与比较。通常AUC值越大，分类器性能越好。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/700px-ROC_space-2.png" alt="ROC曲线"></p>
<p>在基本概念中我们提到了精确率、召回率以及F1值，既然有它们作为二分类的评价指标，为什么还要使用ROC和AUC呢？这是因为ROC曲线有个很好的特性：当测试集中的正负样本分布发生变化时，即正负样本数量相差较大时，ROC曲线仍能保持不变。实际数据集中经常会出现样本数量不平衡现象，并且测试数据中的正负样本的分布也可能随着时间发生变化。下图是两个分类器模型（算法）的ROC曲线比较图：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/roc_com.png" alt="ROC曲线比较"></p>
<h3 id="2-2-P-R曲线"><a href="#2-2-P-R曲线" class="headerlink" title="2.2 P-R曲线"></a>2.2 P-R曲线</h3><p>在P-R曲线中，Precision为横坐标，Recall为纵坐标。在ROC曲线中曲线越凸向左上角约好，在P-R曲线中，曲线越凸向右上角越好。P-R曲线判断模型的好坏要根据具体情况具体分析，有的项目要求召回率较高、有的项目要求精确率较高。P-R曲线的绘制跟ROC曲线的绘制是一样的，在不同的阈值下得到不同的Precision、Recall，得到一系列的点，将它们在P-R图中绘制出来，并依次连接起来就得到了P-R图。两个分类器模型（算法）P-R曲线比较的一个例子如下图所示：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/p-r.png" alt="P-R曲线比较"></p>
<h3 id="2-3-ROC与P-R对比"><a href="#2-3-ROC与P-R对比" class="headerlink" title="2.3 ROC与P-R对比"></a>2.3 ROC与P-R对比</h3><p>从公式计算中可以看出，ROC曲线中真阳性率TPR的计算公式与P-R曲线中的召回率Recall计算公式是一样的，即二者是同一个东西在不同环境下的不同叫法。当正负样本差距不大的情况下，ROC曲线和P-R的趋势是差不多的，但是当负样本很多的时候，ROC曲线效果依然较好，但是P-R曲线效果一般。</p>
<h2 id="3-Demo"><a href="#3-Demo" class="headerlink" title="3. Demo"></a>3. Demo</h2><p>待续。</p>
]]></content>
    
    <summary type="html">
    
      ROC，AUC，Precision，Recall，F1的介绍与计算
    
    </summary>
    
      <category term="机器学习" scheme="noahsnail.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="noahsnail.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode109——Convert Sorted List to Binary Search Tree</title>
    <link href="noahsnail.com/2017/03/24/2017-3-24-Leetcode109%E2%80%94%E2%80%94Convert%20Sorted%20List%20to%20Binary%20Search%20Tree/"/>
    <id>noahsnail.com/2017/03/24/2017-3-24-Leetcode109——Convert Sorted List to Binary Search Tree/</id>
    <published>2017-03-24T01:17:38.000Z</published>
    <updated>2017-03-24T01:30:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>Given a singly linked list where elements are sorted in ascending order, convert it to a height balanced BST.</p>
<h2 id="2-求解"><a href="#2-求解" class="headerlink" title="2. 求解"></a>2. 求解</h2><p>这个题主要是根据一个有序链表构造二叉查找树（树的左结点小于根节点，根节点小于右结点，子树具有同样的性质）。与有序数组最大的不同在于有序链表只能从前往后遍历，不能像有序数组一样访问任意位置的元素。因此构造时需要按顺序构造，其实有序链表是二叉查找树的中序遍历。因此需要按照中序遍历的顺序进行构建，先构建左子树，再构造根节点，最后构造右子树。由于是链表，每次构造之后头结点应该进行移动，Java中用了一个静态变量来保存根节点的位置。构造方法主要是递归，每次构建子树时都需要将数组分成左右两半，左边的构建左子树，右边的构建右子树，中间元素构造根节点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * Definition for singly-linked list.</div><div class="line"> * public class ListNode &#123;</div><div class="line"> *     int val;</div><div class="line"> *     ListNode next;</div><div class="line"> *     ListNode(int x) &#123; val = x; &#125;</div><div class="line"> * &#125;</div><div class="line"> */</div><div class="line">/**</div><div class="line"> * Definition for a binary tree node.</div><div class="line"> * public class TreeNode &#123;</div><div class="line"> *     int val;</div><div class="line"> *     TreeNode left;</div><div class="line"> *     TreeNode right;</div><div class="line"> *     TreeNode(int x) &#123; val = x; &#125;</div><div class="line"> * &#125;</div><div class="line"> */</div><div class="line">public class Solution &#123;</div><div class="line">    static ListNode h;</div><div class="line">    </div><div class="line">    public TreeNode sortedListToBST(ListNode head) &#123;</div><div class="line">        if(head == null) &#123;</div><div class="line">            return null;</div><div class="line">        &#125;</div><div class="line">        int length = 0;</div><div class="line">        h = head;</div><div class="line">        //得到链表长度</div><div class="line">        while(head != null) &#123;</div><div class="line">            length++;</div><div class="line">            head = head.next;</div><div class="line">        &#125;</div><div class="line">        return buildBinarySearchTree(h, 0, length - 1);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    public TreeNode buildBinarySearchTree(ListNode head, int start, int end) &#123;</div><div class="line">        if(start &gt; end) &#123;</div><div class="line">            return null;</div><div class="line">        &#125;</div><div class="line">        int mid = (start + end) / 2;</div><div class="line">        //先构建左子树</div><div class="line">        TreeNode left = buildBinarySearchTree(h, start, mid - 1);</div><div class="line">        //再构造根节点</div><div class="line">        TreeNode root = new TreeNode(h.val);</div><div class="line">        h = h.next;</div><div class="line">        //最后构造右子树</div><div class="line">        TreeNode right = buildBinarySearchTree(h, mid + 1, end);</div><div class="line">        root.left = left;</div><div class="line">        root.right = right;</div><div class="line">        return root;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Leetcode109——Convert Sorted List to Binary Search Tree
    
    </summary>
    
      <category term="基础算法" scheme="noahsnail.com/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="noahsnail.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
