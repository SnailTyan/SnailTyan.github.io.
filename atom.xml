<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="noahsnail.com/"/>
  <updated>2017-07-21T09:24:20.000Z</updated>
  <id>noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AlexNet论文翻译——中文版</title>
    <link href="noahsnail.com/2017/07/18/2017-7-18-AlexNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/07/18/2017-7-18-AlexNet论文翻译——中文版/</id>
    <published>2017-07-18T10:00:15.000Z</published>
    <updated>2017-07-21T09:24:20.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="ImageNet-Classification-with-Deep-Convolutional-Neural-Networks"><a href="#ImageNet-Classification-with-Deep-Convolutional-Neural-Networks" class="headerlink" title="ImageNet Classification with Deep Convolutional Neural Networks"></a>ImageNet Classification with Deep Convolutional Neural Networks</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们训练了一个大型深度卷积神经网络来将<code>ImageNet LSVRC-2010</code>竞赛的120万高分辨率的图像分到1000不同的类别中。在测试数据上，我们得到了<code>top-1 37.5%, top-5 17.0%</code>的错误率，这个结果比目前的最好结果好很多。这个神经网络有6000万参数和650000个神经元，包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，最后是一个1000维的softmax。为了训练的更快，我们使用了非饱和神经元并对卷积操作进行了非常有效的GPU实现。为了减少全连接层的过拟合，我们采用了一个最近开发的名为<code>dropout</code>的正则化方法，结果证明是非常有效的。我们也使用这个模型的一个变种参加了<code>ILSVRC-2012</code>竞赛，赢得了冠军并且与第二名 <code>top-5 26.2%</code>的错误率相比，我们取得了<code>top-5 15.3%</code>的错误率。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>当前的目标识别方法基本上都使用了机器学习方法。为了提高目标识别的性能，我们可以收集更大的数据集，学习更强大的模型，使用更好的技术来防止过拟合。直到最近，标注图像的数据集都相对较小--在几万张图像的数量级上（例如，NORB[16]，Caltech-101/256 [8, 9]和CIFAR-10/100 [12]）。简单的识别任务在这样大小的数据集上可以被解决的相当好，尤其是如果通过标签保留变换进行数据增强的情况下。例如，目前在MNIST数字识别任务上（&lt;0.3%）的最好准确率已经接近了人类水平[4]。但真实环境中的对象表现出了相当大的可变性，因此为了学习识别它们，有必要使用更大的训练数据集。实际上，小图像数据集的缺点已经被广泛认识到（例如，Pinto et al. [21]），但收集上百万图像的标注数据仅在最近才变得的可能。新的更大的数据集包括LabelMe [23]，它包含了数十万张完全分割的图像，ImageNet [6]，它包含了22000个类别上的超过1500万张标注的高分辨率的图像。</p>
<p>为了从数百万张图像中学习几千个对象，我们需要一个有很强学习能力的模型。然而对象识别任务的巨大复杂性意味着这个问题不能被指定，即使通过像ImageNet这样的大数据集，因此我们的模型应该也有许多先验知识来补偿我们所没有的数据。卷积神经网络(CNNs)构成了一个这样的模型[16, 11, 13, 18, 15, 22, 26]。它们的能力可以通过改变它们的广度和深度来控制，它们也可以对图像的本质进行强大且通常正确的假设（也就是说，统计的稳定性和像素依赖的局部性）。因此，与具有层次大小相似的标准前馈神经网络，CNNs有更少的连接和参数，因此它们更容易训练，而它们理论上的最佳性能可能仅比标准前馈神经网络差一点。</p>
<p>尽管CNN具有引人注目的质量，尽管它们的局部架构相当有效，但将它们大规模的应用到到高分辨率图像中仍然是极其昂贵的。幸运的是，目前的GPU，搭配了高度优化的2D卷积实现，强大到足够促进有趣地大量CNN的训练，最近的数据集例如ImageNet包含足够的标注样本来训练这样的模型而没有严重的过拟合。</p>
<p>本文具体的贡献如下：我们在ILSVRC-2010和ILSVRC-2012[2]的ImageNet子集上训练了到目前为止最大的神经网络之一，并取得了迄今为止在这些数据集上报道过的最好结果。我们编写了高度优化的2D卷积GPU实现以及训练卷积神经网络内部的所有其它操作，我们把它公开了。我们的网络包含许多新的不寻常的特性，这些特性提高了神经网络的性能并减少了训练时间，详见第三节。即使使用了120万标注的训练样本，我们的网络尺寸仍然使过拟合成为一个明显的问题，因此我们使用了一些有效的技术来防止过拟合，详见第四节。我们最终的网络包含5个卷积层和3个全连接层，深度似乎是非常重要的：我们发现移除任何卷积层（每个卷积层包含的参数不超过模型参数的1%）都会导致更差的性能。</p>
<p>最后，网络尺寸主要受限于目前GPU的内存容量和我们能忍受的训练时间。我们的网络在两个GTX 580 3GB GPU上训练五六天。我们的所有实验表明我们的结果可以简单地通过等待更快的GPU和更大的可用数据集来提高。</p>
<h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2 数据集"></a>2 数据集</h2><p>ImageNet数据集有超过1500万的标注高分辨率图像，这些图像属于大约22000个类别。这些图像是从网上收集的，使用了Amazon’s Mechanical Turk的众包工具通过人工标注的。从2010年起，作为Pascal视觉对象挑战赛的一部分，每年都会举办ImageNet大规模视觉识别挑战赛（ILSVRC）。ILSVRC使用ImageNet的一个子集，1000个类别每个类别大约1000张图像。总计，大约120万训练图像，50000张验证图像和15万测试图像。</p>
<p>ILSVRC-2010是ILSVRC竞赛中唯一可以获得测试集标签的版本，因此我们大多数实验都是在这个版本上运行的。由于我们也使用我们的模型参加了ILSVRC-2012竞赛，因此在第六节我们也报告了模型在这个版本的数据集上的结果，这个版本的测试标签是不可获得的。在ImageNet上，按照惯例报告两个错误率：<code>top-1</code>和<code>top-5</code>，<code>top-5</code>错误率是指测试图像的正确标签不在模型认为的五个最可能的便签之中。</p>
<p>ImageNet包含各种分辨率的图像，而我们的系统要求不变的输入维度。因此，我们将图像进行下采样到固定的<code>256×256</code>分辨率。给定一个矩形图像，我们首先缩放图像短边长度为256，然后从结果图像中裁剪中心的<code>256×256</code>大小的图像块。除了在训练集上对像素减去平均活跃度外，我们不对图像做任何其它的预处理。因此我们在原始的RGB像素值（中心的）上训练我们的网络。</p>
<h2 id="3-架构"><a href="#3-架构" class="headerlink" title="3 架构"></a>3 架构</h2><p>我们的网络架构概括为图2。它包含八个学习层--5个卷积层和3个全连接层。下面，我们将描述我们网络结构中的一些新奇的不寻常的特性。3.1-3.4小节按照我们对它们评估的重要性进行排序，最重要的最有先。</p>
<h3 id="3-1-ReLU非线性"><a href="#3-1-ReLU非线性" class="headerlink" title="3.1 ReLU非线性"></a>3.1 ReLU非线性</h3><p>将神经元输出<code>f</code>建模为输入<code>x</code>的函数的标准方式是用<code>f(x) = tanh(x)</code>或<code>f(x) = (1 + e−x)−1</code>。考虑到梯度下降的训练时间，这些饱和的非线性比非饱和非线性<code>f(x) = max(0,x)</code>更慢。根据Nair和Hinton[20]的说法，我们将这种非线性神经元称为修正线性单元(ReLU)。采用ReLU的深度卷积神经网络训练时间比等价的<code>tanh</code>单元要快几倍。在图1中，对于一个特定的四层卷积网络，在CIFAR-10数据集上达到25%的训练误差所需要的迭代次数可以证实这一点。这幅图表明，如果我们采用传统的饱和神经元模型，我们将不能在如此大的神经网络上实验该工作。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%201.png" alt="Figure 1"></p>
<p>图1：使用ReLU的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍。为了使训练尽可能快，每个网络的学习率是单独选择的。没有采用任何类型的正则化。影响的大小随着网络结构的变化而变化，这一点已得到证实，但使用ReLU的网络都比等价的饱和神经元快几倍。</p>
<p>我们不是第一个考虑替代CNN中传统神经元模型的人。例如，Jarrett等人[11]声称非线性函数<code>f(x) = |tanh(x)|</code>与其对比度归一化一起，然后是局部均值池化，在Caltech-101数据集上工作的非常好。然而，在这个数据集上主要的关注点是防止过拟合，因此他们观测到的影响不同于我们使用ReLU拟合数据集时的加速能力。更快的学习对大型数据集上大型模型的性能有很大的影响。</p>
<h3 id="3-2-多GPU训练"><a href="#3-2-多GPU训练" class="headerlink" title="3.2 多GPU训练"></a>3.2 多GPU训练</h3><p>单个GTX580 GPU只有3G内存，这限制了可以在GTX580上进行训练的网络最大尺寸。事实证明120万图像用来进行网络训练是足够的，但网络太大因此不能在单个GPU上进行训练。因此我们将网络分布在两个GPU上。目前的GPU非常适合跨GPU并行，因为它们可以直接互相读写内存，而不需要通过主机内存。我们采用的并行方案基本上每个GPU放置一半的核（或神经元），还有一个额外的技巧：只在某些特定的层上进行GPU通信。这意味着，例如，第3层的核会将第2层的所有核映射作为输入。然而，第4层的核只将位于相同GPU上的第3层的核映射作为输入。连接模式的选择是一个交叉验证问题，但这可以让我们准确地调整通信数量，直到它的计算量在可接受的范围内。</p>
<p>除了我们的列不是独立的之外（看图2），最终的架构有点类似于Ciresan等人[5]采用的“columnar” CNN。与每个卷积层一半的核在单GPU上训练的网络相比，这个方案降分别低了我们的<code>top-1 1.7%</code>，<code>top-5 1.2%</code>的错误率。双GPU网络比单GPU网络稍微减少了训练时间。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Fig%202.png" alt="Figure 2"></p>
<p>图 2：我们CNN架构图解，明确描述了两个GPU之间的责任。在图的顶部，一个GPU运行在部分层上，而在图的底部，另一个GPU运行在部分层上。GPU只在特定的层进行通信。网络的输入是150,528维，网络剩下层的神经元数目分别是253,440–186,624–64,896–64,896–43,264–4096–4096–1000（8层）。</p>
<h3 id="3-3-局部响应归一化"><a href="#3-3-局部响应归一化" class="headerlink" title="3.3 局部响应归一化"></a>3.3 局部响应归一化</h3><p>ReLU具有让人满意的特性，它不需要通过输入归一化来防止饱和。如果至少一些训练样本对ReLU产生了正输入，那么那个神经元上将发生学习。然而，我们仍然发现接下来的局部响应归一化有助于泛化。$a_{x,y}^i$表示神经元激活，通过在$(x, y)$位置应用核$i$，然后应用ReLU非线性来计算，响应归一化激活$b^i_{x,y}$通过下式给定：</p>
<p>$$b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta$$</p>
<p>求和运算在n个“毗邻的”核映射的同一位置上执行，N是本层的卷积核数目。核映射的顺序当然是任意的，在训练开始前确定。响应归一化的顺序实现了一种侧抑制形式，灵感来自于真实神经元中发现的类型，为使用不同核进行神经元输出计算的较大活动创造了竞争。常量k，n，α，β是超参数，它们的值通过验证集确定；我们设k=2，n=5，α=0.0001，β=0.75。我们在特定的层使用的ReLU非线性之后应用了这种归一化（请看3.5小节）。</p>
<p>这个方案与Jarrett等人[11]的局部对比度归一化方案有一定的相似性，但我们更恰当的称其为“亮度归一化”，因此我们没有减去均值。响应归一化分别减少了<code>top-1 1.4%</code>，<code>top-5 1.2%</code>的错误率。我们也在CIFAR-10数据集上验证了这个方案的有效性：一个乜嘢归一化的四层CNN取得了13%的错误率，而使用归一化取得了11%的错误率。</p>
<h3 id="3-4-重叠池化"><a href="#3-4-重叠池化" class="headerlink" title="3.4 重叠池化"></a>3.4 重叠池化</h3><p>CNN中的池化层归纳了同一核映射上相邻组神经元的输出。习惯上，相邻池化单元归纳的区域是不重叠的（例如[17, 11, 4]）。更确切的说，池化层可看作由池化单元网格组成，网格间距为$s$个像素，每个网格归纳池化单元中心位置$z × z$大小的邻居。如果设置$s = z$，我们会得到通常在CNN中采用的传统局部池化。如果设置$s &lt; z$，我们会得到重叠池化。这就是我们网络中使用的方法，设置$s = 2$，$z = 3$。这个方案分别降低了<code>top-1 0.4%</code>，<code>top-5 0.3%</code>的错误率，与非重叠方案$s = 2，z = 2$相比，输出的维度是相等的。我们在训练过程中通常观察采用重叠池化的模型，发现它更难过拟合。</p>
<h3 id="3-5-整体架构"><a href="#3-5-整体架构" class="headerlink" title="3.5 整体架构"></a>3.5 整体架构</h3><p>现在我们准备描述我们的CNN的整体架构。如图2所示，我们的网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签的分布。我们的网络最大化多项逻辑回归的目标，这等价于最大化预测分布下训练样本正确标签的对数概率的均值。</p>
<p>第2，4，5卷积层的核只与位于同一GPU上的前一层的核映射相连接（看图2）。第3卷积层的核与第2层的所有核映射相连。全连接层的神经元与前一层的所有神经元相连。第1，2卷积层之后是响应归一化层。3.4节描述的这种最大池化层在响应归一化层和第5卷积层之后。ReLU非线性应用在每个卷积层和全连接层的输出上。</p>
<p>第1卷积层使用96个核对224 × 224 × 3的输入图像进行滤波，核大小为11 × 11 × 3，步长是4个像素（核映射中相邻神经元感受野中心之间的距离）。第2卷积层使用用第1卷积层的输出（响应归一化和池化）作为输入，并使用256个核进行滤波，核大小为5 × 5 × 48。第3，4，5卷积层互相连接，中间没有接入池化层或归一化层。第3卷积层有384个核，核大小为3 × 3 × 256，与第2卷积层的输出（归一化的，池化的）相连。第4卷积层有384个核，核大小为3 × 3 × 192，第5卷积层有256个核，核大小为3 × 3 × 192。每个全连接层有4096个神经元。</p>
<h2 id="4-减少过拟合"><a href="#4-减少过拟合" class="headerlink" title="4 减少过拟合"></a>4 减少过拟合</h2><p>我们的神经网络架构有6000万参数。尽管ILSVRC的1000类使每个训练样本从图像到标签的映射上强加了10比特的约束，但这不足以学习这么多的参数而没有相当大的过拟合。下面，我们会描述我们用来克服过拟合的两种主要方式。</p>
<h3 id="4-1-数据增强"><a href="#4-1-数据增强" class="headerlink" title="4.1 数据增强"></a>4.1 数据增强</h3><p>图像数据上最简单常用的用来减少过拟合的方法是使用标签保留变换（例如[25, 4, 5]）来人工增大数据集。我们使用了两种独特的数据增强方式，这两种方式都可以从原始图像通过非常少的计算量产生变换的图像，因此变换图像不需要存储在硬盘上。在我们的实现中，变换图像通过CPU的Python代码生成，而此时GPU正在训练前一批图像。因此，实际上这些数据增强方案是计算免费的。</p>
<p>第一种数据增强方式包括产生图像变换和水平翻转。我们从256×256图像上通过随机提取224 × 224的图像块实现了这种方式，然后在这些提取的图像块上进行训练。这通过一个2048因子增大了我们的训练集，尽管最终的训练样本是高度相关的。没有这个方案，我们的网络会有大量的过拟合，这会迫使我们使用更小的网络。在测试时，网络会提取5个224 × 224的图像块（四个角上的图像块和中心的图像块）和它们的水平翻转（因此总共10个图像块）进行预测，然后对网络在10个图像块上的softmax层进行平均。</p>
<p>第二种数据增强方式包括改变训练图像的RGB通道的强度。具体地，我们在整个ImageNet训练集上对RGB像素值集合执行PCA。对于每幅训练图像，我们加上多倍找到的主成分，大小成正比的对应特征值乘以一个随机变量，随机变量通过均值为0，标准差为0.1的高斯分布得到。因此对于每幅RGB图像像素$I_xy = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$，我们加上下面的数量：</p>
<p>$$[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T$$</p>
<p>$p_i$，$\lambda_i$分别是RGB像素值3 × 3协方差矩阵的第$i$个特征向量和特征值，$\alpha_i$是前面提到的随机变量。对于某个训练图像的所有像素，每个$\alpha_i$只获取一次，直到图像进行下一次训练时才重新获取。这个方案近似抓住了自然图像的一个重要特性，即光照的颜色和强度发生变化时，目标身份是不变的。这个方案减少了<code>top 1</code>错误率1%以上</p>
<h3 id="4-2-失活-Dropout"><a href="#4-2-失活-Dropout" class="headerlink" title="4.2 失活(Dropout)"></a>4.2 失活(Dropout)</h3><p>将许多不同模型的预测结合起来是降低测试误差[1, 3]的一个非常成功的方法，但对于需要花费几天来训练的大型神经网络来说，这似乎太昂贵了。然而，有一个非常有效的模型结合版本，它只花费两倍的训练成本。这种最近引入的技术，叫做“dropout”[10]，它会以0.5的概率对每个隐层神经元的输出设为0。那些“失活的”的神经元不再进行前向传播并且不参与反向传播。因此每次输入时，神经网络会采样一个不同的架构，但所有架构共享权重。这个技术减少了复杂的神经元互适应，因为一个神经元不能依赖特定的其它神经元的存在。因此，神经元被强迫学习更鲁棒的特征，它在与许多不同的其它神经元的随机子集结合时是有用的。在测试时，我们使用所有的神经元但它们的输出乘以0.5，对指数级的许多失活网络的预测分布进行几何平均，这是一种合理的近似。</p>
<p>我们在图2中的前两个全连接层使用失活。如果没有失活，我们的网络表现出大量的过拟合。失活大致上使要求收敛的迭代次数翻了一倍。</p>
<h2 id="5-学习细节"><a href="#5-学习细节" class="headerlink" title="5 学习细节"></a>5 学习细节</h2><p>我们使用随机梯度下降来训练我们的模型，样本的batch size为128，动量为0.9，权重衰减为0.0005。我们发现少量的权重衰减对于模型的学习是重要的。换句话说，权重衰减不仅仅是一个正则项：它减少了模型的训练误差。权重$w$的更新规则是</p>
<p>$$v_{i+1} := 0.9 \bullet v_i - 0.0005 \bullet \varepsilon \bullet w_i - \varepsilon \bullet \langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$$</p>
<p>$i$是迭代索引，$v$是动量变量，$\varepsilon$是学习率，$\langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$是目标函数对$w$，在$w_i$上的第$i$批微分$D_i$的平均。</p>
<p>我们使用均值为0，标准差为0.01的高斯分布对每一层的权重进行初始化。我们在第2，4，5卷积层和全连接隐层将神经元偏置初始化为常量1。这个初始化通过为ReLU提供正输入加速了学习的早期阶段。我们在剩下的层将神经元偏置初始化为0。</p>
<p>我们对所有的层使用相等的学习率，这个是在整个训练过程中我们手动调整得到的。当验证误差在当前的学习率下停止提供时，我们遵循启发式的方法将学习率除以10。学习率初始化为0.01，在训练停止之前降低三次。我们在120万图像的训练数据集上训练神经网络大约90个循环，在两个NVIDIA GTX 580 3GB GPU上花费了五到六天。</p>
<h2 id="6-结果"><a href="#6-结果" class="headerlink" title="6 结果"></a>6 结果</h2><p>我们在ILSVRC-2010上的结果概括为表1。我们的神经网络取得了<code>top-1 37.5%</code>，<code>top-5 17.0%</code>的错误率。在ILSVRC-2010竞赛中最佳结果是<code>top-1 47.1%</code>，<code>top-5 28.2%</code>，使用的方法是对6个在不同特征上训练的稀疏编码模型生成的预测进行平均，从那时起已公布的最好结果是<code>top-1 45.7%</code>，<code>top-5 25.7%</code>，使用的方法是平均在Fisher向量（FV）上训练的两个分类器的预测结果，Fisher向量是通过两种密集采样特征计算得到的[24]。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%201.png" alt="表1"></p>
<p>表1：ILSVRC-2010测试集上的结果对比。斜体是其它人取得的最好结果。</p>
<p>我们也用我们的模型参加了ILSVRC-2012竞赛并在表2中报告了我们的结果。由于ILSVRC-2012的测试集标签不可以公开得到，我们不能报告我们尝试的所有模型的测试错误率。在这段的其余部分，我们会使用验证误差率和测试误差率互换，因为在我们的实验中它们的差别不会超过0.1%（看图2）。本文中描述的CNN取得了<code>top-5 18.2%</code>的错误率。五个类似的CNN预测的平均误差率为16.4%。为了对ImageNet 2011秋季发布的整个数据集（1500万图像，22000个类别）进行分类，我们在最后的池化层之后有一个额外的第6卷积层，训练了一个CNN，然后在它上面进行“fine-tuning”，在ILSVRC-2012取得了16.6%的错误率。对在ImageNet 2011秋季发布的整个数据集上预训练的两个CNN和前面提到的五个CNN的预测进行平均得到了15.3%的错误率。第二名的最好竞赛输入取得了26.2%的错误率，他的方法是对FV上训练的一些分类器的预测结果进行平均，FV在不同类型密集采样特征计算得到的。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%202.png" alt="表2"></p>
<p>表2：ILSVRC-2012验证集和测试集的误差对比。斜线部分是其它人取得的最好的结果。带星号的是“预训练的”对ImageNet 2011秋季数据集进行分类的模型。更多细节请看第六节。</p>
<p>最后，我们也报告了我们在ImageNet 2009秋季数据集上的误差率，ImageNet 2009秋季数据集有10,184个类，890万图像。在这个数据集上我们按照惯例用一半的图像来训练，一半的图像来测试。由于没有建立测试集，我们的数据集分割有必要不同于以前作者的数据集分割，但这对结果没有明显的影响。我们在这个数据集上的的top-1和top-5错误率是67.4%和40.9%，使用的是上面描述的在最后的池化层之后有一个额外的第6卷积层网络。这个数据集上公开可获得的最好结果是78.1%和60.9%[19]。</p>
<h3 id="6-1-定性评估"><a href="#6-1-定性评估" class="headerlink" title="6.1 定性评估"></a>6.1 定性评估</h3><p>图3显示了网络的两个数据连接层学习到的卷积核。网络学习到了大量的频率核、方向选择核，也学到了各种颜色点。注意两个GPU表现出的专业化，3.5小节中描述的受限连接的结果。GPU 1上的核主要是没有颜色的，而GPU 2上的核主要是针对颜色的。这种专业化在每次运行时都会发生，并且是与任何特别的随机权重初始化（以GPU的重新编号为模）无关的。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%203.png" alt="Figure 3"></p>
<p>图3：第一卷积层在224×224×3的输入图像上学习到的大小为11×11×3的96个卷积核。上面的48个核是在GPU 1上学习到的而下面的48个卷积核是在GPU 2上学习到的。更多细节请看6.1小节。</p>
<p>在图4的左边部分，我们通过在8张测试图像上计算它的top-5预测定性地评估了网络学习到的东西。注意即使是不在图像中心的目标也能被网络识别，例如左上角的小虫。大多数的top-5标签似乎是合理的。例如，对于美洲豹来说，只有其它类型的猫被认为是看似合理的标签。在某些案例（格栅，樱桃）中，网络在意的图片焦点真的很含糊。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%204.png" alt="Figure 4"></p>
<p>图4：（左）8张ILSVRC-2010测试图像和我们的模型认为最可能的5个标签。每张图像的下面是它的正确标签，正确标签的概率用红条表示（如果正确标签在top 5中）。（右）第一列是5张ILSVRC-2010测试图像。剩下的列展示了6张训练图像，这些图像在最后的隐藏层的特征向量与测试图像的特征向量有最小的欧氏距离。</p>
<p>探索网络可视化知识的另一种方式是思考最后的4096维隐藏层在图像上得到的特征激活。如果两幅图像生成的特征激活向量之间有较小的欧式距离，我们可以认为神经网络的更高层特征认为它们是相似的。图4表明根据这个度量标准，测试集的5张图像和训练集的6张图像中的每一张都是最相似的。注意在像素级别，检索到的训练图像与第一列的查询图像在L2上通常是不接近的。例如，检索的狗和大象似乎有很多姿态。我们在补充材料中对更多的测试图像呈现了这种结果。</p>
<p>通过两个4096维实值向量间的欧氏距离来计算相似性是效率低下的，但通过训练一个自动编码器将这些向量压缩为短二值编码可以使其变得高效。这应该会产生一种比将自动编码器应用到原始像素上[14]更好的图像检索方法，自动编码器应用到原始像素上的方法没有使用图像标签，因此会趋向于检索与要检索的图像具有相似边缘模式的图像，无论它们是否是语义上相似。</p>
<h2 id="7-探讨"><a href="#7-探讨" class="headerlink" title="7 探讨"></a>7 探讨</h2><p>我们的结果表明一个大型深度卷积神经网络在一个具有高度挑战性的数据集上使用纯有监督学习可以取得破纪录的结果。值得注意的是，如果移除一个卷积层，我们的网络性能会降低。例如，移除任何中间层都会引起网络损失大约2%的top-1性能。因此深度对于实现我们的结果非常重要。</p>
<p>为了简化我们的实验，我们没有使用任何无监督的预训练，尽管我们希望它会有所帮助，特别是在如果我们能获得足够的计算能力来显著增加网络的大小而标注的数据量没有对应增加的情况下。到目前为止，我们的结果已经提高了，因为我们的网络更大、训练时间更长，但为了匹配人类视觉系统的下颞线（视觉专业术语）我们仍然有许多数量级要达到。最后我们想在视频序列上使用非常大的深度卷积网络，视频序列的时序结构会提供非常有帮助的信息，这些信息在静态图像上是缺失的或远不那么明显。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] R.M.BellandY.Koren.Lessonsfromthenetflixprizechallenge.ACMSIGKDDExplorationsNewsletter, 9(2):75–79, 2007.</p>
<p>[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.imagenet.org/challenges. 2010.</p>
<p>[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.</p>
<p>[4] D. Cires ̧an, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. Arxiv preprint arXiv:1202.2745, 2012.</p>
<p>[5] D.C. Cires ̧an, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classification. Arxiv preprint arXiv:1102.0183, 2011.</p>
<p>[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.</p>
<p>[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL <a href="http://www.image-net.org/challenges/LSVRC/2012/" target="_blank" rel="external">http://www.image-net.org/challenges/LSVRC/2012/</a>.</p>
<p>[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59–70, 2007.</p>
<p>[9] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. URL <a href="http://authors.library.caltech.edu/7694" target="_blank" rel="external">http://authors.library.caltech.edu/7694</a>.</p>
<p>[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.</p>
<p>[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.</p>
<p>[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009.</p>
<p>[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.</p>
<p>[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In ESANN, 2011.</p>
<p>[15] Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, 1990.</p>
<p>[16] Y. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.</p>
<p>[17] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256. IEEE, 2010.</p>
<p>[18] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.</p>
<p>[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer Vision, Florence, Italy, October 2012.</p>
<p>[20] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 2010.</p>
<p>[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computational biology, 4(1):e27, 2008.</p>
<p>[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,2009.</p>
<p>[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for image annotation. International journal of computer vision, 77(1):157–173, 2008.</p>
<p>[24] J.SánchezandF.Perronnin.High-dimensionalsignaturecompressionforlarge-scaleimageclassification. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,2011.</p>
<p>[25] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition, volume 2, pages 958–962, 2003.</p>
<p>[26] S.C.Turaga,J.F.Murray,V.Jain,F.Roth,M.Helmstaedter,K.Briggman,W.Denk,andH.S.Seung.Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22(2):511–538, 2010.</p>
]]></content>
    
    <summary type="html">
    
      AlexNet论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux快捷键总结</title>
    <link href="noahsnail.com/2017/07/18/2017-7-18-Linux%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%80%BB%E7%BB%93/"/>
    <id>noahsnail.com/2017/07/18/2017-7-18-Linux快捷键总结/</id>
    <published>2017-07-18T06:11:02.000Z</published>
    <updated>2017-07-18T08:44:50.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/002.jpg" alt="image"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文的Linux快捷键总结主要是作者使用Linux过程中常用的。</p>
<ul>
<li>清屏，等价于clear命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + l</div></pre></td></tr></table></figure>
<ul>
<li>切换到命令行开始</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + a</div></pre></td></tr></table></figure>
<ul>
<li>切换到命令行末尾</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + e</div></pre></td></tr></table></figure>
<ul>
<li>剪切光标至行首内容，可以用来清除输入的命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + w</div></pre></td></tr></table></figure>
<ul>
<li>粘贴</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + y</div></pre></td></tr></table></figure>
<ul>
<li>终止当前运行的程序</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + c</div></pre></td></tr></table></figure>
<ul>
<li>退出当前shell</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + d</div></pre></td></tr></table></figure>
<ul>
<li>查找历史命令</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ctrl + r</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Linux快捷键总结
    
    </summary>
    
      <category term="Linux" scheme="noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Caffe关闭日志输出</title>
    <link href="noahsnail.com/2017/07/18/2017-7-18-Caffe%E5%85%B3%E9%97%ADlog%E8%BE%93%E5%87%BA/"/>
    <id>noahsnail.com/2017/07/18/2017-7-18-Caffe关闭log输出/</id>
    <published>2017-07-18T05:34:58.000Z</published>
    <updated>2017-07-18T05:49:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>在训练Caffe模型后，部署Caffe服务时我们通常会使用pycaffe来加载模型并处理图像，但是pycaffe加载模型时通常会输出加载模型的日志，影响我们查看自己的日志，因此需要移除Caffe加载模型时的日志。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">os.environ[&apos;GLOG_minloglevel&apos;] = &apos;2&apos;</div><div class="line">import caffe</div></pre></td></tr></table></figure>
<p>Caffe使用的日志是GLOG，其日志级别如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">0 - debug</div><div class="line">1 - info (still a LOT of outputs)</div><div class="line">2 - warnings</div><div class="line">3 - errors</div></pre></td></tr></table></figure>
<p>注意：由于在导入Caffe时Caffe会加载GLOG，因此<code>os.environ[&#39;GLOG_minloglevel&#39;] = &#39;2&#39;</code>需要在<code>import caffe</code>之前。</p>
<p>参考资料</p>
<ol>
<li><a href="https://stackoverflow.com/questions/29788075/setting-glog-minloglevel-1-to-prevent-output-in-shell-from-caffe" target="_blank" rel="external">https://stackoverflow.com/questions/29788075/setting-glog-minloglevel-1-to-prevent-output-in-shell-from-caffe</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Caffe关闭日志输出
    
    </summary>
    
      <category term="Caffe" scheme="noahsnail.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="noahsnail.com/tags/Caffe/"/>
    
  </entry>
  
  <entry>
    <title>Caffe源码调试</title>
    <link href="noahsnail.com/2017/07/17/2017-7-17-Caffe%E4%BB%A3%E7%A0%81%E8%B0%83%E8%AF%95/"/>
    <id>noahsnail.com/2017/07/17/2017-7-17-Caffe代码调试/</id>
    <published>2017-07-17T03:46:44.000Z</published>
    <updated>2017-07-18T08:50:40.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/NASA_Mars_Rover.jpg" alt="Mars Exploration Rovers"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>这篇文件主要介绍如何使用Linux的gdb调试Caffe的源码，源码调试主要是为了阅读并更好的了解Caffe源码。</p>
<h2 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h2><ol>
<li>首先要在编译Caffe源码时打开debug模式，即将<code>Makefile.config</code>中的<code>DEBUG := 1</code>打开。</li>
<li>下载mnist数据集，主要是在mnist数据集上进行调试，执行<code>bash data/mnist/get_mnist.sh</code>。</li>
<li>转换mnist数据集为LMDB，<code>bash examples/mnist/create_mnist.sh</code>。</li>
<li>修改<code>examples/mnist/lenet_solver.prototxt</code>，将GPU改为CPU。</li>
</ol>
<h2 id="2-调试"><a href="#2-调试" class="headerlink" title="2. 调试"></a>2. 调试</h2><h3 id="1-激活GDB"><a href="#1-激活GDB" class="headerlink" title="1. 激活GDB"></a>1. 激活GDB</h3><p>使用GDB启动调试，执行<code>gdb --args build/tools/caffe train --solver examples/mnist/lenet_solver.prototxt</code>，<code>--args</code>表示我们调试时需要输入的参数，调试的命令为<code>build/tools/caffe</code>，caffe命令的参数为<code>--solver examples/mnist/lenet_solver.prototxt</code>。</p>
<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ gdb --args build/tools/caffe train --solver examples/mnist/lenet_solver.prototxt</div><div class="line">GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-94.el7</div><div class="line">Copyright (C) 2013 Free Software Foundation, Inc.</div><div class="line">License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;</div><div class="line">This is free software: you are free to change and redistribute it.</div><div class="line">There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;</div><div class="line">and &quot;show warranty&quot; for details.</div><div class="line">This GDB was configured as &quot;x86_64-redhat-linux-gnu&quot;.</div><div class="line">For bug reporting instructions, please see:</div><div class="line">&lt;http://www.gnu.org/software/gdb/bugs/&gt;...</div><div class="line">Reading symbols from /home/irteam/line-brain/deploy/caffe/.build_debug/tools/caffe.bin...done.</div></pre></td></tr></table></figure>
<h3 id="2-设置断点"><a href="#2-设置断点" class="headerlink" title="2. 设置断点"></a>2. 设置断点</h3><p>执行<code>b src/caffe/layers/base_conv_layer.cpp:117</code>，<code>b</code>表示插入断点（breakpoint），断点的位置是<code>base_conv_layer.cpp</code>文件中的<code>117</code>行。插入断点的命令形式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">b path/to/code.cpp:#line</div></pre></td></tr></table></figure>
<p>118行相关代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">117</span> channels_ = bottom[<span class="number">0</span>]-&gt;shape(channel_axis_);</div><div class="line"><span class="number">118</span> num_output_ = <span class="keyword">this</span>-&gt;layer_param_.convolution_param().num_output();</div><div class="line"><span class="number">119</span> CHECK_GT(num_output_, <span class="number">0</span>);</div></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">(gdb) b src/caffe/layers/base_conv_layer.cpp:117</div><div class="line">No source file named src/caffe/layers/base_conv_layer.cpp.</div><div class="line">Make breakpoint pending on future shared library load? (y or [n]) y</div><div class="line"></div><div class="line">Breakpoint 1 (src/caffe/layers/base_conv_layer.cpp:117) pending.</div></pre></td></tr></table></figure>
<h3 id="3-运行程序"><a href="#3-运行程序" class="headerlink" title="3. 运行程序"></a>3. 运行程序</h3><p>运行程序的命令是<code>r</code>。</p>
<p>执行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div></pre></td><td class="code"><pre><div class="line">Starting program: /*/caffe/build/tools/caffe train --solver examples/mnist/lenet_solver.prototxt</div><div class="line">[Thread debugging using libthread_db enabled]</div><div class="line">Using host libthread_db library &quot;/lib64/libthread_db.so.1&quot;.</div><div class="line">I0718 15:19:19.671941 29986 caffe.cpp:211] Use CPU.</div><div class="line">[New Thread 0x7fffd81c7700 (LWP 29991)]</div><div class="line">[New Thread 0x7fffd79c6700 (LWP 29992)]</div><div class="line">I0718 15:19:20.437239 29986 solver.cpp:44] Initializing solver from parameters:</div><div class="line">test_iter: 100</div><div class="line">test_interval: 500</div><div class="line">base_lr: 0.01</div><div class="line">display: 100</div><div class="line">max_iter: 10000</div><div class="line">lr_policy: &quot;inv&quot;</div><div class="line">gamma: 0.0001</div><div class="line">power: 0.75</div><div class="line">momentum: 0.9</div><div class="line">weight_decay: 0.0005</div><div class="line">snapshot: 5000</div><div class="line">snapshot_prefix: &quot;examples/mnist/lenet&quot;</div><div class="line">solver_mode: CPU</div><div class="line">net: &quot;examples/mnist/lenet_train_test.prototxt&quot;</div><div class="line">train_state &#123;</div><div class="line">  level: 0</div><div class="line">  stage: &quot;&quot;</div><div class="line">&#125;</div><div class="line">I0718 15:19:20.437687 29986 solver.cpp:87] Creating training net from net file: examples/mnist/lenet_train_test.prototxt</div><div class="line">I0718 15:19:20.438357 29986 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist</div><div class="line">I0718 15:19:20.438398 29986 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy</div><div class="line">I0718 15:19:20.438499 29986 net.cpp:51] Initializing net from parameters:</div><div class="line">name: &quot;LeNet&quot;</div><div class="line">state &#123;</div><div class="line">  phase: TRAIN</div><div class="line">  level: 0</div><div class="line">  stage: &quot;&quot;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;mnist&quot;</div><div class="line">  type: &quot;Data&quot;</div><div class="line">  top: &quot;data&quot;</div><div class="line">  top: &quot;label&quot;</div><div class="line">  include &#123;</div><div class="line">    phase: TRAIN</div><div class="line">  &#125;</div><div class="line">  transform_param &#123;</div><div class="line">    scale: 0.00390625</div><div class="line">  &#125;</div><div class="line">  data_param &#123;</div><div class="line">    source: &quot;examples/mnist/mnist_train_lmdb&quot;</div><div class="line">    batch_size: 64</div><div class="line">    backend: LMDB</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;conv1&quot;</div><div class="line">  type: &quot;Convolution&quot;</div><div class="line">  bottom: &quot;data&quot;</div><div class="line">  top: &quot;conv1&quot;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 2</div><div class="line">  &#125;</div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: 20</div><div class="line">    kernel_size: 5</div><div class="line">    stride: 1</div><div class="line">    weight_filler &#123;</div><div class="line">      type: &quot;xavier&quot;</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: &quot;constant&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;pool1&quot;</div><div class="line">  type: &quot;Pooling&quot;</div><div class="line">  bottom: &quot;conv1&quot;</div><div class="line">  top: &quot;pool1&quot;</div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: 2</div><div class="line">    stride: 2</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;conv2&quot;</div><div class="line">  type: &quot;Convolution&quot;</div><div class="line">  bottom: &quot;pool1&quot;</div><div class="line">  top: &quot;conv2&quot;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 2</div><div class="line">  &#125;</div><div class="line">  convolution_param &#123;</div><div class="line">    num_output: 50</div><div class="line">    kernel_size: 5</div><div class="line">    stride: 1</div><div class="line">    weight_filler &#123;</div><div class="line">      type: &quot;xavier&quot;</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: &quot;constant&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;pool2&quot;</div><div class="line">  type: &quot;Pooling&quot;</div><div class="line">  bottom: &quot;conv2&quot;</div><div class="line">  top: &quot;pool2&quot;</div><div class="line">  pooling_param &#123;</div><div class="line">    pool: MAX</div><div class="line">    kernel_size: 2</div><div class="line">    stride: 2</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;ip1&quot;</div><div class="line">  type: &quot;InnerProduct&quot;</div><div class="line">  bottom: &quot;pool2&quot;</div><div class="line">  top: &quot;ip1&quot;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 2</div><div class="line">  &#125;</div><div class="line">  inner_product_param &#123;</div><div class="line">    num_output: 500</div><div class="line">    weight_filler &#123;</div><div class="line">      type: &quot;xavier&quot;</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: &quot;constant&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;relu1&quot;</div><div class="line">  type: &quot;ReLU&quot;</div><div class="line">  bottom: &quot;ip1&quot;</div><div class="line">  top: &quot;ip1&quot;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;ip2&quot;</div><div class="line">  type: &quot;InnerProduct&quot;</div><div class="line">  bottom: &quot;ip1&quot;</div><div class="line">  top: &quot;ip2&quot;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123;</div><div class="line">    lr_mult: 2</div><div class="line">  &#125;</div><div class="line">  inner_product_param &#123;</div><div class="line">    num_output: 10</div><div class="line">    weight_filler &#123;</div><div class="line">      type: &quot;xavier&quot;</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123;</div><div class="line">      type: &quot;constant&quot;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">layer &#123;</div><div class="line">  name: &quot;loss&quot;</div><div class="line">  type: &quot;SoftmaxWithLoss&quot;</div><div class="line">  bottom: &quot;ip2&quot;</div><div class="line">  bottom: &quot;label&quot;</div><div class="line">  top: &quot;loss&quot;</div><div class="line">&#125;</div><div class="line">I0718 15:19:20.439380 29986 layer_factory.hpp:77] Creating layer mnist</div><div class="line">I0718 15:19:20.439625 29986 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb</div><div class="line">I0718 15:19:20.439702 29986 net.cpp:84] Creating Layer mnist</div><div class="line">I0718 15:19:20.439735 29986 net.cpp:380] mnist -&gt; data</div><div class="line">I0718 15:19:20.439853 29986 net.cpp:380] mnist -&gt; label</div><div class="line">I0718 15:19:20.444980 29986 data_layer.cpp:45] output data size: 64,1,28,28</div><div class="line">I0718 15:19:20.445436 29986 base_data_layer.cpp:72] Initializing prefetch</div><div class="line">[New Thread 0x7fffd603d700 (LWP 29993)]</div><div class="line">I0718 15:19:20.448151 29986 base_data_layer.cpp:75] Prefetch initialized.</div><div class="line">I0718 15:19:20.448186 29986 net.cpp:122] Setting up mnist</div><div class="line">I0718 15:19:20.448216 29986 net.cpp:129] Top shape: 64 1 28 28 (50176)</div><div class="line">I0718 15:19:20.448235 29986 net.cpp:129] Top shape: 64 (64)</div><div class="line">I0718 15:19:20.448245 29986 net.cpp:137] Memory required for data: 200960</div><div class="line">I0718 15:19:20.448264 29986 layer_factory.hpp:77] Creating layer conv1</div><div class="line">I0718 15:19:20.448324 29986 net.cpp:84] Creating Layer conv1</div><div class="line">I0718 15:19:20.448345 29986 net.cpp:406] conv1 &lt;- data</div><div class="line">I0718 15:19:20.448393 29986 net.cpp:380] conv1 -&gt; conv1</div><div class="line"></div><div class="line">Breakpoint 1, caffe::BaseConvolutionLayer&lt;float&gt;::LayerSetUp (this=0x91edd70,</div><div class="line">    bottom=std::vector of length 1, capacity 1 = &#123;...&#125;, top=std::vector of length 1, capacity 1 = &#123;...&#125;)</div><div class="line">    at src/caffe/layers/base_conv_layer.cpp:117</div><div class="line">117       channels_ = bottom[0]-&gt;shape(channel_axis_);</div><div class="line">Missing separate debuginfos, use: debuginfo-install OpenEXR-libs-1.7.1-7.el7.x86_64 atk-2.14.0-1.el7.x86_64 atlas-3.10.1-10.el7.x86_64 boost-filesystem-1.53.0-26.el7.x86_64 boost-python-1.53.0-26.el7.x86_64 boost-system-1.53.0-26.el7.x86_64 boost-thread-1.53.0-26.el7.x86_64 cairo-1.14.2-1.el7.x86_64 expat-2.1.0-10.el7_3.x86_64 fontconfig-2.10.95-10.el7.x86_64 freetype-2.4.11-12.el7.x86_64 gdk-pixbuf2-2.31.6-3.el7.x86_64 gflags-2.1.1-6.el7.x86_64 glib2-2.46.2-4.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 glog-0.3.3-8.el7.x86_64 graphite2-1.3.6-1.el7_2.x86_64 gstreamer-0.10.36-7.el7.x86_64 gstreamer-plugins-base-0.10.36-10.el7.x86_64 gtk2-2.24.28-8.el7.x86_64 harfbuzz-0.9.36-1.el7.x86_64 hdf5-1.8.12-8.el7.x86_64 ilmbase-1.0.3-7.el7.x86_64 jasper-libs-1.900.1-29.el7.x86_64 jbigkit-libs-2.0-11.el7.x86_64 leveldb-1.12.0-11.el7.x86_64 libX11-1.6.3-3.el7.x86_64 libXau-1.0.8-2.1.el7.x86_64 libXcomposite-0.4.4-4.1.el7.x86_64 libXcursor-1.1.14-2.1.el7.x86_64 libXdamage-1.1.4-4.1.el7.x86_64 libXext-1.3.3-3.el7.x86_64 libXfixes-5.0.1-2.1.el7.x86_64 libXi-1.7.4-2.el7.x86_64 libXinerama-1.1.3-2.1.el7.x86_64 libXrandr-1.4.2-2.el7.x86_64 libXrender-0.9.8-2.1.el7.x86_64 libffi-3.0.13-18.el7.x86_64 libgcc-4.8.5-11.el7.x86_64 libgfortran-4.8.5-11.el7.x86_64 libjpeg-turbo-1.2.90-5.el7.x86_64 libpng-1.5.13-7.el7_2.x86_64 libquadmath-4.8.5-11.el7.x86_64 libselinux-2.5-6.el7.x86_64 libstdc++-4.8.5-11.el7.x86_64 libtiff-4.0.3-27.el7_3.x86_64 libv4l-0.9.5-4.el7.x86_64 libxcb-1.11-4.el7.x86_64 libxml2-2.9.1-6.el7_2.3.x86_64 lmdb-libs-0.9.18-1.el7.x86_64 opencv-2.4.5-3.el7.x86_64 opencv-core-2.4.5-3.el7.x86_64 orc-0.4.22-5.el7.x86_64 pango-1.36.8-2.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 pixman-0.34.0-1.el7.x86_64 protobuf-2.5.0-8.el7.x86_64 python-libs-2.7.5-48.el7.x86_64 snappy-1.1.0-3.el7.x86_64 xz-libs-5.2.2-1.el7.x86_64 zlib-1.2.7-17.el7.x86_64</div></pre></td></tr></table></figure>
<p><code>Breakpoint 1</code>之前是正常的程序日志输出，程序在断点处暂停。</p>
<p>查看变量命令为<code>p var</code>，命令与结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">(gdb) p channels_</div><div class="line">$1 = 0</div><div class="line"></div><div class="line">(gdb) p channel_axis_</div><div class="line">$2 = 1</div></pre></td></tr></table></figure>
<p>此时，<code>channels_</code>值为<code>0</code>。下一行命令为<code>n</code>，执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) n</div><div class="line">118       num_output_ = this-&gt;layer_param_.convolution_param().num_output();</div></pre></td></tr></table></figure>
<p>此时查看<code>channels_</code>值为<code>1</code>，mnist数据是灰度图像，<code>channels_</code>为<code>1</code>没问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(gdb) p channels_</div><div class="line">$3 = 1</div></pre></td></tr></table></figure>
<p>命令<code>c</code>是继续执行直到下一个断点。</p>
<p>如果需要调试GPU程序，可以使用<code>cuda-gdb</code>，文档地址为：<a href="http://docs.nvidia.com/cuda/cuda-gdb/index.html#axzz4nAAR7ujZ" target="_blank" rel="external">http://docs.nvidia.com/cuda/cuda-gdb/index.html#axzz4nAAR7ujZ</a>。</p>
<p>参考资料</p>
<ol>
<li><a href="http://zhaok.xyz/blog/post/debug-caffe/" target="_blank" rel="external">http://zhaok.xyz/blog/post/debug-caffe/</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Caffe源码调试
    
    </summary>
    
      <category term="Caffe" scheme="noahsnail.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="noahsnail.com/tags/Caffe/"/>
    
  </entry>
  
  <entry>
    <title>vim使用总结</title>
    <link href="noahsnail.com/2017/07/13/2017-7-13-vim%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"/>
    <id>noahsnail.com/2017/07/13/2017-7-13-vim使用总结/</id>
    <published>2017-07-13T02:36:29.000Z</published>
    <updated>2017-07-18T06:17:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是记录使用vim过程中的一些用法，本文中介绍的命令都在vim中使用验证过。</p>
<h2 id="1-删除一行或多行"><a href="#1-删除一行或多行" class="headerlink" title="1. 删除一行或多行"></a>1. 删除一行或多行</h2><ul>
<li>删除一行，命令格式：[:行号d]</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 删除第10行</div><div class="line"></div><div class="line">:10d</div></pre></td></tr></table></figure>
<ul>
<li>删除多行，命令格式：[:起始行号,结束行号d]</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 删除103-104行</div><div class="line"></div><div class="line">:103,104d</div></pre></td></tr></table></figure>
<h2 id="2-显示-不显示行号"><a href="#2-显示-不显示行号" class="headerlink" title="2. 显示/不显示行号"></a>2. 显示/不显示行号</h2><ul>
<li>显示行号</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">:set nu</div></pre></td></tr></table></figure>
<ul>
<li>不显示行号</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">:set nonu</div></pre></td></tr></table></figure>
<h2 id="3-字符串替换"><a href="#3-字符串替换" class="headerlink" title="3. 字符串替换"></a>3. 字符串替换</h2><ul>
<li>命令格式：[:%s/原始字符串/要替换的字符串]</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># test替换为global，默认情况下全部替换</div><div class="line"></div><div class="line">:%s/test/global</div></pre></td></tr></table></figure>
<h2 id="4-跳到指定行"><a href="#4-跳到指定行" class="headerlink" title="4. 跳到指定行"></a>4. 跳到指定行</h2><ul>
<li>命令格式：[:line-number]</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 跳到118行</div><div class="line">:118</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      vim使用总结
    
    </summary>
    
      <category term="开发工具" scheme="noahsnail.com/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="vim" scheme="noahsnail.com/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>Caffe源码解析(一)——caffe.proto</title>
    <link href="noahsnail.com/2017/07/11/2017-7-12-Caffe%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(%E4%B8%80)%E2%80%94%E2%80%94caffe.proto/"/>
    <id>noahsnail.com/2017/07/11/2017-7-12-Caffe源码解析(一)——caffe.proto/</id>
    <published>2017-07-11T10:01:32.000Z</published>
    <updated>2017-07-13T07:14:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>caffe.proto是caffe数据结构定义的主要文件，本文主要是在caffe.proto代码的基础上加上了部分中文注释，其中的内容与caffe的prototxt文件中的结构相对应。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div><div class="line">488</div><div class="line">489</div><div class="line">490</div><div class="line">491</div><div class="line">492</div><div class="line">493</div><div class="line">494</div><div class="line">495</div><div class="line">496</div><div class="line">497</div><div class="line">498</div><div class="line">499</div><div class="line">500</div><div class="line">501</div><div class="line">502</div><div class="line">503</div><div class="line">504</div><div class="line">505</div><div class="line">506</div><div class="line">507</div><div class="line">508</div><div class="line">509</div><div class="line">510</div><div class="line">511</div><div class="line">512</div><div class="line">513</div><div class="line">514</div><div class="line">515</div><div class="line">516</div><div class="line">517</div><div class="line">518</div><div class="line">519</div><div class="line">520</div><div class="line">521</div><div class="line">522</div><div class="line">523</div><div class="line">524</div><div class="line">525</div><div class="line">526</div><div class="line">527</div><div class="line">528</div><div class="line">529</div><div class="line">530</div><div class="line">531</div><div class="line">532</div><div class="line">533</div><div class="line">534</div><div class="line">535</div><div class="line">536</div><div class="line">537</div><div class="line">538</div><div class="line">539</div><div class="line">540</div><div class="line">541</div><div class="line">542</div><div class="line">543</div><div class="line">544</div><div class="line">545</div><div class="line">546</div><div class="line">547</div><div class="line">548</div><div class="line">549</div><div class="line">550</div><div class="line">551</div><div class="line">552</div><div class="line">553</div><div class="line">554</div><div class="line">555</div><div class="line">556</div><div class="line">557</div><div class="line">558</div><div class="line">559</div><div class="line">560</div><div class="line">561</div><div class="line">562</div><div class="line">563</div><div class="line">564</div><div class="line">565</div><div class="line">566</div><div class="line">567</div><div class="line">568</div><div class="line">569</div><div class="line">570</div><div class="line">571</div><div class="line">572</div><div class="line">573</div><div class="line">574</div><div class="line">575</div><div class="line">576</div><div class="line">577</div><div class="line">578</div><div class="line">579</div><div class="line">580</div><div class="line">581</div><div class="line">582</div><div class="line">583</div><div class="line">584</div><div class="line">585</div><div class="line">586</div><div class="line">587</div><div class="line">588</div><div class="line">589</div><div class="line">590</div><div class="line">591</div><div class="line">592</div><div class="line">593</div><div class="line">594</div><div class="line">595</div><div class="line">596</div><div class="line">597</div><div class="line">598</div><div class="line">599</div><div class="line">600</div><div class="line">601</div><div class="line">602</div><div class="line">603</div><div class="line">604</div><div class="line">605</div><div class="line">606</div><div class="line">607</div><div class="line">608</div><div class="line">609</div><div class="line">610</div><div class="line">611</div><div class="line">612</div><div class="line">613</div><div class="line">614</div><div class="line">615</div><div class="line">616</div><div class="line">617</div><div class="line">618</div><div class="line">619</div><div class="line">620</div><div class="line">621</div><div class="line">622</div><div class="line">623</div><div class="line">624</div><div class="line">625</div><div class="line">626</div><div class="line">627</div><div class="line">628</div><div class="line">629</div><div class="line">630</div><div class="line">631</div><div class="line">632</div><div class="line">633</div><div class="line">634</div><div class="line">635</div><div class="line">636</div><div class="line">637</div><div class="line">638</div><div class="line">639</div><div class="line">640</div><div class="line">641</div><div class="line">642</div><div class="line">643</div><div class="line">644</div><div class="line">645</div><div class="line">646</div><div class="line">647</div><div class="line">648</div><div class="line">649</div><div class="line">650</div><div class="line">651</div><div class="line">652</div><div class="line">653</div><div class="line">654</div><div class="line">655</div><div class="line">656</div><div class="line">657</div><div class="line">658</div><div class="line">659</div><div class="line">660</div><div class="line">661</div><div class="line">662</div><div class="line">663</div><div class="line">664</div><div class="line">665</div><div class="line">666</div><div class="line">667</div><div class="line">668</div><div class="line">669</div><div class="line">670</div><div class="line">671</div><div class="line">672</div><div class="line">673</div><div class="line">674</div><div class="line">675</div><div class="line">676</div><div class="line">677</div><div class="line">678</div><div class="line">679</div><div class="line">680</div><div class="line">681</div><div class="line">682</div><div class="line">683</div><div class="line">684</div><div class="line">685</div><div class="line">686</div><div class="line">687</div><div class="line">688</div><div class="line">689</div><div class="line">690</div><div class="line">691</div><div class="line">692</div><div class="line">693</div><div class="line">694</div><div class="line">695</div><div class="line">696</div><div class="line">697</div><div class="line">698</div><div class="line">699</div><div class="line">700</div><div class="line">701</div><div class="line">702</div><div class="line">703</div><div class="line">704</div><div class="line">705</div><div class="line">706</div><div class="line">707</div><div class="line">708</div><div class="line">709</div><div class="line">710</div><div class="line">711</div><div class="line">712</div><div class="line">713</div><div class="line">714</div><div class="line">715</div><div class="line">716</div><div class="line">717</div><div class="line">718</div><div class="line">719</div><div class="line">720</div><div class="line">721</div><div class="line">722</div><div class="line">723</div><div class="line">724</div><div class="line">725</div><div class="line">726</div><div class="line">727</div><div class="line">728</div><div class="line">729</div><div class="line">730</div><div class="line">731</div><div class="line">732</div><div class="line">733</div><div class="line">734</div><div class="line">735</div><div class="line">736</div><div class="line">737</div><div class="line">738</div><div class="line">739</div><div class="line">740</div><div class="line">741</div><div class="line">742</div><div class="line">743</div><div class="line">744</div><div class="line">745</div><div class="line">746</div><div class="line">747</div><div class="line">748</div><div class="line">749</div><div class="line">750</div><div class="line">751</div><div class="line">752</div><div class="line">753</div><div class="line">754</div><div class="line">755</div><div class="line">756</div><div class="line">757</div><div class="line">758</div><div class="line">759</div><div class="line">760</div><div class="line">761</div><div class="line">762</div><div class="line">763</div><div class="line">764</div><div class="line">765</div><div class="line">766</div><div class="line">767</div><div class="line">768</div><div class="line">769</div><div class="line">770</div><div class="line">771</div><div class="line">772</div><div class="line">773</div><div class="line">774</div><div class="line">775</div><div class="line">776</div><div class="line">777</div><div class="line">778</div><div class="line">779</div><div class="line">780</div><div class="line">781</div><div class="line">782</div><div class="line">783</div><div class="line">784</div><div class="line">785</div><div class="line">786</div><div class="line">787</div><div class="line">788</div><div class="line">789</div><div class="line">790</div><div class="line">791</div><div class="line">792</div><div class="line">793</div><div class="line">794</div><div class="line">795</div><div class="line">796</div><div class="line">797</div><div class="line">798</div><div class="line">799</div><div class="line">800</div><div class="line">801</div><div class="line">802</div><div class="line">803</div><div class="line">804</div><div class="line">805</div><div class="line">806</div><div class="line">807</div><div class="line">808</div><div class="line">809</div><div class="line">810</div><div class="line">811</div><div class="line">812</div><div class="line">813</div><div class="line">814</div><div class="line">815</div><div class="line">816</div><div class="line">817</div><div class="line">818</div><div class="line">819</div><div class="line">820</div><div class="line">821</div><div class="line">822</div><div class="line">823</div><div class="line">824</div><div class="line">825</div><div class="line">826</div><div class="line">827</div><div class="line">828</div><div class="line">829</div><div class="line">830</div><div class="line">831</div><div class="line">832</div><div class="line">833</div><div class="line">834</div><div class="line">835</div><div class="line">836</div><div class="line">837</div><div class="line">838</div><div class="line">839</div><div class="line">840</div><div class="line">841</div><div class="line">842</div><div class="line">843</div><div class="line">844</div><div class="line">845</div><div class="line">846</div><div class="line">847</div><div class="line">848</div><div class="line">849</div><div class="line">850</div><div class="line">851</div><div class="line">852</div><div class="line">853</div><div class="line">854</div><div class="line">855</div><div class="line">856</div><div class="line">857</div><div class="line">858</div><div class="line">859</div><div class="line">860</div><div class="line">861</div><div class="line">862</div><div class="line">863</div><div class="line">864</div><div class="line">865</div><div class="line">866</div><div class="line">867</div><div class="line">868</div><div class="line">869</div><div class="line">870</div><div class="line">871</div><div class="line">872</div><div class="line">873</div><div class="line">874</div><div class="line">875</div><div class="line">876</div><div class="line">877</div><div class="line">878</div><div class="line">879</div><div class="line">880</div><div class="line">881</div><div class="line">882</div><div class="line">883</div><div class="line">884</div><div class="line">885</div><div class="line">886</div><div class="line">887</div><div class="line">888</div><div class="line">889</div><div class="line">890</div><div class="line">891</div><div class="line">892</div><div class="line">893</div><div class="line">894</div><div class="line">895</div><div class="line">896</div><div class="line">897</div><div class="line">898</div><div class="line">899</div><div class="line">900</div><div class="line">901</div><div class="line">902</div><div class="line">903</div><div class="line">904</div><div class="line">905</div><div class="line">906</div><div class="line">907</div><div class="line">908</div><div class="line">909</div><div class="line">910</div><div class="line">911</div><div class="line">912</div><div class="line">913</div><div class="line">914</div><div class="line">915</div><div class="line">916</div><div class="line">917</div><div class="line">918</div><div class="line">919</div><div class="line">920</div><div class="line">921</div><div class="line">922</div><div class="line">923</div><div class="line">924</div><div class="line">925</div><div class="line">926</div><div class="line">927</div><div class="line">928</div><div class="line">929</div><div class="line">930</div><div class="line">931</div><div class="line">932</div><div class="line">933</div><div class="line">934</div><div class="line">935</div><div class="line">936</div><div class="line">937</div><div class="line">938</div><div class="line">939</div><div class="line">940</div><div class="line">941</div><div class="line">942</div><div class="line">943</div><div class="line">944</div><div class="line">945</div><div class="line">946</div><div class="line">947</div><div class="line">948</div><div class="line">949</div><div class="line">950</div><div class="line">951</div><div class="line">952</div><div class="line">953</div><div class="line">954</div><div class="line">955</div><div class="line">956</div><div class="line">957</div><div class="line">958</div><div class="line">959</div><div class="line">960</div><div class="line">961</div><div class="line">962</div><div class="line">963</div><div class="line">964</div><div class="line">965</div><div class="line">966</div><div class="line">967</div><div class="line">968</div><div class="line">969</div><div class="line">970</div><div class="line">971</div><div class="line">972</div><div class="line">973</div><div class="line">974</div><div class="line">975</div><div class="line">976</div><div class="line">977</div><div class="line">978</div><div class="line">979</div><div class="line">980</div><div class="line">981</div><div class="line">982</div><div class="line">983</div><div class="line">984</div><div class="line">985</div><div class="line">986</div><div class="line">987</div><div class="line">988</div><div class="line">989</div><div class="line">990</div><div class="line">991</div><div class="line">992</div><div class="line">993</div><div class="line">994</div><div class="line">995</div><div class="line">996</div><div class="line">997</div><div class="line">998</div><div class="line">999</div><div class="line">1000</div><div class="line">1001</div><div class="line">1002</div><div class="line">1003</div><div class="line">1004</div><div class="line">1005</div><div class="line">1006</div><div class="line">1007</div><div class="line">1008</div><div class="line">1009</div><div class="line">1010</div><div class="line">1011</div><div class="line">1012</div><div class="line">1013</div><div class="line">1014</div><div class="line">1015</div><div class="line">1016</div><div class="line">1017</div><div class="line">1018</div><div class="line">1019</div><div class="line">1020</div><div class="line">1021</div><div class="line">1022</div><div class="line">1023</div><div class="line">1024</div><div class="line">1025</div><div class="line">1026</div><div class="line">1027</div><div class="line">1028</div><div class="line">1029</div><div class="line">1030</div><div class="line">1031</div><div class="line">1032</div><div class="line">1033</div><div class="line">1034</div><div class="line">1035</div><div class="line">1036</div><div class="line">1037</div><div class="line">1038</div><div class="line">1039</div><div class="line">1040</div><div class="line">1041</div><div class="line">1042</div><div class="line">1043</div><div class="line">1044</div><div class="line">1045</div><div class="line">1046</div><div class="line">1047</div><div class="line">1048</div><div class="line">1049</div><div class="line">1050</div><div class="line">1051</div><div class="line">1052</div><div class="line">1053</div><div class="line">1054</div><div class="line">1055</div><div class="line">1056</div><div class="line">1057</div><div class="line">1058</div><div class="line">1059</div><div class="line">1060</div><div class="line">1061</div><div class="line">1062</div><div class="line">1063</div><div class="line">1064</div><div class="line">1065</div><div class="line">1066</div><div class="line">1067</div><div class="line">1068</div><div class="line">1069</div><div class="line">1070</div><div class="line">1071</div><div class="line">1072</div><div class="line">1073</div><div class="line">1074</div><div class="line">1075</div><div class="line">1076</div><div class="line">1077</div><div class="line">1078</div><div class="line">1079</div><div class="line">1080</div><div class="line">1081</div><div class="line">1082</div><div class="line">1083</div><div class="line">1084</div><div class="line">1085</div><div class="line">1086</div><div class="line">1087</div><div class="line">1088</div><div class="line">1089</div><div class="line">1090</div><div class="line">1091</div><div class="line">1092</div><div class="line">1093</div><div class="line">1094</div><div class="line">1095</div><div class="line">1096</div><div class="line">1097</div><div class="line">1098</div><div class="line">1099</div><div class="line">1100</div><div class="line">1101</div><div class="line">1102</div><div class="line">1103</div><div class="line">1104</div><div class="line">1105</div><div class="line">1106</div><div class="line">1107</div><div class="line">1108</div><div class="line">1109</div><div class="line">1110</div><div class="line">1111</div><div class="line">1112</div><div class="line">1113</div><div class="line">1114</div><div class="line">1115</div><div class="line">1116</div><div class="line">1117</div><div class="line">1118</div><div class="line">1119</div><div class="line">1120</div><div class="line">1121</div><div class="line">1122</div><div class="line">1123</div><div class="line">1124</div><div class="line">1125</div><div class="line">1126</div><div class="line">1127</div><div class="line">1128</div><div class="line">1129</div><div class="line">1130</div><div class="line">1131</div><div class="line">1132</div><div class="line">1133</div><div class="line">1134</div><div class="line">1135</div><div class="line">1136</div><div class="line">1137</div><div class="line">1138</div><div class="line">1139</div><div class="line">1140</div><div class="line">1141</div><div class="line">1142</div><div class="line">1143</div><div class="line">1144</div><div class="line">1145</div><div class="line">1146</div><div class="line">1147</div><div class="line">1148</div><div class="line">1149</div><div class="line">1150</div><div class="line">1151</div><div class="line">1152</div><div class="line">1153</div><div class="line">1154</div><div class="line">1155</div><div class="line">1156</div><div class="line">1157</div><div class="line">1158</div><div class="line">1159</div><div class="line">1160</div><div class="line">1161</div><div class="line">1162</div><div class="line">1163</div><div class="line">1164</div><div class="line">1165</div><div class="line">1166</div><div class="line">1167</div><div class="line">1168</div><div class="line">1169</div><div class="line">1170</div><div class="line">1171</div><div class="line">1172</div><div class="line">1173</div><div class="line">1174</div><div class="line">1175</div><div class="line">1176</div><div class="line">1177</div><div class="line">1178</div><div class="line">1179</div><div class="line">1180</div><div class="line">1181</div><div class="line">1182</div><div class="line">1183</div><div class="line">1184</div><div class="line">1185</div><div class="line">1186</div><div class="line">1187</div><div class="line">1188</div><div class="line">1189</div><div class="line">1190</div><div class="line">1191</div><div class="line">1192</div><div class="line">1193</div><div class="line">1194</div><div class="line">1195</div><div class="line">1196</div><div class="line">1197</div><div class="line">1198</div><div class="line">1199</div><div class="line">1200</div><div class="line">1201</div><div class="line">1202</div><div class="line">1203</div><div class="line">1204</div><div class="line">1205</div><div class="line">1206</div><div class="line">1207</div><div class="line">1208</div><div class="line">1209</div><div class="line">1210</div><div class="line">1211</div><div class="line">1212</div><div class="line">1213</div><div class="line">1214</div><div class="line">1215</div><div class="line">1216</div><div class="line">1217</div><div class="line">1218</div><div class="line">1219</div><div class="line">1220</div><div class="line">1221</div><div class="line">1222</div><div class="line">1223</div><div class="line">1224</div><div class="line">1225</div><div class="line">1226</div><div class="line">1227</div><div class="line">1228</div><div class="line">1229</div><div class="line">1230</div><div class="line">1231</div><div class="line">1232</div><div class="line">1233</div><div class="line">1234</div><div class="line">1235</div><div class="line">1236</div><div class="line">1237</div><div class="line">1238</div><div class="line">1239</div><div class="line">1240</div><div class="line">1241</div><div class="line">1242</div><div class="line">1243</div><div class="line">1244</div><div class="line">1245</div><div class="line">1246</div><div class="line">1247</div><div class="line">1248</div><div class="line">1249</div><div class="line">1250</div><div class="line">1251</div><div class="line">1252</div><div class="line">1253</div><div class="line">1254</div><div class="line">1255</div><div class="line">1256</div><div class="line">1257</div><div class="line">1258</div><div class="line">1259</div><div class="line">1260</div><div class="line">1261</div><div class="line">1262</div><div class="line">1263</div><div class="line">1264</div><div class="line">1265</div><div class="line">1266</div><div class="line">1267</div><div class="line">1268</div><div class="line">1269</div><div class="line">1270</div><div class="line">1271</div><div class="line">1272</div><div class="line">1273</div><div class="line">1274</div><div class="line">1275</div><div class="line">1276</div><div class="line">1277</div><div class="line">1278</div><div class="line">1279</div><div class="line">1280</div><div class="line">1281</div><div class="line">1282</div><div class="line">1283</div><div class="line">1284</div><div class="line">1285</div><div class="line">1286</div><div class="line">1287</div><div class="line">1288</div><div class="line">1289</div><div class="line">1290</div><div class="line">1291</div><div class="line">1292</div><div class="line">1293</div><div class="line">1294</div><div class="line">1295</div><div class="line">1296</div><div class="line">1297</div><div class="line">1298</div><div class="line">1299</div><div class="line">1300</div><div class="line">1301</div><div class="line">1302</div><div class="line">1303</div><div class="line">1304</div><div class="line">1305</div><div class="line">1306</div><div class="line">1307</div><div class="line">1308</div><div class="line">1309</div><div class="line">1310</div><div class="line">1311</div><div class="line">1312</div><div class="line">1313</div><div class="line">1314</div><div class="line">1315</div><div class="line">1316</div><div class="line">1317</div><div class="line">1318</div><div class="line">1319</div><div class="line">1320</div><div class="line">1321</div><div class="line">1322</div><div class="line">1323</div><div class="line">1324</div><div class="line">1325</div><div class="line">1326</div><div class="line">1327</div><div class="line">1328</div><div class="line">1329</div><div class="line">1330</div><div class="line">1331</div><div class="line">1332</div><div class="line">1333</div><div class="line">1334</div><div class="line">1335</div><div class="line">1336</div><div class="line">1337</div><div class="line">1338</div><div class="line">1339</div><div class="line">1340</div><div class="line">1341</div><div class="line">1342</div><div class="line">1343</div><div class="line">1344</div><div class="line">1345</div><div class="line">1346</div><div class="line">1347</div><div class="line">1348</div><div class="line">1349</div><div class="line">1350</div><div class="line">1351</div><div class="line">1352</div><div class="line">1353</div><div class="line">1354</div><div class="line">1355</div><div class="line">1356</div><div class="line">1357</div><div class="line">1358</div><div class="line">1359</div><div class="line">1360</div><div class="line">1361</div><div class="line">1362</div><div class="line">1363</div><div class="line">1364</div><div class="line">1365</div><div class="line">1366</div><div class="line">1367</div><div class="line">1368</div><div class="line">1369</div><div class="line">1370</div><div class="line">1371</div><div class="line">1372</div><div class="line">1373</div><div class="line">1374</div><div class="line">1375</div><div class="line">1376</div><div class="line">1377</div><div class="line">1378</div><div class="line">1379</div><div class="line">1380</div><div class="line">1381</div><div class="line">1382</div><div class="line">1383</div><div class="line">1384</div><div class="line">1385</div><div class="line">1386</div><div class="line">1387</div><div class="line">1388</div><div class="line">1389</div><div class="line">1390</div><div class="line">1391</div><div class="line">1392</div><div class="line">1393</div><div class="line">1394</div><div class="line">1395</div><div class="line">1396</div><div class="line">1397</div><div class="line">1398</div><div class="line">1399</div><div class="line">1400</div><div class="line">1401</div><div class="line">1402</div><div class="line">1403</div><div class="line">1404</div><div class="line">1405</div><div class="line">1406</div><div class="line">1407</div><div class="line">1408</div><div class="line">1409</div><div class="line">1410</div><div class="line">1411</div><div class="line">1412</div><div class="line">1413</div><div class="line">1414</div><div class="line">1415</div><div class="line">1416</div><div class="line">1417</div><div class="line">1418</div><div class="line">1419</div><div class="line">1420</div><div class="line">1421</div><div class="line">1422</div><div class="line">1423</div><div class="line">1424</div><div class="line">1425</div><div class="line">1426</div><div class="line">1427</div><div class="line">1428</div><div class="line">1429</div><div class="line">1430</div><div class="line">1431</div><div class="line">1432</div><div class="line">1433</div><div class="line">1434</div><div class="line">1435</div><div class="line">1436</div><div class="line">1437</div><div class="line">1438</div><div class="line">1439</div><div class="line">1440</div><div class="line">1441</div><div class="line">1442</div><div class="line">1443</div><div class="line">1444</div><div class="line">1445</div><div class="line">1446</div><div class="line">1447</div><div class="line">1448</div><div class="line">1449</div><div class="line">1450</div><div class="line">1451</div><div class="line">1452</div><div class="line">1453</div><div class="line">1454</div><div class="line">1455</div><div class="line">1456</div><div class="line">1457</div><div class="line">1458</div><div class="line">1459</div><div class="line">1460</div><div class="line">1461</div><div class="line">1462</div><div class="line">1463</div><div class="line">1464</div><div class="line">1465</div><div class="line">1466</div><div class="line">1467</div><div class="line">1468</div><div class="line">1469</div><div class="line">1470</div><div class="line">1471</div><div class="line">1472</div><div class="line">1473</div><div class="line">1474</div><div class="line">1475</div><div class="line">1476</div><div class="line">1477</div><div class="line">1478</div><div class="line">1479</div><div class="line">1480</div><div class="line">1481</div><div class="line">1482</div><div class="line">1483</div><div class="line">1484</div><div class="line">1485</div><div class="line">1486</div><div class="line">1487</div><div class="line">1488</div><div class="line">1489</div><div class="line">1490</div><div class="line">1491</div><div class="line">1492</div><div class="line">1493</div><div class="line">1494</div><div class="line">1495</div><div class="line">1496</div><div class="line">1497</div><div class="line">1498</div><div class="line">1499</div><div class="line">1500</div><div class="line">1501</div><div class="line">1502</div><div class="line">1503</div><div class="line">1504</div><div class="line">1505</div><div class="line">1506</div><div class="line">1507</div><div class="line">1508</div><div class="line">1509</div><div class="line">1510</div><div class="line">1511</div><div class="line">1512</div><div class="line">1513</div><div class="line">1514</div><div class="line">1515</div><div class="line">1516</div><div class="line">1517</div><div class="line">1518</div><div class="line">1519</div><div class="line">1520</div><div class="line">1521</div><div class="line">1522</div><div class="line">1523</div><div class="line">1524</div><div class="line">1525</div><div class="line">1526</div><div class="line">1527</div><div class="line">1528</div><div class="line">1529</div><div class="line">1530</div><div class="line">1531</div><div class="line">1532</div><div class="line">1533</div><div class="line">1534</div><div class="line">1535</div><div class="line">1536</div><div class="line">1537</div><div class="line">1538</div><div class="line">1539</div><div class="line">1540</div><div class="line">1541</div><div class="line">1542</div><div class="line">1543</div><div class="line">1544</div><div class="line">1545</div><div class="line">1546</div><div class="line">1547</div><div class="line">1548</div><div class="line">1549</div><div class="line">1550</div><div class="line">1551</div><div class="line">1552</div><div class="line">1553</div><div class="line">1554</div><div class="line">1555</div><div class="line">1556</div><div class="line">1557</div><div class="line">1558</div><div class="line">1559</div><div class="line">1560</div><div class="line">1561</div><div class="line">1562</div><div class="line">1563</div><div class="line">1564</div><div class="line">1565</div><div class="line">1566</div><div class="line">1567</div><div class="line">1568</div><div class="line">1569</div><div class="line">1570</div><div class="line">1571</div><div class="line">1572</div><div class="line">1573</div><div class="line">1574</div><div class="line">1575</div><div class="line">1576</div><div class="line">1577</div><div class="line">1578</div><div class="line">1579</div><div class="line">1580</div><div class="line">1581</div><div class="line">1582</div><div class="line">1583</div><div class="line">1584</div><div class="line">1585</div><div class="line">1586</div><div class="line">1587</div><div class="line">1588</div><div class="line">1589</div><div class="line">1590</div><div class="line">1591</div><div class="line">1592</div><div class="line">1593</div><div class="line">1594</div><div class="line">1595</div><div class="line">1596</div><div class="line">1597</div><div class="line">1598</div><div class="line">1599</div><div class="line">1600</div><div class="line">1601</div><div class="line">1602</div><div class="line">1603</div><div class="line">1604</div><div class="line">1605</div><div class="line">1606</div><div class="line">1607</div><div class="line">1608</div><div class="line">1609</div><div class="line">1610</div><div class="line">1611</div><div class="line">1612</div><div class="line">1613</div><div class="line">1614</div><div class="line">1615</div><div class="line">1616</div><div class="line">1617</div><div class="line">1618</div><div class="line">1619</div><div class="line">1620</div><div class="line">1621</div><div class="line">1622</div><div class="line">1623</div><div class="line">1624</div><div class="line">1625</div><div class="line">1626</div><div class="line">1627</div><div class="line">1628</div><div class="line">1629</div><div class="line">1630</div><div class="line">1631</div><div class="line">1632</div><div class="line">1633</div><div class="line">1634</div><div class="line">1635</div><div class="line">1636</div><div class="line">1637</div><div class="line">1638</div><div class="line">1639</div><div class="line">1640</div><div class="line">1641</div><div class="line">1642</div><div class="line">1643</div><div class="line">1644</div><div class="line">1645</div><div class="line">1646</div><div class="line">1647</div><div class="line">1648</div><div class="line">1649</div><div class="line">1650</div><div class="line">1651</div><div class="line">1652</div><div class="line">1653</div><div class="line">1654</div><div class="line">1655</div><div class="line">1656</div><div class="line">1657</div><div class="line">1658</div><div class="line">1659</div><div class="line">1660</div><div class="line">1661</div><div class="line">1662</div><div class="line">1663</div><div class="line">1664</div><div class="line">1665</div><div class="line">1666</div><div class="line">1667</div><div class="line">1668</div><div class="line">1669</div><div class="line">1670</div><div class="line">1671</div><div class="line">1672</div><div class="line">1673</div><div class="line">1674</div><div class="line">1675</div><div class="line">1676</div><div class="line">1677</div><div class="line">1678</div><div class="line">1679</div><div class="line">1680</div><div class="line">1681</div><div class="line">1682</div><div class="line">1683</div><div class="line">1684</div><div class="line">1685</div><div class="line">1686</div><div class="line">1687</div><div class="line">1688</div><div class="line">1689</div><div class="line">1690</div><div class="line">1691</div><div class="line">1692</div><div class="line">1693</div><div class="line">1694</div><div class="line">1695</div><div class="line">1696</div><div class="line">1697</div><div class="line">1698</div><div class="line">1699</div><div class="line">1700</div><div class="line">1701</div><div class="line">1702</div><div class="line">1703</div><div class="line">1704</div><div class="line">1705</div><div class="line">1706</div><div class="line">1707</div><div class="line">1708</div><div class="line">1709</div><div class="line">1710</div><div class="line">1711</div><div class="line">1712</div><div class="line">1713</div><div class="line">1714</div><div class="line">1715</div><div class="line">1716</div><div class="line">1717</div><div class="line">1718</div><div class="line">1719</div><div class="line">1720</div><div class="line">1721</div><div class="line">1722</div><div class="line">1723</div><div class="line">1724</div><div class="line">1725</div><div class="line">1726</div><div class="line">1727</div><div class="line">1728</div><div class="line">1729</div><div class="line">1730</div><div class="line">1731</div><div class="line">1732</div><div class="line">1733</div><div class="line">1734</div><div class="line">1735</div><div class="line">1736</div><div class="line">1737</div><div class="line">1738</div><div class="line">1739</div><div class="line">1740</div><div class="line">1741</div><div class="line">1742</div><div class="line">1743</div><div class="line">1744</div><div class="line">1745</div><div class="line">1746</div><div class="line">1747</div><div class="line">1748</div><div class="line">1749</div><div class="line">1750</div><div class="line">1751</div><div class="line">1752</div></pre></td><td class="code"><pre><div class="line">// syntax用来指定protobuf的版本</div><div class="line">syntax = &quot;proto2&quot;;</div><div class="line"></div><div class="line">// package可以看作C++中的namespace，与Caffe C++代码中的namespace caffe对应</div><div class="line">// package用来避免名称冲突</div><div class="line">package caffe;</div><div class="line"></div><div class="line"></div><div class="line">// 在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。</div><div class="line">// 注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。</div><div class="line">// required：一个格式良好的消息一定要含有一个这种字段，表示该值是必须要设置的。</div><div class="line">// optional：消息格式中该字段可以有0个或1个值（不超过1个）。</div><div class="line">// repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留，表示该值可以重复，相当于Java中的List。</div><div class="line"></div><div class="line"></div><div class="line">// Specifies the shape (dimensions) of a Blob.</div><div class="line">// 指定Blob的shape，4-D shape</div><div class="line">message BlobShape &#123;</div><div class="line">  //数据块形状定义为Num * Channel * Height * Wight, 原因在于caffe基于容器的多维嵌套来实现高维数据的封装, 即vector。 </div><div class="line">  repeated int64 dim = 1 [packed = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Blob数据块，包括Blob shape，数据和微分</div><div class="line">message BlobProto &#123;</div><div class="line">  // Blob的shape, 即numpy中的shape</div><div class="line">  optional BlobShape shape = 7;</div><div class="line">  // Blob的数据部分</div><div class="line">  repeated float data = 5 [packed = true];</div><div class="line">  // Blob的微分部分</div><div class="line">  repeated float diff = 6 [packed = true];</div><div class="line">  // Blob中的数据部分(double类型)</div><div class="line">  repeated double double_data = 8 [packed = true];</div><div class="line">  // Blob的微分部分(double类型)</div><div class="line">  repeated double double_diff = 9 [packed = true];</div><div class="line"></div><div class="line">  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.</div><div class="line">  // Blob的4个维度，已被Blob shape代替</div><div class="line">  // Blob中数据的个数(例如卷积核的个数)</div><div class="line">  optional int32 num = 1 [default = 0];</div><div class="line">  // Blob中数据的通道数</div><div class="line">  optional int32 channels = 2 [default = 0];</div><div class="line">  // Blob中数据的高度</div><div class="line">  optional int32 height = 3 [default = 0];</div><div class="line">  // Blob中数据的宽度</div><div class="line">  optional int32 width = 4 [default = 0];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// The BlobProtoVector is simply a way to pass multiple blobproto instances</div><div class="line">// around.</div><div class="line">// BlobProtoVector, 用来保存多个BlobProb对象的Vector</div><div class="line">message BlobProtoVector &#123;</div><div class="line">  repeated BlobProto blobs = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//图像数据, channel-图像通道数, height-高度, width-宽度, data-图像像素数据, label-图像标签, float_data-图像浮点型数据(0-1之间), encoded-图像编码方式</div><div class="line">message Datum &#123;</div><div class="line">  // 图像的通道数</div><div class="line">  optional int32 channels = 1;</div><div class="line">  // 图像的高度</div><div class="line">  optional int32 height = 2;</div><div class="line">  // 图像的宽度</div><div class="line">  optional int32 width = 3;</div><div class="line">  // the actual image data, in bytes</div><div class="line">  // 实际的图像数据，以字节形式(uint8)表示</div><div class="line">  optional bytes data = 4;</div><div class="line">  // 图像对应的标签，必须为整形</div><div class="line">  optional int32 label = 5;</div><div class="line">  // Optionally, the datum could also hold float data.</div><div class="line">  // 可选表示，图像数据表示为float数据，即0-255归一化到0-1之间</div><div class="line">  repeated float float_data = 6;</div><div class="line">  // If true data contains an encoded image that need to be decoded</div><div class="line">  // encoded为true表示图像采用压缩表示，需要解码</div><div class="line">  optional bool encoded = 7 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Filler参数, filler主要对网络权重进行初始化</div><div class="line">// Filler类型分为常量初始化（constant）、高斯分布初始化（gaussian）、positive_unitball初始化、均匀分布初始化（uniform）、xavier初始化、msra初始化、双线性初始化（bilinear）</div><div class="line">message FillerParameter &#123;</div><div class="line">  // The filler type.</div><div class="line">  // Filler的类型</div><div class="line">  optional string type = 1 [default = &apos;constant&apos;];</div><div class="line">  // 常量初始化的值</div><div class="line">  optional float value = 2 [default = 0]; // the value in constant filler</div><div class="line">  // 均匀分布初始化中的最小值</div><div class="line">  optional float min = 3 [default = 0]; // the min value in uniform filler</div><div class="line">  // 均匀分布初始化中的最大值</div><div class="line">  optional float max = 4 [default = 1]; // the max value in uniform filler</div><div class="line">  // 高斯分布初始化中的均值</div><div class="line">  optional float mean = 5 [default = 0]; // the mean value in Gaussian filler</div><div class="line">  // 高斯分布初始化中的标准差</div><div class="line">  optional float std = 6 [default = 1]; // the std value in Gaussian filler</div><div class="line">  // The expected number of non-zero output weights for a given input in</div><div class="line">  // Gaussian filler -- the default -1 means don&apos;t perform sparsification.</div><div class="line">  // 在高斯分布初始化中给定输入及权重，期望输出非0值，默认值-1表示不进行稀疏化</div><div class="line">  optional int32 sparse = 7 [default = -1];</div><div class="line">  // Normalize the filler variance by fan_in, fan_out, or their average.</div><div class="line">  // Applies to &apos;xavier&apos; and &apos;msra&apos; fillers.</div><div class="line">  // 通过fan_in, fan_out或average来归一化filler方差，主要应用到&apos;xavier&apos;和&apos;msra&apos; filler中</div><div class="line">  enum VarianceNorm &#123;</div><div class="line">    FAN_IN = 0;</div><div class="line">    FAN_OUT = 1;</div><div class="line">    AVERAGE = 2;</div><div class="line">  &#125;</div><div class="line">  // 定义filler方差归一化，默认为FAN_IN</div><div class="line">  optional VarianceNorm variance_norm = 8 [default = FAN_IN];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//神经网络参数</div><div class="line">message NetParameter &#123;</div><div class="line">  // 神经网络名字</div><div class="line">  optional string name = 1; // consider giving the network a name</div><div class="line"></div><div class="line">  // DEPRECATED. See InputParameter. The input blobs to the network.</div><div class="line">  // 已废弃。网络的输入部分，具体请看InputParameter。</div><div class="line">  repeated string input = 3;</div><div class="line"></div><div class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</div><div class="line">  // 已废弃。输入blob的shape，具体请看InputParameter。</div><div class="line">  repeated BlobShape input_shape = 8;</div><div class="line"></div><div class="line">  // 4D input dimensions -- deprecated.  Use &quot;input_shape&quot; instead.</div><div class="line">  // If specified, for each input blob there should be four</div><div class="line">  // values specifying the num, channels, height and width of the input blob.</div><div class="line">  // Thus, there should be a total of (4 * #input) numbers.</div><div class="line">  // 已废弃。用input_shape代替。</div><div class="line">  repeated int32 input_dim = 4;</div><div class="line"></div><div class="line">  // Whether the network will force every layer to carry out backward operation.</div><div class="line">  // If set False, then whether to carry out backward is determined</div><div class="line">  // automatically according to the net structure and learning rates.</div><div class="line">  // 网络中是否每一层都执行反向传播的标志，如果设为false，反向传播会根据网络结构和学习率自动进行。</div><div class="line">  optional bool force_backward = 5 [default = false];</div><div class="line"></div><div class="line">  // The current &quot;state&quot; of the network, including the phase, level, and stage.</div><div class="line">  // Some layers may be included/excluded depending on this state and the states</div><div class="line">  // specified in the layers&apos; include and exclude fields.</div><div class="line">  // 网络的当前状态，包括phase, level和stage，(phase应该是对应prototxt文件中的TRAIN,TEST)</div><div class="line">  // 某些层是否included/excluded依赖于层中include，exclue字段指定的state。</div><div class="line">  optional NetState state = 6;</div><div class="line"></div><div class="line">  // Print debugging information about results while running Net::Forward,</div><div class="line">  // Net::Backward, and Net::Update.</div><div class="line">  // 在执行Net::Forward,Net::Backward, Net::Update时是否打印调试信息。</div><div class="line">  optional bool debug_info = 7 [default = false];</div><div class="line"></div><div class="line">  // The layers that make up the net.  Each of their configurations, including</div><div class="line">  // connectivity and behavior, is specified as a LayerParameter.</div><div class="line">  // 构成网络的layer，每一个layer的配置，包括连接性和行为都在LayerParameter中指定。</div><div class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</div><div class="line"></div><div class="line">  // DEPRECATED: use &apos;layer&apos; instead.</div><div class="line">  // 已废弃，用layer代替。</div><div class="line">  repeated V1LayerParameter layers = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// NOTE</div><div class="line">// Update the next available ID when you add a new SolverParameter field.</div><div class="line">// 注意：当你添加一个新的SolverParameter字段时，要更新下一个可获取的ID</div><div class="line">// SolverParameter next available ID: 41 (last added: type)</div><div class="line">// Solver参数</div><div class="line">message SolverParameter &#123;</div><div class="line">  //////////////////////////////////////////////////////////////////////////////</div><div class="line">  // Specifying the train and test networks</div><div class="line">  //</div><div class="line">  // Exactly one train net must be specified using one of the following fields:</div><div class="line">  //     train_net_param, train_net, net_param, net</div><div class="line">  // One or more test nets may be specified using any of the following fields:</div><div class="line">  //     test_net_param, test_net, net_param, net</div><div class="line">  // If more than one test net field is specified (e.g., both net and</div><div class="line">  // test_net are specified), they will be evaluated in the field order given</div><div class="line">  // above: (1) test_net_param, (2) test_net, (3) net_param/net.</div><div class="line">  // A test_iter must be specified for each test_net.</div><div class="line">  // A test_level and/or a test_stage may also be specified for each test_net.</div><div class="line">  //////////////////////////////////////////////////////////////////////////////</div><div class="line"></div><div class="line">  // Proto filename for the train net, possibly combined with one or more test nets.</div><div class="line">  // 训练网络的prototxt文件名，可能结合一个或多个测试网络</div><div class="line">  optional string net = 24;</div><div class="line">  // Inline train net param, possibly combined with one or more test nets.</div><div class="line">  // 内联训练网络参数，可能结合一个或多个测试网络</div><div class="line">  optional NetParameter net_param = 25;</div><div class="line"></div><div class="line">  // 训练网络的proto文件名</div><div class="line">  optional string train_net = 1; // Proto filename for the train net.</div><div class="line">  // 测试网络的proto文件名</div><div class="line">  repeated string test_net = 2; // Proto filenames for the test nets.</div><div class="line">  // 内联训练网络参数</div><div class="line">  optional NetParameter train_net_param = 21; // Inline train net params.</div><div class="line">  // 内联测试网络参数</div><div class="line">  repeated NetParameter test_net_param = 22; // Inline test net params.</div><div class="line"></div><div class="line">  // The states for the train/test nets. Must be unspecified or specified once per net.</div><div class="line">  // By default, all states will have solver = true;</div><div class="line">  // train_state will have phase = TRAIN,</div><div class="line">  // and all test_state&apos;s will have phase = TEST.</div><div class="line">  // Other defaults are set according to the NetState defaults.</div><div class="line">  // train/test网络的状态，必须不指定或每个网络指定一次</div><div class="line">  // 默认情况下，所有的状态都有solver = true，train_state的phase = TRAIN，其它默认情况根据NetState默认值设定。</div><div class="line">  </div><div class="line">  // train网络的状态，必须不指定或每个网络指定一次</div><div class="line">  optional NetState train_state = 26;</div><div class="line">  // test网络的状态，必须不指定或每个网络指定一次</div><div class="line">  repeated NetState test_state = 27;</div><div class="line"></div><div class="line">  // The number of iterations for each test net.</div><div class="line">  // 每个测试网络的迭代次数，即测试数据的迭代次数，测试数据总数=测试迭代次数*测试数据的batch_size。</div><div class="line">  repeated int32 test_iter = 3;</div><div class="line"></div><div class="line">  // The number of iterations between two testing phases.</div><div class="line">  // 两次测试间隔的迭代次数，即训练数据迭代多少次进行一次测试。</div><div class="line">  optional int32 test_interval = 4 [default = 0];</div><div class="line">  // 测试数据的loss，默认情况下不计算</div><div class="line">  optional bool test_compute_loss = 19 [default = false];</div><div class="line">  // If true, run an initial test pass before the first iteration,</div><div class="line">  // ensuring memory availability and printing the starting value of the loss.</div><div class="line">  // 如果为true，在第一次迭代之前进行一次初始测试，从而确保内存可用性并输出初始损失值。</div><div class="line">  optional bool test_initialization = 32 [default = true];</div><div class="line">  // 基本学习率</div><div class="line">  optional float base_lr = 5; // The base learning rate</div><div class="line">  // the number of iterations between displaying info. If display = 0, no info will be displayed.</div><div class="line">  // 执行多少次迭代显示一次信息，如果display = 0，不输出信息。</div><div class="line">  optional int32 display = 6;</div><div class="line">  // Display the loss averaged over the last average_loss iterations</div><div class="line">  // 输出的平均损失是之前多少次迭代的平均损失。</div><div class="line">  optional int32 average_loss = 33 [default = 1];</div><div class="line">  // 训练的最大迭代次数</div><div class="line">  optional int32 max_iter = 7; // the maximum number of iterations</div><div class="line">  // accumulate gradients over `iter_size` x `batch_size` instances</div><div class="line">  // 累积`iter_size` x `batch_size`个实例的梯度</div><div class="line">  optional int32 iter_size = 36 [default = 1];</div><div class="line"></div><div class="line">  // The learning rate decay policy. The currently implemented learning rate</div><div class="line">  // policies are as follows:</div><div class="line">  //    - fixed: always return base_lr.</div><div class="line">  //    - step: return base_lr * gamma ^ (floor(iter / step))</div><div class="line">  //    - exp: return base_lr * gamma ^ iter</div><div class="line">  //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)</div><div class="line">  //    - multistep: similar to step but it allows non uniform steps defined by</div><div class="line">  //      stepvalue</div><div class="line">  //    - poly: the effective learning rate follows a polynomial decay, to be</div><div class="line">  //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)</div><div class="line">  //    - sigmoid: the effective learning rate follows a sigmod decay</div><div class="line">  //      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</div><div class="line">  //</div><div class="line">  // where base_lr, max_iter, gamma, step, stepvalue and power are defined</div><div class="line">  // in the solver parameter protocol buffer, and iter is the current iteration.</div><div class="line"></div><div class="line">  // 学习率的变化策略</div><div class="line">  optional string lr_policy = 8;</div><div class="line">  // 学习率的计算参数</div><div class="line">  optional float gamma = 9; // The parameter to compute the learning rate.</div><div class="line">  // 学习率的计算参数</div><div class="line">  optional float power = 10; // The parameter to compute the learning rate.</div><div class="line">  // 动量参数</div><div class="line">  optional float momentum = 11; // The momentum value.</div><div class="line">  // 权重衰减，权重衰减主要影响神经网络的正则项，具体可参考Caffe文档</div><div class="line">  optional float weight_decay = 12; // The weight decay.</div><div class="line">  // regularization types supported: L1 and L2, controlled by weight_decay</div><div class="line">  // 正则化类型支持L1和L2，受weight_decay控制。</div><div class="line">  optional string regularization_type = 29 [default = &quot;L2&quot;];</div><div class="line">  // the stepsize for learning rate policy &quot;step&quot;</div><div class="line">  // 学习率方案为step时的参数</div><div class="line">  optional int32 stepsize = 13;</div><div class="line">  // the stepsize for learning rate policy &quot;multistep&quot;</div><div class="line">  // 学习率方案为multistep时的参数</div><div class="line">  repeated int32 stepvalue = 34;</div><div class="line"></div><div class="line">  // Set clip_gradients to &gt;= 0 to clip parameter gradients to that L2 norm,</div><div class="line">  // whenever their actual L2 norm is larger.</div><div class="line">  // 设置clip_gradients &gt;= 0可以削减L2范数的梯度，当真实L2范数的梯度大于clip_gradients，将L2范数的梯度设为clip_gradients</div><div class="line">  optional float clip_gradients = 35 [default = -1];</div><div class="line">  // snapshot的间隔，即迭代多少次保存一次snapshot</div><div class="line">  optional int32 snapshot = 14 [default = 0]; // The snapshot interval</div><div class="line">  // snapshot的前缀</div><div class="line">  optional string snapshot_prefix = 15; // The prefix for the snapshot.</div><div class="line">  // whether to snapshot diff in the results or not. Snapshotting diff will help</div><div class="line">  // debugging but the final protocol buffer size will be much larger.</div><div class="line">  // 是否在结果中保存snapshot的差分，snapshot diff有助于调试，但snapshot的文件会更大。</div><div class="line">  optional bool snapshot_diff = 16 [default = false];</div><div class="line">  // snapshot的保存格式（hdf5,binaryproto）。</div><div class="line">  enum SnapshotFormat &#123;</div><div class="line">    HDF5 = 0;</div><div class="line">    BINARYPROTO = 1;</div><div class="line">  &#125;</div><div class="line">  // snapshot默认保存为BINARYPROTO。</div><div class="line">  optional SnapshotFormat snapshot_format = 37 [default = BINARYPROTO];</div><div class="line">  // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default.</div><div class="line">  // 求解神经网络的方式，0 CPU, 1 GPU。默认使用GPU</div><div class="line">  enum SolverMode &#123;</div><div class="line">    CPU = 0;</div><div class="line">    GPU = 1;</div><div class="line">  &#125;</div><div class="line">  // 求解神经网络的模式，0 CPU, 1 GPU。默认使用GPU</div><div class="line">  optional SolverMode solver_mode = 17 [default = GPU];</div><div class="line">  // the device_id will that be used in GPU mode. Use device_id = 0 in default.</div><div class="line">  // device_id是GPU模式下GPU的ID。</div><div class="line">  optional int32 device_id = 18 [default = 0];</div><div class="line">  // If non-negative, the seed with which the Solver will initialize the Caffe</div><div class="line">  // random number generator -- useful for reproducible results. Otherwise,</div><div class="line">  // (and by default) initialize using a seed derived from the system clock.</div><div class="line">  // 如果是非负值，seed用来初始化Caffe的随机数生成器，对于再见结果是很有用的，默认情况下，seed的是从系统时钟获取。</div><div class="line">  optional int64 random_seed = 20 [default = -1];</div><div class="line"></div><div class="line">  // type of the solver</div><div class="line">  // 神经网络求解的类型, 默认为SGD</div><div class="line">  optional string type = 40 [default = &quot;SGD&quot;];</div><div class="line"></div><div class="line">  // numerical stability for RMSProp, AdaGrad and AdaDelta and Adam</div><div class="line">  // RMSProp, AdaGrad, AdaDelta, Adam求解类型的参数</div><div class="line">  optional float delta = 31 [default = 1e-8];</div><div class="line">  // parameters for the Adam solver</div><div class="line">  // Adam求解类型的参数</div><div class="line">  optional float momentum2 = 39 [default = 0.999];</div><div class="line"></div><div class="line">  // RMSProp decay value</div><div class="line">  // MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)</div><div class="line">  // RMSProp类型的衰减值</div><div class="line">  optional float rms_decay = 38 [default = 0.99];</div><div class="line"></div><div class="line">  // If true, print information about the state of the net that may help with</div><div class="line">  // debugging learning problems.</div><div class="line">  // 如果设为true，会输出网络的状态信息，有助于调试</div><div class="line">  optional bool debug_info = 23 [default = false];</div><div class="line"></div><div class="line">  // If false, don&apos;t save a snapshot after training finishes.</div><div class="line">  // 如果设为false，不保存训练结束的snapshot。</div><div class="line">  optional bool snapshot_after_train = 28 [default = true];</div><div class="line"></div><div class="line">  // DEPRECATED: old solver enum types, use string instead</div><div class="line">  // 已废弃，使用string代替</div><div class="line">  enum SolverType &#123;</div><div class="line">    SGD = 0;</div><div class="line">    NESTEROV = 1;</div><div class="line">    ADAGRAD = 2;</div><div class="line">    RMSPROP = 3;</div><div class="line">    ADADELTA = 4;</div><div class="line">    ADAM = 5;</div><div class="line">  &#125;</div><div class="line">  // DEPRECATED: use type instead of solver_type</div><div class="line">  // 已废弃：使用type代替</div><div class="line">  optional SolverType solver_type = 30 [default = SGD];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// A message that stores the solver snapshots</div><div class="line">// 保存solver snapshots</div><div class="line">message SolverState &#123;</div><div class="line">  // 当前的迭代次数</div><div class="line">  optional int32 iter = 1; // The current iteration</div><div class="line">  // 保存学习到的网络</div><div class="line">  optional string learned_net = 2; // The file that stores the learned net.</div><div class="line">  // sgd的求解历史</div><div class="line">  repeated BlobProto history = 3; // The history for sgd solvers</div><div class="line">  // 学习的当前step</div><div class="line">  optional int32 current_step = 4 [default = 0]; // The current step for learning rate</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 定义phase</div><div class="line">enum Phase &#123;</div><div class="line">   TRAIN = 0;</div><div class="line">   TEST = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 网络状态</div><div class="line">message NetState &#123;</div><div class="line">  // 属于哪个phase</div><div class="line">  optional Phase phase = 1 [default = TEST];</div><div class="line">  optional int32 level = 2 [default = 0];</div><div class="line">  repeated string stage = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 网络状态分类</div><div class="line">message NetStateRule &#123;</div><div class="line">  // Set phase to require the NetState have a particular phase (TRAIN or TEST)</div><div class="line">  // to meet this rule.</div><div class="line">  // 设置phase</div><div class="line">  optional Phase phase = 1;</div><div class="line"></div><div class="line">  // Set the minimum and/or maximum levels in which the layer should be used.</div><div class="line">  // Leave undefined to meet the rule regardless of level.</div><div class="line">  // 设置layer的level</div><div class="line">  optional int32 min_level = 2;</div><div class="line">  optional int32 max_level = 3;</div><div class="line"></div><div class="line">  // Customizable sets of stages to include or exclude.</div><div class="line">  // The net must have ALL of the specified stages and NONE of the specified</div><div class="line">  // &quot;not_stage&quot;s to meet the rule.</div><div class="line">  // (Use multiple NetStateRules to specify conjunctions of stages.)</div><div class="line">  // 可定制的stage集合</div><div class="line">  repeated string stage = 4;</div><div class="line">  repeated string not_stage = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Specifies training parameters (multipliers on global learning constants,</div><div class="line">// and the name and other settings used for weight sharing).</div><div class="line">// 指定训练参数及名称以及权重共享的其它设置</div><div class="line">message ParamSpec &#123;</div><div class="line">  // The names of the parameter blobs -- useful for sharing parameters among</div><div class="line">  // layers, but never required otherwise.  To share a parameter between two</div><div class="line">  // layers, give it a (non-empty) name.</div><div class="line">  // 两个layer之间进行参数共享的blob名字</div><div class="line">  optional string name = 1;</div><div class="line"></div><div class="line">  // Whether to require shared weights to have the same shape, or just the same</div><div class="line">  // count -- defaults to STRICT if unspecified.</div><div class="line">  // 参数共享时是否需要具有相同的shape，默认情况下需要有相同的shape</div><div class="line">  optional DimCheckMode share_mode = 2;</div><div class="line">  // 参数共享时的维度检查</div><div class="line">  enum DimCheckMode &#123;</div><div class="line">    // STRICT (default) requires that num, channels, height, width each match.</div><div class="line">    STRICT = 0;</div><div class="line">    // PERMISSIVE requires only the count (num*channels*height*width) to match.</div><div class="line">    PERMISSIVE = 1;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // The multiplier on the global learning rate for this parameter.</div><div class="line">  // 学习率参数, learning rate = base_lr * lr_mult</div><div class="line">  optional float lr_mult = 3 [default = 1.0];</div><div class="line"></div><div class="line">  // The multiplier on the global weight decay for this parameter.</div><div class="line">  // 权重衰减参数, weight = weight_decay * decay_mult</div><div class="line">  optional float decay_mult = 4 [default = 1.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// NOTE</div><div class="line">// Update the next available ID when you add a new LayerParameter field.</div><div class="line">// LayerParameter next available layer-specific ID: 147 (last added: recurrent_param)</div><div class="line">// 注意：当你添加一个新的LayerParameter字段时，要更新下一个可获取的ID</div><div class="line">message LayerParameter &#123;</div><div class="line">  // layer名称</div><div class="line">  optional string name = 1; // the layer name</div><div class="line">  // layer类型</div><div class="line">  optional string type = 2; // the layer type</div><div class="line">  // layer的输入</div><div class="line">  repeated string bottom = 3; // the name of each bottom blob</div><div class="line">  // layer的输出</div><div class="line">  repeated string top = 4; // the name of each top blob</div><div class="line"></div><div class="line">  // The train / test phase for computation.</div><div class="line">  // layer用在train/test phase</div><div class="line">  optional Phase phase = 10;</div><div class="line"></div><div class="line">  // The amount of weight to assign each top blob in the objective.</div><div class="line">  // Each layer assigns a default value, usually of either 0 or 1,</div><div class="line">  // to each top blob.</div><div class="line">  // layer对最终的loss损失值的贡献率</div><div class="line">  repeated float loss_weight = 5;</div><div class="line"></div><div class="line">  // Specifies training parameters (multipliers on global learning constants,</div><div class="line">  // and the name and other settings used for weight sharing).</div><div class="line">  // 指定训练参数</div><div class="line">  repeated ParamSpec param = 6;</div><div class="line"></div><div class="line">  // The blobs containing the numeric parameters of the layer.</div><div class="line">  // layer的blobs</div><div class="line">  repeated BlobProto blobs = 7;</div><div class="line"></div><div class="line">  // Specifies whether to backpropagate to each bottom. If unspecified,</div><div class="line">  // Caffe will automatically infer whether each input needs backpropagation</div><div class="line">  // to compute parameter gradients. If set to true for some inputs,</div><div class="line">  // backpropagation to those inputs is forced; if set false for some inputs,</div><div class="line">  // backpropagation to those inputs is skipped.</div><div class="line">  //</div><div class="line">  // The size must be either 0 or equal to the number of bottoms.</div><div class="line">  // 指定反向传播是否传播到每一个bottom，如果不指定，caffe会自动检查推断是否每一个输入都需要反向传播来计算梯度。如果一些输入设为true,</div><div class="line">  // 则这些layer强制进行反向传播，如果设为false，这些layer将跳过反向传播。</div><div class="line">  repeated bool propagate_down = 11;</div><div class="line"></div><div class="line">  // Rules controlling whether and when a layer is included in the network,</div><div class="line">  // based on the current NetState.  You may specify a non-zero number of rules</div><div class="line">  // to include OR exclude, but not both.  If no include or exclude rules are</div><div class="line">  // specified, the layer is always included.  If the current NetState meets</div><div class="line">  // ANY (i.e., one or more) of the specified rules, the layer is</div><div class="line">  // included/excluded.</div><div class="line">  // 控制layer included/excluded</div><div class="line">  repeated NetStateRule include = 8;</div><div class="line">  repeated NetStateRule exclude = 9;</div><div class="line"></div><div class="line">  // Parameters for data pre-processing.</div><div class="line">  // 数据预处理参数</div><div class="line">  optional TransformationParameter transform_param = 100;</div><div class="line"></div><div class="line">  // Parameters shared by loss layers.</div><div class="line">  // loss layer的参数共享</div><div class="line">  optional LossParameter loss_param = 101;</div><div class="line"></div><div class="line">  // Layer type-specific parameters.</div><div class="line">  //</div><div class="line">  // Note: certain layers may have more than one computational engine</div><div class="line">  // for their implementation. These layers include an Engine type and</div><div class="line">  // engine parameter for selecting the implementation.</div><div class="line">  // The default for the engine is set by the ENGINE switch at compile-time.</div><div class="line"></div><div class="line">  // 特定layer的参数</div><div class="line">  optional AccuracyParameter accuracy_param = 102;</div><div class="line">  optional ArgMaxParameter argmax_param = 103;</div><div class="line">  optional BatchNormParameter batch_norm_param = 139;</div><div class="line">  optional BiasParameter bias_param = 141;</div><div class="line">  optional ConcatParameter concat_param = 104;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = 105;</div><div class="line">  optional ConvolutionParameter convolution_param = 106;</div><div class="line">  optional CropParameter crop_param = 144;</div><div class="line">  optional DataParameter data_param = 107;</div><div class="line">  optional DropoutParameter dropout_param = 108;</div><div class="line">  optional DummyDataParameter dummy_data_param = 109;</div><div class="line">  optional EltwiseParameter eltwise_param = 110;</div><div class="line">  optional ELUParameter elu_param = 140;</div><div class="line">  optional EmbedParameter embed_param = 137;</div><div class="line">  optional ExpParameter exp_param = 111;</div><div class="line">  optional FlattenParameter flatten_param = 135;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = 112;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 113;</div><div class="line">  optional HingeLossParameter hinge_loss_param = 114;</div><div class="line">  optional ImageDataParameter image_data_param = 115;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = 116;</div><div class="line">  optional InnerProductParameter inner_product_param = 117;</div><div class="line">  optional InputParameter input_param = 143;</div><div class="line">  optional LogParameter log_param = 134;</div><div class="line">  optional LRNParameter lrn_param = 118;</div><div class="line">  optional MemoryDataParameter memory_data_param = 119;</div><div class="line">  optional MVNParameter mvn_param = 120;</div><div class="line">  optional ParameterParameter parameter_param = 145;</div><div class="line">  optional PoolingParameter pooling_param = 121;</div><div class="line">  optional PowerParameter power_param = 122;</div><div class="line">  optional PReLUParameter prelu_param = 131;</div><div class="line">  optional PythonParameter python_param = 130;</div><div class="line">  optional RecurrentParameter recurrent_param = 146;</div><div class="line">  optional ReductionParameter reduction_param = 136;</div><div class="line">  optional ReLUParameter relu_param = 123;</div><div class="line">  optional ReshapeParameter reshape_param = 133;</div><div class="line">  optional ScaleParameter scale_param = 142;</div><div class="line">  optional SigmoidParameter sigmoid_param = 124;</div><div class="line">  optional SoftmaxParameter softmax_param = 125;</div><div class="line">  optional SPPParameter spp_param = 132;</div><div class="line">  optional SliceParameter slice_param = 126;</div><div class="line">  optional TanHParameter tanh_param = 127;</div><div class="line">  optional ThresholdParameter threshold_param = 128;</div><div class="line">  optional TileParameter tile_param = 138;</div><div class="line">  optional WindowDataParameter window_data_param = 129;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used to apply transformation to the data layer&apos;s data</div><div class="line">// 用来进行数据层（图像）变换的参数</div><div class="line">message TransformationParameter &#123;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  // 像素归一化，归一化之前会减去均值</div><div class="line">  optional float scale = 1 [default = 1];</div><div class="line">  // Specify if we want to randomly mirror data.</div><div class="line">  // 图像进行随机mirror操作</div><div class="line">  optional bool mirror = 2 [default = false];</div><div class="line">  // Specify if we would like to randomly crop an image.</div><div class="line">  // 图像随机crop操作</div><div class="line">  optional uint32 crop_size = 3 [default = 0];</div><div class="line">  // mean_file and mean_value cannot be specified at the same time</div><div class="line">  // 图像的均值文件</div><div class="line">  optional string mean_file = 4;</div><div class="line">  // if specified can be repeated once (would subtract it from all the channels)</div><div class="line">  // or can be repeated the same number of times as channels</div><div class="line">  // (would subtract them from the corresponding channel)</div><div class="line">  // 图像的均值，手动指定，通常是三个</div><div class="line">  repeated float mean_value = 5;</div><div class="line">  // Force the decoded image to have 3 color channels.</div><div class="line">  // 强制图像必须有三个颜色通道</div><div class="line">  optional bool force_color = 6 [default = false];</div><div class="line">  // Force the decoded image to have 1 color channels.</div><div class="line">  // 强制图像为灰度图像</div><div class="line">  optional bool force_gray = 7 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters shared by loss layers</div><div class="line">// loss层参数</div><div class="line">message LossParameter &#123;</div><div class="line">  // If specified, ignore instances with the given label.</div><div class="line">  // 如果指定，则label等于ignore_label的样本将不参与Loss计算，并且反向传播时梯度直接置0。</div><div class="line">  optional int32 ignore_label = 1;</div><div class="line">  // How to normalize the loss for loss layers that aggregate across batches,</div><div class="line">  // spatial dimensions, or other dimensions.  Currently only implemented in</div><div class="line">  // SoftmaxWithLoss and SigmoidCrossEntropyLoss layers.</div><div class="line">  // 指定loss归一化的方式</div><div class="line">  enum NormalizationMode &#123;</div><div class="line">    // Divide by the number of examples in the batch times spatial dimensions.</div><div class="line">    // Outputs that receive the ignore label will NOT be ignored in computing</div><div class="line">    // the normalization factor.</div><div class="line">    // 所有样本都参与计算，包括ignore label</div><div class="line">    FULL = 0;</div><div class="line">    // Divide by the total number of output locations that do not take the</div><div class="line">    // ignore_label.  If ignore_label is not set, this behaves like FULL.</div><div class="line">    // 所有样本都参与计算，不包括ignore label</div><div class="line">    VALID = 1;</div><div class="line">    // Divide by the batch size.</div><div class="line">    // 除以给定的batch size。</div><div class="line">    BATCH_SIZE = 2;</div><div class="line">    // Do not normalize the loss.</div><div class="line">    // 不归一化loss</div><div class="line">    NONE = 3;</div><div class="line">  &#125;</div><div class="line">  // For historical reasons, the default normalization for</div><div class="line">  // SigmoidCrossEntropyLoss is BATCH_SIZE and *not* VALID.</div><div class="line">  // loss归一化方式</div><div class="line">  optional NormalizationMode normalization = 3 [default = VALID];</div><div class="line">  // Deprecated.  Ignored if normalization is specified.  If normalization</div><div class="line">  // is not specified, then setting this to false will be equivalent to</div><div class="line">  // normalization = BATCH_SIZE to be consistent with previous behavior.</div><div class="line">  // 已废弃。Loss会除以参与计算的样本总数；否则Loss等于直接求和</div><div class="line">  optional bool normalize = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Messages that store parameters used by individual layer types follow, in</div><div class="line">// alphabetical order.</div><div class="line">// accuracy层参数</div><div class="line">message AccuracyParameter &#123;</div><div class="line">  // When computing accuracy, count as correct by comparing the true label to</div><div class="line">  // the top k scoring classes.  By default, only compare to the top scoring</div><div class="line">  // class (i.e. argmax).</div><div class="line">  // 计算前top-k的准确率，默认计算top-1准确率</div><div class="line">  optional uint32 top_k = 1 [default = 1];</div><div class="line"></div><div class="line">  // The &quot;label&quot; axis of the prediction blob, whose argmax corresponds to the</div><div class="line">  // predicted label -- may be negative to index from the end (e.g., -1 for the</div><div class="line">  // last axis).  For example, if axis == 1 and the predictions are</div><div class="line">  // (N x C x H x W), the label blob is expected to contain N*H*W ground truth</div><div class="line">  // labels with integer values in &#123;0, 1, ..., C-1&#125;.</div><div class="line">  // 指定在哪个维度上计算label</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line"></div><div class="line">  // If specified, ignore instances with the given label.</div><div class="line">  // 如果指定，则忽略给定标签的实例</div><div class="line">  optional int32 ignore_label = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// 标签最大化参数，标签最大化即确定概率最大的label</div><div class="line">message ArgMaxParameter &#123;</div><div class="line">  // If true produce pairs (argmax, maxval)</div><div class="line">  // 如果为真，则生成(argmax, maxval)</div><div class="line">  optional bool out_max_val = 1 [default = false];</div><div class="line">  // 类别的top-k</div><div class="line">  optional uint32 top_k = 2 [default = 1];</div><div class="line">  // The axis along which to maximise -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).</div><div class="line">  // By default ArgMaxLayer maximizes over the flattened trailing dimensions</div><div class="line">  // for each index of the first / num dimension.</div><div class="line">  // 根据axis进行标签最大化</div><div class="line">  optional int32 axis = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 参数拼接，在deconv的prototxt文件中见过</div><div class="line">message ConcatParameter &#123;</div><div class="line">  // The axis along which to concatenate -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).  Other axes must have the</div><div class="line">  // same dimension for all the bottom blobs.</div><div class="line">  // By default, ConcatLayer concatenates blobs along the &quot;channels&quot; axis (1).</div><div class="line">  // 参数拼接时的维度，按axis进行拼接</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line"></div><div class="line">  // DEPRECATED: alias for &quot;axis&quot; -- does not support negative indexing.</div><div class="line">  // 已废弃。与axis一样。</div><div class="line">  optional uint32 concat_dim = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// batch norm层的相关参数, batch norm layer通常配与scale layer一起使用，具体用法可参考Resnet结构</div><div class="line">message BatchNormParameter &#123;</div><div class="line">  // If false, accumulate global mean/variance values via a moving average. If</div><div class="line">  // true, use those accumulated values instead of computing mean/variance</div><div class="line">  // across the batch.</div><div class="line">  // 如果设为false，累计全部的mean/variance，如果为true，使用累计值代替batch上mean/variance的计算</div><div class="line">  // true是使用了caffe内部的均值和方差，false是使用了每个Batch里的数据的均值和方差</div><div class="line">  optional bool use_global_stats = 1;</div><div class="line">  // How much does the moving average decay each iteration?</div><div class="line">  // 每次迭代平均值衰减比例</div><div class="line">  optional float moving_average_fraction = 2 [default = .999];</div><div class="line">  // Small value to add to the variance estimate so that we don&apos;t divide by</div><div class="line">  // zero.</div><div class="line">  // variance估计时为了使除数不为0，需要加上eps</div><div class="line">  optional float eps = 3 [default = 1e-5];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// bias层参数，没找到实际的应用例子</div><div class="line">message BiasParameter &#123;</div><div class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</div><div class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  //</div><div class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</div><div class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</div><div class="line">  // following shapes (for the given value of axis):</div><div class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</div><div class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</div><div class="line">  //    (axis == 2 == -2)                   40;       40x60</div><div class="line">  //    (axis == 3 == -1)                                60</div><div class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</div><div class="line">  // &quot;axis&quot;) -- a scalar bias.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // (num_axes is ignored unless just one bottom is given and the bias is</div><div class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</div><div class="line">  // number of axes by the second bottom.)</div><div class="line">  // The number of axes of the input (bottom[0]) covered by the bias</div><div class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</div><div class="line">  // Set num_axes := 0, to add a zero-axis Blob: a scalar.</div><div class="line">  optional int32 num_axes = 2 [default = 1];</div><div class="line"></div><div class="line">  // (filler is ignored unless just one bottom is given and the bias is</div><div class="line">  // a learned parameter of the layer.)</div><div class="line">  // The initialization for the learned bias parameter.</div><div class="line">  // Default is the zero (0) initialization, resulting in the BiasLayer</div><div class="line">  // initially performing the identity operation.</div><div class="line">  optional FillerParameter filler = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 对比损失层，siamese network中使用了对比损失</div><div class="line">message ContrastiveLossParameter &#123;</div><div class="line">  // margin for dissimilar pair</div><div class="line">  // 不相似的样本对的距离保持在margin以上</div><div class="line">  optional float margin = 1 [default = 1.0];</div><div class="line">  // The first implementation of this cost did not exactly match the cost of</div><div class="line">  // Hadsell et al 2006 -- using (margin - d^2) instead of (margin - d)^2.</div><div class="line">  // legacy_version = false (the default) uses (margin - d)^2 as proposed in the</div><div class="line">  // Hadsell paper. New models should probably use this version.</div><div class="line">  // legacy_version = true uses (margin - d^2). This is kept to support /</div><div class="line">  // reproduce existing models and results</div><div class="line">  // 第一版对比损失没有完全按论文写，如果为false，则按照论文原来的公式计算</div><div class="line">  optional bool legacy_version = 2 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 卷积层参数</div><div class="line">message ConvolutionParameter &#123;</div><div class="line">  // 输出数据的个数</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // 是否有偏置项</div><div class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</div><div class="line"></div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in all spatial dimensions, or once per spatial dimension.</div><div class="line">  // 卷积padding的大小</div><div class="line">  repeated uint32 pad = 3; // The padding size; defaults to 0</div><div class="line">  // 卷积核的大小</div><div class="line">  repeated uint32 kernel_size = 4; // The kernel size</div><div class="line">  // 卷积的步长</div><div class="line">  repeated uint32 stride = 6; // The stride; defaults to 1</div><div class="line">  // Factor used to dilate the kernel, (implicitly) zero-filling the resulting</div><div class="line">  // holes. (Kernel dilation is sometimes referred to by its use in the</div><div class="line">  // algorithme à trous from Holschneider et al. 1987.)</div><div class="line">  // 卷积膨胀，在卷积的时候可以skip一定长度的像素</div><div class="line">  repeated uint32 dilation = 18; // The dilation; defaults to 1</div><div class="line"></div><div class="line">  // For 2D convolution only, the *_h and *_w versions may also be used to</div><div class="line">  // specify both spatial dimensions.</div><div class="line">  // padding, kernel, stride的宽度和高度</div><div class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height (2D only)</div><div class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width (2D only)</div><div class="line">  optional uint32 kernel_h = 11; // The kernel height (2D only)</div><div class="line">  optional uint32 kernel_w = 12; // The kernel width (2D only)</div><div class="line">  optional uint32 stride_h = 13; // The stride height (2D only)</div><div class="line">  optional uint32 stride_w = 14; // The stride width (2D only)</div><div class="line"></div><div class="line">  // 来自于AlexNet论文</div><div class="line">  optional uint32 group = 5 [default = 1]; // The group size for group conv</div><div class="line"></div><div class="line">  // 权重初始化</div><div class="line">  optional FillerParameter weight_filler = 7; // The filler for the weight</div><div class="line">  // 偏置初始化</div><div class="line">  optional FillerParameter bias_filler = 8; // The filler for the bias</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 卷积的方式的选择，default是正常的卷积，caffe是矩阵乘法的卷积，cudnn是cuda库流并行式的卷积</div><div class="line">  optional Engine engine = 15 [default = DEFAULT];</div><div class="line"></div><div class="line">  // The axis to interpret as &quot;channels&quot; when performing convolution.</div><div class="line">  // Preceding dimensions are treated as independent inputs;</div><div class="line">  // succeeding dimensions are treated as &quot;spatial&quot;.</div><div class="line">  // With (N, C, H, W) inputs, and axis == 1 (the default), we perform</div><div class="line">  // N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</div><div class="line">  // groups g&gt;1) filters across the spatial axes (H, W) of the input.</div><div class="line">  // With (N, C, D, H, W) inputs, and axis == 1, we perform</div><div class="line">  // N independent 3D convolutions, sliding (C/g)-channels</div><div class="line">  // filters across the spatial axes (D, H, W) of the input.</div><div class="line">  // 通道channel所在的维度</div><div class="line">  optional int32 axis = 16 [default = 1];</div><div class="line"></div><div class="line">  // Whether to force use of the general ND convolution, even if a specific</div><div class="line">  // implementation for blobs of the appropriate number of spatial dimensions</div><div class="line">  // is available. (Currently, there is only a 2D-specific convolution</div><div class="line">  // implementation; for input blobs with num_axes != 2, this option is</div><div class="line">  // ignored and the ND implementation will be used.)</div><div class="line">  // 如果输入数据维度等于2，则执行通用的ND卷积，否则正常执行卷积</div><div class="line">  optional bool force_nd_im2col = 17 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 图像裁剪参数</div><div class="line">message CropParameter &#123;</div><div class="line">  // To crop, elements of the first bottom are selected to fit the dimensions</div><div class="line">  // of the second, reference bottom. The crop is configured by</div><div class="line">  // - the crop `axis` to pick the dimensions for cropping</div><div class="line">  // - the crop `offset` to set the shift for all/each dimension</div><div class="line">  // to align the cropped bottom with the reference bottom.</div><div class="line">  // All dimensions up to but excluding `axis` are preserved, while</div><div class="line">  // the dimensions including and trailing `axis` are cropped.</div><div class="line">  // If only one `offset` is set, then all dimensions are offset by this amount.</div><div class="line">  // Otherwise, the number of offsets must equal the number of cropped axes to</div><div class="line">  // shift the crop in each dimension accordingly.</div><div class="line">  // Note: standard dimensions are N,C,H,W so the default is a spatial crop,</div><div class="line">  // and `axis` may be negative to index from the end (e.g., -1 for the last</div><div class="line">  // axis).</div><div class="line">  // axis是在哪个维度上进行裁剪，会裁剪轴2及之后的所有轴</div><div class="line">  optional int32 axis = 1 [default = 2];</div><div class="line">  // offset设置是每个维度进行裁剪时的偏移量</div><div class="line">  repeated uint32 offset = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 数据层参数</div><div class="line">message DataParameter &#123;</div><div class="line">  enum DB &#123;</div><div class="line">    LEVELDB = 0;</div><div class="line">    LMDB = 1;</div><div class="line">  &#125;</div><div class="line">  // Specify the data source.</div><div class="line">  // 设定数据源路径</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 指定一次处理的图片数量</div><div class="line">  optional uint32 batch_size = 4;</div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  // DEPRECATED. Each solver accesses a different subset of the database.</div><div class="line">  // rand_skip跳过指定的数据点，避免异步的sgd从同一个数据点开始</div><div class="line">  optional uint32 rand_skip = 7 [default = 0];</div><div class="line">  // 使用的数据库类型，LMDB or LEVELDB</div><div class="line">  optional DB backend = 8 [default = LEVELDB];</div><div class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</div><div class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</div><div class="line">  // mean subtraction is always carried out before scaling.</div><div class="line">  // 已废弃。图像归一化，在TransformationParameter中。</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 已废弃。均值文件，在TransformationParameter中。</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</div><div class="line">  // crop an image.</div><div class="line">  // 已废弃。图像裁剪，在TransformationParameter中。</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</div><div class="line">  // data.</div><div class="line">  // 已废弃。图像翻转，在TransformationParameter中。</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // Force the encoded image to have 3 color channels</div><div class="line">  // 强制图像数据有三个颜色通道</div><div class="line">  optional bool force_encoded_color = 9 [default = false];</div><div class="line">  // Prefetch queue (Number of batches to prefetch to host memory, increase if</div><div class="line">  // data access bandwidth varies).</div><div class="line">  // 预先拉取batch的数目</div><div class="line">  optional uint32 prefetch = 10 [default = 4];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// dropout层参数</div><div class="line">message DropoutParameter &#123;</div><div class="line">  // 为了避免过拟合，参数随机失活的比例</div><div class="line">  optional float dropout_ratio = 1 [default = 0.5]; // dropout ratio</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// DummyDataLayer fills any number of arbitrarily shaped blobs with random</div><div class="line">// (or constant) data generated by &quot;Fillers&quot; (see &quot;message FillerParameter&quot;).</div><div class="line">// DummyData层的参数</div><div class="line">message DummyDataParameter &#123;</div><div class="line">  // This layer produces N &gt;= 1 top blobs.  DummyDataParameter must specify 1 or N</div><div class="line">  // shape fields, and 0, 1 or N data_fillers.</div><div class="line">  // If 0 data_fillers are specified, ConstantFiller with a value of 0 is used.</div><div class="line">  // If 1 data_filler is specified, it is applied to all top blobs.  If N are</div><div class="line">  // specified, the ith is applied to the ith top blob.</div><div class="line">  // blob数据的生成方式</div><div class="line">  repeated FillerParameter data_filler = 1;</div><div class="line">  // 数据的维度</div><div class="line">  repeated BlobShape shape = 6;</div><div class="line"></div><div class="line">  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.</div><div class="line">  // 已废弃。使用shape代替。</div><div class="line">  repeated uint32 num = 2;</div><div class="line">  repeated uint32 channels = 3;</div><div class="line">  repeated uint32 height = 4;</div><div class="line">  repeated uint32 width = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">//Eltwise层的参数</div><div class="line">message EltwiseParameter &#123;</div><div class="line">  // 操作的类型</div><div class="line">  enum EltwiseOp &#123;</div><div class="line">    PROD = 0;</div><div class="line">    SUM = 1;</div><div class="line">    MAX = 2;</div><div class="line">  &#125;</div><div class="line">  // 数据操作分三种：点乘，相加，取最大值</div><div class="line">  optional EltwiseOp operation = 1 [default = SUM]; // element-wise operation</div><div class="line">  // SUM操作时各个blob对应的系数</div><div class="line">  repeated float coeff = 2; // blob-wise coefficient for SUM operation</div><div class="line"></div><div class="line">  // Whether to use an asymptotically slower (for &gt;2 inputs) but stabler method</div><div class="line">  // of computing the gradient for the PROD operation. (No effect for SUM op.)</div><div class="line">  // 在进行PROD操作，即乘法时是否使用异步操作来计算梯度，更慢但更稳定。</div><div class="line">  optional bool stable_prod_grad = 3 [default = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ELULayer</div><div class="line">// ELU层的参数，具体看论文</div><div class="line">message ELUParameter &#123;</div><div class="line">  // Described in:</div><div class="line">  // Clevert, D.-A., Unterthiner, T., &amp; Hochreiter, S. (2015). Fast and Accurate</div><div class="line">  // Deep Network Learning by Exponential Linear Units (ELUs). arXiv</div><div class="line">  optional float alpha = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by EmbedLayer</div><div class="line">// Embed层的参数，主要用于LSTM等翻译网络</div><div class="line">message EmbedParameter &#123;</div><div class="line">  // Embed层的输出</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // The input is given as integers to be interpreted as one-hot</div><div class="line">  // vector indices with dimension num_input.  Hence num_input should be</div><div class="line">  // 1 greater than the maximum possible input value.</div><div class="line">  // Embed层的输入</div><div class="line">  optional uint32 input_dim = 2;</div><div class="line">  // 是否使用偏置项</div><div class="line">  optional bool bias_term = 3 [default = true]; // Whether to use a bias term</div><div class="line">  // 权重生成</div><div class="line">  optional FillerParameter weight_filler = 4; // The filler for the weight</div><div class="line">  // 偏置生成</div><div class="line">  optional FillerParameter bias_filler = 5; // The filler for the bias</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ExpLayer</div><div class="line">// Exp层的参数，即指数层参数</div><div class="line">message ExpParameter &#123;</div><div class="line">  // ExpLayer computes outputs y = base ^ (shift + scale * x), for base &gt; 0.</div><div class="line">  // Or if base is set to the default (-1), base is set to e,</div><div class="line">  // so y = exp(shift + scale * x).</div><div class="line">  // 指数层的计算是y = base ^ (shift + scale * x)，下面分别是公式中的三个参数</div><div class="line">  optional float base = 1 [default = -1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Message that stores parameters used by FlattenLayer</div><div class="line">// Flatten层的参数，主要是按某个轴展开（平铺），mnist demo的mnist_autoencode就使用了Flatten层</div><div class="line">message FlattenParameter &#123;</div><div class="line">  // The first axis to flatten: all preceding axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</div><div class="line">  // 从哪一层开始展开</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // The last axis to flatten: all following axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., the default -1 for the last</div><div class="line">  // axis).</div><div class="line">  // 展开到哪一层结束</div><div class="line">  optional int32 end_axis = 2 [default = -1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by HDF5DataLayer</div><div class="line">// HDF5数据层的参数</div><div class="line">message HDF5DataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // HDF5层输入数据的数据源</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 2;</div><div class="line"></div><div class="line">  // Specify whether to shuffle the data.</div><div class="line">  // If shuffle == true, the ordering of the HDF5 files is shuffled,</div><div class="line">  // and the ordering of data within any given HDF5 file is shuffled,</div><div class="line">  // but data between different files are not interleaved; all of a file&apos;s</div><div class="line">  // data are output (in a random order) before moving onto another file.</div><div class="line">  // 是否对HDF5的输入数据进行shuffle</div><div class="line">  optional bool shuffle = 3 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// HDF5输出层参数</div><div class="line">message HDF5OutputParameter &#123;</div><div class="line">  // 输出的HDF5文件的文件名</div><div class="line">  optional string file_name = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// HingeLoss层参数</div><div class="line">message HingeLossParameter &#123;</div><div class="line">  enum Norm &#123;</div><div class="line">    L1 = 1;</div><div class="line">    L2 = 2;</div><div class="line">  &#125;</div><div class="line">  // Specify the Norm to use L1 or L2</div><div class="line">  // 指定HingeLoss的类型</div><div class="line">  optional Norm norm = 1 [default = L1];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// ImageData层参数，网络中直接输入原图</div><div class="line">message ImageDataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // 描述图像路径及标签的文件</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch size</div><div class="line">  optional uint32 batch_size = 4 [default = 1];</div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  // rand_skip跳过指定的数据点，避免异步的sgd从同一个数据点开始，与Data层中是一样的</div><div class="line">  optional uint32 rand_skip = 7 [default = 0];</div><div class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</div><div class="line">  // 是否对图像顺序进行shuffle</div><div class="line">  optional bool shuffle = 8 [default = false];</div><div class="line">  // It will also resize images if new_height or new_width are not zero.</div><div class="line">  // 图像resize的高度</div><div class="line">  optional uint32 new_height = 9 [default = 0];</div><div class="line">  // 图像resize的宽度</div><div class="line">  optional uint32 new_width = 10 [default = 0];</div><div class="line">  // Specify if the images are color or gray</div><div class="line">  // 指定图像彩色图像还是灰度图像，默认彩色</div><div class="line">  optional bool is_color = 11 [default = true];</div><div class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</div><div class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</div><div class="line">  // mean subtraction is always carried out before scaling.</div><div class="line">  // 已废弃。参考TransformationParameter中的scale</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 指定均值文件</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</div><div class="line">  // crop an image.</div><div class="line">  // 已废弃。参考TransformationParameter中的crop_size</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</div><div class="line">  // data.</div><div class="line">  // 已废弃，参考TransformationParameter的mirror。</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // 不太清楚root_folder具体是什么</div><div class="line">  optional string root_folder = 12 [default = &quot;&quot;];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 信息增益损失层参数</div><div class="line">message InfogainLossParameter &#123;</div><div class="line">  // Specify the infogain matrix source.</div><div class="line">  // 指定存储信息增益矩阵的源文件</div><div class="line">  optional string source = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// InnerProduct层的参数</div><div class="line">message InnerProductParameter &#123;</div><div class="line">  // InnerProduct层的输出</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // 是否有偏置项</div><div class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</div><div class="line">  // 权重初始化，随机生成</div><div class="line">  optional FillerParameter weight_filler = 3; // The filler for the weight</div><div class="line">  // 偏置初始化，随机生成</div><div class="line">  optional FillerParameter bias_filler = 4; // The filler for the bias</div><div class="line"></div><div class="line">  // The first axis to be lumped into a single inner product computation;</div><div class="line">  // all preceding axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</div><div class="line">  // 从某一维度开始进行内积计算，前面的维度保留</div><div class="line">  optional int32 axis = 5 [default = 1];</div><div class="line">  // Specify whether to transpose the weight matrix or not.</div><div class="line">  // If transpose == true, any operations will be performed on the transpose</div><div class="line">  // of the weight matrix. The weight matrix itself is not going to be transposed</div><div class="line">  // but rather the transfer flag of operations will be toggled accordingly.</div><div class="line">  // 是否对权重矩阵进行转置</div><div class="line">  optional bool transpose = 6 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Input参数，caffe网络部署时会用到</div><div class="line">message InputParameter &#123;</div><div class="line">  // This layer produces N &gt;= 1 top blob(s) to be assigned manually.</div><div class="line">  // Define N shapes to set a shape for each top.</div><div class="line">  // Define 1 shape to set the same shape for every top.</div><div class="line">  // Define no shape to defer to reshaping manually.</div><div class="line">  // 输入数据的shape</div><div class="line">  repeated BlobShape shape = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by LogLayer</div><div class="line">// Log层参数，对数据进行Log运算</div><div class="line">message LogParameter &#123;</div><div class="line">  // LogLayer computes outputs y = log_base(shift + scale * x), for base &gt; 0.</div><div class="line">  // Or if base is set to the default (-1), base is set to e,</div><div class="line">  // so y = ln(shift + scale * x) = log_e(shift + scale * x)</div><div class="line">  // Log层计算公式为y = log_base(shift + scale * x)，下面分别是公式中的三个参数</div><div class="line">  optional float base = 1 [default = -1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by LRNLayer</div><div class="line">// LRN层的参数，局部归一化，AlexNet中的LRN</div><div class="line">message LRNParameter &#123;</div><div class="line">  // 如果是跨通道LRN，则表示求和的通道数；如果是在通道内LRN，则表示求和的正方形区域长度。</div><div class="line">  optional uint32 local_size = 1 [default = 5];</div><div class="line">  // 归一化公式中的参数</div><div class="line">  optional float alpha = 2 [default = 1.];</div><div class="line">  optional float beta = 3 [default = 0.75];</div><div class="line">  enum NormRegion &#123;</div><div class="line">    ACROSS_CHANNELS = 0;</div><div class="line">    WITHIN_CHANNEL = 1;</div><div class="line">  &#125;</div><div class="line">  // 归一化的区域，分为通道内和跨通道两种</div><div class="line">  optional NormRegion norm_region = 4 [default = ACROSS_CHANNELS];</div><div class="line">  optional float k = 5 [default = 1.];</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 与前面的engine是一样的</div><div class="line">  optional Engine engine = 6 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 内存数据层参数</div><div class="line">message MemoryDataParameter &#123;</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 1;</div><div class="line">  // 图像通道数</div><div class="line">  optional uint32 channels = 2;</div><div class="line">  // 图像高度</div><div class="line">  optional uint32 height = 3;</div><div class="line">  // 图像宽度</div><div class="line">  optional uint32 width = 4;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// mean-variance normalization层参数</div><div class="line">message MVNParameter &#123;</div><div class="line">  // This parameter can be set to false to normalize mean only</div><div class="line">  // 是否对方差进行归一化</div><div class="line">  optional bool normalize_variance = 1 [default = true];</div><div class="line"></div><div class="line">  // This parameter can be set to true to perform DNN-like MVN</div><div class="line">  // 是否进行跨通道的MVN</div><div class="line">  optional bool across_channels = 2 [default = false];</div><div class="line"></div><div class="line">  // Epsilon for not dividing by zero while normalizing variance</div><div class="line">  // 避免除数为0，与前面的一样</div><div class="line">  optional float eps = 3 [default = 1e-9];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 参数层参数</div><div class="line">message ParameterParameter &#123;</div><div class="line">  // 用户自己定义的shape</div><div class="line">  optional BlobShape shape = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 池化层参数</div><div class="line">message PoolingParameter &#123;</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  // 池化的方式</div><div class="line">  optional PoolMethod pool = 1 [default = MAX]; // The pooling method</div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in height and width or as Y, X pairs.</div><div class="line">  // padding的大小</div><div class="line">  optional uint32 pad = 4 [default = 0]; // The padding size (equal in Y, X)</div><div class="line">  // padding的高度</div><div class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height</div><div class="line">  // padding的宽度</div><div class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width</div><div class="line">  // 池化的核大小</div><div class="line">  optional uint32 kernel_size = 2; // The kernel size (square)</div><div class="line">  // 核高度</div><div class="line">  optional uint32 kernel_h = 5; // The kernel height</div><div class="line">  // 核宽度</div><div class="line">  optional uint32 kernel_w = 6; // The kernel width</div><div class="line">  // 池化的步长</div><div class="line">  optional uint32 stride = 3 [default = 1]; // The stride (equal in Y, X)</div><div class="line">  // 步长的高度</div><div class="line">  optional uint32 stride_h = 7; // The stride height</div><div class="line">  // 步长的宽度</div><div class="line">  optional uint32 stride_w = 8; // The stride width</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行池化操作的类型，与前面的一样</div><div class="line">  optional Engine engine = 11 [default = DEFAULT];</div><div class="line">  // If global_pooling then it will pool over the size of the bottom by doing</div><div class="line">  // kernel_h = bottom-&gt;height and kernel_w = bottom-&gt;width</div><div class="line">  // global_pooling是对多个通道进行pooling，例如从三通道pooling为单通道</div><div class="line">  optional bool global_pooling = 12 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Power层参数</div><div class="line">message PowerParameter &#123;</div><div class="line">  // PowerLayer computes outputs y = (shift + scale * x) ^ power.</div><div class="line">  // Power的计算公式为y = (shift + scale * x) ^ power，下面是公式中的参数</div><div class="line">  optional float power = 1 [default = 1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// python layer参数，在faster rcnn中有应用</div><div class="line">message PythonParameter &#123;</div><div class="line">  // python模块名称</div><div class="line">  optional string module = 1;</div><div class="line">  // python模块中层的名字，即类名</div><div class="line">  optional string layer = 2;</div><div class="line">  // This value is set to the attribute `param_str` of the `PythonLayer` object</div><div class="line">  // in Python before calling the `setup()` method. This could be a number,</div><div class="line">  // string, dictionary in Python dict format, JSON, etc. You may parse this</div><div class="line">  // string in `setup` method and use it in `forward` and `backward`.</div><div class="line">  // 可以用来设置参数，key-value形式，可以参考faster rcnn中模型的train.prototxt</div><div class="line">  optional string param_str = 3 [default = &apos;&apos;];</div><div class="line">  // Whether this PythonLayer is shared among worker solvers during data parallelism.</div><div class="line">  // If true, each worker solver sequentially run forward from this layer.</div><div class="line">  // This value should be set true if you are using it as a data layer.</div><div class="line">  // 是否需要在并行时共享layer</div><div class="line">  optional bool share_in_parallel = 4 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Message that stores parameters used by RecurrentLayer</div><div class="line">// Recurrent层参数</div><div class="line">message RecurrentParameter &#123;</div><div class="line">  // The dimension of the output (and usually hidden state) representation --</div><div class="line">  // must be explicitly set to non-zero.</div><div class="line">  // Recurrent层的输出——必须非零</div><div class="line">  optional uint32 num_output = 1 [default = 0];</div><div class="line">  // 权重初始化，随机生成初始化</div><div class="line">  optional FillerParameter weight_filler = 2; // The filler for the weight</div><div class="line">  // 偏置初始化，随机生成</div><div class="line">  optional FillerParameter bias_filler = 3; // The filler for the bias</div><div class="line"></div><div class="line">  // Whether to enable displaying debug_info in the unrolled recurrent net.</div><div class="line">  // 是否输出调试信息</div><div class="line">  optional bool debug_info = 4 [default = false];</div><div class="line"></div><div class="line">  // Whether to add as additional inputs (bottoms) the initial hidden state</div><div class="line">  // blobs, and add as additional outputs (tops) the final timestep hidden state</div><div class="line">  // blobs.  The number of additional bottom/top blobs required depends on the</div><div class="line">  // recurrent architecture -- e.g., 1 for RNNs, 2 for LSTMs.</div><div class="line">  // 是否添加额外的输入</div><div class="line">  optional bool expose_hidden = 5 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ReductionLayer</div><div class="line">// Reduction层参数</div><div class="line">message ReductionParameter &#123;</div><div class="line">  enum ReductionOp &#123;</div><div class="line">    SUM = 1;</div><div class="line">    ASUM = 2;</div><div class="line">    SUMSQ = 3;</div><div class="line">    MEAN = 4;</div><div class="line">  &#125;</div><div class="line">  // 通过reduction操作来将数据减少到一维，可以通过上面的四种方式</div><div class="line">  optional ReductionOp operation = 1 [default = SUM]; // reduction operation</div><div class="line"></div><div class="line">  // The first axis to reduce to a scalar -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).</div><div class="line">  // (Currently, only reduction along ALL &quot;tail&quot; axes is supported; reduction</div><div class="line">  // of axis M through N, where N &lt; num_axes - 1, is unsupported.)</div><div class="line">  // Suppose we have an n-axis bottom Blob with shape:</div><div class="line">  //     (d0, d1, d2, ..., d(m-1), dm, d(m+1), ..., d(n-1)).</div><div class="line">  // If axis == m, the output Blob will have shape</div><div class="line">  //     (d0, d1, d2, ..., d(m-1)),</div><div class="line">  // and the ReductionOp operation is performed (d0 * d1 * d2 * ... * d(m-1))</div><div class="line">  // times, each including (dm * d(m+1) * ... * d(n-1)) individual data.</div><div class="line">  // If axis == 0 (the default), the output Blob always has the empty shape</div><div class="line">  // (count 1), performing reduction across the entire input --</div><div class="line">  // often useful for creating new loss functions.</div><div class="line">  // 在哪个轴上执行reduction操作</div><div class="line">  optional int32 axis = 2 [default = 0];</div><div class="line">  // 输出系数</div><div class="line">  optional float coeff = 3 [default = 1.0]; // coefficient for output</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ReLULayer</div><div class="line">// ReLU层参数</div><div class="line">message ReLUParameter &#123;</div><div class="line">  // Allow non-zero slope for negative inputs to speed up optimization</div><div class="line">  // Described in:</div><div class="line">  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities</div><div class="line">  // improve neural network acoustic models. In ICML Workshop on Deep Learning</div><div class="line">  // for Audio, Speech, and Language Processing.</div><div class="line">  // ReLUU操作的阈值</div><div class="line">  optional float negative_slope = 1 [default = 0];</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行ReLU操作的类型，与前面的一样</div><div class="line">  optional Engine engine = 2 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Reshape层参数，与numpy中的Reshape作用是一样的</div><div class="line">message ReshapeParameter &#123;</div><div class="line">  // Specify the output dimensions. If some of the dimensions are set to 0,</div><div class="line">  // the corresponding dimension from the bottom layer is used (unchanged).</div><div class="line">  // Exactly one dimension may be set to -1, in which case its value is</div><div class="line">  // inferred from the count of the bottom blob and the remaining dimensions.</div><div class="line">  // For example, suppose we want to reshape a 2D blob &quot;input&quot; with shape 2 x 8:</div><div class="line">  //</div><div class="line">  //   layer &#123;</div><div class="line">  //     type: &quot;Reshape&quot; bottom: &quot;input&quot; top: &quot;output&quot;</div><div class="line">  //     reshape_param &#123; ... &#125;</div><div class="line">  //   &#125;</div><div class="line">  //</div><div class="line">  // If &quot;input&quot; is 2D with shape 2 x 8, then the following reshape_param</div><div class="line">  // specifications are all equivalent, producing a 3D blob &quot;output&quot; with shape</div><div class="line">  // 2 x 2 x 4:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  2  dim: 2  dim:  4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim:  4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim: -1 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim:-1  dim:  4 &#125; &#125;</div><div class="line">  // reshape之后输出的维度</div><div class="line">  optional BlobShape shape = 1;</div><div class="line"></div><div class="line">  // axis and num_axes control the portion of the bottom blob&apos;s shape that are</div><div class="line">  // replaced by (included in) the reshape. By default (axis == 0 and</div><div class="line">  // num_axes == -1), the entire bottom blob shape is included in the reshape,</div><div class="line">  // and hence the shape field must specify the entire output shape.</div><div class="line">  //</div><div class="line">  // axis may be non-zero to retain some portion of the beginning of the input</div><div class="line">  // shape (and may be negative to index from the end; e.g., -1 to begin the</div><div class="line">  // reshape after the last axis, including nothing in the reshape,</div><div class="line">  // -2 to include only the last axis, etc.).</div><div class="line">  //</div><div class="line">  // For example, suppose &quot;input&quot; is a 2D blob with shape 2 x 8.</div><div class="line">  // Then the following ReshapeLayer specifications are all equivalent,</div><div class="line">  // producing a blob &quot;output&quot; with shape 2 x 2 x 4:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 2  dim: 4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis:  1 &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis: -3 &#125;</div><div class="line">  //</div><div class="line">  // num_axes specifies the extent of the reshape.</div><div class="line">  // If num_axes &gt;= 0 (and axis &gt;= 0), the reshape will be performed only on</div><div class="line">  // input axes in the range [axis, axis+num_axes].</div><div class="line">  // num_axes may also be -1, the default, to include all remaining axes</div><div class="line">  // (starting from axis).</div><div class="line">  //</div><div class="line">  // For example, suppose &quot;input&quot; is a 2D blob with shape 2 x 8.</div><div class="line">  // Then the following ReshapeLayer specifications are equivalent,</div><div class="line">  // producing a blob &quot;output&quot; with shape 1 x 2 x 8.</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  dim:  8 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  &#125;  num_axes: 1 &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  &#125;  num_axes: 0 &#125;</div><div class="line">  //</div><div class="line">  // On the other hand, these would produce output blob shape 2 x 1 x 8:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 1  dim: 8  &#125;  &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 1 &#125;  axis: 1  num_axes: 0 &#125;</div><div class="line"></div><div class="line">  optional int32 axis = 2 [default = 0];</div><div class="line">  optional int32 num_axes = 3 [default = -1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Scale层参数，与batch norm layer配合使用，可参考Resnet结构</div><div class="line">message ScaleParameter &#123;</div><div class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</div><div class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  //</div><div class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</div><div class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</div><div class="line">  // following shapes (for the given value of axis):</div><div class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</div><div class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</div><div class="line">  //    (axis == 2 == -2)                   40;       40x60</div><div class="line">  //    (axis == 3 == -1)                                60</div><div class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</div><div class="line">  // &quot;axis&quot;) -- a scalar multiplier.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // (num_axes is ignored unless just one bottom is given and the scale is</div><div class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</div><div class="line">  // number of axes by the second bottom.)</div><div class="line">  // The number of axes of the input (bottom[0]) covered by the scale</div><div class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</div><div class="line">  // Set num_axes := 0, to multiply with a zero-axis Blob: a scalar.</div><div class="line">  optional int32 num_axes = 2 [default = 1];</div><div class="line"></div><div class="line">  // (filler is ignored unless just one bottom is given and the scale is</div><div class="line">  // a learned parameter of the layer.)</div><div class="line">  // The initialization for the learned scale parameter.</div><div class="line">  // Default is the unit (1) initialization, resulting in the ScaleLayer</div><div class="line">  // initially performing the identity operation.</div><div class="line">  optional FillerParameter filler = 3;</div><div class="line"></div><div class="line">  // Whether to also learn a bias (equivalent to a ScaleLayer+BiasLayer, but</div><div class="line">  // may be more efficient).  Initialized with bias_filler (defaults to 0).</div><div class="line">  // 是否使用偏置项</div><div class="line">  optional bool bias_term = 4 [default = false];</div><div class="line">  // 偏置项初始化</div><div class="line">  optional FillerParameter bias_filler = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Sigmoid层参数</div><div class="line">message SigmoidParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 使用哪种sigmoid实现</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Slice层参数</div><div class="line">message SliceParameter &#123;</div><div class="line">  // The axis along which to slice -- may be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  // By default, SliceLayer concatenates blobs along the &quot;channels&quot; axis (1).</div><div class="line">  // 在哪个维度上进行拆分</div><div class="line">  optional int32 axis = 3 [default = 1];</div><div class="line">  // 指定拆分点</div><div class="line">  repeated uint32 slice_point = 2;</div><div class="line"></div><div class="line">  // DEPRECATED: alias for &quot;axis&quot; -- does not support negative indexing.</div><div class="line">  // 已废弃。</div><div class="line">  optional uint32 slice_dim = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer</div><div class="line">// Softmax层参数</div><div class="line">message SoftmaxParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 使用哪种softmax实现</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line"></div><div class="line">  // The axis along which to perform the softmax -- may be negative to index</div><div class="line">  // from the end (e.g., -1 for the last axis).</div><div class="line">  // Any other axes will be evaluated as independent softmaxes.</div><div class="line">  // 在哪个维度上进行softmax</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// TanH层参数</div><div class="line">message TanHParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行tanh激活函数的类型</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by TileLayer</div><div class="line">// Tile层参数，扩大某一维度</div><div class="line">message TileParameter &#123;</div><div class="line">  // The index of the axis to tile.</div><div class="line">  // 扩大哪个维度</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // The number of copies (tiles) of the blob to output.</div><div class="line">  // 创建多少个副本</div><div class="line">  optional int32 tiles = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ThresholdLayer</div><div class="line">// Threshold层参数，主要用来测试输入是否超过阈值</div><div class="line">message ThresholdParameter &#123;</div><div class="line">  // 设置阈值</div><div class="line">  optional float threshold = 1 [default = 0]; // Strictly positive values</div><div class="line">&#125;</div><div class="line"></div><div class="line">// WindowData层参数</div><div class="line">message WindowDataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // 指定数据源</div><div class="line">  optional string source = 1;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  // 是否归一化</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 图像均值文件</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 4;</div><div class="line">  // Specify if we would like to randomly crop an image.</div><div class="line">  // 是否随机crop</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // Specify if we want to randomly mirror data.</div><div class="line">  // 是否随机mirror</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // Foreground (object) overlap threshold</div><div class="line">  // 前景重叠阈值</div><div class="line">  optional float fg_threshold = 7 [default = 0.5];</div><div class="line">  // Background (non-object) overlap threshold</div><div class="line">  // 背景重叠阈值</div><div class="line">  optional float bg_threshold = 8 [default = 0.5];</div><div class="line">  // Fraction of batch that should be foreground objects</div><div class="line">  // 前景比例</div><div class="line">  optional float fg_fraction = 9 [default = 0.25];</div><div class="line">  // Amount of contextual padding to add around a window</div><div class="line">  // (used only by the window_data_layer)</div><div class="line">  // 是否padding</div><div class="line">  optional uint32 context_pad = 10 [default = 0];</div><div class="line">  // Mode for cropping out a detection window</div><div class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</div><div class="line">  // square: the tightest square around the window is cropped</div><div class="line">  // crop的方式</div><div class="line">  optional string crop_mode = 11 [default = &quot;warp&quot;];</div><div class="line">  // cache_images: will load all images in memory for faster access</div><div class="line">  // 是否缓存图像，即将图像都转入内存</div><div class="line">  optional bool cache_images = 12 [default = false];</div><div class="line">  // append root_folder to locate images</div><div class="line">  // 图像文件的根目录</div><div class="line">  optional string root_folder = 13 [default = &quot;&quot;];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// SPP层参数，SPP是spatial pyramid pooling，空间金字塔池化，具体可参考何凯明论文Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</div><div class="line">message SPPParameter &#123;</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  // 空间金字塔高度</div><div class="line">  optional uint32 pyramid_height = 1;</div><div class="line">  // 池化方法</div><div class="line">  optional PoolMethod pool = 2 [default = MAX]; // The pooling method</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行SPP的方式</div><div class="line">  optional Engine engine = 6 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DEPRECATED: use LayerParameter.</div><div class="line">// 已废弃，使用LayerParameter。</div><div class="line">message V1LayerParameter &#123;</div><div class="line">  repeated string bottom = 2;</div><div class="line">  repeated string top = 3;</div><div class="line">  optional string name = 4;</div><div class="line">  repeated NetStateRule include = 32;</div><div class="line">  repeated NetStateRule exclude = 33;</div><div class="line">  enum LayerType &#123;</div><div class="line">    NONE = 0;</div><div class="line">    ABSVAL = 35;</div><div class="line">    ACCURACY = 1;</div><div class="line">    ARGMAX = 30;</div><div class="line">    BNLL = 2;</div><div class="line">    CONCAT = 3;</div><div class="line">    CONTRASTIVE_LOSS = 37;</div><div class="line">    CONVOLUTION = 4;</div><div class="line">    DATA = 5;</div><div class="line">    DECONVOLUTION = 39;</div><div class="line">    DROPOUT = 6;</div><div class="line">    DUMMY_DATA = 32;</div><div class="line">    EUCLIDEAN_LOSS = 7;</div><div class="line">    ELTWISE = 25;</div><div class="line">    EXP = 38;</div><div class="line">    FLATTEN = 8;</div><div class="line">    HDF5_DATA = 9;</div><div class="line">    HDF5_OUTPUT = 10;</div><div class="line">    HINGE_LOSS = 28;</div><div class="line">    IM2COL = 11;</div><div class="line">    IMAGE_DATA = 12;</div><div class="line">    INFOGAIN_LOSS = 13;</div><div class="line">    INNER_PRODUCT = 14;</div><div class="line">    LRN = 15;</div><div class="line">    MEMORY_DATA = 29;</div><div class="line">    MULTINOMIAL_LOGISTIC_LOSS = 16;</div><div class="line">    MVN = 34;</div><div class="line">    POOLING = 17;</div><div class="line">    POWER = 26;</div><div class="line">    RELU = 18;</div><div class="line">    SIGMOID = 19;</div><div class="line">    SIGMOID_CROSS_ENTROPY_LOSS = 27;</div><div class="line">    SILENCE = 36;</div><div class="line">    SOFTMAX = 20;</div><div class="line">    SOFTMAX_LOSS = 21;</div><div class="line">    SPLIT = 22;</div><div class="line">    SLICE = 33;</div><div class="line">    TANH = 23;</div><div class="line">    WINDOW_DATA = 24;</div><div class="line">    THRESHOLD = 31;</div><div class="line">  &#125;</div><div class="line">  optional LayerType type = 5;</div><div class="line">  repeated BlobProto blobs = 6;</div><div class="line">  repeated string param = 1001;</div><div class="line">  repeated DimCheckMode blob_share_mode = 1002;</div><div class="line">  enum DimCheckMode &#123;</div><div class="line">    STRICT = 0;</div><div class="line">    PERMISSIVE = 1;</div><div class="line">  &#125;</div><div class="line">  repeated float blobs_lr = 7;</div><div class="line">  repeated float weight_decay = 8;</div><div class="line">  repeated float loss_weight = 35;</div><div class="line">  optional AccuracyParameter accuracy_param = 27;</div><div class="line">  optional ArgMaxParameter argmax_param = 23;</div><div class="line">  optional ConcatParameter concat_param = 9;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = 40;</div><div class="line">  optional ConvolutionParameter convolution_param = 10;</div><div class="line">  optional DataParameter data_param = 11;</div><div class="line">  optional DropoutParameter dropout_param = 12;</div><div class="line">  optional DummyDataParameter dummy_data_param = 26;</div><div class="line">  optional EltwiseParameter eltwise_param = 24;</div><div class="line">  optional ExpParameter exp_param = 41;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = 13;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 14;</div><div class="line">  optional HingeLossParameter hinge_loss_param = 29;</div><div class="line">  optional ImageDataParameter image_data_param = 15;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = 16;</div><div class="line">  optional InnerProductParameter inner_product_param = 17;</div><div class="line">  optional LRNParameter lrn_param = 18;</div><div class="line">  optional MemoryDataParameter memory_data_param = 22;</div><div class="line">  optional MVNParameter mvn_param = 34;</div><div class="line">  optional PoolingParameter pooling_param = 19;</div><div class="line">  optional PowerParameter power_param = 21;</div><div class="line">  optional ReLUParameter relu_param = 30;</div><div class="line">  optional SigmoidParameter sigmoid_param = 38;</div><div class="line">  optional SoftmaxParameter softmax_param = 39;</div><div class="line">  optional SliceParameter slice_param = 31;</div><div class="line">  optional TanHParameter tanh_param = 37;</div><div class="line">  optional ThresholdParameter threshold_param = 25;</div><div class="line">  optional WindowDataParameter window_data_param = 20;</div><div class="line">  optional TransformationParameter transform_param = 36;</div><div class="line">  optional LossParameter loss_param = 42;</div><div class="line">  optional V0LayerParameter layer = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DEPRECATED: V0LayerParameter is the old way of specifying layer parameters</div><div class="line">// in Caffe.  We keep this message type around for legacy support.</div><div class="line">// 已废弃。</div><div class="line">message V0LayerParameter &#123;</div><div class="line">  optional string name = 1; // the layer name</div><div class="line">  optional string type = 2; // the string to specify the layer type</div><div class="line"></div><div class="line">  // Parameters to specify layers with inner products.</div><div class="line">  optional uint32 num_output = 3; // The number of outputs for the layer</div><div class="line">  optional bool biasterm = 4 [default = true]; // whether to have bias terms</div><div class="line">  optional FillerParameter weight_filler = 5; // The filler for the weight</div><div class="line">  optional FillerParameter bias_filler = 6; // The filler for the bias</div><div class="line"></div><div class="line">  optional uint32 pad = 7 [default = 0]; // The padding size</div><div class="line">  optional uint32 kernelsize = 8; // The kernel size</div><div class="line">  optional uint32 group = 9 [default = 1]; // The group size for group conv</div><div class="line">  optional uint32 stride = 10 [default = 1]; // The stride</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  optional PoolMethod pool = 11 [default = MAX]; // The pooling method</div><div class="line">  optional float dropout_ratio = 12 [default = 0.5]; // dropout ratio</div><div class="line"></div><div class="line">  optional uint32 local_size = 13 [default = 5]; // for local response norm</div><div class="line">  optional float alpha = 14 [default = 1.]; // for local response norm</div><div class="line">  optional float beta = 15 [default = 0.75]; // for local response norm</div><div class="line">  optional float k = 22 [default = 1.];</div><div class="line"></div><div class="line">  // For data layers, specify the data source</div><div class="line">  optional string source = 16;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  optional float scale = 17 [default = 1];</div><div class="line">  optional string meanfile = 18;</div><div class="line">  // For data layers, specify the batch size.</div><div class="line">  optional uint32 batchsize = 19;</div><div class="line">  // For data layers, specify if we would like to randomly crop an image.</div><div class="line">  optional uint32 cropsize = 20 [default = 0];</div><div class="line">  // For data layers, specify if we want to randomly mirror data.</div><div class="line">  optional bool mirror = 21 [default = false];</div><div class="line"></div><div class="line">  // The blobs containing the numeric parameters of the layer</div><div class="line">  repeated BlobProto blobs = 50;</div><div class="line">  // The ratio that is multiplied on the global learning rate. If you want to</div><div class="line">  // set the learning ratio for one blob, you need to set it for all blobs.</div><div class="line">  repeated float blobs_lr = 51;</div><div class="line">  // The weight decay that is multiplied on the global weight decay.</div><div class="line">  repeated float weight_decay = 52;</div><div class="line"></div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  optional uint32 rand_skip = 53 [default = 0];</div><div class="line"></div><div class="line">  // Fields related to detection (det_*)</div><div class="line">  // foreground (object) overlap threshold</div><div class="line">  optional float det_fg_threshold = 54 [default = 0.5];</div><div class="line">  // background (non-object) overlap threshold</div><div class="line">  optional float det_bg_threshold = 55 [default = 0.5];</div><div class="line">  // Fraction of batch that should be foreground objects</div><div class="line">  optional float det_fg_fraction = 56 [default = 0.25];</div><div class="line"></div><div class="line">  // optional bool OBSOLETE_can_clobber = 57 [default = true];</div><div class="line"></div><div class="line">  // Amount of contextual padding to add around a window</div><div class="line">  // (used only by the window_data_layer)</div><div class="line">  optional uint32 det_context_pad = 58 [default = 0];</div><div class="line"></div><div class="line">  // Mode for cropping out a detection window</div><div class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</div><div class="line">  // square: the tightest square around the window is cropped</div><div class="line">  optional string det_crop_mode = 59 [default = &quot;warp&quot;];</div><div class="line"></div><div class="line">  // For ReshapeLayer, one needs to specify the new dimensions.</div><div class="line">  optional int32 new_num = 60 [default = 0];</div><div class="line">  optional int32 new_channels = 61 [default = 0];</div><div class="line">  optional int32 new_height = 62 [default = 0];</div><div class="line">  optional int32 new_width = 63 [default = 0];</div><div class="line"></div><div class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</div><div class="line">  // It will also resize images if new_height or new_width are not zero.</div><div class="line">  optional bool shuffle_images = 64 [default = false];</div><div class="line"></div><div class="line">  // For ConcatLayer, one needs to specify the dimension for concatenation, and</div><div class="line">  // the other dimensions must be the same for all the bottom blobs.</div><div class="line">  // By default it will concatenate blobs along the channels dimension.</div><div class="line">  optional uint32 concat_dim = 65 [default = 1];</div><div class="line"></div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 1001;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// PReLU层参数，ReLU的进化版本</div><div class="line">message PReLUParameter &#123;</div><div class="line">  // Parametric ReLU described in K. He et al, Delving Deep into Rectifiers:</div><div class="line">  // Surpassing Human-Level Performance on ImageNet Classification, 2015.</div><div class="line"></div><div class="line">  // Initial value of a_i. Default is a_i=0.25 for all i.</div><div class="line">  // 参数初始化</div><div class="line">  optional FillerParameter filler = 1;</div><div class="line">  // Whether or not slope parameters are shared across channels.</div><div class="line">  // 是否在各通道共享参数</div><div class="line">  optional bool channel_shared = 2 [default = false];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      Caffe源码解析(一)——caffe.proto
    
    </summary>
    
      <category term="Caffe" scheme="noahsnail.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="noahsnail.com/tags/Caffe/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/07/04/2017-7-4-AlexNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    <id>noahsnail.com/2017/07/04/2017-7-4-AlexNet论文翻译/</id>
    <published>2017-07-04T10:04:42.000Z</published>
    <updated>2017-07-21T09:24:13.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://ocs628urt.bkt.clouddn.com/we-need-to-go-deeper.jpg" alt="Deep Learning"></p>
<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="ImageNet-Classification-with-Deep-Convolutional-Neural-Networks"><a href="#ImageNet-Classification-with-Deep-Convolutional-Neural-Networks" class="headerlink" title="ImageNet Classification with Deep Convolutional Neural Networks"></a>ImageNet Classification with Deep Convolutional Neural Networks</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们训练了一个大型深度卷积神经网络来将<code>ImageNet LSVRC-2010</code>竞赛的120万高分辨率的图像分到1000不同的类别中。在测试数据上，我们得到了<code>top-1 37.5%, top-5 17.0%</code>的错误率，这个结果比目前的最好结果好很多。这个神经网络有6000万参数和650000个神经元，包含5个卷积层（某些卷积层后面带有池化层）和3个全连接层，最后是一个1000维的softmax。为了训练的更快，我们使用了非饱和神经元并对卷积操作进行了非常有效的GPU实现。为了减少全连接层的过拟合，我们采用了一个最近开发的名为<code>dropout</code>的正则化方法，结果证明是非常有效的。我们也使用这个模型的一个变种参加了<code>ILSVRC-2012</code>竞赛，赢得了冠军并且与第二名 <code>top-5 26.2%</code>的错误率相比，我们取得了<code>top-5 15.3%</code>的错误率。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively small -- on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the current best error rate on the MNIST digit-recognition task (&lt;0.3%) approaches human performance [4]. But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>当前的目标识别方法基本上都使用了机器学习方法。为了提高目标识别的性能，我们可以收集更大的数据集，学习更强大的模型，使用更好的技术来防止过拟合。直到最近，标注图像的数据集都相对较小--在几万张图像的数量级上（例如，NORB[16]，Caltech-101/256 [8, 9]和CIFAR-10/100 [12]）。简单的识别任务在这样大小的数据集上可以被解决的相当好，尤其是如果通过标签保留变换进行数据增强的情况下。例如，目前在MNIST数字识别任务上（&lt;0.3%）的最好准确率已经接近了人类水平[4]。但真实环境中的对象表现出了相当大的可变性，因此为了学习识别它们，有必要使用更大的训练数据集。实际上，小图像数据集的缺点已经被广泛认识到（例如，Pinto et al. [21]），但收集上百万图像的标注数据仅在最近才变得的可能。新的更大的数据集包括LabelMe [23]，它包含了数十万张完全分割的图像，ImageNet[6]，它包含了22000个类别上的超过1500万张标注的高分辨率的图像。</p>
<p>To learn about thousands of objects from millions of images, we need a model with a large learning capacity. However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.</p>
<p>为了从数百万张图像中学习几千个对象，我们需要一个有很强学习能力的模型。然而对象识别任务的巨大复杂性意味着这个问题不能被指定，即使通过像ImageNet这样的大数据集，因此我们的模型应该也有许多先验知识来补偿我们所没有的数据。卷积神经网络(CNNs)构成了一个这样的模型[16, 11, 13, 18, 15, 22, 26]。它们的能力可以通过改变它们的广度和深度来控制，它们也可以对图像的本质进行强大且通常正确的假设（也就是说，统计的稳定性和像素依赖的局部性）。因此，与具有层次大小相似的标准前馈神经网络，CNNs有更少的连接和参数，因此它们更容易训练，而它们理论上的最佳性能可能仅比标准前馈神经网络差一点。</p>
<p>Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images. Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting.</p>
<p>尽管CNN具有引人注目的质量，尽管它们的局部架构相当有效，但将它们大规模的应用到到高分辨率图像中仍然是极其昂贵的。幸运的是，目前的GPU，搭配了高度优化的2D卷积实现，强大到足够促进有趣地大量CNN的训练，最近的数据集例如ImageNet包含足够的标注样本来训练这样的模型而没有严重的过拟合。</p>
<p>The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets. We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly. Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3. The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4. Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in inferior performance.</p>
<p>本文具体的贡献如下：我们在ILSVRC-2010和ILSVRC-2012[2]的ImageNet子集上训练了到目前为止最大的神经网络之一，并取得了迄今为止在这些数据集上报道过的最好结果。我们编写了高度优化的2D卷积GPU实现以及训练卷积神经网络内部的所有其它操作，我们把它公开了。我们的网络包含许多新的不寻常的特性，这些特性提高了神经网络的性能并减少了训练时间，详见第三节。即使使用了120万标注的训练样本，我们的网络尺寸仍然使过拟合成为一个明显的问题，因此我们使用了一些有效的技术来防止过拟合，详见第四节。我们最终的网络包含5个卷积层和3个全连接层，深度似乎是非常重要的：我们发现移除任何卷积层（每个卷积层包含的参数不超过模型参数的1%）都会导致更差的性能。</p>
<p>In the end, the network’s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate. Our network takes between five and six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.</p>
<p>最后，网络尺寸主要受限于目前GPU的内存容量和我们能忍受的训练时间。我们的网络在两个GTX 580 3GB GPU上训练五六天。我们的所有实验表明我们的结果可以简单地通过等待更快的GPU和更大的可用数据集来提高。</p>
<h2 id="2-The-Dataset"><a href="#2-The-Dataset" class="headerlink" title="2 The Dataset"></a>2 The Dataset</h2><p>ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.</p>
<h2 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2 数据集"></a>2 数据集</h2><p>ImageNet数据集有超过1500万的标注高分辨率图像，这些图像属于大约22000个类别。这些图像是从网上收集的，使用了Amazon’s Mechanical Turk的众包工具通过人工标注的。从2010年起，作为Pascal视觉对象挑战赛的一部分，每年都会举办ImageNet大规模视觉识别挑战赛（ILSVRC）。ILSVRC使用ImageNet的一个子集，1000个类别每个类别大约1000张图像。总计，大约120万训练图像，50000张验证图像和15万测试图像。</p>
<p>ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is the version on which we performed most of our experiments. Since we also entered our model in the ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as well, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model.</p>
<p>ILSVRC-2010是ILSVRC竞赛中唯一可以获得测试集标签的版本，因此我们大多数实验都是在这个版本上运行的。由于我们也使用我们的模型参加了ILSVRC-2012竞赛，因此在第六节我们也报告了模型在这个版本的数据集上的结果，这个版本的测试标签是不可获得的。在ImageNet上，按照惯例报告两个错误率：<code>top-1</code>和<code>top-5</code>，<code>top-5</code>错误率是指测试图像的正确标签不在模型认为的五个最可能的便签之中。</p>
<p>ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality. Therefore, we down-sampled the images to a fixed resolution of 256 × 256. Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256×256 patch from the resulting image. We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.</p>
<p>ImageNet包含各种分辨率的图像，而我们的系统要求不变的输入维度。因此，我们将图像进行下采样到固定的<code>256×256</code>分辨率。给定一个矩形图像，我们首先缩放图像短边长度为256，然后从结果图像中裁剪中心的<code>256×256</code>大小的图像块。除了在训练集上对像素减去平均活跃度外，我们不对图像做任何其它的预处理。因此我们在原始的RGB像素值（中心的）上训练我们的网络。</p>
<h2 id="3-The-Architecture"><a href="#3-The-Architecture" class="headerlink" title="3 The Architecture"></a>3 The Architecture</h2><p>The architecture of our network is summarized in Figure 2. It contains eight learned layers — five convolutional and three fully-connected. Below, we describe some of the novel or unusual features of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of their importance, with the most important first.</p>
<h2 id="3-架构"><a href="#3-架构" class="headerlink" title="3 架构"></a>3 架构</h2><p>我们的网络架构概括为图2。它包含八个学习层--5个卷积层和3个全连接层。下面，我们将描述我们网络结构中的一些新奇的不寻常的特性。3.1-3.4小节按照我们对它们评估的重要性进行排序，最重要的最有先。</p>
<h3 id="3-1-ReLU-Nonlinearity"><a href="#3-1-ReLU-Nonlinearity" class="headerlink" title="3.1 ReLU Nonlinearity"></a>3.1 ReLU Nonlinearity</h3><p>The standard way to model a neuron’s output <code>f</code> as a function of its input <code>x</code> is with <code>f(x) = tanh(x)</code> or <code>f(x) = (1 + e−x)−1</code>. In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity <code>f(x) = max(0,x)</code>. Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units. This is demonstrated in Figure 1, which shows the number of iterations required to reach 25% training error on the CIFAR-10 dataset for a particular four-layer convolutional network. This plot shows that we would not have been able to experiment with such large neural networks for this work if we had used traditional saturating neuron models.</p>
<h3 id="3-1-ReLU非线性"><a href="#3-1-ReLU非线性" class="headerlink" title="3.1 ReLU非线性"></a>3.1 ReLU非线性</h3><p>将神经元输出<code>f</code>建模为输入<code>x</code>的函数的标准方式是用<code>f(x) = tanh(x)</code>或<code>f(x) = (1 + e−x)−1</code>。考虑到梯度下降的训练时间，这些饱和的非线性比非饱和非线性<code>f(x) = max(0,x)</code>更慢。根据Nair和Hinton[20]的说法，我们将这种非线性神经元称为修正线性单元(ReLU)。采用ReLU的深度卷积神经网络训练时间比等价的<code>tanh</code>单元要快几倍。在图1中，对于一个特定的四层卷积网络，在CIFAR-10数据集上达到25%的训练误差所需要的迭代次数可以证实这一点。这幅图表明，如果我们采用传统的饱和神经元模型，我们将不能在如此大的神经网络上实验该工作。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%201.png" alt="Figure 1"></p>
<p>Figure 1: A four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line). The learning rates for each network were chosen independently to make training as fast as possible. No regularization of any kind was employed. The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons.</p>
<p>图1：使用ReLU的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍。为了使训练尽可能快，每个网络的学习率是单独选择的。没有采用任何类型的正则化。影响的大小随着网络结构的变化而变化，这一点已得到证实，但使用ReLU的网络都比等价的饱和神经元快几倍。</p>
<p>We are not the first to consider alternatives to traditional neuron models in CNNs. For example, Jarrett et al. [11] claim that the nonlinearity <code>f(x) = |tanh(x)|</code> works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset. However, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs. Faster learning has a great influence on the performance of large models trained on large datasets.</p>
<p>我们不是第一个考虑替代CNN中传统神经元模型的人。例如，Jarrett等人[11]声称非线性函数<code>f(x) = |tanh(x)|</code>与其对比度归一化一起，然后是局部均值池化，在Caltech-101数据集上工作的非常好。然而，在这个数据集上主要的关注点是防止过拟合，因此他们观测到的影响不同于我们使用ReLU拟合数据集时的加速能力。更快的学习对大型数据集上大型模型的性能有很大的影响。</p>
<h3 id="3-2-Training-on-Multiple-GPUs"><a href="#3-2-Training-on-Multiple-GPUs" class="headerlink" title="3.2 Training on Multiple GPUs"></a>3.2 Training on Multiple GPUs</h3><p>A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another’s memory directly, without going through host machine memory. The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers. This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation.</p>
<h3 id="3-2-多GPU训练"><a href="#3-2-多GPU训练" class="headerlink" title="3.2 多GPU训练"></a>3.2 多GPU训练</h3><p>单个GTX580 GPU只有3G内存，这限制了可以在GTX580上进行训练的网络最大尺寸。事实证明120万图像用来进行网络训练是足够的，但网络太大因此不能在单个GPU上进行训练。因此我们将网络分布在两个GPU上。目前的GPU非常适合跨GPU并行，因为它们可以直接互相读写内存，而不需要通过主机内存。我们采用的并行方案基本上每个GPU放置一半的核（或神经元），还有一个额外的技巧：只在某些特定的层上进行GPU通信。这意味着，例如，第3层的核会将第2层的所有核映射作为输入。然而，第4层的核只将位于相同GPU上的第3层的核映射作为输入。连接模式的选择是一个交叉验证问题，但这可以让我们准确地调整通信数量，直到它的计算量在可接受的范围内。</p>
<p>The resultant architecture is somewhat similar to that of the “columnar” CNN employed by Ciresan et al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as<br>many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU net.</p>
<p>除了我们的列不是独立的之外（看图2），最终的架构有点类似于Ciresan等人[5]采用的“columnar” CNN。与每个卷积层一半的核在单GPU上训练的网络相比，这个方案降分别低了我们的<code>top-1 1.7%</code>，<code>top-5 1.2%</code>的错误率。双GPU网络比单GPU网络稍微减少了训练时间。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Fig%202.png" alt="Figure 2"></p>
<p>Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and the number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264– 4096–4096–1000.</p>
<p>图 2：我们CNN架构图解，明确描述了两个GPU之间的责任。在图的顶部，一个GPU运行在部分层上，而在图的底部，另一个GPU运行在部分层上。GPU只在特定的层进行通信。网络的输入是150,528维，网络剩下层的神经元数目分别是253,440–186,624–64,896–64,896–43,264–4096–4096–1000（8层）。</p>
<h3 id="3-3-Local-Response-Normalization"><a href="#3-3-Local-Response-Normalization" class="headerlink" title="3.3 Local Response Normalization"></a>3.3 Local Response Normalization</h3><p>ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization. Denoting by $a_{x,y}^i$ the activity of a neuron computed by applying kernel $i$ at position $(x, y)$ and then applying the ReLU nonlinearity, the response-normalized activity $b^i_{x,y}$ is given by the expression</p>
<p>$$b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta$$</p>
<p>where the sum runs over n “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined before training begins. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. The constants k, n, α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = 0.0001, and β = 0.75. We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).</p>
<h3 id="3-3-局部响应归一化"><a href="#3-3-局部响应归一化" class="headerlink" title="3.3 局部响应归一化"></a>3.3 局部响应归一化</h3><p>ReLU具有让人满意的特性，它不需要通过输入归一化来防止饱和。如果至少一些训练样本对ReLU产生了正输入，那么那个神经元上将发生学习。然而，我们仍然发现接下来的局部响应归一化有助于泛化。$a_{x,y}^i$表示神经元激活，通过在$(x, y)$位置应用核$i$，然后应用ReLU非线性来计算，响应归一化激活$b^i_{x,y}$通过下式给定：</p>
<p>$$b^i_{x,y} = a_{x,y}^i / ( k + \alpha \sum _{j = max(0, i-n / 2)} ^{min(N-1, i+n / 2)} (a_{x,y}^i)^2 )^\beta$$</p>
<p>求和运算在n个“毗邻的”核映射的同一位置上执行，N是本层的卷积核数目。核映射的顺序当然是任意的，在训练开始前确定。响应归一化的顺序实现了一种侧抑制形式，灵感来自于真实神经元中发现的类型，为使用不同核进行神经元输出计算的较大活动创造了竞争。常量k，n，α，β是超参数，它们的值通过验证集确定；我们设k=2，n=5，α=0.0001，β=0.75。我们在特定的层使用的ReLU非线性之后应用了这种归一化（请看3.5小节）。</p>
<p>This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11], but ours would be more correctly termed “brightness normalization”, since we do not subtract the mean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization.</p>
<p>这个方案与Jarrett等人[11]的局部对比度归一化方案有一定的相似性，但我们更恰当的称其为“亮度归一化”，因此我们没有减去均值。响应归一化分别减少了<code>top-1 1.4%</code>，<code>top-5 1.2%</code>的错误率。我们也在CIFAR-10数据集上验证了这个方案的有效性：一个乜嘢归一化的四层CNN取得了13%的错误率，而使用归一化取得了11%的错误率。</p>
<h3 id="3-4-Overlapping-Pooling"><a href="#3-4-Overlapping-Pooling" class="headerlink" title="3.4 Overlapping Pooling"></a>3.4 Overlapping Pooling</h3><p>Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced $s$ pixels apart, each summarizing a neighborhood of size $z × z$ centered at the location of the pooling unit. If we set $s = z$, we obtain traditional local pooling as commonly employed in CNNs. If we set $s &lt; z$, we obtain overlapping pooling. This is what we use throughout our network, with $s = 2$ and $z = 3$. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme $s = 2, z = 2$, which produces output of equivalent dimensions. We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.</p>
<h3 id="3-4-重叠池化"><a href="#3-4-重叠池化" class="headerlink" title="3.4 重叠池化"></a>3.4 重叠池化</h3><p>CNN中的池化层归纳了同一核映射上相邻组神经元的输出。习惯上，相邻池化单元归纳的区域是不重叠的（例如[17, 11, 4]）。更确切的说，池化层可看作由池化单元网格组成，网格间距为$s$个像素，每个网格归纳池化单元中心位置$z × z$大小的邻居。如果设置$s = z$，我们会得到通常在CNN中采用的传统局部池化。如果设置$s &lt; z$，我们会得到重叠池化。这就是我们网络中使用的方法，设置$s = 2$，$z = 3$。这个方案分别降低了<code>top-1 0.4%</code>，<code>top-5 0.3%</code>的错误率，与非重叠方案$s = 2，z = 2$相比，输出的维度是相等的。我们在训练过程中通常观察采用重叠池化的模型，发现它更难过拟合。</p>
<h3 id="3-5-Overall-Architecture"><a href="#3-5-Overall-Architecture" class="headerlink" title="3.5 Overall Architecture"></a>3.5 Overall Architecture</h3><p>Now we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fully-connected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.</p>
<h3 id="3-5-整体架构"><a href="#3-5-整体架构" class="headerlink" title="3.5 整体架构"></a>3.5 整体架构</h3><p>现在我们准备描述我们的CNN的整体架构。如图2所示，我们的网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签的分布。我们的网络最大化多项逻辑回归的目标，这等价于最大化预测分布下训练样本正确标签的对数概率的均值。</p>
<p>The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third convolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-connected layers are connected to all neurons in the previous layer. Response-normalization layers follow the first and second convolutional layers. Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer. The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.</p>
<p>第2，4，5卷积层的核只与位于同一GPU上的前一层的核映射相连接（看图2）。第3卷积层的核与第2层的所有核映射相连。全连接层的神经元与前一层的所有神经元相连。第1，2卷积层之后是响应归一化层。3.4节描述的这种最大池化层在响应归一化层和第5卷积层之后。ReLU非线性应用在每个卷积层和全连接层的输出上。</p>
<p>The first convolutional layer filters the 224 × 224 × 3 input image with 96 kernels of size 11 × 11 × 3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each.</p>
<p>第1卷积层使用96个核对224 × 224 × 3的输入图像进行滤波，核大小为11 × 11 × 3，步长是4个像素（核映射中相邻神经元感受野中心之间的距离）。第2卷积层使用用第1卷积层的输出（响应归一化和池化）作为输入，并使用256个核进行滤波，核大小为5 × 5 × 48。第3，4，5卷积层互相连接，中间没有接入池化层或归一化层。第3卷积层有384个核，核大小为3 × 3 × 256，与第2卷积层的输出（归一化的，池化的）相连。第4卷积层有384个核，核大小为3 × 3 × 192，第5卷积层有256个核，核大小为3 × 3 × 192。每个全连接层有4096个神经元。</p>
<h2 id="4-Reducing-Overfitting"><a href="#4-Reducing-Overfitting" class="headerlink" title="4 Reducing Overfitting"></a>4 Reducing Overfitting</h2><p>Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting.</p>
<h2 id="4-减少过拟合"><a href="#4-减少过拟合" class="headerlink" title="4 减少过拟合"></a>4 减少过拟合</h2><p>我们的神经网络架构有6000万参数。尽管ILSVRC的1000类使每个训练样本从图像到标签的映射上强加了10比特的约束，但这不足以学习这么多的参数而没有相当大的过拟合。下面，我们会描述我们用来克服过拟合的两种主要方式。</p>
<h3 id="4-1-Data-Augmentation"><a href="#4-1-Data-Augmentation" class="headerlink" title="4.1 Data Augmentation"></a>4.1 Data Augmentation</h3><p>The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these data augmentation schemes are, in effect, computationally free.</p>
<h3 id="4-1-数据增强"><a href="#4-1-数据增强" class="headerlink" title="4.1 数据增强"></a>4.1 数据增强</h3><p>图像数据上最简单常用的用来减少过拟合的方法是使用标签保留变换（例如[25, 4, 5]）来人工增大数据集。我们使用了两种独特的数据增强方式，这两种方式都可以从原始图像通过非常少的计算量产生变换的图像，因此变换图像不需要存储在硬盘上。在我们的实现中，变换图像通过CPU的Python代码生成，而此时GPU正在训练前一批图像。因此，实际上这些数据增强方案是计算免费的。</p>
<p>The first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patches. This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent. Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks. At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network’s softmax layer on the ten patches.</p>
<p>第一种数据增强方式包括产生图像变换和水平翻转。我们从256×256图像上通过随机提取224 × 224的图像块实现了这种方式，然后在这些提取的图像块上进行训练。这通过一个2048因子增大了我们的训练集，尽管最终的训练样本是高度相关的。没有这个方案，我们的网络会有大量的过拟合，这会迫使我们使用更小的网络。在测试时，网络会提取5个224 × 224的图像块（四个角上的图像块和中心的图像块）和它们的水平翻转（因此总共10个图像块）进行预测，然后对网络在10个图像块上的softmax层进行平均。</p>
<p>The second form of data augmentation consists of altering the intensities of the RGB channels in training images. Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set. To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel $I_xy = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$ we add the following quantity: </p>
<p>$$[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T$$</p>
<p>where $p_i$ and $\lambda_i$ are $i$th eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and $\alpha_i$ is the aforementioned random variable. Each $\alpha_i$ is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn. This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. This scheme reduces the top-1 error rate by over 1%.</p>
<p>第二种数据增强方式包括改变训练图像的RGB通道的强度。具体地，我们在整个ImageNet训练集上对RGB像素值集合执行PCA。对于每幅训练图像，我们加上多倍找到的主成分，大小成正比的对应特征值乘以一个随机变量，随机变量通过均值为0，标准差为0.1的高斯分布得到。因此对于每幅RGB图像像素$I_xy = [I^R_{xy} , I^G_{xy} , I^B_{xy} ]^T$，我们加上下面的数量：</p>
<p>$$[p_1, p_2, p_3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T$$</p>
<p>$p_i$，$\lambda_i$分别是RGB像素值3 × 3协方差矩阵的第$i$个特征向量和特征值，$\alpha_i$是前面提到的随机变量。对于某个训练图像的所有像素，每个$\alpha_i$只获取一次，直到图像进行下一次训练时才重新获取。这个方案近似抓住了自然图像的一个重要特性，即光照的颜色和强度发生变化时，目标身份是不变的。这个方案减少了<code>top 1</code>错误率1%以上。</p>
<h3 id="4-2-Dropout"><a href="#4-2-Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h3><p>Combining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back-propagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.</p>
<h3 id="4-2-失活-Dropout"><a href="#4-2-失活-Dropout" class="headerlink" title="4.2 失活(Dropout)"></a>4.2 失活(Dropout)</h3><p>将许多不同模型的预测结合起来是降低测试误差[1, 3]的一个非常成功的方法，但对于需要花费几天来训练的大型神经网络来说，这似乎太昂贵了。然而，有一个非常有效的模型结合版本，它只花费两倍的训练成本。这种最近引入的技术，叫做“dropout”[10]，它会以0.5的概率对每个隐层神经元的输出设为0。那些“失活的”的神经元不再进行前向传播并且不参与反向传播。因此每次输入时，神经网络会采样一个不同的架构，但所有架构共享权重。这个技术减少了复杂的神经元互适应，因为一个神经元不能依赖特定的其它神经元的存在。因此，神经元被强迫学习更鲁棒的特征，它在与许多不同的其它神经元的随机子集结合时是有用的。在测试时，我们使用所有的神经元但它们的输出乘以0.5，对指数级的许多失活网络的预测分布进行几何平均，这是一种合理的近似。</p>
<p>We use dropout in the first two fully-connected layers of Figure 2. Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.</p>
<p>我们在图2中的前两个全连接层使用失活。如果没有失活，我们的网络表现出大量的过拟合。失活大致上使要求收敛的迭代次数翻了一倍。</p>
<h2 id="5-Details-of-learning"><a href="#5-Details-of-learning" class="headerlink" title="5 Details of learning"></a>5 Details of learning</h2><p>We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s training error. The update rule for weight $w$ was</p>
<p>$$v_{i+1} := 0.9 \bullet v_i - 0.0005 \bullet \varepsilon \bullet w_i - \varepsilon \bullet \langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$$</p>
<p>where $i$ is the iteration index, $v$ is the momentum variable, $\varepsilon$ is the learning rate, and $\langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$ is the average over the $i$th batch $D_i$ of the derivative of the objective with respect to $w$, evaluated at $w_i$.</p>
<h2 id="5-学习细节"><a href="#5-学习细节" class="headerlink" title="5 学习细节"></a>5 学习细节</h2><p>我们使用随机梯度下降来训练我们的模型，样本的batch size为128，动量为0.9，权重衰减为0.0005。我们发现少量的权重衰减对于模型的学习是重要的。换句话说，权重衰减不仅仅是一个正则项：它减少了模型的训练误差。权重$w$的更新规则是</p>
<p>$$v_{i+1} := 0.9 \bullet v_i - 0.0005 \bullet \varepsilon \bullet w_i - \varepsilon \bullet \langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$$</p>
<p>$i$是迭代索引，$v$是动量变量，$\varepsilon$是学习率，$\langle \frac{\partial L} {\partial w} |_{w_i}\rangle _{D_i}$是目标函数对$w$，在$w_i$上的第$i$批微分$D_i$的平均。</p>
<p>We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron biases in the remaining layers with the constant 0.</p>
<p>我们使用均值为0，标准差为0.01的高斯分布对每一层的权重进行初始化。我们在第2，4，5卷积层和全连接隐层将神经元偏置初始化为常量1。这个初始化通过为ReLU提供正输入加速了学习的早期阶段。我们在剩下的层将神经元偏置初始化为0。</p>
<p>We used an equal learning rate for all layers, which we adjusted manually throughout training. The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination. We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.</p>
<p>我们对所有的层使用相等的学习率，这个是在整个训练过程中我们手动调整得到的。当验证误差在当前的学习率下停止提供时，我们遵循启发式的方法将学习率除以10。学习率初始化为0.01，在训练停止之前降低三次。我们在120万图像的训练数据集上训练神经网络大约90个循环，在两个NVIDIA GTX 580 3GB GPU上花费了五到六天。</p>
<h2 id="6-Results"><a href="#6-Results" class="headerlink" title="6 Results"></a>6 Results</h2><p>Our results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%. The best performance achieved during the ILSVRC-2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%201.png" alt="Table 1"></p>
<p>Table 1: Comparison of results on ILSVRC-2010 test set.In italics are best results achieved by others.</p>
<h2 id="6-结果"><a href="#6-结果" class="headerlink" title="6 结果"></a>6 结果</h2><p>我们在ILSVRC-2010上的结果概括为表1。我们的神经网络取得了<code>top-1 37.5%</code>，<code>top-5 17.0%</code>的错误率。在ILSVRC-2010竞赛中最佳结果是<code>top-1 47.1%</code>，<code>top-5 28.2%</code>，使用的方法是对6个在不同特征上训练的稀疏编码模型生成的预测进行平均，从那时起已公布的最好结果是<code>top-1 45.7%</code>，<code>top-5 25.7%</code>，使用的方法是平均在Fisher向量（FV）上训练的两个分类器的预测结果，Fisher向量是通过两种密集采样特征计算得到的[24]。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%201.png" alt="表1"></p>
<p>表1：ILSVRC-2010测试集上的结果对比。斜体是其它人取得的最好结果。</p>
<p>We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then “fine-tuning” it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7].</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%202.png" alt="Table 2"></p>
<p>Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk were “pre-trained” to classify the entire ImageNet 2011 Fall release. See Section 6 for details. </p>
<p>我们也用我们的模型参加了ILSVRC-2012竞赛并在表2中报告了我们的结果。由于ILSVRC-2012的测试集标签不可以公开得到，我们不能报告我们尝试的所有模型的测试错误率。在这段的其余部分，我们会使用验证误差率和测试误差率互换，因为在我们的实验中它们的差别不会超过0.1%（看图2）。本文中描述的CNN取得了<code>top-5 18.2%</code>的错误率。五个类似的CNN预测的平均误差率为16.4%。为了对ImageNet 2011秋季发布的整个数据集（1500万图像，22000个类别）进行分类，我们在最后的池化层之后有一个额外的第6卷积层，训练了一个CNN，然后在它上面进行“fine-tuning”，在ILSVRC-2012取得了16.6%的错误率。对在ImageNet 2011秋季发布的整个数据集上预训练的两个CNN和前面提到的五个CNN的预测进行平均得到了15.3%的错误率。第二名的最好竞赛输入取得了26.2%的错误率，他的方法是对FV上训练的一些分类器的预测结果进行平均，FV在不同类型密集采样特征计算得到的。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Table%202.png" alt="表2"></p>
<p>表2：ILSVRC-2012验证集和测试集的误差对比。斜线部分是其它人取得的最好的结果。带星号的是“预训练的”对ImageNet 2011秋季数据集进行分类的模型。更多细节请看第六节。</p>
<p>Finally, we also report our error rates on the Fall 2009 version of ImageNet with 10,184 categories and 8.9 million images. On this dataset we follow the convention in the literature of using half of the images for training and half for testing. Since there is no established test set, our split necessarily differs from the splits used by previous authors, but this does not affect the results appreciably. Our top-1 and top-5 error rates on this dataset are 67.4% and 40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].</p>
<p>最后，我们也报告了我们在ImageNet 2009秋季数据集上的误差率，ImageNet 2009秋季数据集有10,184个类，890万图像。在这个数据集上我们按照惯例用一半的图像来训练，一半的图像来测试。由于没有建立测试集，我们的数据集分割有必要不同于以前作者的数据集分割，但这对结果没有明显的影响。我们在这个数据集上的的top-1和top-5错误率是67.4%和40.9%，使用的是上面描述的在最后的池化层之后有一个额外的第6卷积层网络。这个数据集上公开可获得的最好结果是78.1%和60.9%[19]。</p>
<h3 id="6-1-Qualitative-Evaluations"><a href="#6-1-Qualitative-Evaluations" class="headerlink" title="6.1 Qualitative Evaluations"></a>6.1 Qualitative Evaluations</h3><p>Figure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The network has learned a variety of frequency and orientation-selective kernels, as well as various colored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific. This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs).</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%203.png" alt="Figure 3"></p>
<p>Figure 3: 96 convolutional kernels of size 11×11×3 learned by the first convolutional layer on the 224×224×3 input images. The top 48 kernels were learned on GPU 1 while the bottom 48 kernels were learned on GPU 2. See Section 6.1 for details.</p>
<h3 id="6-1-定性评估"><a href="#6-1-定性评估" class="headerlink" title="6.1 定性评估"></a>6.1 定性评估</h3><p>图3显示了网络的两个数据连接层学习到的卷积核。网络学习到了大量的频率核、方向选择核，也学到了各种颜色点。注意两个GPU表现出的专业化，3.5小节中描述的受限连接的结果。GPU 1上的核主要是没有颜色的，而GPU 2上的核主要是针对颜色的。这种专业化在每次运行时都会发生，并且是与任何特别的随机权重初始化（以GPU的重新编号为模）无关的。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%203.png" alt="Figure 3"></p>
<p>图3：第一卷积层在224×224×3的输入图像上学习到的大小为11×11×3的96个卷积核。上面的48个核是在GPU 1上学习到的而下面的48个卷积核是在GPU 2上学习到的。更多细节请看6.1小节。</p>
<p>In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its top-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the top-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example, only other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry) there is genuine ambiguity about the intended focus of the photograph.</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%204.png" alt="Figure 4"></p>
<p>Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model. The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.</p>
<p>在图4的左边部分，我们通过在8张测试图像上计算它的top-5预测定性地评估了网络学习到的东西。注意即使是不在图像中心的目标也能被网络识别，例如左上角的小虫。大多数的top-5标签似乎是合理的。例如，对于美洲豹来说，只有其它类型的猫被认为是看似合理的标签。在某些案例（格栅，樱桃）中，网络在意的图片焦点真的很含糊。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/Figure%204.png" alt="Figure 4"></p>
<p>图4：（左）8张ILSVRC-2010测试图像和我们的模型认为最可能的5个标签。每张图像的下面是它的正确标签，正确标签的概率用红条表示（如果正确标签在top 5中）。（右）第一列是5张ILSVRC-2010测试图像。剩下的列展示了6张训练图像，这些图像在最后的隐藏层的特征向量与测试图像的特征向量有最小的欧氏距离。</p>
<p>Another way to probe the network’s visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer. If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar. Figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure. Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column. For example, the retrieved dogs and elephants appear in a variety of poses. We present the results for many more test images in the supplementary material.</p>
<p>探索网络可视化知识的另一种方式是思考最后的4096维隐藏层在图像上得到的特征激活。如果两幅图像生成的特征激活向量之间有较小的欧式距离，我们可以认为神经网络的更高层特征认为它们是相似的。图4表明根据这个度量标准，测试集的5张图像和训练集的6张图像中的每一张都是最相似的。注意在像素级别，检索到的训练图像与第一列的查询图像在L2上通常是不接近的。例如，检索的狗和大象似乎有很多姿态。我们在补充材料中对更多的测试图像呈现了这种结果。</p>
<p>Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vectors is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes. This should produce a much better image retrieval method than applying auto-encoders to the raw pixels [14], which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar.</p>
<p>通过两个4096维实值向量间的欧氏距离来计算相似性是效率低下的，但通过训练一个自动编码器将这些向量压缩为短二值编码可以使其变得高效。这应该会产生一种比将自动编码器应用到原始像素上[14]更好的图像检索方法，自动编码器应用到原始像素上的方法没有使用图像标签，因此会趋向于检索与要检索的图像具有相似边缘模式的图像，无论它们是否是语义上相似。</p>
<h2 id="7-Discussion"><a href="#7-Discussion" class="headerlink" title="7 Discussion"></a>7 Discussion</h2><p>Our results show that a large, deep convolutional neural network is capable of achieving record-breaking results on a highly challenging dataset using purely supervised learning. It is notable that our network’s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results.</p>
<h2 id="7-探讨"><a href="#7-探讨" class="headerlink" title="7 探讨"></a>7 探讨</h2><p>我们的结果表明一个大型深度卷积神经网络在一个具有高度挑战性的数据集上使用纯有监督学习可以取得破纪录的结果。值得注意的是，如果移除一个卷积层，我们的网络性能会降低。例如，移除任何中间层都会引起网络损失大约2%的top-1性能。因此深度对于实现我们的结果非常重要。</p>
<p>To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data. Thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system. Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.</p>
<p>为了简化我们的实验，我们没有使用任何无监督的预训练，尽管我们希望它会有所帮助，特别是在如果我们能获得足够的计算能力来显著增加网络的大小而标注的数据量没有对应增加的情况下。到目前为止，我们的结果已经提高了，因为我们的网络更大、训练时间更长，但为了匹配人类视觉系统的下颞线（视觉专业术语）我们仍然有许多数量级要达到。最后我们想在视频序列上使用非常大的深度卷积网络，视频序列的时序结构会提供非常有帮助的信息，这些信息在静态图像上是缺失的或远不那么明显。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] R.M.BellandY.Koren.Lessonsfromthenetflixprizechallenge.ACMSIGKDDExplorationsNewsletter, 9(2):75–79, 2007.</p>
<p>[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.imagenet.org/challenges. 2010.</p>
<p>[3] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.</p>
<p>[4] D. Cires ̧an, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. Arxiv preprint arXiv:1202.2745, 2012.</p>
<p>[5] D.C. Cires ̧an, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classification. Arxiv preprint arXiv:1102.0183, 2011.</p>
<p>[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.</p>
<p>[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012, 2012. URL <a href="http://www.image-net.org/challenges/LSVRC/2012/" target="_blank" rel="external">http://www.image-net.org/challenges/LSVRC/2012/</a>.</p>
<p>[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):59–70, 2007.</p>
<p>[9] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. URL <a href="http://authors.library.caltech.edu/7694" target="_blank" rel="external">http://authors.library.caltech.edu/7694</a>.</p>
<p>[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.</p>
<p>[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In International Conference on Computer Vision, pages 2146–2153. IEEE, 2009.</p>
<p>[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009.</p>
<p>[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 2010.</p>
<p>[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In ESANN, 2011.</p>
<p>[15] Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, 1990.</p>
<p>[16] Y. LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–97. IEEE, 2004.</p>
<p>[17] Y. LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253–256. IEEE, 2010.</p>
<p>[18] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.</p>
<p>[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer Vision, Florence, Italy, October 2012.</p>
<p>[20] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 2010.</p>
<p>[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computational biology, 4(1):e27, 2008.</p>
<p>[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579,2009.</p>
<p>[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for image annotation. International journal of computer vision, 77(1):157–173, 2008.</p>
<p>[24] J.SánchezandF.Perronnin.High-dimensionalsignaturecompressionforlarge-scaleimageclassification. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1665–1672. IEEE,2011.</p>
<p>[25] P.Y. Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition, volume 2, pages 958–962, 2003.</p>
<p>[26] S.C.Turaga,J.F.Murray,V.Jain,F.Roth,M.Helmstaedter,K.Briggman,W.Denk,andH.S.Seung.Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22(2):511–538, 2010.</p>
]]></content>
    
    <summary type="html">
    
      AlexNet论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>LeNet在caffe中的实现分析</title>
    <link href="noahsnail.com/2017/07/04/2017-7-4-LeNet%E5%88%86%E6%9E%90%E5%8F%8A%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>noahsnail.com/2017/07/04/2017-7-4-LeNet分析及可视化/</id>
    <published>2017-07-04T09:29:07.000Z</published>
    <updated>2017-07-04T09:35:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是对Caffe中mnist数据集上训练的LeNet模型进行结构分析和可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> caffe</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义LeNet模型信息</span></div><div class="line">deploy = <span class="string">'lenet.prototxt'</span></div><div class="line">model = <span class="string">'lenet_iter_10000.caffemodel'</span></div><div class="line"></div><div class="line"><span class="comment"># 加载模型</span></div><div class="line">net = caffe.Net(deploy, model, caffe.TEST)</div><div class="line"></div><div class="line"><span class="comment"># 计算均值</span></div><div class="line"><span class="comment"># blob = caffe.proto.caffe_pb2.BlobProto()</span></div><div class="line"><span class="comment"># bin_mean = open(mean_file, 'rb' ).read()</span></div><div class="line"><span class="comment"># blob.ParseFromString(bin_mean)</span></div><div class="line"><span class="comment"># arr = np.array(caffe.io.blobproto_to_array(blob))</span></div><div class="line"><span class="comment"># npy_mean = arr[0]</span></div><div class="line"><span class="comment"># mu = npy_mean.mean(1).mean(1)</span></div><div class="line"></div><div class="line"><span class="comment"># init transformer</span></div><div class="line">transformer = caffe.io.Transformer(&#123;<span class="string">'data'</span>: net.blobs[<span class="string">'data'</span>].data.shape&#125;)</div><div class="line">transformer.set_transpose(<span class="string">'data'</span>, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</div><div class="line"><span class="comment"># transformer.set_mean('data', mu)</span></div><div class="line">transformer.set_raw_scale(<span class="string">'data'</span>, <span class="number">255</span>)</div><div class="line"><span class="comment"># transformer.set_channel_swap('data', (2, 1, 0))</span></div><div class="line"></div><div class="line"><span class="comment"># get certain layer feature</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(pimg, lay_name)</span>:</span></div><div class="line">    <span class="keyword">global</span> transformer</div><div class="line">    <span class="keyword">global</span> net</div><div class="line">    image = caffe.io.load_image(pimg, color = <span class="keyword">False</span>)</div><div class="line">    image</div><div class="line">    transformed_image = transformer.preprocess(<span class="string">'data'</span>, image)</div><div class="line">    net.blobs[<span class="string">'data'</span>].data[...] = transformed_image</div><div class="line">    output = net.forward()</div><div class="line">    result = output[lay_name]</div><div class="line">    <span class="keyword">return</span> result</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Test</span></div><div class="line">result = init(<span class="string">'test.jpg'</span>, <span class="string">'prob'</span>)</div><div class="line"><span class="keyword">print</span> result.shape</div><div class="line"><span class="keyword">print</span> result</div></pre></td></tr></table></figure>
<pre><code>(1, 10)
[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
</code></pre><p><strong>LeNet网络的所有layer以及layer的输出数据</strong><br>data: 输入图片数据大小为28*28<br>conv1: 20个卷积核，卷积之后feature map大小24*24<br>pool1: pooling后feature map大小变为12*12, 共20层<br>conv2: 50个卷积核, 卷积之后feature map大小为8*8<br>pool2: pooling后feature map大小变为4*4, 共50层<br>ip1: 全连接层一, 500个结点<br>ip2: 全连接层二, 10个结点<br>prob: 对ip2进行softmax</p>
<p>备注: conv1之后得到20个feature map, conv2有50个卷积核, 每个卷积核在20个feature map卷积之后, 20个卷积之后的feature map对应位置上的点的数据累加之后取激活函数(ReLU)得到该卷积核的对应的feature map, 因此conv2执行之后的feature map个数为50, 而不是50*20.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all layer name and blob shape</span></div><div class="line"><span class="comment"># blob shape is (batch_size, channel_dim, height, width).</span></div><div class="line"><span class="keyword">for</span> layer_name, blob <span class="keyword">in</span> net.blobs.iteritems():</div><div class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(blob.data.shape)</div></pre></td></tr></table></figure>
<pre><code>data  (1, 1, 28, 28)
conv1 (1, 20, 24, 24)
pool1 (1, 20, 12, 12)
conv2 (1, 50, 8, 8)
pool2 (1, 50, 4, 4)
ip1 (1, 500)
ip2 (1, 10)
prob  (1, 10)
</code></pre><p><strong>LeNet网络的权重(weights + biases)</strong><br>conv1: 20个卷积核, weights大小为5*5, 20个biases<br>conv2: 50个卷积核, weights大小为5*5, 50个biases<br>ip1: conv2之后得到50个4*4大小的feature map, 排列起来大小为800, 与ip1的500个结点进行全连接, weights个数为500*800, biases个数为500<br>ip2: ip1的500个结点与ip2的10个结点进行全连接, weights个数为500*10, biases个数为10</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all layer name and parameters shape</span></div><div class="line"><span class="comment"># param[0] is weights, param[1] is biases</span></div><div class="line"><span class="comment"># weights shape is (output_channels, input_channels, filter_height, filter_width)</span></div><div class="line"><span class="comment"># biases shape is (output_channels,)</span></div><div class="line"><span class="keyword">for</span> layer_name, param <span class="keyword">in</span> net.params.iteritems():</div><div class="line">    <span class="keyword">print</span> layer_name + <span class="string">'\t'</span> + str(param[<span class="number">0</span>].data.shape) + <span class="string">'\t'</span> + str(param[<span class="number">1</span>].data.shape)</div></pre></td></tr></table></figure>
<pre><code>conv1 (20, 1, 5, 5) (20,)
conv2 (50, 20, 5, 5)  (50,)
ip1 (500, 800)  (500,)
ip2 (10, 500) (10,)
</code></pre><p><strong>numpy pad</strong><br>padding分为四部分<br>第一部分: (0, n ** 2 - data.shape[0]), 补充方阵的缺少的部分, 0表示前面不补, 后面补n ** 2 - data.shape[0]列<br>第二部分: (0, 1)表示每个filter的前面不补, 后面补1列, filter补了一行<br>第三部分: (0, 1)表示每个filter的前面不补, 后面补1列, filter补了一列<br>第四部分: (0, 0)剩下的不补充数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># param(weights) visualization</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualization</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="comment"># normalize data for display</span></div><div class="line">    data = (data - data.min()) / (data.max() - data.min())</div><div class="line">    </div><div class="line">    <span class="comment"># force the number of filters to be square</span></div><div class="line">    n = int(np.ceil(np.sqrt(data.shape[<span class="number">0</span>])))</div><div class="line">    </div><div class="line">    <span class="comment"># add some space between filters</span></div><div class="line">    padding = (((<span class="number">0</span>, n ** <span class="number">2</span> - data.shape[<span class="number">0</span>]), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">1</span>)) + ((<span class="number">0</span>, <span class="number">0</span>),) * (data.ndim - <span class="number">3</span>)) </div><div class="line">    data = np.pad(data, padding, mode = <span class="string">'constant'</span>, constant_values = <span class="number">1</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># tile the filters into an image</span></div><div class="line">    data = data.reshape((n, n) + data.shape[<span class="number">1</span>:]).transpose((<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>) + tuple(range(<span class="number">4</span>, data.ndim + <span class="number">1</span>)))</div><div class="line">    data = data.reshape((n * data.shape[<span class="number">1</span>], n * data.shape[<span class="number">3</span>]) + data.shape[<span class="number">4</span>:])</div><div class="line">    plt.imshow(data, cmap=<span class="string">'gray'</span>)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># feature map visualization</span></div><div class="line">feature_map = net.blobs[<span class="string">'conv1'</span>].data[<span class="number">0</span>]</div><div class="line">visualization(feature_map)</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/output_8_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># filter visualization</span></div><div class="line">filters = net.params[<span class="string">'conv1'</span>][<span class="number">0</span>].data</div><div class="line">visualization(filters.reshape(<span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>))</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/output_9_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      LeNet在caffe中的实现分析
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>keras的基本用法(五)——图像predict</title>
    <link href="noahsnail.com/2017/06/12/2017-6-12-keras%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%94)%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8Fpredict/"/>
    <id>noahsnail.com/2017/06/12/2017-6-12-keras的基本用法(五)——图像predict/</id>
    <published>2017-06-12T07:48:52.000Z</published>
    <updated>2017-06-12T07:57:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要介绍Keras的一些基本用法，主要是根据已有模型预测图像的类别，以ResNet50为例。</p>
<ul>
<li>Demo</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">from keras.layers import Dense</div><div class="line">from keras.models import Model</div><div class="line">from keras.preprocessing import image</div><div class="line">from keras.applications.resnet50 import ResNet50</div><div class="line"></div><div class="line"># 使用ResNet的结构，不包括最后一层</div><div class="line">base_model = ResNet50(include_top = False, pooling = &apos;avg&apos;)</div><div class="line"></div><div class="line"># 定义网络结构最后一层</div><div class="line">predictions = Dense(3, activation=&apos;softmax&apos;)(base_model.output)</div><div class="line"></div><div class="line"># 定义模型</div><div class="line">model = Model(inputs=base_model.input, outputs=predictions)</div><div class="line"></div><div class="line"># 加载训练好的模型</div><div class="line">model.load_weights(&apos;./weights.h5&apos;)</div><div class="line"></div><div class="line">image_path = &apos;./lena.jpg&apos;</div><div class="line"></div><div class="line"># 加载图像</div><div class="line">img = image.load_img(image_path, target_size=(224, 224))</div><div class="line"></div><div class="line"># 图像预处理</div><div class="line">x = image.img_to_array(img)</div><div class="line">x = np.expand_dims(x, axis=0)</div><div class="line">x = preprocess_input(x)</div><div class="line"></div><div class="line"># 对图像进行分类</div><div class="line">preds = model.predict(x)</div><div class="line"></div><div class="line"># 输出预测概率</div><div class="line">print &apos;Predicted:&apos;, preds</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      keras的基本用法(五)——图像predict
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>keras的基本用法(四)——Fine Tuning神经网络</title>
    <link href="noahsnail.com/2017/06/07/2017-6-7-keras%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%9B%9B)%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9CFine%20Tuning/"/>
    <id>noahsnail.com/2017/06/07/2017-6-7-keras的基本用法(四)——网络Fine Tuning/</id>
    <published>2017-06-07T01:53:09.000Z</published>
    <updated>2017-06-12T08:23:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要介绍Keras的一些基本用法，主要涉及已有网络的fine tuning，以ResNet50为例。</p>
<ul>
<li>Demo</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">from keras.models import Model</div><div class="line">from keras.layers import Dense</div><div class="line">from keras.applications.resnet50 import ResNet50</div><div class="line">from keras.preprocessing.image import ImageDataGenerator</div><div class="line"></div><div class="line"># 训练的batch_size</div><div class="line">batch_size = 16</div><div class="line"># 训练的epoch</div><div class="line">epochs = 100</div><div class="line"></div><div class="line"># 图像Generator，用来构建输入数据</div><div class="line">train_datagen = ImageDataGenerator(</div><div class="line">        width_shift_range=0.1,</div><div class="line">        height_shift_range=0.1,</div><div class="line">        zoom_range=0.2,</div><div class="line">        horizontal_flip=True)</div><div class="line"></div><div class="line"># 从文件中读取数据，目录结构应为train下面是各个类别的子目录，每个子目录中为对应类别的图像</div><div class="line">train_generator = train_datagen.flow_from_directory(&apos;./train&apos;, target_size = (224, 224), batch_size = batch_size)</div><div class="line"></div><div class="line"># 输出类别信息</div><div class="line">print train_generator.class_indices</div><div class="line"></div><div class="line"># 生成测试数据</div><div class="line">test_datagen = ImageDataGenerator()</div><div class="line">validation_generator = test_datagen.flow_from_directory(&apos;./validation&apos;, target_size = (224, 224), batch_size = batch_size)</div><div class="line"></div><div class="line"># 使用ResNet的结构，不包括最后一层，且加载ImageNet的预训练参数</div><div class="line">base_model = ResNet50(weights = &apos;imagenet&apos;, include_top = False, pooling = &apos;avg&apos;)</div><div class="line"></div><div class="line"># 构建网络的最后一层，3是自己的数据的类别</div><div class="line">predictions = Dense(3, activation=&apos;softmax&apos;)(base_model.output)</div><div class="line"></div><div class="line"># 定义整个模型</div><div class="line">model = Model(inputs=base_model.input, outputs=predictions)</div><div class="line"></div><div class="line"># 编译模型，loss为交叉熵损失</div><div class="line">model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;categorical_crossentropy&apos;)</div><div class="line"></div><div class="line"># 训练模型</div><div class="line">model.fit_generator(train_generator,steps_per_epoch = batch_size, epochs = epochs, validation_data = validation_generator, validation_steps = batch_size)</div><div class="line"></div><div class="line"># 保存训练得到的模型</div><div class="line">model.save_weights(&apos;weights.h5&apos;)</div></pre></td></tr></table></figure>
<ul>
<li>部分结果</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">&#123;&apos;Type_3&apos;: 2, &apos;Type_2&apos;: 1, &apos;Type_1&apos;: 0&#125;</div><div class="line">Found 761 images belonging to 3 classes.</div><div class="line">Epoch 1/40</div><div class="line"> 1/16 [&gt;.............................] - ETA: 119s - loss: 1.33922017-06-07 10:18:48.246289: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2521 get requests, put_count=2161 evicted_count=1000 eviction_rate=0.462749 and unsatisfied allocation rate=0.579135</div><div class="line">2017-06-07 10:18:48.246348: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110</div><div class="line">16/16 [==============================] - 120s - loss: 2.3753 - val_loss: 10.8293</div><div class="line">Epoch 2/40</div><div class="line"> 1/16 [&gt;.............................] - ETA: 5s - loss: 1.00542017-06-07 10:20:40.464589: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2270 get requests, put_count=2642 evicted_count=1000 eviction_rate=0.378501 and unsatisfied allocation rate=0.286784</div><div class="line">2017-06-07 10:20:40.464643: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281</div><div class="line">16/16 [==============================] - 83s - loss: 1.7988 - val_loss: 11.5219</div><div class="line">Epoch 3/40</div><div class="line">16/16 [==============================] - 81s - loss: 1.6640 - val_loss: 11.0043</div><div class="line">Epoch 4/40</div><div class="line"> 3/16 [====&gt;.........................] - ETA: 4s - loss: 1.87452017-06-07 10:23:26.725923: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 11057 get requests, put_count=11071 evicted_count=1000 eviction_rate=0.0903261 and unsatisfied allocation rate=0.0945103</div><div class="line">2017-06-07 10:23:26.725986: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720</div><div class="line">16/16 [==============================] - 83s - loss: 1.7237 - val_loss: 11.7738</div><div class="line">Epoch 5/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.6304 - val_loss: 10.6538</div><div class="line">Epoch 6/40</div><div class="line">16/16 [==============================] - 80s - loss: 1.2182 - val_loss: 4.5027</div><div class="line">Epoch 7/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.3179 - val_loss: 11.5891</div><div class="line">Epoch 8/40</div><div class="line">16/16 [==============================] - 82s - loss: 1.1806 - val_loss: 10.5800</div><div class="line">Epoch 9/40</div><div class="line">16/16 [==============================] - 81s - loss: 1.1935 - val_loss: 11.1477</div><div class="line">Epoch 10/40</div><div class="line">16/16 [==============================] - 80s - loss: 1.1727 - val_loss: 7.0913</div><div class="line">Epoch 11/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.2058 - val_loss: 6.4474</div><div class="line">Epoch 12/40</div><div class="line">16/16 [==============================] - 82s - loss: 1.2702 - val_loss: 7.7678</div><div class="line">Epoch 13/40</div><div class="line">16/16 [==============================] - 84s - loss: 1.2060 - val_loss: 7.9961</div><div class="line">Epoch 14/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.0768 - val_loss: 11.2121</div><div class="line">Epoch 15/40</div><div class="line">16/16 [==============================] - 80s - loss: 1.1401 - val_loss: 13.2052</div><div class="line">Epoch 16/40</div><div class="line">16/16 [==============================] - 83s - loss: 1.1961 - val_loss: 13.0330</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      keras的基本用法(四)——Fine Tuning神经网络
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>pandas总结(一)——Series的使用</title>
    <link href="noahsnail.com/2017/06/06/2017-6-6-pandas%E6%80%BB%E7%BB%93(%E4%B8%80)%E2%80%94%E2%80%94Series%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>noahsnail.com/2017/06/06/2017-6-6-pandas总结(一)——Series的使用/</id>
    <published>2017-06-06T11:33:34.000Z</published>
    <updated>2017-06-06T11:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># pandas是一个用来进行数据分析的基于numpy的库</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Series是一个一维的数据结构</span></div><div class="line"></div><div class="line"><span class="comment"># 用list构建Series</span></div><div class="line">series1 = pd.Series([<span class="number">3</span>, <span class="number">5</span>, <span class="string">'test'</span>, <span class="number">-5</span>, <span class="number">0.3</span>])</div><div class="line"><span class="keyword">print</span> series1</div></pre></td></tr></table></figure>
<pre><code>0       3
1       5
2    test
3      -5
4     0.3
dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 用list, index构建Series</span></div><div class="line">series2 = pd.Series([<span class="number">3</span>, <span class="number">5</span>, <span class="string">'test'</span>, <span class="number">-5</span>, <span class="number">0.3</span>], index = [<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>])</div><div class="line"><span class="keyword">print</span> series2</div></pre></td></tr></table></figure>
<pre><code>A       3
B       5
C    test
D      -5
E     0.3
dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 通过dict构建Series</span></div><div class="line">companies = &#123;<span class="string">'Baidu'</span>: <span class="number">400</span>, <span class="string">'Alibaba'</span>: <span class="number">500</span>, <span class="string">'Tecent'</span>: <span class="number">600</span>, <span class="string">'Jingdong'</span>: <span class="number">300</span>&#125;</div><div class="line">series3 = pd.Series(companies)</div><div class="line"><span class="keyword">print</span> series3</div></pre></td></tr></table></figure>
<pre><code>Alibaba     500
Baidu       400
Jingdong    300
Tecent      600
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Series数据选择</span></div><div class="line"></div><div class="line"><span class="comment"># 通过index选择数据</span></div><div class="line"><span class="keyword">print</span> series3[<span class="string">'Baidu'</span>]</div><div class="line"></div><div class="line"><span class="comment"># 选择多个数据</span></div><div class="line"><span class="keyword">print</span> series3[[<span class="string">'Baidu'</span>, <span class="string">'Tecent'</span>]]</div></pre></td></tr></table></figure>
<pre><code>400
Baidu     400
Tecent    600
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 根据条件选择数据</span></div><div class="line"><span class="keyword">print</span> series3[series3 &lt; <span class="number">500</span>]</div></pre></td></tr></table></figure>
<pre><code>Baidu       400
Jingdong    300
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># 条件选择原理</span></div><div class="line"><span class="keyword">print</span> series3 &lt; <span class="number">500</span></div><div class="line">temp = series3 &lt; <span class="number">500</span></div><div class="line"><span class="keyword">print</span> series3[temp]</div></pre></td></tr></table></figure>
<pre><code>Alibaba     False
Baidu        True
Jingdong     True
Tecent      False
dtype: bool
Baidu       400
Jingdong    300
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Series元素赋值</span></div><div class="line"><span class="keyword">print</span> <span class="string">'old value: '</span>, series3[<span class="string">'Baidu'</span>]</div><div class="line">series3[<span class="string">'Baidu'</span>] = <span class="number">450</span></div><div class="line"><span class="keyword">print</span> <span class="string">'new value: '</span>, series3[<span class="string">'Baidu'</span>]</div></pre></td></tr></table></figure>
<pre><code>old value:  400
new value:  450
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 根据条件赋值</span></div><div class="line"><span class="keyword">print</span> <span class="string">'old series: '</span></div><div class="line"><span class="keyword">print</span> series3</div><div class="line">series3[series3 &lt; <span class="number">500</span>] = <span class="number">500</span></div><div class="line"><span class="keyword">print</span> <span class="string">'new series: '</span></div><div class="line"><span class="keyword">print</span> series3</div></pre></td></tr></table></figure>
<pre><code>old series: 
Alibaba     500
Baidu       400
Jingdong    300
Tecent      600
dtype: int64
new series: 
Alibaba     500
Baidu       500
Jingdong    500
Tecent      600
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Series数学运算</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Division: '</span></div><div class="line"><span class="keyword">print</span> series3 / <span class="number">2</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Square: '</span></div><div class="line"><span class="keyword">print</span> series3 ** <span class="number">2</span></div><div class="line"><span class="keyword">print</span> np.square(series3)</div></pre></td></tr></table></figure>
<pre><code>Division: 
Alibaba     250.0
Baidu       250.0
Jingdong    250.0
Tecent      300.0
dtype: float64
Square: 
Alibaba     250000
Baidu       250000
Jingdong    250000
Tecent      360000
dtype: int64
Alibaba     250000
Baidu       250000
Jingdong    250000
Tecent      360000
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义新的Series, 公司人数</span></div><div class="line">people = &#123;<span class="string">'Baidu'</span>: <span class="number">50000</span>, <span class="string">'Alibaba'</span>: <span class="number">45000</span>, <span class="string">'Tecent'</span>: <span class="number">60000</span>, <span class="string">'Jingdong'</span>: <span class="number">80000</span>, <span class="string">'Netease'</span>: <span class="number">30000</span>&#125;</div><div class="line">series4 = pd.Series(people)</div><div class="line"><span class="keyword">print</span> series4</div></pre></td></tr></table></figure>
<pre><code>Alibaba     45000
Baidu       50000
Jingdong    80000
Netease     30000
Tecent      60000
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Series相加, series3没有Netease, 因此结果为NaN</span></div><div class="line"><span class="keyword">print</span> series3 + series4</div></pre></td></tr></table></figure>
<pre><code>Alibaba     45500.0
Baidu       50500.0
Jingdong    80500.0
Netease         NaN
Tecent      60600.0
dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 判断数据是否数据缺失</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Netease'</span> <span class="keyword">in</span> series3</div><div class="line"><span class="keyword">print</span> <span class="string">'Baidu'</span> <span class="keyword">in</span> series3</div></pre></td></tr></table></figure>
<pre><code>False
True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 找出数据为null或非null的元素</span></div><div class="line">result = series3 + series4</div><div class="line"><span class="keyword">print</span> result.notnull()</div><div class="line"><span class="keyword">print</span> result.isnull()</div><div class="line"></div><div class="line"><span class="keyword">print</span> result[result.isnull()]</div><div class="line"><span class="keyword">print</span> result[result.isnull() != <span class="keyword">True</span>]</div></pre></td></tr></table></figure>
<pre><code>Alibaba      True
Baidu        True
Jingdong     True
Netease     False
Tecent       True
dtype: bool
Alibaba     False
Baidu       False
Jingdong    False
Netease      True
Tecent      False
dtype: bool
Netease   NaN
dtype: float64
Alibaba     45500.0
Baidu       50500.0
Jingdong    80500.0
Tecent      60600.0
dtype: float64
</code></pre>]]></content>
    
    <summary type="html">
    
      pandas总结(一)——Series的使用
    
    </summary>
    
      <category term="tensorflow" scheme="noahsnail.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="noahsnail.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Unix、Posix和标准UniX规范</title>
    <link href="noahsnail.com/2017/06/03/2017-6-3-Unix%E3%80%81Posix%E5%92%8C%E6%A0%87%E5%87%86UniX%E8%A7%84%E8%8C%83/"/>
    <id>noahsnail.com/2017/06/03/2017-6-3-Unix、Posix和标准UniX规范/</id>
    <published>2017-06-03T10:06:44.000Z</published>
    <updated>2017-06-03T10:14:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>20世纪60年代是大型、复杂操作系统盛行的年代，比如IBM的OS/360和Honeywell的Multics系统。OS/360是历史上最成功的软件项目之一，而 Multics虽然持续存在了多年，却从来没有被广泛应用过。贝尔实验室曾经是Multics项目的最初参与者，但是因为考虑到该项目的复杂性和缺乏进展而于1969年退出。鉴于Mutics项目不愉快的经历，一群贝尔实验室的研究人员Ken Thompson、 Dennis Ritchie、 Doug Mcllroy和 Joe Ossanna，从1969年开始在DEC PDP-7计算机上完全用机器语言编写了一个简单得多的操作系统。这个新系统中的很多思想，比如层次文件系统、作为用户级进程的 shell概念，都是来自于 Multics，只不过在一个更小、更简单的程序包里实现。1970年，Brian Kernighan给新系统命名为“Unix”，这也是一个双关语，暗指“Multics”的复杂性。1973年用C重新编写其内核，1974年，Unix开始正式对外发布。</p>
<p>贝尔实验室以慷慨的条件向学校提供源代码，所以Unix在大专院校里获得了很多支持并得以持续发展。最有影响的工作发生在20世纪70年代晚期到80年代早期，在美国加州大学伯克利分校，研究人员在一系列发布版本中增加了虚拟内存和Internet协议，称为Unix4.xBSD(Berkeley Software Distribution)。与此同时，贝尔实验室也在发布自己的版本，称为System V Unix。其他厂商的版本，比如Sun Microsystems的Solaris系统,则是从这些原始的BSD和System V版本中衍生而来。</p>
<p>20世纪80年代中期，Unix厂商试图通过加入新的、往往不兼容的特性来使它们的程序与众不同，麻烦也就随之而来了。为了阻止这种趋势，IEEE(电气和电子工程师协会)开始努力标准化Unix的开发，后来由 Richard Stallman命名为“Posix”。结果就得到了一系列的标准，称作Posix标准。这套标准涵盖了很多方面，比如Unix系统调用的C语言接口、shell程序和工具、线程及网络编程。最近，一个被称为“标准Unix规范”的独立标准化工作已经与Posix一起创建了统一的Unix系统标准。这些标准化工作的结果是Unix版本之间的差异已经基本消失。</p>
<p>参考资料：</p>
<ol>
<li>深度理解计算机系统（P11）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Unix、Posix和标准UniX规范
    
    </summary>
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/categories/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>GNU项目</title>
    <link href="noahsnail.com/2017/06/03/2017-6-3-GNU%E9%A1%B9%E7%9B%AE/"/>
    <id>noahsnail.com/2017/06/03/2017-6-3-GNU项目/</id>
    <published>2017-06-03T09:57:06.000Z</published>
    <updated>2017-06-03T10:03:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>GCC是GNU(GNU是GNU’s Not Unix的缩写)项目开发出来的众多有用工具之。GNU项目是1984年由Richard Stallman发起的一个免税的慈善项目。该项目的目标非常宏大，就是开发出一个完整的类Unix的系统，其源代码能够不受限制地被修改和传播。GNU项目已经开发出了一个包含Unix操作系统的所有主要部件的环境，但内核除外，内核是由 Linux项目独立发展而来的。GNU环境包括 EMACS编辑器、GCC编译器、GDB调试器、汇编器、链接器、处理二进制文件的工具以及其他一些部件。GCC编译器已经发展到支持许多不同的语言，能够为许多不同的机器生成代码。支持的语言包括C、C++、 Fortran、Java、Pascal、面向对象C语言(Objective-C)和Ada。</p>
<p>GNU项目取得了非凡的成绩，但是却常常被忽略。现代开放源码运动(通常和Linux联系在一起)的思想起源是GNU项目中自由软件(free software)的概念。(此处的free为自由言论(free speech)中的“自由”之意，而非免费啤酒(free beer)中的“免费”之意。) 而且，Linux如此受欢迎在很大程度上还要归功于GNU工具，它们给Linux内核提供了环境。</p>
<p>参考资料：</p>
<ol>
<li>深度理解计算机系统（P4）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      GNU项目
    
    </summary>
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/categories/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>C语言的起源</title>
    <link href="noahsnail.com/2017/06/03/2017-6-3-C%E8%AF%AD%E8%A8%80%E7%9A%84%E8%B5%B7%E6%BA%90/"/>
    <id>noahsnail.com/2017/06/03/2017-6-3-C语言的起源/</id>
    <published>2017-06-03T09:43:06.000Z</published>
    <updated>2017-06-03T09:54:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>C语言是贝尔实验室的Dennis Ritchie于1969年~1973年间创建的。美国国家标准学会（American national standards institute，ANSI）在1989年颁布了ANSI C的标准，后来语言的标准化成了国际标准化组织（International Standards Organization，ISO)的责任。这些标准定义了C语言和一系列函数库,即所谓的<strong>C标准库</strong>。Kernighan和 Ritchie在他们的经典著作中描述了ANSI C，这本著作被人们满怀感情地称为“K8R”。用Ritchie的话来说，C语言是“古怪的、有缺陷的，但同时也是一个巨大的成功”。为什么会成功呢?</p>
<ul>
<li><p>C语言与Unⅸ操作系统关系密切。<br>C从一开始就是作为一种用于Unix系统的程府语言开发出来的。大部分Unix内核(操作系统的核心部分)，以及所有支撑工具和函数库都是用C语言编写的。20世纪70年代后期到80年代初期，Unix风行于高等院校，许多人开始接触C语言并喜欢上它。因为Unix几乎全部是用C编写的，它可以很方便地移植到新的机器上，这种特点为C和Unix赢得了更为广泛的支持。</p>
</li>
<li><p>C语言小而简单。<br>C语言的设计是由一个人而非一个协会掌控的，因此这是一个简洁明了、没有什么冗赘的设计。K&amp;R这本书用大量的例子和练习描述了完整的C语言及其标准库，而全书不过261页。C语言的简单使它相对而言易于学习，也易于移植到不同的计算机上。</p>
</li>
<li><p>C语言是为实践目的设计的。C语言是设计用来实现Unix操作系统的。后来其他人发现能够用这门语言无障碍地编写他们想要的程序。</p>
</li>
</ul>
<p>C语言是系统级编程的首选，同时它也非常适用于应用级程序的编写。然而，它也并非适用于所有的程序员和所有的情况。C语言的指针是造成程序员困惑和程序错误的一个常见原因。同时，C语言还缺乏对非常有用的抽象的显式支持，例如类、对象和异常，像C++和Java这样针对应用级程序的新程序语言解决了这些问题。</p>
<p>参考资料：</p>
<ol>
<li>深度理解计算机系统（P2，3）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      C语言的起源
    
    </summary>
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/categories/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="深入理解计算机系统" scheme="noahsnail.com/tags/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Caffe神经网络结构汇总</title>
    <link href="noahsnail.com/2017/06/01/2017-6-1-Caffe%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E6%80%BB%E7%BB%93/"/>
    <id>noahsnail.com/2017/06/01/2017-6-1-Caffe网络结构总结/</id>
    <published>2017-06-01T07:16:35.000Z</published>
    <updated>2017-06-01T07:56:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>自2012年Alexnet赢得了ImageNet竞赛以来，深度学习（神经网络）得到了飞速发展，产生了许多的神经网络结构，本文主要总结Caffe中使用的神经网络(分类的神经网络)，本文的神经网络作者都使用Caffe训练过，并在Kaggle的Intel癌症预测比赛中进行了测试与使用（top 8%）。</p>
<h2 id="1-Alexnet"><a href="#1-Alexnet" class="headerlink" title="1. Alexnet"></a>1. Alexnet</h2><p>Alexnet，2012年ImageNet竞赛冠军，深度学习的里程碑。</p>
<p>网络结构地址：<a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet" target="_blank" rel="external">https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet</a></p>
<p>预训练模型地址：<a href="http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel" target="_blank" rel="external">http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel</a></p>
<h2 id="2-Squeezenet"><a href="#2-Squeezenet" class="headerlink" title="2. Squeezenet"></a>2. Squeezenet</h2><p>Squeezenet设计目标不是为了提高识别的准确率，而是希望简化网络复杂度。squeezenet的模型结构确实很小，没压缩的情况下才5M左右，而且识别的精度还可以。</p>
<p>网络结构地址：<a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="external">https://github.com/DeepScale/SqueezeNet</a></p>
<p>预训练模型地址：<a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="external">https://github.com/DeepScale/SqueezeNet</a></p>
<h2 id="3-VGG系列"><a href="#3-VGG系列" class="headerlink" title="3. VGG系列"></a>3. VGG系列</h2><p>VGG和GoogLenet是2014年imagenet竞赛的双雄，VGG主要分为VGG16和VGG19。其网络结构与预训练模型的地址如下：</p>
<p>VGG16的网络结构：<a href="https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md" target="_blank" rel="external">https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md</a></p>
<p>VGG16的预训练模型：<a href="http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel" target="_blank" rel="external"> http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel</a></p>
<p>VGG19的网络结构：<a href="https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md" target="_blank" rel="external">https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md</a></p>
<p>VGG19的预训练模型：<a href="http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel" target="_blank" rel="external">http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel</a></p>
<p>备注：上面的网络结构需要进行细微调整才能在Caffe中直接训练，主要是网络结构中的Type类型。</p>
<h2 id="4-Resnet系列"><a href="#4-Resnet系列" class="headerlink" title="4. Resnet系列"></a>4. Resnet系列</h2><p>Resnet网络，2015年ImageNet竞赛冠军，网络结构主要分为Resnet-50、Resnet-101、Resnet-152三种，当然也有一些其它的结构，例如Resnet-18，Resnet-14。</p>
<p>Github地址：<a href="https://github.com/KaimingHe/deep-residual-networks" target="_blank" rel="external">https://github.com/KaimingHe/deep-residual-networks</a></p>
<p>Resnet-50、Resnet-101、Resnet-152的网络结构及预训练模型的下载地址：<a href="https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&amp;id=4006CBB8476FF777%2117887&amp;cid=4006CBB8476FF777" target="_blank" rel="external">https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&amp;id=4006CBB8476FF777%2117887&amp;cid=4006CBB8476FF777</a></p>
<h2 id="5-Inception系列"><a href="#5-Inception系列" class="headerlink" title="5. Inception系列"></a>5. Inception系列</h2><p>Inception系列是Google发明的一系列神经网络结构。</p>
<p>Inception-v1：</p>
<p>Inception-v1，即大名鼎鼎的GoogLenet，2014年ImageNet竞赛冠军。</p>
<p>网络结构地址：<a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet" target="_blank" rel="external">https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet</a></p>
<p>预训练模型地址：<a href="http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel" target="_blank" rel="external">http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel</a></p>
<p>Inception-v2：</p>
<p>即Inception V1 + Batch Normalization。</p>
<p>网络结构地址：<a href="https://github.com/pertusa/InceptionBN-21K-for-Caffe" target="_blank" rel="external">https://github.com/pertusa/InceptionBN-21K-for-Caffe</a></p>
<p>预训练模型地址：<a href="http://www.dlsi.ua.es/~pertusa/deep/Inception21k.caffemodel" target="_blank" rel="external">http://www.dlsi.ua.es/~pertusa/deep/Inception21k.caffemodel</a></p>
<p>Inception-v3：</p>
<p>网络结构地址：<a href="https://pan.baidu.com/s/1boC0HEf#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1boC0HEf#list/path=%2F</a></p>
<p>预训练模型地址：<a href="https://pan.baidu.com/s/1boC0HEf#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1boC0HEf#list/path=%2F</a></p>
<p>Inception-v4：</p>
<p>网络结构地址：<a href="https://pan.baidu.com/s/1c6D150#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1c6D150#list/path=%2F</a></p>
<p>预训练模型地址：<a href="https://pan.baidu.com/s/1c6D150#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1c6D150#list/path=%2F</a></p>
<p>Inception-resnet-v2：</p>
<p>网络结构地址：<a href="https://pan.baidu.com/s/1jHPJCX4#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1jHPJCX4#list/path=%2F</a></p>
<p>预训练模型地址：<a href="https://pan.baidu.com/s/1jHPJCX4#list/path=%2F" target="_blank" rel="external">https://pan.baidu.com/s/1jHPJCX4#list/path=%2F</a></p>
]]></content>
    
    <summary type="html">
    
      Caffe神经网络结构汇总
    
    </summary>
    
      <category term="Caffe" scheme="noahsnail.com/categories/Caffe/"/>
    
    
      <category term="Caffe" scheme="noahsnail.com/tags/Caffe/"/>
    
  </entry>
  
  <entry>
    <title>Pandas应用总结</title>
    <link href="noahsnail.com/2017/05/27/2017-5-27-Pandas%E5%BA%94%E7%94%A8%E6%80%BB%E7%BB%93/"/>
    <id>noahsnail.com/2017/05/27/2017-5-27-Pandas应用总结/</id>
    <published>2017-05-27T10:33:49.000Z</published>
    <updated>2017-05-27T10:39:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Pandas是一个基于numpy的、用来进行数据分析的库。</p>
<h2 id="1-Series的应用"><a href="#1-Series的应用" class="headerlink" title="1. Series的应用"></a>1. Series的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># Series是一个一维的数据结构</div><div class="line"></div><div class="line"># 用list构建Series</div><div class="line">s = pd.Series([3, 5, &apos;test&apos;, -5, 0.3])</div><div class="line">print s</div><div class="line"></div><div class="line">0       3</div><div class="line">1       5</div><div class="line">2    test</div><div class="line">3      -5</div><div class="line">4     0.3</div><div class="line">dtype: object</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      Pandas应用总结
    
    </summary>
    
      <category term="Machine Learning" scheme="noahsnail.com/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="noahsnail.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux下启动和停止apache服务</title>
    <link href="noahsnail.com/2017/05/27/2017-5-27-Linux%E4%B8%8B%E5%90%AF%E5%8A%A8%E5%92%8C%E5%81%9C%E6%AD%A2apache%E6%9C%8D%E5%8A%A1/"/>
    <id>noahsnail.com/2017/05/27/2017-5-27-Linux下启动和停止apache服务/</id>
    <published>2017-05-27T06:17:50.000Z</published>
    <updated>2017-05-27T10:27:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文使用的Linux系统为CentOS 7，下面将介绍apache服务的启动、关闭与设置。apache在CentOS 7中一般是默认安装的，而且服务名字为<code>httpd</code>。</p>
<h2 id="1-安装apache及查看相关配置"><a href="#1-安装apache及查看相关配置" class="headerlink" title="1. 安装apache及查看相关配置"></a>1. 安装apache及查看相关配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># apache安装命令</div><div class="line"></div><div class="line">$ sudo yum install httpd</div><div class="line">Loaded plugins: fastestmirror, langpacks</div><div class="line">Loading mirror speeds from cached hostfile</div><div class="line">Package httpd-2.4.6-45.el7.centos.4.x86_64 already installed and latest version</div><div class="line">Nothing to do</div><div class="line"></div><div class="line"></div><div class="line"># 查看apache文件的位置</div><div class="line"></div><div class="line">$ sudo find / -name httpd</div><div class="line">/run/httpd</div><div class="line">/etc/logrotate.d/httpd</div><div class="line">/etc/sysconfig/httpd</div><div class="line">/etc/httpd</div><div class="line">/var/log/httpd</div><div class="line">/var/cache/httpd</div><div class="line">/usr/sbin/httpd</div><div class="line">/usr/lib64/httpd</div><div class="line">/usr/share/httpd</div><div class="line">/usr/include/httpd</div><div class="line">/usr/libexec/initscripts/legacy-actions/httpd</div></pre></td></tr></table></figure>
<p>Apache配置文件位于<code>/etc/httpd/conf</code>，主要的配置文件是<code>/etc/httpd/conf/httpd.conf</code>, apache相关的配置信息都可以在这个文件中看到。</p>
<h2 id="2-apache服务的启动与关闭"><a href="#2-apache服务的启动与关闭" class="headerlink" title="2. apache服务的启动与关闭"></a>2. apache服务的启动与关闭</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 启动服务</div><div class="line">$ sudo systemctl start httpd.service</div><div class="line"></div><div class="line"># 查看服务</div><div class="line">$ ps aux | grep httpd</div><div class="line">root     *  0.3  0.0 220444  4956 ?        Ss   15:46   0:00 /usr/sbin/httpd -DFOREGROUND</div><div class="line">apache   *  0.0  0.0 220444  2492 ?        S    15:46   0:00 /usr/sbin/httpd -DFOREGROUND</div><div class="line">apache   *  0.0  0.0 220444  2488 ?        S    15:46   0:00 /usr/sbin/httpd -DFOREGROUND</div><div class="line">apache   *  0.0  0.0 220444  2488 ?        S    15:46   0:00 /usr/sbin/httpd -DFOREGROUND</div><div class="line">apache   *  0.0  0.0 220444  2488 ?        S    15:46   0:00 /usr/sbin/httpd -DFOREGROUND</div><div class="line">apache   *  0.0  0.0 220444  2488 ?        S    15:46   0:00 /usr/sbin/httpd -DFOREGROUND</div><div class="line"></div><div class="line"># 停止服务</div><div class="line">$ sudo systemctl stop httpd.service</div><div class="line"></div><div class="line"># 重启服务</div><div class="line">$ sudo systemctl restart httpd.service</div></pre></td></tr></table></figure>
<p>启动服务后，可以在外网通过服务器的IP地址访问。可以看到如下界面：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/apache.png" alt="image"></p>
<h2 id="3-配置自己可以在外部访问的内容"><a href="#3-配置自己可以在外部访问的内容" class="headerlink" title="3. 配置自己可以在外部访问的内容"></a>3. 配置自己可以在外部访问的内容</h2><p>可以在<code>/var/www/html</code>下创建一个软链接，链接到你想要在外部访问的内容，同时要修改要访问目录的权限。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo ln -s images your_directory</div><div class="line">$ sudo chmod 755 your_directory</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      Linux下启动和停止apache服务
    
    </summary>
    
      <category term="Linux" scheme="noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux的find命令</title>
    <link href="noahsnail.com/2017/05/26/2017-5-26-Linux%E7%9A%84find%E5%91%BD%E4%BB%A4/"/>
    <id>noahsnail.com/2017/05/26/2017-5-26-Linux的find命令/</id>
    <published>2017-05-26T05:33:22.000Z</published>
    <updated>2017-05-27T06:17:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Linux的find命令主要用来查找系统中的文件。命令格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">find命令的格式： find  [-path ..] -options [-print -exec -ok]</div><div class="line">path：要查找的目录路径。</div><div class="line">~ 表示$HOME目录</div><div class="line">. 表示当前目录</div><div class="line">/ 表示根目录</div><div class="line">-print ：表示将结果输出到标准输出</div><div class="line">-exec ：对匹配的文件执行该参数所给出的shell命令。形式为 command  &#123;&#125; \; ，注意&#123;&#125;与\; 之间有空格</div><div class="line">-ok ：与-exec作用相同，区别在于，在执行命令之前，都会给出提示，让用户确认是否执行</div><div class="line"></div><div class="line">options常用的有下选项：</div><div class="line">-name 按照名字查找</div><div class="line">-perm 安装权限查找</div><div class="line">-prune 不再当前指定的目录下查找</div><div class="line">-user 文件属主来查找</div><div class="line">-group 所属组来查找</div><div class="line">-nogroup 查找无有效所属组的文件</div><div class="line">-nouser 查找无有效属主的文件</div><div class="line">-type 按照文件类型查找</div></pre></td></tr></table></figure>
<ul>
<li>Demo</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"># 当前路径下查找名为source_code的文件或文件夹（递归子文件夹）</div><div class="line">$ find . -name source_code</div><div class="line">./source_code</div><div class="line"></div><div class="line"># 查找当前路径下的txt文件</div><div class="line">$ find . -name &quot;*.txt&quot;</div><div class="line">./crontab.txt</div><div class="line"></div><div class="line"># 根据文件权限查找，查找权限为777的文件</div><div class="line">$ find . -perm 777</div><div class="line">./test.sh</div><div class="line"></div><div class="line"># 根据类型查找，查找符号链接</div><div class="line">$ find . -type l</div><div class="line">./Chapter14/multem</div><div class="line"></div><div class="line"># 根据修改时间查找</div><div class="line">$ find . -mtime 7 -type f</div><div class="line">./model-zoo/.DS_Store</div><div class="line">./model-zoo/vgg19/train_val.prototxt</div><div class="line">./model-zoo/vgg19/VGG_ILSVRC_19_layers.caffemodel</div><div class="line"></div><div class="line"># 根据文件大小查找</div><div class="line">$ find . -size +1k -type f</div><div class="line">./.DS_Store</div><div class="line">./Chapter14/.DS_Store</div><div class="line">./Chapter15/.DS_Store</div><div class="line">./Chapter16/.DS_Store</div><div class="line">./Chapter17/.DS_Store</div><div class="line">./filename.txt</div><div class="line"></div><div class="line"># 查找文件并删除</div><div class="line">$ find . -size +1k -type f -ok rm &#123;&#125; \;</div><div class="line">&quot;rm ./.DS_Store&quot;? y</div><div class="line">&quot;rm ./Chapter14/.DS_Store&quot;? y</div><div class="line">&quot;rm ./Chapter15/.DS_Store&quot;? y</div><div class="line">&quot;rm ./Chapter16/.DS_Store&quot;? y</div><div class="line">&quot;rm ./Chapter17/.DS_Store&quot;? y</div><div class="line">&quot;rm ./filename.txt&quot;? n</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Linux的find命令
    
    </summary>
    
      <category term="Linux" scheme="noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Centos 7下Docker的安装与非root配置</title>
    <link href="noahsnail.com/2017/05/22/2017-5-22-Centos%207%E4%B8%8BDocker%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>noahsnail.com/2017/05/22/2017-5-22-Centos 7下Docker的安装与配置/</id>
    <published>2017-05-22T05:03:30.000Z</published>
    <updated>2017-05-22T05:21:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-CentOS-7下docker的安装"><a href="#1-CentOS-7下docker的安装" class="headerlink" title="1. CentOS 7下docker的安装"></a>1. CentOS 7下docker的安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. install yum-utils</span></div><div class="line">$ sudo yum install -y yum-utils</div><div class="line">Loaded plugins: fastestmirror, langpacks</div><div class="line">base                                                                                          | 3.6 kB  00:00:00</div><div class="line">epel                                                                                          | 4.3 kB  00:00:00</div><div class="line">extras                                                                                        | 3.4 kB  00:00:00</div><div class="line">update                                                                                        | 3.4 kB  00:00:00</div><div class="line">(1/3): epel/7/x86_64/updateinfo                                                               | 797 kB  00:00:00</div><div class="line">(2/3): epel/7/x86_64/primary_db                                                               | 4.7 MB  00:00:00</div><div class="line">(3/3): update/7/x86_64/primary_db                                                             | 4.8 MB  00:00:00</div><div class="line">Loading mirror speeds from cached hostfile</div><div class="line">Package yum-utils-1.1.31-40.el7.noarch already installed and latest version</div><div class="line">Nothing to <span class="keyword">do</span></div><div class="line"></div><div class="line"><span class="comment"># 2. set up the stable repository</span></div><div class="line"><span class="variable">$sudo</span> yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</div><div class="line">Loaded plugins: fastestmirror, langpacks</div><div class="line">adding repo from: https://download.docker.com/linux/centos/docker-ce.repo</div><div class="line">grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo</div><div class="line">repo saved to /etc/yum.repos.d/docker-ce.repo</div><div class="line"></div><div class="line"><span class="comment"># 3. install docker</span></div><div class="line">$ sudo yum install docker-ce</div><div class="line">Loaded plugins: fastestmirror, langpacks</div><div class="line">docker-ce-stable                                                                              | 2.9 kB  00:00:00</div><div class="line">docker-ce-stable/x86_64/primary_db                                                            | 4.8 kB  00:00:00</div><div class="line">Loading mirror speeds from cached hostfile</div><div class="line">Resolving Dependencies</div><div class="line">--&gt; Running transaction check</div><div class="line">---&gt; Package docker-ce.x86_64 0:17.03.1.ce-1.el7.centos will be installed</div><div class="line">--&gt; Processing Dependency: docker-ce-selinux &gt;= 17.03.1.ce-1.el7.centos <span class="keyword">for</span> package: docker-ce-17.03.1.ce-1.el7.centos.x86_64</div><div class="line">--&gt; Processing Dependency: libcgroup <span class="keyword">for</span> package: docker-ce-17.03.1.ce-1.el7.centos.x86_64</div><div class="line">--&gt; Processing Dependency: libseccomp.so.2()(64bit) <span class="keyword">for</span> package: docker-ce-17.03.1.ce-1.el7.centos.x86_64</div><div class="line">--&gt; Running transaction check</div><div class="line">---&gt; Package docker-ce-selinux.noarch 0:17.03.1.ce-1.el7.centos will be installed</div><div class="line">--&gt; Processing Dependency: policycoreutils-python <span class="keyword">for</span> package: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch</div><div class="line">---&gt; Package libcgroup.x86_64 0:0.41-11.el7 will be installed</div><div class="line">---&gt; Package libseccomp.x86_64 0:2.3.1-2.el7 will be installed</div><div class="line">--&gt; Running transaction check</div><div class="line">---&gt; Package policycoreutils-python.x86_64 0:2.5-11.el7_3 will be installed</div><div class="line">--&gt; Processing Dependency: setools-libs &gt;= 3.3.8-1 <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: libsemanage-python &gt;= 2.5-5 <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: audit-libs-python &gt;= 2.1.3-4 <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: python-IPy <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: libapol.so.4(VERS_4.0)(64bit) <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: checkpolicy <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: libqpol.so.1()(64bit) <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Processing Dependency: libapol.so.4()(64bit) <span class="keyword">for</span> package: policycoreutils-python-2.5-11.el7_3.x86_64</div><div class="line">--&gt; Running transaction check</div><div class="line">---&gt; Package audit-libs-python.x86_64 0:2.6.5-3.el7_3.1 will be installed</div><div class="line">---&gt; Package checkpolicy.x86_64 0:2.5-4.el7 will be installed</div><div class="line">---&gt; Package libsemanage-python.x86_64 0:2.5-5.1.el7_3 will be installed</div><div class="line">---&gt; Package python-IPy.noarch 0:0.75-6.el7 will be installed</div><div class="line">---&gt; Package setools-libs.x86_64 0:3.3.8-1.1.el7 will be installed</div><div class="line">--&gt; Finished Dependency Resolution</div><div class="line"></div><div class="line">Dependencies Resolved</div><div class="line"></div><div class="line">=====================================================================================================================</div><div class="line"> Package                         Arch            Version                             Repository                 Size</div><div class="line">=====================================================================================================================</div><div class="line">Installing:</div><div class="line"> docker-ce                       x86_64          17.03.1.ce-1.el7.centos             docker-ce-stable           19 M</div><div class="line">Installing <span class="keyword">for</span> dependencies:</div><div class="line"> audit-libs-python               x86_64          2.6.5-3.el7_3.1                     update                     70 k</div><div class="line"> checkpolicy                     x86_64          2.5-4.el7                           base                      290 k</div><div class="line"> docker-ce-selinux               noarch          17.03.1.ce-1.el7.centos             docker-ce-stable           28 k</div><div class="line"> libcgroup                       x86_64          0.41-11.el7                         base                       65 k</div><div class="line"> libseccomp                      x86_64          2.3.1-2.el7                         base                       56 k</div><div class="line"> libsemanage-python              x86_64          2.5-5.1.el7_3                       update                    104 k</div><div class="line"> policycoreutils-python          x86_64          2.5-11.el7_3                        update                    445 k</div><div class="line"> python-IPy                      noarch          0.75-6.el7                          base                       32 k</div><div class="line"> setools-libs                    x86_64          3.3.8-1.1.el7                       base                      612 k</div><div class="line"></div><div class="line">Transaction Summary</div><div class="line">=====================================================================================================================</div><div class="line">Install  1 Package (+9 Dependent packages)</div><div class="line"></div><div class="line">Total download size: 20 M</div><div class="line">Installed size: 24 M</div><div class="line">Is this ok [y/d/N]: y</div><div class="line">Downloading packages:</div><div class="line">(1/10): audit-libs-python-2.6.5-3.el7_3.1.x86_64.rpm                                          |  70 kB  00:00:00</div><div class="line">(2/10): checkpolicy-2.5-4.el7.x86_64.rpm                                                      | 290 kB  00:00:00</div><div class="line">(3/10): libseccomp-2.3.1-2.el7.x86_64.rpm                                                     |  56 kB  00:00:00</div><div class="line">(4/10): libcgroup-0.41-11.el7.x86_64.rpm                                                      |  65 kB  00:00:00</div><div class="line">(5/10): policycoreutils-python-2.5-11.el7_3.x86_64.rpm                                        | 445 kB  00:00:00</div><div class="line">(6/10): setools-libs-3.3.8-1.1.el7.x86_64.rpm                                                 | 612 kB  00:00:00</div><div class="line">(7/10): libsemanage-python-2.5-5.1.el7_3.x86_64.rpm                                           | 104 kB  00:00:00</div><div class="line">(8/10): python-IPy-0.75-6.el7.noarch.rpm                                                      |  32 kB  00:00:00</div><div class="line">warning: /var/cache/yum/x86_64/7/docker-ce-stable/packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm: Header V4 RSA/SHA512 Signature, key ID 621e9f35: NOKEY</div><div class="line">Public key <span class="keyword">for</span> docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm is not installed</div><div class="line">(9/10): docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm                                  |  28 kB  00:00:00</div><div class="line">(10/10): docker-ce-17.03.1.ce-1.el7.centos.x86_64.rpm                                         |  19 MB  00:00:00</div><div class="line">---------------------------------------------------------------------------------------------------------------------</div><div class="line">Total                                                                                 23 MB/s |  20 MB  00:00:00</div><div class="line">Retrieving key from https://download.docker.com/linux/centos/gpg</div><div class="line">Importing GPG key 0x621E9F35:</div><div class="line"> Userid     : <span class="string">"Docker Release (CE rpm) &lt;docker@docker.com&gt;"</span></div><div class="line"> Fingerprint: 060a 61c5 1b55 8a7f 742b 77aa c52f eb6b 621e 9f35</div><div class="line"> From       : https://download.docker.com/linux/centos/gpg</div><div class="line">Is this ok [y/N]: y</div><div class="line">Running transaction check</div><div class="line">Running transaction <span class="built_in">test</span></div><div class="line">Transaction <span class="built_in">test</span> succeeded</div><div class="line">Running transaction</div><div class="line">  Installing : libcgroup-0.41-11.el7.x86_64                                                                     1/10</div><div class="line">  Installing : setools-libs-3.3.8-1.1.el7.x86_64                                                                2/10</div><div class="line">  Installing : checkpolicy-2.5-4.el7.x86_64                                                                     3/10</div><div class="line">  Installing : libsemanage-python-2.5-5.1.el7_3.x86_64                                                          4/10</div><div class="line">  Installing : audit-libs-python-2.6.5-3.el7_3.1.x86_64                                                         5/10</div><div class="line">  Installing : python-IPy-0.75-6.el7.noarch                                                                     6/10</div><div class="line">  Installing : policycoreutils-python-2.5-11.el7_3.x86_64                                                       7/10</div><div class="line">  Installing : docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch                                                 8/10</div><div class="line">setsebool:  SELinux is disabled.</div><div class="line">libsemanage.semanage_direct_install_info: Overriding docker module at lower priority 100 with module at priority 400.</div><div class="line">  Installing : libseccomp-2.3.1-2.el7.x86_64                                                                    9/10</div><div class="line">  Installing : docker-ce-17.03.1.ce-1.el7.centos.x86_64                                                        10/10</div><div class="line">  Verifying  : libseccomp-2.3.1-2.el7.x86_64                                                                    1/10</div><div class="line">  Verifying  : python-IPy-0.75-6.el7.noarch                                                                     2/10</div><div class="line">  Verifying  : audit-libs-python-2.6.5-3.el7_3.1.x86_64                                                         3/10</div><div class="line">  Verifying  : libsemanage-python-2.5-5.1.el7_3.x86_64                                                          4/10</div><div class="line">  Verifying  : docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch                                                 5/10</div><div class="line">  Verifying  : libcgroup-0.41-11.el7.x86_64                                                                     6/10</div><div class="line">  Verifying  : policycoreutils-python-2.5-11.el7_3.x86_64                                                       7/10</div><div class="line">  Verifying  : docker-ce-17.03.1.ce-1.el7.centos.x86_64                                                         8/10</div><div class="line">  Verifying  : checkpolicy-2.5-4.el7.x86_64                                                                     9/10</div><div class="line">  Verifying  : setools-libs-3.3.8-1.1.el7.x86_64                                                               10/10</div><div class="line"></div><div class="line">Installed:</div><div class="line">  docker-ce.x86_64 0:17.03.1.ce-1.el7.centos</div><div class="line"></div><div class="line">Dependency Installed:</div><div class="line">  audit-libs-python.x86_64 0:2.6.5-3.el7_3.1                    checkpolicy.x86_64 0:2.5-4.el7</div><div class="line">  docker-ce-selinux.noarch 0:17.03.1.ce-1.el7.centos            libcgroup.x86_64 0:0.41-11.el7</div><div class="line">  libseccomp.x86_64 0:2.3.1-2.el7                               libsemanage-python.x86_64 0:2.5-5.1.el7_3</div><div class="line">  policycoreutils-python.x86_64 0:2.5-11.el7_3                  python-IPy.noarch 0:0.75-6.el7</div><div class="line">  setools-libs.x86_64 0:3.3.8-1.1.el7</div><div class="line"></div><div class="line">Complete!</div><div class="line"></div><div class="line"><span class="comment"># 4. start docker</span></div><div class="line">$ sudo systemctl start docker</div><div class="line"></div><div class="line"><span class="comment"># 5. verify docker</span></div><div class="line">$ sudo docker run hello-world</div><div class="line">Unable to find image <span class="string">'hello-world:latest'</span> locally</div><div class="line">latest: Pulling from library/hello-world</div><div class="line">78445dd45222: Pull complete</div><div class="line">Digest: sha256:c5515758d4c5e1e838e9<span class="built_in">cd</span>307f6c6a0d620b5e07e6f927b07d05f6d12a1ac8d7</div><div class="line">Status: Downloaded newer image <span class="keyword">for</span> hello-world:latest</div><div class="line"></div><div class="line">Hello from Docker!</div><div class="line">This message shows that your installation appears to be working correctly.</div><div class="line"></div><div class="line">To generate this message, Docker took the following steps:</div><div class="line"> 1. The Docker client contacted the Docker daemon.</div><div class="line"> 2. The Docker daemon pulled the <span class="string">"hello-world"</span> image from the Docker Hub.</div><div class="line"> 3. The Docker daemon created a new container from that image <span class="built_in">which</span> runs the</div><div class="line">    executable that produces the output you are currently reading.</div><div class="line"> 4. The Docker daemon streamed that output to the Docker client, <span class="built_in">which</span> sent it</div><div class="line">    to your terminal.</div><div class="line"></div><div class="line">To try something more ambitious, you can run an Ubuntu container with:</div><div class="line"> $ docker run -it ubuntu bash</div><div class="line"></div><div class="line">Share images, automate workflows, and more with a free Docker ID:</div><div class="line"> https://cloud.docker.com/</div><div class="line"></div><div class="line">For more examples and ideas, visit:</div><div class="line"> https://docs.docker.com/engine/userguide/</div></pre></td></tr></table></figure>
<h2 id="2-docker非root配置"><a href="#2-docker非root配置" class="headerlink" title="2. docker非root配置"></a>2. docker非root配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 查看是否存在docker用户组</div><div class="line">$ cat /etc/group | grep docker</div><div class="line"></div><div class="line"># 如果不存在docker用户组，则创建</div><div class="line">$ sudo groupadd docker</div><div class="line"></div><div class="line"># 添加当前用户到docker组中</div><div class="line">$ sudo gpasswd -a $&#123;USER&#125; docker</div><div class="line"></div><div class="line"># 重启docker</div><div class="line">$ sudo systemctl restart docker</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Centos 7下Docker的安装与非root配置
    
    </summary>
    
      <category term="Docker学习笔记" scheme="noahsnail.com/categories/Docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Docker" scheme="noahsnail.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Linux的/bin、/sbin、/usr/sbin、/usr/bin、/usr/local/bin、/usr/local/sbin</title>
    <link href="noahsnail.com/2017/05/22/2017-5-22-Linux%E7%9A%84:bin%E3%80%81:sbin%E3%80%81:usr:sbin%E3%80%81:usr:bin/"/>
    <id>noahsnail.com/2017/05/22/2017-5-22-Linux的:bin、:sbin、:usr:sbin、:usr:bin/</id>
    <published>2017-05-22T01:51:31.000Z</published>
    <updated>2017-05-26T05:33:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>可以参考<a href="https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard" target="_blank" rel="external">https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard</a></p>
<p>1、Linux的/bin、/sbin、/usr/sbin、/usr/bin、/usr/local/bin、/usr/local/sbin</p>
<ul>
<li><p>/bin<br>bin为binary的简写，包含基本的用户命令，可被所有用户使用。包含能够同时被用户和系统管理员使用的命令（二进制程序），并且可以在不挂载任何其它文件系统的情况下使用。</p>
</li>
<li><p>/sbin<br>存放系统管理员以及其他需要root权限来运行的工具。同类型的工具同时也可以保存在<code>/usr/bin</code>、<code>/usr/local/sbin</code>。但是<code>/sbin</code>中保存的工具是在系统启动、复原、恢复和修复的过程中，作为<code>/bin</code>中工具的补充。</p>
</li>
</ul>
<ul>
<li><p>/usr/bin<br>非基本的命令二进制文件，所有用户可用。主要放置一些应用软件工具的必备执行档。</p>
</li>
<li><p>/usr/sbin<br>存放了系统管理员使用的、对于boot启动时非必须的二进制程序文件。在<code>/usr</code>确保被挂载的情况下，运行的管理员程序一般存放在<code>/usr/sbin</code>中，在本地安装的管理员程序则应当被存放在<code>/usr/local/sbin</code>中。</p>
</li>
<li><p>/usr/local/bin<br>本地站点用户使用的二进制程序文件。</p>
</li>
<li><p>/usr/local/sbin<br>本地站点管理员使用的二进制程序文件。</p>
</li>
</ul>
<p>总结：如果是用户和管理员必备的二进制文件，通常放在/bin。如果是系统管理员必备，但是一般用户根本不会用到的二进制文件，通常放在/sbin。相对而言，如果不是用户必备的二进制文件，通常会放在/usr/bin；如果不是系统管理员必备的工具，通常会放在/usr/sbin。</p>
<h2 id="2-Linux中的目录介绍"><a href="#2-Linux中的目录介绍" class="headerlink" title="2. Linux中的目录介绍"></a>2. Linux中的目录介绍</h2><p>•主目录：/root、/home/username<br>•用户可执行文件：/bin、/usr/bin、/usr/local/bin<br>•系统可执行文件：/sbin、/usr/sbin、/usr/local/sbin<br>•其他挂载点：/media、/mnt<br>•配置：/etc<br>•临时文件：/tmp<br>•内核和Bootloader：/boot<br>•服务器数据：/var、/srv<br>•系统信息：/proc、/sys<br>•共享库：/lib、/usr/lib、/usr/local/lib</p>
<p>参考资料：</p>
<ol>
<li><a href="http://www.ruanyifeng.com/blog/2012/02/a_history_of_unix_directory_structure.html" target="_blank" rel="external">http://www.ruanyifeng.com/blog/2012/02/a_history_of_unix_directory_structure.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Linux的/bin、/sbin、/usr/sbin、/usr/bin、/usr/local/bin、/usr/local/sbin
    
    </summary>
    
      <category term="Linux" scheme="noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="noahsnail.com/tags/Linux/"/>
    
  </entry>
  
</feed>
