<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="noahsnail.com/"/>
  <updated>2018-03-01T10:51:59.000Z</updated>
  <id>noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>动手学深度学习(二)——欠拟合和过拟合</title>
    <link href="noahsnail.com/2018/03/01/2018-03-01-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>noahsnail.com/2018/03/01/2018-03-01-动手学深度学习(二)——欠拟合和过拟合/</id>
    <published>2018-03-01T10:49:47.000Z</published>
    <updated>2018-03-01T10:51:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h1 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h1><h3 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h3><p>机器学习模型在训练数据集上表现出的误差叫做训练误差，在任意一个测试数据样本上表现出的误差的期望值叫做泛化误差。</p>
<p>统计学习理论的一个假设是：训练数据集和测试数据集里的每一个数据样本都是从同一个概率分布中相互独立地生成出的（独立同分布假设）。</p>
<p>一个重要结论是：训练误差的降低不一定意味着泛化误差的降低。机器学习既需要降低训练误差，又需要降低泛化误差。</p>
<h3 id="欠拟合和过拟合-1"><a href="#欠拟合和过拟合-1" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h3><ul>
<li>欠拟合：机器学习模型无法得到较低训练误差。</li>
<li>过拟合：机器学习模型的训练误差远小于其在测试数据集上的误差。</li>
</ul>
<h3 id="模型的选择"><a href="#模型的选择" class="headerlink" title="模型的选择"></a>模型的选择</h3><p>模型拟合能力和误差之间的关系如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f50a8a6737e3927b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="error_model_complexity.png"></p>
<h3 id="训练数据集的大小"><a href="#训练数据集的大小" class="headerlink" title="训练数据集的大小"></a>训练数据集的大小</h3><p>一般来说，如果训练数据集过小，特别是比模型参数数量更小时，过拟合更容易发生。除此之外，泛化误差不会随训练数据集里样本数量增加而增大。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-7e957140a2d8242c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="model_vs_data.png"></p>
<h3 id="多项式拟合"><a href="#多项式拟合" class="headerlink" title="多项式拟合"></a>多项式拟合</h3><p>给定一个<strong>标量</strong>数据点集合<code>x</code>和对应的标量目标值<code>y</code>，多项式拟合的目标是找一个K阶多项式，其由向量<code>w</code>和位移<code>b</code>组成，来最好地近似每个样本<code>x</code>和<code>y</code>。用数学符号来表示就是我们将学<code>w</code>和<code>b</code>来预测</p>
<p>$$\hat{y} = b + \sum_{k=1}^K x^k w_k$$</p>
<p>并以平方误差为损失函数，一阶多项式拟合又叫线性拟合。</p>
<h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><p>使用二阶多项式来生成每一个数据样本，$y=1.2x−3.4x^2+5.6x^3+5.0+noise$，噪音服从均值0和标准差为0.1的正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">100</span></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"><span class="comment"># 多项式权重</span></div><div class="line">true_w = [<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>]</div><div class="line"><span class="comment"># 多项式偏置</span></div><div class="line">true_b = <span class="number">5.0</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成随机数据x</span></div><div class="line">x = nd.random.normal(shape=(num_train + num_test, <span class="number">1</span>))</div><div class="line"><span class="comment"># 计算x的多项式值</span></div><div class="line">X = nd.concat(x, nd.power(x, <span class="number">2</span>), nd.power(x, <span class="number">3</span>))</div><div class="line"><span class="comment"># 计算y</span></div><div class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_w[<span class="number">2</span>] * X[:, <span class="number">2</span>] + true_b</div><div class="line"><span class="comment"># 查看数据</span></div><div class="line">(<span class="string">'x:'</span>, x[:<span class="number">5</span>], <span class="string">'X:'</span>, X[:<span class="number">5</span>], <span class="string">'y:'</span>, y[:<span class="number">5</span>])</div></pre></td></tr></table></figure>
<pre><code>(200L,)
</code></pre><h2 id="定义训练和测试步骤"><a href="#定义训练和测试步骤" class="headerlink" title="定义训练和测试步骤"></a>定义训练和测试步骤</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义训练过程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X_train, X_test, y_train, y_test)</span>:</span></div><div class="line">    <span class="comment"># 定义线性回归模型</span></div><div class="line">    net = gluon.nn.Sequential()</div><div class="line">    <span class="keyword">with</span> net.name_scope():</div><div class="line">        net.add(gluon.nn.Dense(<span class="number">1</span>))</div><div class="line">    <span class="comment"># 权重初始化</span></div><div class="line">    net.initialize()</div><div class="line">    <span class="comment"># 学习率</span></div><div class="line">    learning_rate = <span class="number">0.01</span></div><div class="line">    <span class="comment"># 迭代周期</span></div><div class="line">    epochs = <span class="number">100</span></div><div class="line">    <span class="comment"># 训练的批数据大小</span></div><div class="line">    batch_size = min(<span class="number">10</span>, y_train.shape[<span class="number">0</span>])</div><div class="line">    <span class="comment"># 创建训练数据集</span></div><div class="line">    dataset_train = gluon.data.ArrayDataset(X_train, y_train)</div><div class="line">    <span class="comment"># 读取数据</span></div><div class="line">    data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 训练方法SGD</span></div><div class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: learning_rate&#125;)</div><div class="line">    <span class="comment"># 定义损失函数</span></div><div class="line">    square_loss = gluon.loss.L2Loss()</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="comment"># 进行训练</span></div><div class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter_train:</div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                <span class="comment"># 进行预测</span></div><div class="line">                output = net(data)</div><div class="line">                <span class="comment"># 计算预测值与实际值之间的损失</span></div><div class="line">                loss = square_loss(output, label)</div><div class="line">            <span class="comment"># 损失进行反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新权重</span></div><div class="line">            trainer.step(batch_size)</div><div class="line">        <span class="comment"># 保存训练损失</span></div><div class="line">        train_loss.append(square_loss(net(X_train), y_train).mean().asscalar())</div><div class="line">        <span class="comment"># 保存测试损失</span></div><div class="line">        test_loss.append(square_loss(net(X_test), y_test).mean().asscalar())</div><div class="line">    <span class="comment"># 绘制损失</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>,<span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> (<span class="string">'learned weight'</span>, net[<span class="number">0</span>].weight.data(), <span class="string">'learned bias'</span>, net[<span class="number">0</span>].bias.data())</div></pre></td></tr></table></figure>
<h2 id="三阶多项式拟合（正常）"><a href="#三阶多项式拟合（正常）" class="headerlink" title="三阶多项式拟合（正常）"></a>三阶多项式拟合（正常）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(X[:num_train, :], X[num_train:, :], y[:num_train], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-4500dba51617c3ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 1.22117233 -3.39606118  5.59531116]]
 &lt;NDArray 1x3 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [ 4.98550272]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="线性拟合（欠拟合）"><a href="#线性拟合（欠拟合）" class="headerlink" title="线性拟合（欠拟合）"></a>线性拟合（欠拟合）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(x[:num_train, :], x[num_train:, :], y[:num_train], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cac456292c149670.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 19.74101448]]
 &lt;NDArray 1x1 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [-0.23861444]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="训练量不足（过拟合）"><a href="#训练量不足（过拟合）" class="headerlink" title="训练量不足（过拟合）"></a>训练量不足（过拟合）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(X[<span class="number">0</span>:<span class="number">2</span>, :], X[num_train:, :], y[<span class="number">0</span>:<span class="number">2</span>], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cc94f0d732ebd573.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 3.10832024 -0.740421    4.85165691]]
 &lt;NDArray 1x3 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [ 0.29450524]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>训练误差的降低并不一定意味着泛化误差的降低。</li>
<li>欠拟合和过拟合都是需要尽量避免的。我们要注意模型的选择和训练量的大小。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——欠拟合和过拟合
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——多层感知机(gluon)</title>
    <link href="noahsnail.com/2018/02/28/2018-02-28-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(gluon)/"/>
    <id>noahsnail.com/2018/02/28/2018-02-28-动手学深度学习(二)——多层感知机(gluon)/</id>
    <published>2018-02-28T08:42:14.000Z</published>
    <updated>2018-02-28T08:42:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 按顺序堆叠网络层</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># name_scope作用, 方便管理参数名称</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    net.add(gluon.nn.Flatten())</div><div class="line">    net.add(gluon.nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</div><div class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 参数初始化</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练器和优化方法</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.819048, Train acc 0.728666, Test acc 0.768530
Epoch 1. Loss: 0.550646, Train acc 0.808644, Test acc 0.823618
Epoch 2. Loss: 0.488554, Train acc 0.829210, Test acc 0.845553
Epoch 3. Loss: 0.457407, Train acc 0.839493, Test acc 0.842448
Epoch 4. Loss: 0.438059, Train acc 0.845486, Test acc 0.852063
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——多层感知机(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——多层感知机(从零开始)</title>
    <link href="noahsnail.com/2018/02/27/2018-02-27-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>noahsnail.com/2018/02/27/2018-02-27-动手学深度学习(二)——多层感知机(从零开始)/</id>
    <published>2018-02-27T10:35:46.000Z</published>
    <updated>2018-02-27T10:35:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy, SGD</div></pre></td></tr></table></figure>
<h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输入数据大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"><span class="comment"># 输出数据大小, 分为10类</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 隐藏单元个数</span></div><div class="line">num_hidden = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 正态分布的标准差</span></div><div class="line">weight_scale = <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化输入层权重</span></div><div class="line">W1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale)</div><div class="line">b1 = nd.zeros(num_hidden)</div><div class="line"></div><div class="line"><span class="comment"># 随机初始化隐藏层权重</span></div><div class="line">W2 = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale)</div><div class="line">b2 = nd.zeros(num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W1, b1, W2, b2]</div><div class="line"></div><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 激活函数使用ReLU, relu(x)=max(x,0)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.maximum(X, <span class="number">0</span>)</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># 输入数据重排</span></div><div class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</div><div class="line">    <span class="comment"># 计算激活值</span></div><div class="line">    h1 = relu(nd.dot(X, W1) + b1)</div><div class="line">    <span class="comment"># 计算输出</span></div><div class="line">    output = nd.dot(h1, W2) + b2</div><div class="line">    <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment">## 定义学习率</span></div><div class="line">learning_rate = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        SGD(params, learning_rate/batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 1.042064, Train acc 0.630976, Test acc 0.776142
Epoch 1. Loss: 0.601578, Train acc 0.788862, Test acc 0.815204
Epoch 2. Loss: 0.525148, Train acc 0.816556, Test acc 0.835136
Epoch 3. Loss: 0.486619, Train acc 0.829427, Test acc 0.833033
Epoch 4. Loss: 0.459395, Train acc 0.836104, Test acc 0.835136
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——多层感知机(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——逻辑回归(gluon)</title>
    <link href="noahsnail.com/2018/02/22/2018-02-22-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(gluon)/"/>
    <id>noahsnail.com/2018/02/22/2018-02-22-动手学深度学习(一)——逻辑回归(gluon)/</id>
    <published>2018-02-22T09:58:07.000Z</published>
    <updated>2018-02-22T09:58:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div></pre></td></tr></table></figure>
<h2 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="定义和初始化模型"><a href="#定义和初始化模型" class="headerlink" title="定义和初始化模型"></a>定义和初始化模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个空的模型</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># name_scope作用, 方便管理参数命名</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    <span class="comment"># 加入一个平铺层, 其会将输入数据平铺为batch_size*?维</span></div><div class="line">    net.add(gluon.nn.Flatten())</div><div class="line">    <span class="comment"># 加入一个全连接层, 输出为10类</span></div><div class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 参数初始化</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练器和优化方法</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.793821, Train acc 0.744107, Test acc 0.786659
Epoch 1. Loss: 0.575076, Train acc 0.809879, Test acc 0.820112
Epoch 2. Loss: 0.530560, Train acc 0.822583, Test acc 0.831731
Epoch 3. Loss: 0.506161, Train acc 0.829728, Test acc 0.835837
Epoch 4. Loss: 0.488752, Train acc 0.834769, Test acc 0.834135
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——逻辑回归(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——逻辑回归(从零开始)</title>
    <link href="noahsnail.com/2018/02/09/2018-02-09-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>noahsnail.com/2018/02/09/2018-02-09-动手学深度学习(一)——逻辑回归(从零开始)/</id>
    <published>2018-02-09T06:23:27.000Z</published>
    <updated>2018-02-09T07:18:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<p>注意：mxnet随机种子设为1时，loss一直为nan，经测试，种子为2时，jupyter-notebook有时会出现nan，但在命令行执行python文件多次都不会出现nan。</p>
<h2 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 数据预处理</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label)</span>:</span></div><div class="line">    <span class="keyword">return</span> data.astype(<span class="string">'float32'</span>) / <span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 加载训练数据</span></div><div class="line">mnist_train = gluon.data.vision.FashionMNIST(train=<span class="keyword">True</span>, transform=transform)</div><div class="line"></div><div class="line"><span class="comment"># 加载测试数据</span></div><div class="line">mnist_test = gluon.data.vision.FashionMNIST(train=<span class="keyword">False</span>, transform=transform)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 取出单条数据</span></div><div class="line">data, label = mnist_train[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="comment"># 查看数据</span></div><div class="line">(<span class="string">'example shape: '</span>, data.shape, <span class="string">'label:'</span>, label)</div></pre></td></tr></table></figure>
<pre><code>(&apos;example shape: &apos;, (28L, 28L, 1L), &apos;label:&apos;, 2.0)
</code></pre><h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 显示图像</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span><span class="params">(images)</span>:</span></div><div class="line">    <span class="comment"># 获得图像的数量</span></div><div class="line">    n = images.shape[<span class="number">0</span>]</div><div class="line">    <span class="comment"># 绘制子图</span></div><div class="line">    _, figs = plt.subplots(<span class="number">1</span>, n, figsize=(<span class="number">15</span>, <span class="number">15</span>))</div><div class="line">    <span class="comment"># 遍历绘制图像</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(n):</div><div class="line">        <span class="comment"># 显示图像</span></div><div class="line">        figs[i].imshow(images[i].reshape((<span class="number">28</span>, <span class="number">28</span>)).asnumpy())</div><div class="line">        </div><div class="line">        <span class="comment"># 显示灰度图</span></div><div class="line">        <span class="comment">#figs[i].imshow(images[i].reshape((28, 28)).asnumpy(), cmap="gray")</span></div><div class="line">        </div><div class="line">        <span class="comment"># 不显示坐标轴</span></div><div class="line">        figs[i].axes.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">        figs[i].axes.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    plt.show()</div><div class="line">    </div><div class="line"><span class="comment"># 获取图像对应的文本标签</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text_labels</span><span class="params">(labels)</span>:</span></div><div class="line">    <span class="comment"># 图像标签对应的文本</span></div><div class="line">    text_labels = [</div><div class="line">        <span class="string">'t-shirt'</span>, <span class="string">'trouser'</span>, <span class="string">'pullover'</span>, <span class="string">'dress,'</span>, <span class="string">'coat'</span>,</div><div class="line">        <span class="string">'sandal'</span>, <span class="string">'shirt'</span>, <span class="string">'sneaker'</span>, <span class="string">'bag'</span>, <span class="string">'ankle boot'</span></div><div class="line">    ]</div><div class="line">    <span class="comment"># 返回图像标签对应的文本</span></div><div class="line">    <span class="keyword">return</span> [text_labels[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</div><div class="line"></div><div class="line"><span class="comment"># 取出训练集的前9条数据</span></div><div class="line">data, label = mnist_train[<span class="number">0</span>:<span class="number">9</span>]</div><div class="line"></div><div class="line"><span class="comment"># 显示数据</span></div><div class="line">show_images(data)</div><div class="line"><span class="comment"># 显示标签</span></div><div class="line"><span class="keyword">print</span> get_text_labels(label)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8cb1872a2eba78be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure"></p>
<pre><code>[&apos;pullover&apos;, &apos;ankle boot&apos;, &apos;shirt&apos;, &apos;t-shirt&apos;, &apos;dress,&apos;, &apos;coat&apos;, &apos;coat&apos;, &apos;sandal&apos;, &apos;coat&apos;]
</code></pre><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 读取训练数据</span></div><div class="line">train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># 读取测试数据</span></div><div class="line">test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输入数据大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"><span class="comment"># 输出数据大小, 分为10类</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化权重</span></div><div class="line">W = nd.random_normal(shape=(num_inputs, num_outputs))</div><div class="line"><span class="comment"># 随机初始化偏置</span></div><div class="line">b = nd.random_normal(shape=num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W, b]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义softmax</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># 计算e^x</span></div><div class="line">    exp = nd.exp(X)</div><div class="line">    <span class="comment"># 假设exp是矩阵，这里对行进行求和，并要求保留axis 1, 就是返回 (nrows, 1) 形状的矩阵</span></div><div class="line">    partition = exp.sum(axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 对exp进行归一化</span></div><div class="line">    <span class="keyword">return</span> exp / partition</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试softmax</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化数据</span></div><div class="line">X = nd.random_normal(shape=(<span class="number">2</span>, <span class="number">5</span>))</div><div class="line"><span class="comment"># 求softmax</span></div><div class="line">X_prob = softmax(X)</div><div class="line"><span class="comment"># 输出结果</span></div><div class="line"><span class="keyword">print</span> X_prob</div><div class="line"><span class="comment"># 对每行概率求和, 如果和为1, 说明没问题</span></div><div class="line"><span class="keyword">print</span> X_prob.sum(axis=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<pre><code>[[ 0.06774449  0.5180918   0.1474141   0.11459844  0.1521512 ]
 [ 0.23102701  0.47666225  0.10536087  0.09706162  0.08988826]]
&lt;NDArray 2x5 @cpu(0)&gt;

[ 1.  1.]
&lt;NDArray 2 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义模型</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> softmax(nd.dot(X.reshape((<span class="number">-1</span>, num_inputs)), W) + b)</div></pre></td></tr></table></figure>
<h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(yhat, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> -nd.pick(nd.log(yhat), y)</div></pre></td></tr></table></figure>
<h2 id="计算精度"><a href="#计算精度" class="headerlink" title="计算精度"></a>计算精度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算分类准确率</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>) == label).asscalar()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 评估整个数据集的分类精度</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net)</span>:</span></div><div class="line">    <span class="comment"># 最终的准确率</span></div><div class="line">    acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 遍历测试数据集</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</div><div class="line">        output = net(data)</div><div class="line">        <span class="comment"># 累加准确率</span></div><div class="line">        acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 返回平均准确率</span></div><div class="line">    <span class="keyword">return</span> acc / len(data_iterator)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试随机初始化参数的分类准确率</span></div><div class="line">evaluate_accuracy(test_data, net)</div></pre></td></tr></table></figure>
<pre><code>0.15156249999999999
</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义SGD</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></div><div class="line">    <span class="comment"># 对参数进行梯度下降</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        <span class="comment"># 这样写不会创建新的param, 而是会写在原来的param里, 新的param没有梯度</span></div><div class="line">        param[:] = param - lr * param.grad</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 学习率</span></div><div class="line">learning_rate = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代巡礼</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 将梯度做平均，这样学习率会对batch size不那么敏感, 避免学习率与batch_size耦合</span></div><div class="line">        SGD(params, learning_rate / batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 3.725043, Train acc 0.487916, Test acc 0.602344
Epoch 1. Loss: 1.965692, Train acc 0.632812, Test acc 0.666895
Epoch 2. Loss: 1.630362, Train acc 0.677349, Test acc 0.691016
Epoch 3. Loss: 1.450728, Train acc 0.701690, Test acc 0.710352
Epoch 4. Loss: 1.329035, Train acc 0.717996, Test acc 0.720410
</code></pre><ul>
<li>learning_rate = 1</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Epoch 0. Loss: 18.803541, Train acc 0.081366, Test acc 0.085840</div><div class="line">Epoch 1. Loss: 18.806381, Train acc 0.081394, Test acc 0.085840</div><div class="line">Epoch 2. Loss: 18.795504, Train acc 0.081449, Test acc 0.085840</div><div class="line">Epoch 3. Loss: 18.798995, Train acc 0.081505, Test acc 0.085840</div><div class="line">Epoch 4. Loss: 18.794157, Train acc 0.081505, Test acc 0.085840</div></pre></td></tr></table></figure>
<ul>
<li>learning_rate = 0.01</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Epoch 0. Loss: 10.406917, Train acc 0.115165, Test acc 0.169629</div><div class="line">Epoch 1. Loss: 5.512807, Train acc 0.246349, Test acc 0.304590</div><div class="line">Epoch 2. Loss: 3.911078, Train acc 0.373232, Test acc 0.412695</div><div class="line">Epoch 3. Loss: 3.218969, Train acc 0.454189, Test acc 0.471191</div><div class="line">Epoch 4. Loss: 2.855459, Train acc 0.502371, Test acc 0.509082</div></pre></td></tr></table></figure>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 从测试集中取数据</span></div><div class="line">data, label = mnist_test[<span class="number">0</span>:<span class="number">9</span>]</div><div class="line"><span class="comment"># 显示图片及真实标签</span></div><div class="line">show_images(data)</div><div class="line"><span class="keyword">print</span> <span class="string">'True labels: '</span></div><div class="line"><span class="keyword">print</span> get_text_labels(label)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 预测标签</span></div><div class="line">predicted_labels = net(data).argmax(axis=<span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">'Predicted labels: '</span></div><div class="line"><span class="keyword">print</span> get_text_labels(predicted_labels.asnumpy())</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3d41ebd81c95b691.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure"></p>
<pre><code>True labels: 
[&apos;t-shirt&apos;, &apos;trouser&apos;, &apos;pullover&apos;, &apos;pullover&apos;, &apos;dress,&apos;, &apos;pullover&apos;, &apos;bag&apos;, &apos;shirt&apos;, &apos;sandal&apos;]
Predicted labels: 
[&apos;shirt&apos;, &apos;trouser&apos;, &apos;pullover&apos;, &apos;pullover&apos;, &apos;dress,&apos;, &apos;bag&apos;, &apos;bag&apos;, &apos;shirt&apos;, &apos;sandal&apos;]
</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>FashionMNIST<br><a href="https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/vision.html#FashionMNIST" target="_blank" rel="external">https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/vision.html#FashionMNIST</a>  </li>
</ul>
<h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——逻辑回归(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python中的__all__</title>
    <link href="noahsnail.com/2018/02/08/2018-02-08-Python%E4%B8%AD%E7%9A%84__all__/"/>
    <id>noahsnail.com/2018/02/08/2018-02-08-Python中的__all__/</id>
    <published>2018-02-08T08:19:15.000Z</published>
    <updated>2018-02-08T08:43:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-动机"><a href="#1-动机" class="headerlink" title="1. 动机"></a>1. 动机</h2><p>今天看MXNet的gluon源码时发现<code>mxnet.gluon.data.vision</code>有<code>__all__</code>如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">__all__ = [&apos;MNIST&apos;, &apos;FashionMNIST&apos;, &apos;CIFAR10&apos;, &apos;CIFAR100&apos;,</div><div class="line">           &apos;ImageRecordDataset&apos;, &apos;ImageFolderDataset&apos;]</div></pre></td></tr></table></figure>
<h2 id="2-作用"><a href="#2-作用" class="headerlink" title="2. 作用"></a>2. 作用</h2><p><code>__all__</code>是一个字符串list，用来定义模块中对于<code>from XXX import *</code>时要对外导出的符号，即要暴露的借口，但它只对<code>import *</code>起作用，对<code>from XXX import XXX</code>不起作用。</p>
<h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3. 测试"></a>3. 测试</h2><p><code>all.py</code>文件时要导出的模块，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">__all__ = [&apos;x&apos;, &apos;y&apos;, &apos;test&apos;]</div><div class="line"></div><div class="line">x = 2</div><div class="line">y = 3</div><div class="line">z = 4</div><div class="line"></div><div class="line">def test():</div><div class="line">	print(&apos;test&apos;)</div></pre></td></tr></table></figure>
<ul>
<li>测试文件一</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from foo import *</div><div class="line">print(&apos;x: &apos;, x)</div><div class="line"></div><div class="line">print(&apos;y: &apos;, y)</div><div class="line">print(&apos;z: &apos;, z)</div><div class="line"></div><div class="line">test()</div></pre></td></tr></table></figure>
<ul>
<li>运行结果：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x:  2</div><div class="line">y:  3</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;test.py&quot;, line 6, in &lt;module&gt;</div><div class="line">    print(&apos;z: &apos;, z)</div><div class="line">NameError: name &apos;z&apos; is not defined</div></pre></td></tr></table></figure>
<ul>
<li>测试文件二</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from foo import *</div><div class="line">from foo import z</div><div class="line">print(&apos;x: &apos;, x)</div><div class="line"></div><div class="line">print(&apos;y: &apos;, y)</div><div class="line">print(&apos;z: &apos;, z)</div><div class="line"></div><div class="line">test()</div></pre></td></tr></table></figure>
<ul>
<li>运行结果</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x:  2</div><div class="line">y:  3</div><div class="line">z:  4</div><div class="line">test</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python" target="_blank" rel="external">https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python中的__all__
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>使用Docker搭建Anaconda Python3.6的练习环境</title>
    <link href="noahsnail.com/2018/02/08/2018-02-08-%E4%BD%BF%E7%94%A8Docker%E6%90%AD%E5%BB%BAAnaconda%20Python3.6%E7%9A%84%E7%BB%83%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    <id>noahsnail.com/2018/02/08/2018-02-08-使用Docker搭建Anaconda Python3.6的练习环境/</id>
    <published>2018-02-08T07:34:43.000Z</published>
    <updated>2018-02-08T08:18:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>最近在看Python 3的相关内容，由于电脑里已经装了Anaconda 2.7，因此就在Docker里搭建了一个Anaconda Python3.6的练习环境。Dockerfile如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">FROM nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04</div><div class="line">MAINTAINER Tyan &lt;tyan.liu.git@gmail.com&gt;</div><div class="line"></div><div class="line"></div><div class="line"># Install basic dependencies</div><div class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</div><div class="line">        build-essential \</div><div class="line">        cmake \</div><div class="line">        git \</div><div class="line">        wget \</div><div class="line">        libopencv-dev \</div><div class="line">        libsnappy-dev \</div><div class="line">        python-dev \</div><div class="line">        python-pip \</div><div class="line">        tzdata \</div><div class="line">        vim</div><div class="line"></div><div class="line"></div><div class="line"># Install anaconda for python 3.6</div><div class="line">RUN wget --quiet https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh -O ~/anaconda.sh &amp;&amp; \</div><div class="line">    /bin/bash ~/anaconda.sh -b -p /opt/conda &amp;&amp; \</div><div class="line">    rm ~/anaconda.sh &amp;&amp; \</div><div class="line">    echo &quot;export PATH=/opt/conda/bin:$PATH&quot; &gt;&gt; ~/.bashrc</div><div class="line"></div><div class="line"></div><div class="line"># Set timezone</div><div class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</div><div class="line"></div><div class="line"></div><div class="line"># Set locale</div><div class="line">ENV LANG C.UTF-8 LC_ALL=C.UTF-8</div><div class="line"></div><div class="line"></div><div class="line"># Initialize workspace</div><div class="line">RUN mkdir /workspace</div><div class="line">WORKDIR /workspace</div></pre></td></tr></table></figure>
<p>构建docker image命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker build -t python:3.6 .</div></pre></td></tr></table></figure>
<p>运行docker image命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -ti --rm python:3.6</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      使用Docker搭建Anaconda Python3.6的练习环境
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>nohup python缓存问题</title>
    <link href="noahsnail.com/2018/02/07/2018-02-07-nohup%20python%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98/"/>
    <id>noahsnail.com/2018/02/07/2018-02-07-nohup python缓存问题/</id>
    <published>2018-02-07T09:08:12.000Z</published>
    <updated>2018-02-08T07:28:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>深度学习用python跑数据时，经常会用到<code>nohup</code>命令，通常的命令格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup python [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<p>如果没有指定输出文件，<code>nohup</code>会将输出放到<code>nohup.out</code>文件中，但在程序运行过程中<code>nohup.out</code>文件中不能实时的看到python的输出，原因是python的输出有缓冲。</p>
<p>解决方案如下：</p>
<ul>
<li>方案一</li>
</ul>
<p>使用<code>-u</code>参数，使python输出不进行缓冲，命令格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup python -u [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<ul>
<li>方案二</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export PYTHONUNBUFFERED=1</div><div class="line">nohup python [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://stackoverflow.com/questions/12919980/nohup-is-not-writing-log-to-output-file" target="_blank" rel="external">https://stackoverflow.com/questions/12919980/nohup-is-not-writing-log-to-output-file</a></p>
]]></content>
    
    <summary type="html">
    
      nohup python缓存问题
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——线性回归(gluon)</title>
    <link href="noahsnail.com/2018/02/06/2018-02-06-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(gluon)/"/>
    <id>noahsnail.com/2018/02/06/2018-02-06-动手学深度学习(一)——线性回归(gluon)/</id>
    <published>2018-02-06T10:59:38.000Z</published>
    <updated>2018-02-09T06:31:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 导入mxnet的gluon, ndarray, autograd</span></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">1</span>)</div><div class="line">random.seed(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据的维度</span></div><div class="line">num_inputs = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># 训练数据的样本数量</span></div><div class="line">num_examples = <span class="number">1000</span></div><div class="line"></div><div class="line"><span class="comment"># 实际的权重w</span></div><div class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</div><div class="line"></div><div class="line"><span class="comment"># 实际的偏置b</span></div><div class="line">true_b = <span class="number">4.2</span></div><div class="line"></div><div class="line"><span class="comment"># 随机生成均值为0, 方差为1, 服从正态分布的训练数据X, </span></div><div class="line">X = nd.random_normal(shape=(num_examples, num_inputs))</div><div class="line"></div><div class="line"><span class="comment"># 根据X, w, b生成对应的输出y</span></div><div class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_b </div><div class="line"></div><div class="line"><span class="comment"># 给y加上随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div></pre></td></tr></table></figure>
<h2 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 绘制数据的散点图 </span></div><div class="line">plt.scatter(X[:, <span class="number">1</span>].asnumpy(), y.asnumpy())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1ddbc52187302d98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="数据展示"></p>
<h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练时的批数据大小</span></div><div class="line">batch_size = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 创建数据集</span></div><div class="line">dataset = gluon.data.ArrayDataset(X, y)</div><div class="line"></div><div class="line"><span class="comment"># 读取数据</span></div><div class="line">data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 查看数据</span></div><div class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">    <span class="keyword">print</span> data, label</div><div class="line">    <span class="keyword">break</span></div></pre></td></tr></table></figure>
<pre><code>[[-2.11255503  0.61242002]
 [ 2.18546367 -0.48856559]
 [ 0.91085583  0.38985687]
 [-0.56097323  1.44421673]
 [ 0.31765923 -1.75729597]
 [-0.57738042  2.03963804]
 [-0.91808975  0.64181799]
 [-0.20269176  0.21012937]
 [-0.22549874  0.19895147]
 [ 1.42844415  0.06982213]]
&lt;NDArray 10x2 @cpu(0)&gt; 
[ -2.11691356  10.22533131   4.70613146  -1.82755637  10.82125568
  -3.88111711   0.17608714   3.07074499   3.06542921   6.82972908]
&lt;NDArray 10 @cpu(0)&gt;
</code></pre><h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个空的模型</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># 加入一个Dense层</span></div><div class="line">net.add(gluon.nn.Dense(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">square_loss = gluon.loss.L2Loss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.01</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练的迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</div><div class="line">    <span class="comment"># 总的loss</span></div><div class="line">    total_loss = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算预测值</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算loss</span></div><div class="line">            loss = square_loss(output, label)</div><div class="line">        <span class="comment"># 根据loss进行反向传播计算梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 更新权重, batch_size用来进行梯度平均</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 计算总的loss</span></div><div class="line">        total_loss += nd.sum(loss).asscalar()</div><div class="line">        </div><div class="line">    <span class="keyword">print</span> <span class="string">"Epoch %d, average loss: %f"</span> % (epoch, total_loss/num_examples)</div></pre></td></tr></table></figure>
<pre><code>Epoch 0, average loss: 7.403182
Epoch 1, average loss: 0.854247
Epoch 2, average loss: 0.099864
Epoch 3, average loss: 0.011887
Epoch 4, average loss: 0.001479
</code></pre><h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><p>ArrayDataset<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.ArrayDataset" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.ArrayDataset</a>  </p>
</li>
<li><p>DataLoader<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataload#mxnet.gluon.data.DataLoader" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataload#mxnet.gluon.data.DataLoader</a>  </p>
</li>
<li><p>Sequential<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=gluon.nn.sequential#mxnet.gluon.nn.Sequential" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=gluon.nn.sequential#mxnet.gluon.nn.Sequential</a>  </p>
</li>
<li><p>L2Loss<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html?highlight=l2loss#mxnet.gluon.loss.L2Loss" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/loss.html?highlight=l2loss#mxnet.gluon.loss.L2Loss</a>  </p>
</li>
<li><p>Trainer<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=trainer#mxnet.gluon.Trainer" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=trainer#mxnet.gluon.Trainer</a>  </p>
</li>
</ul>
<h2 id="代码地址-1"><a href="#代码地址-1" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——线性回归(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Model Zoo</title>
    <link href="noahsnail.com/2018/02/02/2018-02-02-Caffe%20Model%20Zoo/"/>
    <id>noahsnail.com/2018/02/02/2018-02-02-Caffe Model Zoo/</id>
    <published>2018-02-02T10:59:55.000Z</published>
    <updated>2018-02-02T10:59:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Caffe有许多分类的预训练模型及网络结构，我自己训练过的模型总结在Github上，基本上涵盖了大部分的分类模型，包括AlexNet，VGG，GoogLeNet，Inception系列，ResNet，SENet，DenseNet，SqueezeNet。</p>
<p>其中会碰到不少坑，例如VGG给的结构已经太旧了，需要根据新版本Caffe的进行修改，DenseNet训练有些地方需要修改等。鉴于以上原因，我自己整理了一个Caffe Model Zoo，都是已经使用Caffe训练过模型的。</p>
<p>Github地址：<a href="https://github.com/SnailTyan/caffe-model-zoo" target="_blank" rel="external">https://github.com/SnailTyan/caffe-model-zoo</a></p>
]]></content>
    
    <summary type="html">
    
      Caffe Model Zoo
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习模型训练流程</title>
    <link href="noahsnail.com/2018/02/02/2018-02-02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/"/>
    <id>noahsnail.com/2018/02/02/2018-02-02-深度学习模型训练流程/</id>
    <published>2018-02-02T08:37:32.000Z</published>
    <updated>2018-02-04T14:02:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>工作中训练了很多的深度学习模型，目前到了上升到方法论的角度来看了。日常工作中有的人可能已经在遵循方法论做事，可能自己没有注意，有的人可能没有遵循方法论在做事，虽然可能最后的结果差不多，但花费的时间和精力应该会差别很大，当然这是我自己的感受。我们不必完全按照方法论来做，但基本流程跟方法论应该一致。</p>
<p>下面的具体步骤以图像分类，识别图像中的猫为例。</p>
<h2 id="1-问题定义"><a href="#1-问题定义" class="headerlink" title="1. 问题定义"></a>1. 问题定义</h2><p>问题定义，“what，how，why”中的what，首先要弄清楚自己要干什么，然后调研相关的技术，确定解决方案。例如这一步中我的工作是进行图像分类，问题定义是图像分类——识别猫，相关的技术包括各种分类模型，各种深度学习框架。我选择的是BN-Inception + Caffe。</p>
<h2 id="2-确定评估标准"><a href="#2-确定评估标准" class="headerlink" title="2. 确定评估标准"></a>2. 确定评估标准</h2><p>根据问题定义，确定了相关技术之后，不要着急动手去做，先确定评估标准，怎么评价模型的好坏，例如分类猫可以通过准确率（Precision）、召回率（Recall）、F1、ROC曲线、AUC面积等。确定了评估标准之后，评估数据集也要准备好。</p>
<h2 id="3-确定Baseline和Target"><a href="#3-确定Baseline和Target" class="headerlink" title="3. 确定Baseline和Target"></a>3. 确定Baseline和Target</h2><ul>
<li>Baseline</li>
</ul>
<p>有了评估标准后，需要确定一个Baseline，例如可以简单快速的训练一个模型或已经有一个Pretrained Model，在评估数据集上进行评估，得到一个指标作为Baseline，然后在Baseline的基础上进行提高，确定Baseline类似于敏捷开发中的快速原型开发。</p>
<ul>
<li>Target</li>
</ul>
<p>有了Baseline之后，可以确定一个目标，但这个目标不能是拍脑袋出来的，如果你的业务与别人的业务类似，例如色情识别，可以使用大厂（BAT)的模型先在评估数据集上得出一个结果，目标定为达到他们的水平或超过他们的水平。如果不跟别人的业务类似，那么需要根据具体的业务需求确定一个目标。目标还是要有的，起码确定一个方向。</p>
<h2 id="4-模型训练"><a href="#4-模型训练" class="headerlink" title="4. 模型训练"></a>4. 模型训练</h2><p>模型训练这部分就没太多说的了，深度学习工程师的基本功。</p>
<h2 id="5-模型评估"><a href="#5-模型评估" class="headerlink" title="5. 模型评估"></a>5. 模型评估</h2><p>将训练的模型在评估数据集上进行评估，分析评估结果，与上一次的模型结果以及Target进行对比。将错误的数据取出来，分析存在的问题，讨论调整的方向，记录实验结果。</p>
<h2 id="6-模型再训练"><a href="#6-模型再训练" class="headerlink" title="6. 模型再训练"></a>6. 模型再训练</h2><p>重复步骤4、5，直至达到目标。如果模型还不错，可以将模型放到Beta环境测试，分析线上的结果，重复步骤4、5。</p>
<h2 id="7-服务部署"><a href="#7-服务部署" class="headerlink" title="7. 服务部署"></a>7. 服务部署</h2><p>如果模型在Beta环境也不错，则可以进行线上测试，继续重复步骤4、5，因为有的模型需要不断的进行迭代更新。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://towardsdatascience.com/machine-learning-in-practice-what-are-the-steps-a4b15ee18546" target="_blank" rel="external">https://towardsdatascience.com/machine-learning-in-practice-what-are-the-steps-a4b15ee18546</a></p>
]]></content>
    
    <summary type="html">
    
      深度学习模型训练流程
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中英文对照</title>
    <link href="noahsnail.com/2018/02/02/2018-02-02-Detecting%20Text%20in%20Natural%20Image%20with%20Connectionist%20Text%20Proposal%20Network%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2018/02/02/2018-02-02-Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中英文对照/</id>
    <published>2018-02-02T07:15:37.000Z</published>
    <updated>2018-03-07T10:52:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<h1 id="Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network"><a href="#Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network" class="headerlink" title="Detecting Text in Natural Image with Connectionist Text Proposal Network"></a>Detecting Text in Natural Image with Connectionist Text Proposal Network</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8,35] by a large margin. The CTPN is computationally efficient with 0.14s/image, by using the very deep VGG16 model [27]. Online demo is available at: <a href="http://textdet.com/" target="_blank" rel="external">http://textdet.com/</a>.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。CTPN直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。我们开发了一个垂直锚点机制，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。这使得CTPN可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。CTPN在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。在线演示获取地址：<a href="http://textdet.com/" target="_blank" rel="external">http://textdet.com/</a>。</p>
<h2 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h2><p>Scene text detection, convolutional network, recurrent neural network, anchor mechanism</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>场景文本检测；卷积网络；循环神经网络；锚点机制</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Reading text in natural image has recently attracted increasing attention in computer vision [8,14,15,10,35,11,9,1,28,32]. This is due to its numerous practical applications such as image OCR, multi-language translation, image retrieval, etc. It includes two sub tasks: text detection and recognition. This work focus on the detection task [14,1,28,32], which is more challenging than recognition task carried out on a well-cropped word image [15,9]. Large variance of text patterns and highly cluttered background pose main challenge of accurate text localization.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在自然图像中阅读文本最近在计算机视觉中引起越来越多的关注[8，14，15，10，35，11，9，1，28，32]。这是由于它的许多实际应用，如图像OCR，多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。这项工作的重点是检测任务[14，1，28，32]，这是比在一个良好的裁剪字图像[15，9]进行的识别任务更具有挑战性。文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。</p>
<p>Current approaches for text detection mostly employ a bottom-up pipeline [28,1,14,32,33]. They commonly start from low-level character or stroke detection, which is typically followed by a number of subsequent steps: non-text component filtering, text line construction and text line verification. These multi-step bottom-up approaches are generally complicated with less robustness and reliability. Their performance heavily rely on the results of character detection, and connected-components methods or sliding-window methods have been proposed. These methods commonly explore low-level features (e.g., based on SWT [3,13], MSER [14,33,23], or HoG [28]) to distinguish text candidates from background. However, they are not robust by identifying individual strokes or characters separately, without context information. For example, it is more confident for people to identify a sequence of characters than an individual one, especially when a character is extremely ambiguous. These limitations often result in a large number of non-text components in character detection, causing main difficulties for handling them in following steps. Furthermore, these false detections are easily accumulated sequentially in bottom-up pipeline, as pointed out in [28]. To address these problems, we exploit strong deep features for detecting text information directly in convolutional maps. We develop text anchor mechanism that accurately predicts text locations in fine scale. Then, an in-network recurrent architecture is proposed to connect these fine-scale text proposals in sequences, allowing them to encode rich context information.</p>
<p>目前的文本检测方法大多采用自下而上的流程[28，1，14，32，33]。它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。它们的性能很大程度上依赖于字符检测的结果，并且已经提出了连接组件方法或滑动窗口方法。这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。但是，如果没有上下文信息，他们不能鲁棒的单独识别各个笔划或字符。例如，相比单个字符人们更信任一个字符序列，特别是当一个字符非常模糊时。这些限制在字符检测中通常会导致大量非文本组件，在后续步骤中的主要困难是处理它们。此外，正如[28]所指出的，这些误检很容易在自下而上的过程中连续累积。为了解决这些问题，我们利用强大的深度特征直接在卷积映射中检测文本信息。我们开发的文本锚点机制能在细粒度上精确预测文本位置。然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</p>
<p>Deep Convolutional Neural Networks (CNN) have recently advanced general object detection substantially [25,5,6]. The state-of-the-art method is Faster Region-CNN (R-CNN) system [25] where a Region Proposal Network (RPN) is proposed to generate high-quality class-agnostic object proposals directly from convolutional feature maps. Then the RPN proposals are fed into a Fast R-CNN [5] model for further classification and refinement, leading to the state-of-the-art performance on generic object detection. However, it is difficult to apply these general object detection systems directly to scene text detection, which generally requires a higher localization accuracy. In generic object detection, each object has a well-defined closed boundary [2], while such a well-defined boundary may not exist in text, since a text line or word is composed of a number of separate characters or strokes. For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (e.g., the PASCAL standard [4]), since people can recognize an object easily from major part of it. By contrast, reading text comprehensively is a fine-grained recognition task which requires a correct detection that covers a full region of a text line or word. Therefore, text detection generally requires a more accurate localization, leading to a different evaluation standard, e.g., the Wolf’s standard [30] which is commonly employed by text benchmarks [19,21].</p>
<p>深度卷积神经网络（CNN）最近已经基本实现了一般物体检测[25，5，6]。最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。然而，很难将这些通用目标检测系统直接应用于场景文本检测，这通常需要更高的定位精度。在通用目标检测中，每个目标都有一个明确的封闭边界[2]，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠&gt;0.5，因为人们可以容易地从目标的主要部分识别它。相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的Wolf标准[19，21]。</p>
<p>In this work, we fill this gap by extending the RPN architecture [25] to accurate text line localization. We present several technical developments that tailor generic object detection model elegantly towards our problem. We strive for a further step by proposing an in-network recurrent mechanism that allows our model to detect text sequence directly in the convolutional maps, avoiding further post-processing by an additional costly CNN detection model.</p>
<p>在这项工作中，我们通过将RPN架构[25]扩展到准确的文本行定义来填补这个空白。我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的CNN检测模型进行进一步的后处理。</p>
<h3 id="1-1-Contributions"><a href="#1-1-Contributions" class="headerlink" title="1.1 Contributions"></a>1.1 Contributions</h3><p>We propose a novel Connectionist Text Proposal Network (CTPN) that directly localizes text sequences in convolutional layers. This overcomes a number of main limitations raised by previous bottom-up approaches building on character detection. We leverage the advantages of strong deep convolutional features and sharing computation mechanism, and propose the CTPN architecture which is described in Fig. 1. It makes the following major contributions:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3c885b0226d1b773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>Fig. 1: (a) Architecture of the Connectionist Text Proposal Network (CTPN). We densely slide a 3×3 spatial window through the last convolutional maps (conv5 ) of the VGG16 model [27]. The sequential windows in each row are recurrently connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs). The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which jointly predicts text/non-text scores, y-axis coordinates and side-refinement offsets of $k$ anchors. (b) The CTPN outputs sequential fixed-width fine-scale text proposals. Color of each box indicates the text/non-text score. Only the boxes with positive scores are presented.</p>
<h3 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1 贡献"></a>1.1 贡献</h3><p>我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。这克服了以前的建立在字符检测基础上的自下而上方法带来的一些主要限制。我们利用强深度卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。主要贡献如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3c885b0226d1b773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1：（a）连接文本提议网络（CTPN）的架构。我们通过VGG16模型[27]的最后一个卷积映射（conv5）密集地滑动3×3空间窗口。每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。RNN层连接到512维的全连接层，接着是输出层，联合预测$k$个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。（b）CTPN输出连续的固定宽度细粒度文本提议。每个框的颜色表示文本/非文本分数。只显示文本框正例的分数。</p>
<p>First, we cast the problem of text detection into localizing a sequence of fine-scale text proposals. We develop an anchor regression mechanism that jointly predicts vertical location and text/non-text score of each text proposal, resulting in an excellent localization accuracy. This departs from the RPN prediction of a whole object, which is difficult to provide a satisfied localization accuracy.</p>
<p>首先，我们将文本检测的问题转化为一系列细粒度的文本提议。我们开发了一个锚点回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。这背离了整个目标的RPN预测，RPN预测难以提供令人满意的定位精度。</p>
<p>Second, we propose an in-network recurrence mechanism that elegantly connects sequential text proposals in the convolutional feature maps. This connection allows our detector to explore meaningful context information of text line, making it powerful to detect extremely challenging text reliably.</p>
<p>其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。通过这种连接，我们的检测器可以探索文本行有意义的上下文信息，使其能够可靠地检测极具挑战性的文本。</p>
<p>Third, both methods are integrated seamlessly to meet the nature of text sequence, resulting in a unified end-to-end trainable model. Our method is able to handle multi-scale and multi-lingual text in a single process, avoiding further post filtering or refinement.</p>
<p>第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</p>
<p>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015). Furthermore, it is computationally efficient, resulting in a 0.14s/image running time (on the ICDAR 2013) by using the very deep VGG16 model [27].</p>
<p>第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Text detection.</strong> Past works in scene text detection have been dominated by bottom-up approaches which are generally built on stroke or character detection. They can be roughly grouped into two categories, connected-components (CCs) based approaches and sliding-window based methods. The CCs based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are greedily grouped into stroke or character candidates, by using low-level properties, e.g., intensity, color, gradient, etc. [33,14,32,13,3]. The sliding-window based methods detect character candidates by densely moving a multi-scale window through an image. The character or non-character window is discriminated by a pre-trained classifier, by using manually-designed features [28,29], or recent CNN features [16]. However, both groups of methods commonly suffer from poor performance of character detection, causing accumulated errors in following component filtering and text line construction steps. Furthermore, robustly filtering out non-character components or confidently verifying detected text lines are even difficult themselves [1,33,14]. Another limitation is that the sliding-window methods are computationally expensive, by running a classifier on a huge number of the sliding windows.</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>文本检测</strong>。过去在场景文本检测中的工作一直以自下而上的方法为主，一般建立在笔画或字符检测上。它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。基于滑动窗口的方法通过在图像中密集地滑动多尺度窗口来检测候选字符。字符或非字符窗口通过预先训练的分类器，使用手动设计的特征[28，29]或最近的CNN特征[16]进行区分。然而，这两种方法通常都会受到较差的字符检测性能的影响，导致在接下来的组件过滤和文本行构建步骤中出现累积的错误。此外，强大地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难[1，33，14]。另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。</p>
<p><strong>Object detection.</strong> Convolutional Neural Networks (CNN) have recently advanced general object detection substantially [25,5,6]. A common strategy is to generate a number of object proposals by employing inexpensive low-level features, and then a strong CNN classifier is applied to further classify and refine the generated proposals. Selective Search (SS) [4] which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5]. Recently, Ren et al. [25] proposed a Faster R-CNN system for object detection. They proposed a Region Proposal Network (RPN) that generates high-quality class-agnostic object proposals directly from the convolutional feature maps. The RPN is fast by sharing convolutional computation. However, the RPN proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5]. More importantly, text is different significantly from general objects, making it difficult to directly apply general object detection system to this highly domain-specific task.</p>
<p><strong>目标检测。</strong>卷积神经网络（CNN）近来在通用目标检测[25，5，6]上已经取得了实质的进步。一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强CNN分类器来进一步对生成的提议进行分类和细化。生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。最近，Ren等人[25]提出了Faster R-CNN目标检测系统。他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。通过共享卷积计算RPN是快速的。然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。</p>
<h2 id="3-Connectionist-Text-Proposal-Network"><a href="#3-Connectionist-Text-Proposal-Network" class="headerlink" title="3. Connectionist Text Proposal Network"></a>3. Connectionist Text Proposal Network</h2><p>This section presents details of the Connectionist Text Proposal Network (CTPN). It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, recurrent connectionist text proposals, and side-refinement.</p>
<h2 id="3-连接文本提议网络"><a href="#3-连接文本提议网络" class="headerlink" title="3. 连接文本提议网络"></a>3. 连接文本提议网络</h2><p>本节介绍连接文本提议网络（CTPN）的细节。它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</p>
<h3 id="3-1-Detecting-Text-in-Fine-scale-Proposals"><a href="#3-1-Detecting-Text-in-Fine-scale-Proposals" class="headerlink" title="3.1 Detecting Text in Fine-scale Proposals"></a>3.1 Detecting Text in Fine-scale Proposals</h3><p>Similar to Region Proposal Network (RPN) [25], the CTPN is essentially a fully convolutional network that allows an input image of arbitrary size. It detects a text line by densely sliding a small window in the convolutional feature maps, and outputs a sequence of fine-scale (e.g., fixed 16-pixel width) text proposals, as shown in Fig. 1 (b).</p>
<h3 id="3-1-在细粒度提议中检测文本"><a href="#3-1-在细粒度提议中检测文本" class="headerlink" title="3.1 在细粒度提议中检测文本"></a>3.1 在细粒度提议中检测文本</h3><p>类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的16个像素）文本提议，如图1（b）所示。</p>
<p>We take the very deep 16-layer vggNet (VGG16) [27] as an example to describe our approach, which is readily applicable to other deep models. Architecture of the CTPN is presented in Fig. 1 (a). We use a small spatial window, 3×3, to slide the feature maps of last convolutional layer (e.g., the conv5 of the VGG16). The size of conv5 feature maps is determined by the size of input image, while the total stride and receptive field are fixed as 16 and 228 pixels, respectively. Both the total stride and receptive field are fixed by the network architecture. Using a sliding window in the convolutional layer allows it to share convolutional computation, which is the key to reduce computation of the costly sliding-window based methods.</p>
<p>我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。CTPN的架构如图1（a）所示。我们使用一个小的空间窗口3×3来滑动最后的卷积层特征映射（例如，VGG16的conv5）。conv5特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为16个和228个像素。网络架构决定总步长和感受野。在卷积层中使用滑动窗口允许它共享卷积计算，这是减少昂贵的基于滑动窗口的方法的计算量的关键。</p>
<p>Generally, sliding-window methods adopt multi-scale windows to detect objects of different sizes, where one window scale is fixed to objects of similar size. In [25], Ren et al. proposed an efficient anchor regression mechanism that allows the RPN to detect multi-scale objects with a single-scale window. The key insight is that a single window is able to predict objects in a wide range of scales and aspect ratios, by using a number of flexible anchors. We wish to extend this efficient anchor mechanism to our text task. However, text differs from generic objects substantially, which generally have a well-defined enclosed boundary and center, allowing inferring whole object from even a part of it [2]. Text is a sequence which does not have an obvious closed boundary. It may include multi-level components, such as stroke, character, word, text line and text region, which are not distinguished clearly between each other. Text detection is defined in word or text line level, so that it may be easy to make an incorrect detection by defining it as a single object, e.g., detecting part of a word. Therefore, directly predicting the location of a text line or word may be difficult or unreliable, making it hard to get a satisfied accuracy. An example is shown in Fig. 2, where the RPN is directly trained for localizing text lines in an image.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1d1b210f78d93fed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>Fig. 2: Left: RPN proposals. Right: Fine-scale text proposals.</p>
<p>通常，滑动窗口方法采用多尺度窗口来检测不同尺寸的目标，其中一个窗口尺度被固定到与目标的尺寸相似。在[25]中，Ren等人提出了一种有效的锚点回归机制，允许RPN使用单尺度窗口检测多尺度目标。关键的洞察力是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和长宽比的目标。我们希望将这种有效的锚点机制扩展到我们的文本任务。然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标[2]。文本是一个没有明显封闭边界的序列。它可能包含多层次的组件，如笔划，字符，单词，文本行和文本区域等，这些组件之间没有明确区分。文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。因此，直接预测文本行或单词的位置可能很难或不可靠，因此很难获得令人满意的准确性。一个例子如图2所示，其中RPN直接被训练用于定位图像中的文本行。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1d1b210f78d93fed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2：左：RPN提议。右：细粒度的文本提议。</p>
<p>We look for a unique property of text that is able to generalize well to text components in all levels. We observed that word detection by the RPN is difficult to accurately predict the horizontal sides of words, since each character within a word is isolated or separated, making it confused to find the start and end locations of a word. Obviously, a text line is a sequence which is the main difference between text and generic objects. It is natural to consider a text line as a sequence of fine-scale text proposals, where each proposal generally represents a small part of a text line, e.g., a text piece with 16-pixel width. Each proposal may include a single or multiple strokes, a part of a character, a single or multiple characters, etc. We believe that it would be more accurate to just predict the vertical location of each proposal, by fixing its horizontal location which may be more difficult to predict. This reduces the search space, compared to the RPN which predicts 4 coordinates of an object. We develop a vertical anchor mechanism that simultaneously predicts a text/non-text score and y-axis location of each fine-scale proposal. It is also more reliable to detect a general fixed-width text proposal than identifying an isolate character, which is easily confused with part of a character or multiple characters. Furthermore, detecting a text line in a sequence of fixed-width text proposals also works reliably on text of multiple scales and multiple aspect ratios.</p>
<p>我们寻找文本的独特属性，能够很好地概括各个层次的文本组件。我们观察到由RPN进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。显然，文本行是一个序列，它是文本和通用目标之间的主要区别。将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16个像素的文本块。每个提议可能包含单个或多个笔划，字符的一部分，单个或多个字符等。我们认为，通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测。与预测目标4个坐标的RPN相比，这减少了搜索空间。我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。检测一般固定宽度的文本提议比识别分隔的字符更可靠，分隔字符容易与字符或多个字符的一部分混淆。此外，检测一系列固定宽度文本提议中的文本行也可以在多个尺度和多个长宽比的文本上可靠地工作。</p>
<p>To this end, we design the fine-scale text proposal as follow. Our detector investigates each spatial location in the <em>conv5</em> densely. A text proposal is defined to have a fixed width of 16 pixels (in the input image). This is equal to move the detector densely through the <em>conv5</em> maps, where the total stride is exactly 16 pixels. Then we design $k$ vertical anchors to  predict  $y$-coordinates for each proposal. The $k$ anchors have a same horizontal location with a fixed width of 16 pixels, but their vertical locations are varied in $k$ different heights. In our experiments, we use ten anchors for each proposal, $k=10$, whose heights are varied from 11 to 273 pixels (by $\div 0.7$ each time) in the input image. The explicit vertical coordinates are measured by the height and $y$-axis center of a proposal bounding box. We compute relative predicted vertical coordinates ($\textbf{v}$) with respect to the bounding box location of an anchor as, $$v_c=(c_y-c_y^a)/h^a, \qquad v_h=\log (h/h^a) \tag{1} $$$$v^*_c=(c^*_y-c_y^a)/h^a, \qquad v^*_h=\log (h^*/h^a)\tag{2}$$ where $\textbf{v}=\lbrace v_c,v_h \rbrace$ and $\textbf{v}^*=\lbrace v^*_c,v^*_h\rbrace$ are the relative predicted coordinates and ground truth coordinates, respectively. $c_y^a$ and $h^a$ are the center ($y$-axis) and height of the anchor box, which can be pre-computed from an input image. $c_y$ and $h$ are the predicted $y$-axis coordinates in the input image, while $c^*_y$ and $h^*$ are the ground truth coordinates. Therefore, each predicted text proposal has a bounding box with size of $h\times 16$ (in the input image), as shown in Fig. 1 (b) and Fig. 2 (right). Generally, an text proposal is largely smaller than its effective receptive field which is 228$\times$228.</p>
<p>为此，我们设计如下的细粒度文本提议。我们的检测器密集地调查了<em>conv5</em>中的每个空间位置。文本提议被定义为具有16个像素的固定宽度（在输入图像中）。这相当于在<em>conv5</em>的映射上密集地移动检测器，其中总步长恰好为16个像素。然后，我们设计$k$个垂直锚点来预测每个提议的$y$坐标。$k$个锚点具有相同的水平位置，固定宽度为16个像素，但其垂直位置在$k$个不同的高度变化。在我们的实验中，我们对每个提议使用十个锚点，$k=10$，其高度在输入图像中从11个像素变化到273个像素（每次$\div 0.7$）。明确的垂直坐标是通过提议边界框的高度和$y$轴中心来度量的。我们计算相对于锚点的边界框位置的相对预测的垂直坐标（$\textbf{v}$），如下所示：$$v_c=(c_y-c_y^a)/h^a, \qquad v_h=\log (h/h^a) \tag{1} $$$$v^*_c=(c^*_y-c_y^a)/h^a, \qquad v^*_h=\log (h^*/h^a)\tag{2}$$ 其中$\textbf{v}=\lbrace v_c,v_h \rbrace$和$\textbf{v}^*=\lbrace v^*_c,v^*_h\rbrace$分别是相对于预测坐标和实际坐标。$c_y^a$和$h^a$是锚盒的中心（$y$轴）和高度，可以从输入图像预先计算。$c_y$和$h$是输入图像中预测的$y$轴坐标，而$c^*_y$和$h^*$是实际坐标。因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为$h\times 16$的边界框（在输入图像中）。一般来说，文本提议在很大程度上要比它的有效感受野228$\times$228要小。</p>
<p>The detection processing is summarised as follow. Given an input image, we have $W \times H \times C$  <em>conv5</em> features maps (by using the VGG16 model), where $C$ is the number of feature maps or channels, and $W \times H$ is the spatial arrangement. When our detector is sliding a 3$\times$3 window densely through the conv5, each sliding-window takes a convolutional feature of $3 \times 3 \times C$ for producing the prediction. For each prediction, the horizontal location ($x$-coordinates) and $k$-anchor locations are fixed, which can be pre-computed by mapping the spatial window location in the <em>conv5</em> onto the input image. Our detector outputs the text/non-text scores and the predicted $y$-coordinates ($\textbf{v}$) for $k$ anchors at each window location. The detected text proposals are generated from the anchors having a text/non-text score of $&gt;0.7$  (with non-maximum suppression). By the designed vertical anchor and fine-scale detection strategy, our detector is able to handle text lines in a wide range of scales and aspect ratios by using a single-scale image. This further reduces its computation, and at the same time, predicting accurate localizations of the text lines. Compared to the RPN or Faster R-CNN system [25], our fine-scale detection provides more detailed supervised information that naturally leads to a more accurate detection.</p>
<p>检测处理总结如下。给定输入图像，我们有$W \times H \times C$ <em>conv5</em>特征映射（通过使用VGG16模型），其中$C$是特征映射或通道的数目，并且$W \times H$是空间布置。当我们的检测器通过conv5密集地滑动3$\times$3窗口时，每个滑动窗口使用$3 \times 3 \times C$的卷积特征来产生预测。对于每个预测，水平位置（$x$轴坐标）和$k$个锚点位置是固定的，可以通过将<em>conv5</em>中的空间窗口位置映射到输入图像上来预先计算。我们的检测器在每个窗口位置输出$k$个锚点的文本/非文本分数和预测的$y$轴坐标（$\textbf{v}$）。检测到的文本提议是从具有$&gt; 0.7 $（具有非极大值抑制）的文本/非文本分数的锚点生成的。通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。这进一步减少了计算量，同时预测了文本行的准确位置。与RPN或Faster R-CNN系统[25]相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</p>
<h3 id="3-2-Recurrent-Connectionist-Text-Proposals"><a href="#3-2-Recurrent-Connectionist-Text-Proposals" class="headerlink" title="3.2 Recurrent Connectionist Text Proposals"></a>3.2 Recurrent Connectionist Text Proposals</h3><p>To improve localization accuracy, we split a text line into a sequence of fine-scale text proposals, and predict each of them separately. Obviously, it is not robust to regard each isolated proposal independently. This may lead to a number of false detections on non-text objects which have a similar structure as text patterns, such as windows, bricks, leaves, etc. (referred as text-like outliers in [13]). It is also possible to discard some ambiguous patterns which contain weak text information. Several examples are presented in Fig. 3 (top). Text have strong sequential characteristics where the sequential context information is crucial to make a reliable decision. This has been verified by recent work [9] where a recurrent neural network (RNN) is applied to encode this context information for text recognition. Their results have shown that the sequential context information is greatly facilitate the recognition task on cropped word images.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f0023bd7bb92ba91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>Fig. 3: Top: CTPN without RNN. Bottom: CTPN with RNN connection.</p>
<h3 id="3-2-循环连接文本提议"><a href="#3-2-循环连接文本提议" class="headerlink" title="3.2 循环连接文本提议"></a>3.2 循环连接文本提议</h3><p>为了提高定位精度，我们将文本行分成一系列细粒度的文本提议，并分别预测每个文本提议。显然，将每个孤立的提议独立考虑并不鲁棒。这可能会导致对与文本模式类似的非文本目标的误检，如窗口，砖块，树叶等（在文献[13]中称为类文本异常值）。还可以丢弃一些含有弱文本信息的模糊模式。图3给出了几个例子（上）。文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。最近的工作已经证实了这一点[9]，其中应用递归神经网络（RNN）来编码用于文本识别的上下文信息。他们的结果表明，序列上下文信息极大地促进了对裁剪的单词图像的识别任务。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f0023bd7bb92ba91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3：上：没有RNN的CTPN。下：有RNN连接的CTPN。</p>
<p>Motivated from this work, we believe that this context information may also be of importance for our detection task. Our detector should be able to explore this important context information to make a more reliable decision, when it works on each individual proposal. Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and seamless in-network connection of the fine-scale text proposals. RNN provides a natural choice for encoding this information recurrently using its hidden layers. To this end, we propose to design a RNN layer upon the conv5, which takes the convolutional feature of each window as sequential inputs, and updates its internal state recurrently in the hidden layer, $H_t$, $$H_{t}=\varphi(H_{t-1}, X_t),  \qquad t=1,2,…,W \tag{3}$$ where $X_t \in R^{3\times 3 \times C}$ is the input <em>conv5</em> feature from $t$-th  sliding-window (3$\times$3). The sliding-window moves densely from left to right, resulting in $t=1,2,…,W$ sequential features for each row.  $W$ is the width of the <em>conv5</em>. $H_t$ is a recurrent internal state that is computed jointly from  both current input ($X_t$) and previous states encoded in $H_{t-1}$. The recurrence is computed by using a non-linear function $\varphi$, which defines exact form of the recurrent model. We exploit the long short-term memory (LSTM) architecture [12] for our RNN layer. The LSTM was proposed specially to address vanishing gradient problem, by introducing three additional multiplicative gates: the <em>input gate</em>, <em>forget gate</em> and <em>output gate</em>. Details can be found in [12]. Hence the internal state in RNN hidden layer accesses the sequential context information scanned by all previous windows through the recurrent connection.  We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the recurrent context in both directions, so that the connectionist receipt field is able to cover the whole image width, e.g., 228 $\times$ width. We use a 128D hidden layer for each LSTM, resulting in a 256D RNN hidden layer, $H_t \in R^{256}$.</p>
<p>受到这项工作的启发，我们认为这种上下文信息对于我们的检测任务也很重要。我们的检测器应该能够探索这些重要的上下文信息，以便在每个单独的提议中都可以做出更可靠的决策。此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。RNN提供了一种自然选择，使用其隐藏层对这些信息进行循环编码。为此，我们提出在conv5上设计一个RNN层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，$$H_{t}=\varphi(H_{t-1}, X_t),  \qquad t=1,2,…,W \tag{3}$$其中$X_t \in R^{3\times 3 \times C}$是第$t$个滑动窗口(3$\times$3)的输入<em>conv5</em>特征。滑动窗口从左向右密集移动，导致每行的$t=1,2,…,W$序列特征。$W$是<em>conv5</em>的宽度。$H_t$是从当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算的循环内部状态。递归是通过使用非线性函数$\varphi$来计算的，它定义了循环模型的确切形式。我们利用长短时记忆（LSTM）架构[12]作为我们的RNN层。通过引入三个附加乘法门：<em>输入门</em>，<em>忘记门</em>和<em>输出门</em>，专门提出了LSTM以解决梯度消失问题。细节可以在[12]中找到。因此，RNN隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如228$\times$width。我们对每个LSTM使用一个128维的隐藏层，从而产生256维的RNN隐藏层$H_t \in R^{256}$。</p>
<p>The internal state in $H_t$ is mapped to the following FC layer, and output layer for computing the predictions of the t-th proposal. Therefore, our integration with the RNN layer is elegant, resulting in an efficient model that is end-to-end trainable without additional cost. The efficiency of the RNN connection is demonstrated in Fig. 3. Obviously, it reduces false detections considerably, and at the same time, recovers many missed text proposals which contain very weak text information.</p>
<p>$H_t$中的内部状态被映射到后面的FC层，并且输出层用于计算第$t$个提议的预测。因此，我们与RNN层的集成非常优雅，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。RNN连接的功效如图3所示。显然，它大大减少了错误检测，同时还能够恢复很多包含非常弱的文本信息的遗漏文本提议。</p>
<h3 id="3-3-Side-refinement"><a href="#3-3-Side-refinement" class="headerlink" title="3.3 Side-refinement"></a>3.3 Side-refinement</h3><p>The fine-scale text proposals are detected accurately and reliably by our CTPN. Text line construction is straightforward by connecting continuous text proposals whose text/non-text score is $&gt;0.7$. Text lines are constructed as follow. First, we define a paired neighbour ($B_j$) for a proposal $B_i$ as $B_j-&gt;B_i$, when (i) $B_j$ is the nearest horizontal distance to $B_i$, and (ii) this distance is less than 50 pixels, and (iii) their vertical overlap is $&gt;0.7$. Second, two proposals are grouped into a pair, if $B_j-&gt;B_i$ and $B_i-&gt;B_j$. Then a text line is constructed by sequentially connecting the pairs having a same proposal.</p>
<h3 id="3-3-边缘细化"><a href="#3-3-边缘细化" class="headerlink" title="3.3 边缘细化"></a>3.3 边缘细化</h3><p>我们的CTPN能够准确可靠地检测细粒度的文本提议。通过连接其文本/非文本分数为$&gt;0.7$的连续文本提议，文本行的构建非常简单。文本行构建如下。首先，我们为提议$B_i$定义一个配对邻居（$B_j$）作为$B_j-&gt;B_i$，当（i）$B_j$是最接近$B_i$的水平距离，（ii）该距离小于50像素，并且（iii）它们的垂直重叠是$&gt;0.7$时。其次，如果$B_j-&gt;B_i$和$B_i-&gt;B_j$，则将两个提议分组为一对。然后通过顺序连接具有相同提议的对来构建文本行。</p>
<p>The fine-scale detection and RNN connection are able to predict accurate localizations in vertical direction. In horizontal direction, the image is divided into a sequence of equal 16-pixel width proposals. This may lead to an inaccurate localization when the text proposals in both horizontal sides are not exactly covered by a ground truth text line area, or some side proposals are discarded (e.g., having a low text score), as shown in Fig. 4. This inaccuracy may be not crucial in generic object detection, but should not be ignored in text detection, particularly for those small-scale text lines or words. To address this problem, we propose a side-refinement approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as side-anchor or side-proposal). Similar to the y-coordinate prediction, we compute relative offset as, $$o=(x_{side}-c_x^a)/w^a, \quad o^*=(x^*_{side}-c_x^a)/w^a$$ where $x_{side}$ is the predicted $x$-coordinate of the nearest horizontal side (e.g., left or right side) to current anchor. $x^*_{side}$ is the ground truth (GT) side coordinate in $x$-axis, which is pre-computed from the GT bounding box and anchor location. $c_x^a$ is the center of anchor in $x$-axis. $w^a$ is the width of anchor, which is fixed, $w^a=16$ . The side-proposals are defined as the start and end proposals when we connect a sequence of detected fine-scale text proposals into a text line. We only use the offsets of the side-proposals to refine the final text line bounding box. Several detection examples improved by side-refinement are presented in Fig. 4. The side-refinement further improves the localization accuracy, leading to about $2\%$ performance improvements on the SWT and Multi-Lingual datasets. Notice that the offset for side-refinement is predicted simultaneously by our model, as shown in Fig. 1. It is not computed from an additional post-processing step.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-af7aa618291f03f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>Fig.4: CTPN detection with (red box) and without (yellow dashed box) the side-refinement. Color of fine-scale proposal box indicate a text/non-text score.</p>
<p>细粒度的检测和RNN连接可以预测垂直方向的精确位置。在水平方向上，图像被分成一系列相等的宽度为16个像素的提议。如图4所示，当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。与y坐标预测类似，我们计算相对偏移为：$$o=(x_{side}-c_x^a)/w^a, \quad o^*=(x^*_{side}-c_x^a)/w^a$$，其中$x_{side}$是最接近水平边（例如，左边或右边）到当前锚点的预测的$x$坐标。$x^*_{side}$是$x$轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。$c_x^a$是$x$轴的锚点的中心。$w^a$是固定的锚点宽度，$w^a=16$。当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。我们只使用边缘提议的偏移量来优化最终的文本行边界框。通过边缘细化改进的几个检测示例如图4所示。边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约$2\%$。请注意，我们的模型同时预测了边缘细化的偏移量，如图1所示。它不是通过额外的后处理步骤计算的。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-af7aa618291f03f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。细粒度提议边界框的颜色表示文本/非文本分数。</p>
<h3 id="3-4-Model-Outputs-and-Loss-Functions"><a href="#3-4-Model-Outputs-and-Loss-Functions" class="headerlink" title="3.4 Model Outputs and Loss Functions"></a>3.4 Model Outputs and Loss Functions</h3><p>The proposed CTPN has three outputs which are jointly connected to the last FC layer, as shown in Fig. 1 (a). The three outputs simultaneously predict text/non-text scores ($\textbf{s}$), vertical coordinates ($\textbf{v}=\lbrace v_c, v_h\rbrace$) in E.q. (2) and side-refinement offset ($\textbf{o}$). We explore $k$ anchors to predict them on each spatial location in the <em>conv5</em>,  resulting in $2k$, $2k$ and $k$ parameters in the output layer, respectively.</p>
<h3 id="3-4-模型输出与损失函数"><a href="#3-4-模型输出与损失函数" class="headerlink" title="3.4 模型输出与损失函数"></a>3.4 模型输出与损失函数</h3><p>提出的CTPN有三个输出共同连接到最后的FC层，如图1（a）所示。这三个输出同时预测公式（2）中的文本/非文本分数（$ \ textbf {s} $），垂直坐标（$\textbf{v}=\lbrace v_c, v_h\rbrace$）和边缘细化偏移（$\textbf{o}$）。我们将探索$k$个锚点来预测它们在<em>conv5</em>中的每个空间位置，从而在输出层分别得到$2k$，$2k$和$k$个参数。</p>
<p>We employ multi-task learning to jointly optimize model parameters. We introduce three loss functions, $L^{cl}_s$, $L^{re}_v$ and $l^{re}_o$, which compute errors of text/non-text score, coordinate and side-refinement, respectively. With these considerations, we follow the multi-task loss applied in [5,25], and minimize an overall objective function ($L$) for an image as, $$L(\textbf{s}_i, \textbf{v}_j, \textbf{o}_k) =\frac1{N_{s}}\sum_iL^{cl}_{s}(\textbf{s}_i, \textbf{s}_i^*) +\frac{\lambda_1}{N_v}\sum_j L^{re}_v(\textbf{v}_j, \textbf{v}_j^*) +\frac{\lambda_2}{N_o}\sum_k L^{re}_o(\textbf{o}_k, \textbf{o}_k^*) \tag{5}$$ where each anchor is a training sample, and $i$ is the index of an anchor in a mini-batch. $\textbf{s}_i$ is the predicted probability of anchor $i$ being a true text. $\textbf{s}_i^*=\lbrace 0,1\rbrace$ is the ground truth. $j$ is the index of an anchor in the set of valid anchors for $y$-coordinates regression, which are defined as follow. A valid anchor is a defined positive anchor ($\textbf{s}_j^*=1$, described below), or has an Intersection-over-Union (IoU) $&gt;0.5$ overlap  with a ground truth text proposal. $\textbf{v}_j$ and $\textbf{v}_j^*$ are the prediction and ground truth $y$-coordinates associated with the $j$-{th} anchor. $k$ is the index of a side-anchor, which is defined as a set of anchors within a horizontal distance (e.g.,  32-pixel) to the left or right side of a ground truth text line bounding box. $\textbf{o}_k$ and $\textbf{o}_k^*$ are the predicted and ground truth offsets in $x$-axis associated to the $k$-{th} anchor. $L^{cl}_s$ is the classification loss which we use Softmax loss to distinguish text and non-text. $L^{re}_v$ and $L^{re}_o$ are the regression loss. We follow previous work by using the smooth $L_1$ function to compute them [5, 25]. $\lambda_1$ and $\lambda_2$ are loss weights to balance different tasks, which are empirically set to 1.0 and 2.0. $N_{s}$ $N_{v}$ and $N_{o}$ are normalization parameters, denoting the total number of anchors used by $L^{cl}_s$, $L^{re}_v$ and $L^{re}_o$, respectively.</p>
<p>我们采用多任务学习来联合优化模型参数。我们引入了三种损失函数：$L^{cl}_s$，$L^{re}_v$和$l^{re}_o$，其分别计算文本/非文本分数，坐标和边缘细化。考虑到这些因素，我们遵循[5，25]中应用的多任务损失，并最小化图像的总体目标函数（$L$）最小化：$$L(\textbf{s}_i, \textbf{v}_j, \textbf{o}_k) =\frac1{N_{s}}\sum_iL^{cl}_{s}(\textbf{s}_i, \textbf{s}_i^*) +\frac{\lambda_1}{N_v}\sum_j L^{re}_v(\textbf{v}_j, \textbf{v}_j^*) +\frac{\lambda_2}{N_o}\sum_k L^{re}_o(\textbf{o}_k, \textbf{o}_k^*) \tag{5}$$，其中每个锚点都是一个训练样本，$i$是一个小批量数据中一个锚点的索引。$\textbf{s}_i$是预测的锚点$i$作为实际文本的预测概率。$\textbf{s}_i^*=\lbrace 0,1\rbrace$是真实值。$j$是$y$坐标回归中有效锚点集合中锚点的索引，定义如下。有效的锚点是定义的正锚点（$\textbf{s}_j^*=1$，如下所述），或者与实际文本提议重叠的交并比（IoU）$&gt;0.5$。$\textbf{v}_j$和$\textbf{v}_j^*$是与第$j$个锚点关联的预测的和真实的$y$坐标。$k$是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第$k$个锚点关联的$x$轴的预测和实际偏移量。$L^{cl}_s$是我们使用Softmax损失区分文本和非文本的分类损失。$L^{re}_v$和$L^{re}_o$是回归损失。我们遵循以前的工作，使用平滑$L_1$函数来计算它们[5，25]。$\lambda_1$和$\lambda_2$是损失权重，用来平衡不同的任务，将它们经验地设置为1.0和2.0。$N_{s}$ $N_{v}$和$N_{o}$是标准化参数，表示$L^{cl}_s$，$L^{re}_v$，$L^{re}_o$分别使用的锚点总数。</p>
<h3 id="3-5-Training-and-Implementation-Details"><a href="#3-5-Training-and-Implementation-Details" class="headerlink" title="3.5 Training and Implementation Details"></a>3.5 Training and Implementation Details</h3><p>The CTPN can be trained end-to-end by using the standard back-propagation and stochastic gradient descent (SGD). Similar to RPN [25], training samples are the anchors, whose locations can be pre computed in input image, so that the training labels of each anchor can be computed from corresponding GT box.</p>
<h3 id="3-5-训练和实现细节"><a href="#3-5-训练和实现细节" class="headerlink" title="3.5 训练和实现细节"></a>3.5 训练和实现细节</h3><p>通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。与RPN[25]类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</p>
<p><strong>Training labels.</strong> For text/non-text classification, a binary label is assigned to each positive (text) or negative (non-text) anchor. It is defined by computing the IoU overlap with the GT bounding box (divided by anchor location). A positive anchor is defined as : (i) an anchor that has an $&gt; 0.7$ IoU overlap with any GT box; or (ii) the anchor with the highest IoU overlap with a GT box. <em>By the condition (ii), even a very small text pattern can assign a positive anchor. This is crucial to detect small-scale text patterns, which is one of key advantages of the CTPN.</em> This is different from generic object detection where the impact of condition (ii) may be not significant. The negative anchors are defined as $&lt;0.5$ IoU overlap with all GT boxes. The training labels for the $y$-coordinate regression ($\textbf{v}^*$) and offset regression ($\textbf{o}^*$) are computed as E.q. (2) and (4) respectively.</p>
<p><strong>训练标签</strong>。对于文本/非文本分类，二值标签分配给每个正（文本）锚点或负（非文本）锚点。它通过计算与实际边界框的IoU重叠（除以锚点位置）来定义。正锚点被定义为：（i）与任何实际边界框具有$&gt;0.7$的IoU重叠；或者（ii）与实际边界框具有最高IoU重叠。<em>通过条件（ii），即使是非常小的文本模式也可以分为正锚点。这对于检测小规模文本模式至关重要，这是CTPN的主要优势之一。</em>这不同于通用目标检测，通用目标检测中条件（ii）的影响可能不显著。负锚点定义为与所有实际边界框具有$&lt;0.5$的IoU重叠。$y$坐标回归（$\textbf{v}^*$）和偏移回归（$\textbf{o}^*$）的训练标签分别按公式（2）和（4）计算。</p>
<p><strong>Training data.</strong> In the training process, each mini-batch samples are collected randomly from a single image. The number of anchors for each mini-batch is fixed to  $N_s=128$, with 1:1 ratio for positive and negative samples. A mini-patch is pad with negative samples if the number of positive ones is fewer than 64. Our model was trained on 3,000 natural images, including 229 images from the ICDAR 2013 training set. We collected the other images ourselves and manually labelled them with text line bounding boxes. All self-collected training images are not overlapped with any test image in all benchmarks.  The input image is resized by setting its short side to 600 for training, while keeping its original aspect ratio.</p>
<p><strong>训练数据</strong>。在训练过程中，每个小批量样本从单张图像中随机收集。每个小批量数据的锚点数量固定为$N_s=128$，正负样本的比例为1：1。如果正样本的数量少于64，则会用小图像块填充负样本。我们的模型在3000张自然图像上训练，其中包括来自ICDAR 2013训练集的229张图像。我们自己收集了其他图像，并用文本行边界框进行了手工标注。在所有基准测试集中，所有自我收集的训练图像都不与任何测试图像重叠。为了训练，通过将输入图像的短边设置为600来调整输入图像的大小，同时保持其原始长宽比。</p>
<p><strong>Implementation Details.</strong> We follow the standard practice, and explore the very deep VGG16 model [27] pre-trained on the ImageNet data [26]. We initialize the new layers (e.g., the RNN and output layers) by using random weights with Gaussian distribution of 0 mean and 0.01 standard deviation. The model was trained end-to-end by fixing the parameters in the first two convolutional layers. We used 0.9 momentum and 0.0005 weight decay. The learning rate was set to 0.001 in the first 16K iterations, followed by another 4K iterations with 0.0001 learning rate. Our model was implemented in Caffe framework [17].</p>
<p><strong>实现细节。</strong>我们遵循标准实践，并在ImageNet数据[26]上探索预先训练的非常深的VGG16模型[27]。我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。该模型通过固定前两个卷积层中的参数进行端对端的训练。我们使用0.9的动量和0.0005的重量衰减。在前16K次迭代中，学习率被设置为0.001，随后以0.0001的学习率再进行4K次迭代。我们的模型在Caffe框架[17]中实现。</p>
<h2 id="4-Experimental-Results-and-Discussions"><a href="#4-Experimental-Results-and-Discussions" class="headerlink" title="4. Experimental Results and Discussions"></a>4. Experimental Results and Discussions</h2><p>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24]. In our experiments, we first verify the efficiency of each proposed component individually, e.g., the fine-scale text proposal detection or in-network recurrent connection. The ICDAR 2013 is used for this component evaluation.</p>
<h2 id="4-实验结果和讨论"><a href="#4-实验结果和讨论" class="headerlink" title="4. 实验结果和讨论"></a>4. 实验结果和讨论</h2><p>我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。ICDAR 2013用于该组件的评估。</p>
<h3 id="4-1-Benchmarks-and-Evaluation-Metric"><a href="#4-1-Benchmarks-and-Evaluation-Metric" class="headerlink" title="4.1 Benchmarks and Evaluation Metric"></a>4.1 Benchmarks and Evaluation Metric</h3><p>The ICDAR 2011 dataset [21] consists of 229 training images and 255 testing ones, where the images are labelled in word level. The ICDAR 2013 [19] is similar as the ICDAR 2011, and has in total 462 images, including 229 images and 233 images for training and testing, respectively. The ICDAR 2015 (Incidental Scene Text - Challenge 4) [18] includes 1,500 images which were collected by using the Google Glass. The training set has 1,000 images, and the remained 500 images are used for test. This dataset is more challenging than previous ones by including arbitrary orientation, very small-scale and low resolution text. The Multilingual scene text dataset is collected by [24]. It contains 248 images for training and 239 for testing. The images include multi-languages text, and the ground truth is labelled in text line level. Epshtein et al. [3] introduced the SWT dataset containing 307 images which include many extremely small-scale text.</p>
<h3 id="4-1-基准数据集和评估标准"><a href="#4-1-基准数据集和评估标准" class="headerlink" title="4.1 基准数据集和评估标准"></a>4.1 基准数据集和评估标准</h3><p>ICDAR 2011数据集[21]由229张训练图像和255张测试图像组成，图像以字级别标记。ICDAR 2013[19]与ICDAR 2011类似，共有462张图像，其中包括229张训练图像和233张测试图像。ICDAR 2015年（Incidental Scene Text —— Challenge 4）[18]包括使用Google Glass收集的1500张图像。训练集有1000张图像，剩余的500张图像用于测试。这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。Multilingual场景文本数据集由[24]收集。它包含248张训练图像和239张测试图像。图像包含多种语言的文字，并且真实值以文本行级别标注。Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</p>
<p>We follow previous work by using standard evaluation protocols which are provided by the dataset creators or competition organizers. For the ICDAR 2011 we use the standard protocol proposed by [30], the evaluation on the ICDAR 2013 follows the standard in [19]. For the ICDAR 2015, we used the online evaluation system provided by the organizers as in [18]. The evaluations on the SWT and Multilingual datasets follow the protocols defined in [3] and [24] respectively.</p>
<p>我们遵循以前的工作，使用由数据集创建者或竞赛组织者提供的标准评估协议。对于ICDAR 2011，我们使用[30]提出的标准协议，对ICDAR 2013的评估遵循[19]中的标准。对于ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。SWT和Multilingual数据集的评估分别遵循[3]和[24]中定义的协议。</p>
<h3 id="4-2-Fine-Scale-Text-Proposal-Network-with-Faster-R-CNN"><a href="#4-2-Fine-Scale-Text-Proposal-Network-with-Faster-R-CNN" class="headerlink" title="4.2 Fine-Scale Text Proposal Network with Faster R-CNN"></a>4.2 Fine-Scale Text Proposal Network with Faster R-CNN</h3><p>We first discuss our fine-scale detection strategy against the RPN and Faster R-CNN system [25]. As can be found in Table 1 (left), the individual RPN is difficult to perform accurate text localization, by generating a large amount of false detections (low precision). By refining the RPN proposals with a Fast R-CNN detection model [5], the Faster R-CNN system improves localization accuracy considerably, with a F-measure of 0.75. One observation is that the Faster R-CNN also increases the recall of original RPN. This may benefit from joint bounding box regression mechanism of the Fast R-CNN, which improves the accuracy of a predicted bounding box. The RPN proposals may roughly localize a major part of a text line or word, but they are not accurate enough by the ICDAR 2013 standard. Obviously, the proposed fine-scale text proposal network (FTPN) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.</p>
<p>Table 1: Component evaluation on the ICDAR 2013, and State-of-the-art results on the SWT and MULTILINGUAL.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-a4d735190f65caf2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<h3 id="4-2-具有Faster-R-CNN的细粒度文本提议网络"><a href="#4-2-具有Faster-R-CNN的细粒度文本提议网络" class="headerlink" title="4.2 具有Faster R-CNN的细粒度文本提议网络"></a>4.2 具有Faster R-CNN的细粒度文本提议网络</h3><p>我们首先讨论我们关于RPN和Faster R-CNN系统[25]的细粒度检测策略。如表1（左）所示，通过产生大量的错误检测（低精度），单独的RPN难以执行准确的文本定位。通过使用Fast R-CNN检测模型[5]完善RPN提议，Faster R-CNN系统显著提高了定位精度，其F-measure为0.75。一个观察结果是Faster R-CNN也增加了原始RPN的召回率。这可能受益于Fast R-CNN的联合边界框回归机制，其提高了预测边界框的准确性。RPN提议可以粗略定位文本行或文字的主要部分，但根据ICDAR 2013的标准这不够准确。显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</p>
<p>表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-a4d735190f65caf2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<h3 id="4-3-Recurrent-Connectionist-Text-Proposals"><a href="#4-3-Recurrent-Connectionist-Text-Proposals" class="headerlink" title="4.3 Recurrent Connectionist Text Proposals"></a>4.3 Recurrent Connectionist Text Proposals</h3><p>We discuss impact of recurrent connection on our CTPN. As shown in Fig. 3, the context information is greatly helpful to reduce false detections, such as text-like outliers. It is of great importance for recovering highly ambiguous text (e.g., extremely small-scale ones), which is one of main advantages of our CTPN, as demonstrated in Fig. 6. These appealing properties result in a significant performance boost. As shown in Table 1 (left), with our recurrent connection, the CTPN improves the FTPN substantially from a F-measure of 0.80 to 0.88.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-d36a956d9c67e6f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 6"></p>
<p>Fig.6: CTPN detection results on extremely small-scale cases (in red boxes), where some ground truth boxes are missed. Yellow boxes are the ground truth.</p>
<h3 id="4-3-循环连接文本提议"><a href="#4-3-循环连接文本提议" class="headerlink" title="4.3 循环连接文本提议"></a>4.3 循环连接文本提议</h3><p>我们讨论循环连接对CTPN的影响。如图3所示，上下文信息对于减少误检非常有用，例如类似文本的异常值。对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。这些吸引人的属性可显著提升性能。如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-d36a956d9c67e6f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 6"></p>
<p>图6：在极小尺度的情况下（红色框内）CTPN检测结果，其中一些真实边界框被遗漏。黄色边界箱是真实值。</p>
<p><strong>Running time</strong>. The implementation time of our CTPN (for whole detection processing) is about 0.14s per image with a fixed short side of 600, by using a single GPU. The CTPN without the RNN connection takes about 0.13s/image GPU time. Therefore, the proposed in-network recurrent mechanism increase model computation marginally, with considerable performance gain obtained.</p>
<p><strong>运行时间</strong>。通过使用单个GPU，我们的CTPN（用于整个检测处理）的执行时间为每张图像大约0.14s，固定短边为600。没有RNN连接的CTPN每张图像GPU时间大约需要0.13s。因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</p>
<h3 id="4-4-Comparisons-with-state-of-the-art-results"><a href="#4-4-Comparisons-with-state-of-the-art-results" class="headerlink" title="4.4 Comparisons with state-of-the-art results"></a>4.4 Comparisons with state-of-the-art results</h3><p>Our detection results on several challenging images are presented in Fig. 5. As can be found, the CTPN works perfectly on these challenging cases, some of which are difficult for many previous methods. It is able to handle multi-scale and multi-language efficiently (e.g., Chinese and Korean).</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-14f888029b71dcf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 5"></p>
<p>Fig. 5: CTPN detection results several challenging images, including multi-scale and multi-language text lines. Yellow boxes are the ground truth.</p>
<h3 id="4-4-与最新结果的比较"><a href="#4-4-与最新结果的比较" class="headerlink" title="4.4 与最新结果的比较"></a>4.4 与最新结果的比较</h3><p>我们在几个具有挑战性的图像上的检测结果如图5所示。可以发现，CTPN在这些具有挑战性的情况上可以完美的工作，其中一些对于许多以前的方法来说是困难的。它能够有效地处理多尺度和多语言（例如中文和韩文）。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-14f888029b71dcf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 5"></p>
<p>图5：CTPN在几个具有挑战性的图像上的检测结果，包括多尺度和多语言文本行。黄色边界框是真实值。</p>
<p>The full evaluation was conducted on five benchmarks. Image resolution is varied significantly in different datasets. We set short side of images to 2000 for the SWT and ICDAR 2015, and 600 for the other three. We compare our performance against recently published results in [1,28,34]. As shown in Table 1 and 2, our CTPN achieves the best performance on all five datasets. On the SWT, our improvements are significant on both recall and F-measure, with marginal gain on precision. Our detector performs favourably against the TextFlow on the Multilingual, suggesting that our method generalize well to various languages. On the ICDAR 2013, it outperforms recent TextFlow [28] and FASText [1] remarkably by improving the F-measure from 0.80 to 0.88. The gains are considerable in both precision and recall, with more than $+5\%$ and $+7\%$ improvements, respectively. In addition, we further compare our method against [8,11,35], which were published after our initial submission. It consistently obtains substantial improvements on F-measure and recall. This may due to strong capability of CTPN for detecting extremely challenging text, e.g., very small-scale ones, some of which are even difficult for human. As shown in Fig. 6, those challenging ones are detected correctly by our detector, but some of them are even missed by the GT labelling, which may reduce our precision in evaluation.</p>
<p>Table 2: State-of-the-art results on the ICDAR 2011, 2013 and 2015.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-2dd358bcd114b84e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>全面评估是在五个基准数据集上进行的。图像分辨率在不同的数据集中显著不同。我们为SWT和ICDAR 2015设置图像短边为2000，其他三个的短边为600。我们将我们的性能与最近公布的结果[1,28,34]进行了比较。如表1和表2所示，我们的CTPN在所有的五个数据集上都实现了最佳性能。在SWT上，我们的改进对于召回和F-measure都非常重要，并在精确度上取得了很小的收益。我们的检测器在Multilingual上比TextFlow表现更好，表明我们的方法能很好地泛化到各种语言。在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。精确度和召回率都有显著提高，改进分别超过$+5\%$和$+7\%$。此外，我们进一步与[8,11,35]比较了我们的方法，它们是在我们的首次提交后发布的。它始终在F-measure和召回率方面取得重大进展。这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。如图6所示，我们的检测器可以正确地检测到那些具有挑战性的图像，但有些甚至会被真实标签遗漏，这可能会降低我们的评估精度。</p>
<p>表2：ICDAR 2011，2013和2015上的最新结果。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-2dd358bcd114b84e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>We further investigate running time of various methods, as compared in Table 2. FASText [1] achieves 0.15s/image CPU time. Our method is slightly faster than it by obtaining 0.14s/image, but in GPU time. Though it is not fair to compare them directly, the GPU computation has become mainstream with recent great success of deep learning approaches on object detection [25,5,6]. Regardless of running time, our method outperforms the FASText substantially with $11\%$ improvement on F-measure. Our time can be reduced by using a smaller image scale. By using the scale of 450, it is reduced to 0.09s/image, while obtaining P/R/F of 0.92/0.77/0.84 on the ICDAR 2013, which are compared competitively against Gupta et al.’s approach [8] using 0.07s/image with GPU.</p>
<p>我们进一步调查了各种方法的运行时间，在表2中进行了比较。FASText[1]达到0.15s每张图像的CPU时间。我们的方法比它快一点，取得了0.14s每张图像，但是在GPU时间上。尽管直接比较它们是不公平的，但GPU计算已经成为主流，最近在目标检测方面的深度学习方法[25,5,6]上取得了很大成功。无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了$11%$。我们的时间可以通过使用较小的图像尺度来缩短。在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</p>
<h2 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5. Conclusions"></a>5. Conclusions</h2><p>We have presented a Connectionist Text Proposal Network (CTPN) —— an efficient text detector that is end-to-end trainable. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional maps. We develop vertical anchor mechanism that jointly predicts precise location and text/non-text score for each proposal, which is the key to realize accurate localization of text. We propose an in-network RNN layer that connects sequential text proposals elegantly, allowing it to explore meaningful context information. These key technical developments result in a powerful ability to detect highly challenging text, with less false detections. The CTPN is efficient by achieving new state-of-the-art performance on five benchmarks, with 0.14s/image running time.</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。CTPN直接在卷积映射的一系列细粒度文本提议中检测文本行。我们开发了垂直锚点机制，联合预测每个提议的精确位置和文本/非文本分数，这是实现文本准确定位的关键。我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。这些关键技术的发展带来了检测极具挑战性的文本的强大能力，同时减少了误检。通过在五个基准数据集测试中实现了最佳性能，每张图像运行时间为0.14s，CTPN是有效的。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p>Busta, M., Neumann, L., Matas, J.: Fastext: Efficient unconstrained scene text detector (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Cheng, M., Zhang, Z., Lin, W., Torr, P.: Bing: Binarized normed gradients for objectness estimation at 300fps (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Epshtein, B., Ofek, E., Wexler, Y.: Detecting text in natural scenes with stroke width transform (2010), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Everingham, M., Gool, L.V., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV) 88(2), 303–338 (2010)</p>
</li>
<li><p>Girshick, R.: Fast r-cnn (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Graves, A., Schmidhuber, J.: Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks 18(5), 602–610 (2005)</p>
</li>
<li><p>Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>He,P.,Huang,W.,Qiao,Y.,Loy,C.C.,Tang,X.:Readingscenetextindeepconvo- lutional sequences (2016), in The 30th AAAI Conference on Artificial Intelligence (AAAI-16)</p>
</li>
<li><p>He, T., Huang, W., Qiao, Y., Yao, J.: Accurate text localization in natural image with cascaded convolutional text network (2016), arXiv:1603.09423</p>
</li>
<li><p>He, T., Huang, W., Qiao, Y., Yao, J.: Text-attentional convolutional neural net- works for scene text detection. IEEE Trans. Image Processing (TIP) 25, 2529–2541 (2016)</p>
</li>
<li><p>Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Networks 9(8), 1735–1780 (1997)</p>
</li>
<li><p>Huang, W., Lin, Z., Yang, J., Wang, J.: Text localization in natural images using stroke feature transform and text covariance descriptors (2013), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolutional neural networks induced mser trees (2014), in European Conference on Computer Vision (ECCV)</p>
</li>
<li><p>Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the wild with convolutional neural networks. International Journal of Computer Vision (IJCV) (2016)</p>
</li>
<li><p>Jaderberg, M., Vedaldi, A., Zisserman, A.: Deep features for text spotting (2014), in European Conference on Computer Vision (ECCV)</p>
</li>
<li><p>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding (2014), in ACM International Conference on Multimedia (ACM MM)</p>
</li>
<li><p>Karatzas,D.,Gomez-Bigorda,L.,Nicolaou,A.,Ghosh,S.,Bagdanov,A.,Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S.,Valveny, E.: Icdar 2015 competition on robust reading (2015), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R., Mas, J., Mota, D.F., Almazan, J.A., de las Heras., L.P.: Icdar 2013 robust reading competition (2013), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Mao, J., Li, H., Zhou, W., Yan, S., Tian, Q.: Scale based region growing for scene text detection (2013), in ACM International Conference on Multimedia (ACM MM)</p>
</li>
<li><p>Minetto, R., Thome, N., Cord, M., Fabrizio, J., Marcotegui, B.: Snoopertext: A multiresolution system for text detection in complex visual scenes (2010), in IEEE International Conference on Pattern Recognition (ICIP)</p>
</li>
<li><p>Neumann, L., Matas, J.: Efficient scene text localization and recognition with local character refinement (2015), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Neumann, L., Matas, J.: Real-time lexicon-free scene text localization and recognition. In IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) (2015)</p>
</li>
<li><p>Pan, Y., Hou, X., Liu, C.: Hybrid approach to detect and localize texts in natural scene images. IEEE Trans. Image Processing (TIP) 20, 800–813 (2011)</p>
</li>
<li><p>Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks (2015), in Neural Information Processing Systems (NIPS)</p>
</li>
<li><p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Li, F.: Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV) 115(3), 211–252 (2015)</p>
</li>
<li><p>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2015), in International Conference on Learning Representation (ICLR)</p>
</li>
<li><p>Tian, S., Pan, Y., Huang, C., Lu, S., Yu, K., Tan, C.L.: Text flow: A unified text detection system in natural scene images (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition (2011), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Wolf, C., Jolion, J.: Object count / area graphs for the evaluation of object detection and segmentation algorithms. International Journal of Document Analysis 8, 280–296 (2006)</p>
</li>
<li><p>Yao, C., Bai, X., Liu, W.: A unified framework for multioriented text detection and recognition. IEEE Trans. Image Processing (TIP) 23(11), 4737–4749 (2014)</p>
</li>
<li><p>Yin, X.C., Pei, W.Y., Zhang, J., Hao, H.W.: Multi-orientation scene text detection with adaptive clustering. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 37, 1930–1937 (2015)</p>
</li>
<li><p>Yin, X.C., Yin, X., Huang, K., Hao, H.W.: Robust text detection in natural scene images. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 36, 970–983 (2014)</p>
</li>
<li><p>Zhang, Z., Shen, W., Yao, C., Bai, X.: Symmetry-based text line detection in natural scenes (2015), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Zhang, Z., Zhang, C., Shen, W., Yao, C., Liu, W., Bai, X.: Multi-oriented text de- tection with fully convolutional networks (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中文版</title>
    <link href="noahsnail.com/2018/02/02/2018-02-02-Detecting%20Text%20in%20Natural%20Image%20with%20Connectionist%20Text%20Proposal%20Network%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2018/02/02/2018-02-02-Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中文版/</id>
    <published>2018-02-02T07:14:50.000Z</published>
    <updated>2018-03-07T10:53:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<h1 id="Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network"><a href="#Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network" class="headerlink" title="Detecting Text in Natural Image with Connectionist Text Proposal Network"></a>Detecting Text in Natural Image with Connectionist Text Proposal Network</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。CTPN直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。我们开发了一个垂直锚点机制，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。这使得CTPN可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。CTPN在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。在线演示获取地址：<a href="http://textdet.com/" target="_blank" rel="external">http://textdet.com/</a>。</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>场景文本检测；卷积网络；循环神经网络；锚点机制</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在自然图像中阅读文本最近在计算机视觉中引起越来越多的关注[8，14，15，10，35，11，9，1，28，32]。这是由于它的许多实际应用，如图像OCR，多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。这项工作的重点是检测任务[14，1，28，32]，这是比在一个良好的裁剪字图像[15，9]进行的识别任务更具有挑战性。文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。</p>
<p>目前的文本检测方法大多采用自下而上的流程[28，1，14，32，33]。它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。它们的性能很大程度上依赖于字符检测的结果，并且已经提出了连接组件方法或滑动窗口方法。这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。但是，如果没有上下文信息，他们不能鲁棒的单独识别各个笔划或字符。例如，相比单个字符人们更信任一个字符序列，特别是当一个字符非常模糊时。这些限制在字符检测中通常会导致大量非文本组件，在后续步骤中的主要困难是处理它们。此外，正如[28]所指出的，这些误检很容易在自下而上的过程中连续累积。为了解决这些问题，我们利用强大的深度特征直接在卷积映射中检测文本信息。我们开发的文本锚点机制能在细粒度上精确预测文本位置。然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</p>
<p>深度卷积神经网络（CNN）最近已经基本实现了一般物体检测[25，5，6]。最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。然而，很难将这些通用目标检测系统直接应用于场景文本检测，这通常需要更高的定位精度。在通用目标检测中，每个目标都有一个明确的封闭边界[2]，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠&gt;0.5，因为人们可以容易地从目标的主要部分识别它。相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的Wolf标准[19，21]。</p>
<p>在这项工作中，我们通过将RPN架构[25]扩展到准确的文本行定义来填补这个空白。我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的CNN检测模型进行进一步的后处理。</p>
<h3 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1 贡献"></a>1.1 贡献</h3><p>我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。这克服了以前的建立在字符检测基础上的自下而上方法带来的一些主要限制。我们利用强深度卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。主要贡献如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3c885b0226d1b773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1：（a）连接文本提议网络（CTPN）的架构。我们通过VGG16模型[27]的最后一个卷积映射（conv5）密集地滑动3×3空间窗口。每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。RNN层连接到512维的全连接层，接着是输出层，联合预测$k$个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。（b）CTPN输出连续的固定宽度细粒度文本提议。每个框的颜色表示文本/非文本分数。只显示文本框正例的分数。</p>
<p>首先，我们将文本检测的问题转化为一系列细粒度的文本提议。我们开发了一个锚点回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。这背离了整个目标的RPN预测，RPN预测难以提供令人满意的定位精度。</p>
<p>其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。通过这种连接，我们的检测器可以探索文本行有意义的上下文信息，使其能够可靠地检测极具挑战性的文本。</p>
<p>第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</p>
<p>第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>文本检测</strong>。过去在场景文本检测中的工作一直以自下而上的方法为主，一般建立在笔画或字符检测上。它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。基于滑动窗口的方法通过在图像中密集地滑动多尺度窗口来检测候选字符。字符或非字符窗口通过预先训练的分类器，使用手动设计的特征[28，29]或最近的CNN特征[16]进行区分。然而，这两种方法通常都会受到较差的字符检测性能的影响，导致在接下来的组件过滤和文本行构建步骤中出现累积的错误。此外，强大地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难[1，33，14]。另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。</p>
<p><strong>目标检测。</strong>卷积神经网络（CNN）近来在通用目标检测[25，5，6]上已经取得了实质的进步。一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强CNN分类器来进一步对生成的提议进行分类和细化。生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。最近，Ren等人[25]提出了Faster R-CNN目标检测系统。他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。通过共享卷积计算RPN是快速的。然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。</p>
<h2 id="3-连接文本提议网络"><a href="#3-连接文本提议网络" class="headerlink" title="3. 连接文本提议网络"></a>3. 连接文本提议网络</h2><p>本节介绍连接文本提议网络（CTPN）的细节。它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</p>
<h3 id="3-1-在细粒度提议中检测文本"><a href="#3-1-在细粒度提议中检测文本" class="headerlink" title="3.1 在细粒度提议中检测文本"></a>3.1 在细粒度提议中检测文本</h3><p>类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的16个像素）文本提议，如图1（b）所示。</p>
<p>我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。CTPN的架构如图1（a）所示。我们使用一个小的空间窗口3×3来滑动最后的卷积层特征映射（例如，VGG16的conv5）。conv5特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为16个和228个像素。网络架构决定总步长和感受野。在卷积层中使用滑动窗口允许它共享卷积计算，这是减少昂贵的基于滑动窗口的方法的计算量的关键。</p>
<p>通常，滑动窗口方法采用多尺度窗口来检测不同尺寸的目标，其中一个窗口尺度被固定到与目标的尺寸相似。在[25]中，Ren等人提出了一种有效的锚点回归机制，允许RPN使用单尺度窗口检测多尺度目标。关键的洞察力是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和长宽比的目标。我们希望将这种有效的锚点机制扩展到我们的文本任务。然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标[2]。文本是一个没有明显封闭边界的序列。它可能包含多层次的组件，如笔划，字符，单词，文本行和文本区域等，这些组件之间没有明确区分。文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。因此，直接预测文本行或单词的位置可能很难或不可靠，因此很难获得令人满意的准确性。一个例子如图2所示，其中RPN直接被训练用于定位图像中的文本行。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1d1b210f78d93fed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2：左：RPN提议。右：细粒度的文本提议。</p>
<p>我们寻找文本的独特属性，能够很好地概括各个层次的文本组件。我们观察到由RPN进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。显然，文本行是一个序列，它是文本和通用目标之间的主要区别。将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16个像素的文本块。每个提议可能包含单个或多个笔划，字符的一部分，单个或多个字符等。我们认为，通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测。与预测目标4个坐标的RPN相比，这减少了搜索空间。我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。检测一般固定宽度的文本提议比识别分隔的字符更可靠，分隔字符容易与字符或多个字符的一部分混淆。此外，检测一系列固定宽度文本提议中的文本行也可以在多个尺度和多个长宽比的文本上可靠地工作。</p>
<p>为此，我们设计如下的细粒度文本提议。我们的检测器密集地调查了<em>conv5</em>中的每个空间位置。文本提议被定义为具有16个像素的固定宽度（在输入图像中）。这相当于在<em>conv5</em>的映射上密集地移动检测器，其中总步长恰好为16个像素。然后，我们设计$k$个垂直锚点来预测每个提议的$y$坐标。$k$个锚点具有相同的水平位置，固定宽度为16个像素，但其垂直位置在$k$个不同的高度变化。在我们的实验中，我们对每个提议使用十个锚点，$k=10$，其高度在输入图像中从11个像素变化到273个像素（每次$\div 0.7$）。明确的垂直坐标是通过提议边界框的高度和$y$轴中心来度量的。我们计算相对于锚点的边界框位置的相对预测的垂直坐标（$\textbf{v}$），如下所示：$$v_c=(c_y-c_y^a)/h^a, \qquad v_h=\log (h/h^a) \tag{1} $$$$v^*_c=(c^*_y-c_y^a)/h^a, \qquad v^*_h=\log (h^*/h^a)\tag{2}$$ 其中$\textbf{v}=\lbrace v_c,v_h \rbrace$和$\textbf{v}^*=\lbrace v^*_c,v^*_h\rbrace$分别是相对于预测坐标和实际坐标。$c_y^a$和$h^a$是锚盒的中心（$y$轴）和高度，可以从输入图像预先计算。$c_y$和$h$是输入图像中预测的$y$轴坐标，而$c^*_y$和$h^*$是实际坐标。因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为$h\times 16$的边界框（在输入图像中）。一般来说，文本提议在很大程度上要比它的有效感受野228$\times$228要小。</p>
<p>检测处理总结如下。给定输入图像，我们有$W \times H \times C$ <em>conv5</em>特征映射（通过使用VGG16模型），其中$C$是特征映射或通道的数目，并且$W \times H$是空间布置。当我们的检测器通过conv5密集地滑动3$\times$3窗口时，每个滑动窗口使用$3 \times 3 \times C$的卷积特征来产生预测。对于每个预测，水平位置（$x$轴坐标）和$k$个锚点位置是固定的，可以通过将<em>conv5</em>中的空间窗口位置映射到输入图像上来预先计算。我们的检测器在每个窗口位置输出$k$个锚点的文本/非文本分数和预测的$y$轴坐标（$\textbf{v}$）。检测到的文本提议是从具有$&gt; 0.7 $（具有非极大值抑制）的文本/非文本分数的锚点生成的。通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。这进一步减少了计算量，同时预测了文本行的准确位置。与RPN或Faster R-CNN系统[25]相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</p>
<h3 id="3-2-循环连接文本提议"><a href="#3-2-循环连接文本提议" class="headerlink" title="3.2 循环连接文本提议"></a>3.2 循环连接文本提议</h3><p>为了提高定位精度，我们将文本行分成一系列细粒度的文本提议，并分别预测每个文本提议。显然，将每个孤立的提议独立考虑并不鲁棒。这可能会导致对与文本模式类似的非文本目标的误检，如窗口，砖块，树叶等（在文献[13]中称为类文本异常值）。还可以丢弃一些含有弱文本信息的模糊模式。图3给出了几个例子（上）。文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。最近的工作已经证实了这一点[9]，其中应用递归神经网络（RNN）来编码用于文本识别的上下文信息。他们的结果表明，序列上下文信息极大地促进了对裁剪的单词图像的识别任务。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f0023bd7bb92ba91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3：上：没有RNN的CTPN。下：有RNN连接的CTPN。</p>
<p>受到这项工作的启发，我们认为这种上下文信息对于我们的检测任务也很重要。我们的检测器应该能够探索这些重要的上下文信息，以便在每个单独的提议中都可以做出更可靠的决策。此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。RNN提供了一种自然选择，使用其隐藏层对这些信息进行循环编码。为此，我们提出在conv5上设计一个RNN层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，$$H_{t}=\varphi(H_{t-1}, X_t),  \qquad t=1,2,…,W \tag{3}$$其中$X_t \in R^{3\times 3 \times C}$是第$t$个滑动窗口(3$\times$3)的输入<em>conv5</em>特征。滑动窗口从左向右密集移动，导致每行的$t=1,2,…,W$序列特征。$W$是<em>conv5</em>的宽度。$H_t$是从当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算的循环内部状态。递归是通过使用非线性函数$\varphi$来计算的，它定义了循环模型的确切形式。我们利用长短时记忆（LSTM）架构[12]作为我们的RNN层。通过引入三个附加乘法门：<em>输入门</em>，<em>忘记门</em>和<em>输出门</em>，专门提出了LSTM以解决梯度消失问题。细节可以在[12]中找到。因此，RNN隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如228$\times$width。我们对每个LSTM使用一个128维的隐藏层，从而产生256维的RNN隐藏层$H_t \in R^{256}$。</p>
<p>$H_t$中的内部状态被映射到后面的FC层，并且输出层用于计算第$t$个提议的预测。因此，我们与RNN层的集成非常优雅，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。RNN连接的功效如图3所示。显然，它大大减少了错误检测，同时还能够恢复很多包含非常弱的文本信息的遗漏文本提议。</p>
<h3 id="3-3-边缘细化"><a href="#3-3-边缘细化" class="headerlink" title="3.3 边缘细化"></a>3.3 边缘细化</h3><p>我们的CTPN能够准确可靠地检测细粒度的文本提议。通过连接其文本/非文本分数为$&gt;0.7$的连续文本提议，文本行的构建非常简单。文本行构建如下。首先，我们为提议$B_i$定义一个配对邻居（$B_j$）作为$B_j-&gt;B_i$，当（i）$B_j$是最接近$B_i$的水平距离，（ii）该距离小于50像素，并且（iii）它们的垂直重叠是$&gt;0.7$时。其次，如果$B_j-&gt;B_i$和$B_i-&gt;B_j$，则将两个提议分组为一对。然后通过顺序连接具有相同提议的对来构建文本行。</p>
<p>细粒度的检测和RNN连接可以预测垂直方向的精确位置。在水平方向上，图像被分成一系列相等的宽度为16个像素的提议。如图4所示，当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。与y坐标预测类似，我们计算相对偏移为：$$o=(x_{side}-c_x^a)/w^a, \quad o^*=(x^*_{side}-c_x^a)/w^a$$，其中$x_{side}$是最接近水平边（例如，左边或右边）到当前锚点的预测的$x$坐标。$x^*_{side}$是$x$轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。$c_x^a$是$x$轴的锚点的中心。$w^a$是固定的锚点宽度，$w^a=16$。当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。我们只使用边缘提议的偏移量来优化最终的文本行边界框。通过边缘细化改进的几个检测示例如图4所示。边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约$2\%$。请注意，我们的模型同时预测了边缘细化的偏移量，如图1所示。它不是通过额外的后处理步骤计算的。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-af7aa618291f03f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。细粒度提议边界框的颜色表示文本/非文本分数。</p>
<h3 id="3-4-模型输出与损失函数"><a href="#3-4-模型输出与损失函数" class="headerlink" title="3.4 模型输出与损失函数"></a>3.4 模型输出与损失函数</h3><p>提出的CTPN有三个输出共同连接到最后的FC层，如图1（a）所示。这三个输出同时预测公式（2）中的文本/非文本分数（$ \ textbf {s} $），垂直坐标（$\textbf{v}=\lbrace v_c, v_h\rbrace$）和边缘细化偏移（$\textbf{o}$）。我们将探索$k$个锚点来预测它们在<em>conv5</em>中的每个空间位置，从而在输出层分别得到$2k$，$2k$和$k$个参数。</p>
<p>我们采用多任务学习来联合优化模型参数。我们引入了三种损失函数：$L^{cl}_s$，$L^{re}_v$和$l^{re}_o$，其分别计算文本/非文本分数，坐标和边缘细化。考虑到这些因素，我们遵循[5，25]中应用的多任务损失，并最小化图像的总体目标函数（$L$）最小化：$$L(\textbf{s}_i, \textbf{v}_j, \textbf{o}_k) =\frac1{N_{s}}\sum_iL^{cl}_{s}(\textbf{s}_i, \textbf{s}_i^*) +\frac{\lambda_1}{N_v}\sum_j L^{re}_v(\textbf{v}_j, \textbf{v}_j^*) +\frac{\lambda_2}{N_o}\sum_k L^{re}_o(\textbf{o}_k, \textbf{o}_k^*) $$，其中每个锚点都是一个训练样本，$i$是一个小批量数据中一个锚点的索引。$\textbf{s}_i$是预测的锚点$i$作为实际文本的预测概率。$\textbf{s}_i^*=\lbrace 0,1\rbrace$是真实值。$j$是$y$坐标回归中有效锚点集合中锚点的索引，定义如下。有效的锚点是定义的正锚点（$\textbf{s}_j^*=1$，如下所述），或者与实际文本提议重叠的交并比（IoU）$&gt;0.5$。$\textbf{v}_j$和$\textbf{v}_j^*$是与第$j$个锚点关联的预测的和真实的$y$坐标。$k$是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第$k$个锚点关联的$x$轴的预测和实际偏移量。$L^{cl}_s$是我们使用Softmax损失区分文本和非文本的分类损失。$L^{re}_v$和$L^{re}_o$是回归损失。我们遵循以前的工作，使用平滑$L_1$函数来计算它们[5，25]。$\lambda_1$和$\lambda_2$是损失权重，用来平衡不同的任务，将它们经验地设置为1.0和2.0。$N_{s}$ $N_{v}$和$N_{o}$是标准化参数，表示$L^{cl}_s$，$L^{re}_v$，$L^{re}_o$分别使用的锚点总数。</p>
<h3 id="3-5-训练和实现细节"><a href="#3-5-训练和实现细节" class="headerlink" title="3.5 训练和实现细节"></a>3.5 训练和实现细节</h3><p>通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。与RPN[25]类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</p>
<p><strong>训练标签</strong>。对于文本/非文本分类，二值标签分配给每个正（文本）锚点或负（非文本）锚点。它通过计算与实际边界框的IoU重叠（除以锚点位置）来定义。正锚点被定义为：（i）与任何实际边界框具有$&gt;0.7$的IoU重叠；或者（ii）与实际边界框具有最高IoU重叠。<em>通过条件（ii），即使是非常小的文本模式也可以分为正锚点。这对于检测小规模文本模式至关重要，这是CTPN的主要优势之一。</em>这不同于通用目标检测，通用目标检测中条件（ii）的影响可能不显著。负锚点定义为与所有实际边界框具有$&lt;0.5$的IoU重叠。$y$坐标回归（$\textbf{v}^*$）和偏移回归（$\textbf{o}^*$）的训练标签分别按公式（2）和（4）计算。</p>
<p><strong>训练数据</strong>。在训练过程中，每个小批量样本从单张图像中随机收集。每个小批量数据的锚点数量固定为$N_s=128$，正负样本的比例为1：1。如果正样本的数量少于64，则会用小图像块填充负样本。我们的模型在3000张自然图像上训练，其中包括来自ICDAR 2013训练集的229张图像。我们自己收集了其他图像，并用文本行边界框进行了手工标注。在所有基准测试集中，所有自我收集的训练图像都不与任何测试图像重叠。为了训练，将输入图像的短边设置为600来调整输入图像的大小，同时保持其原始长宽比。</p>
<p><strong>实现细节。</strong>我们遵循标准实践，并在ImageNet数据[26]上探索预先训练的非常深的VGG16模型[27]。我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。该模型通过固定前两个卷积层中的参数进行端对端的训练。我们使用0.9的动量和0.0005的重量衰减。在前16K次迭代中，学习率被设置为0.001，随后以0.0001的学习率再进行4K次迭代。我们的模型在Caffe框架[17]中实现。</p>
<h2 id="4-实验结果和讨论"><a href="#4-实验结果和讨论" class="headerlink" title="4. 实验结果和讨论"></a>4. 实验结果和讨论</h2><p>我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。ICDAR 2013用于该组件的评估。</p>
<h3 id="4-1-基准数据集和评估标准"><a href="#4-1-基准数据集和评估标准" class="headerlink" title="4.1 基准数据集和评估标准"></a>4.1 基准数据集和评估标准</h3><p>ICDAR 2011数据集[21]由229张训练图像和255张测试图像组成，图像以字级别标记。ICDAR 2013[19]与ICDAR 2011类似，共有462张图像，其中包括229张训练图像和233张测试图像。ICDAR 2015年（Incidental Scene Text —— Challenge 4）[18]包括使用Google Glass收集的1500张图像。训练集有1000张图像，剩余的500张图像用于测试。这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。Multilingual场景文本数据集由[24]收集。它包含248张训练图像和239张测试图像。图像包含多种语言的文字，并且真实值以文本行级别标注。Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</p>
<p>我们遵循以前的工作，使用由数据集创建者或竞赛组织者提供的标准评估协议。对于ICDAR 2011，我们使用[30]提出的标准协议，对ICDAR 2013的评估遵循[19]中的标准。对于ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。SWT和Multilingual数据集的评估分别遵循[3]和[24]中定义的协议。</p>
<h3 id="4-2-具有Faster-R-CNN的细粒度文本提议网络"><a href="#4-2-具有Faster-R-CNN的细粒度文本提议网络" class="headerlink" title="4.2 具有Faster R-CNN的细粒度文本提议网络"></a>4.2 具有Faster R-CNN的细粒度文本提议网络</h3><p>我们首先讨论我们关于RPN和Faster R-CNN系统[25]的细粒度检测策略。如表1（左）所示，通过产生大量的错误检测（低精度），单独的RPN难以执行准确的文本定位。通过使用Fast R-CNN检测模型[5]完善RPN提议，Faster R-CNN系统显著提高了定位精度，其F-measure为0.75。一个观察结果是Faster R-CNN也增加了原始RPN的召回率。这可能受益于Fast R-CNN的联合边界框回归机制，其提高了预测边界框的准确性。RPN提议可以粗略定位文本行或文字的主要部分，但根据ICDAR 2013的标准这不够准确。显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</p>
<p>表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-a4d735190f65caf2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<h3 id="4-3-循环连接文本提议"><a href="#4-3-循环连接文本提议" class="headerlink" title="4.3 循环连接文本提议"></a>4.3 循环连接文本提议</h3><p>我们讨论循环连接对CTPN的影响。如图3所示，上下文信息对于减少误检非常有用，例如类似文本的异常值。对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。这些吸引人的属性可显著提升性能。如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-d36a956d9c67e6f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 6"></p>
<p>图6：在极小尺度的情况下（红色框内）CTPN检测结果，其中一些真实边界框被遗漏。黄色边界箱是真实值。</p>
<p><strong>运行时间</strong>。通过使用单个GPU，我们的CTPN（用于整个检测处理）的执行时间为每张图像大约0.14s，固定短边为600。没有RNN连接的CTPN每张图像GPU时间大约需要0.13s。因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</p>
<h3 id="4-4-与最新结果的比较"><a href="#4-4-与最新结果的比较" class="headerlink" title="4.4 与最新结果的比较"></a>4.4 与最新结果的比较</h3><p>我们在几个具有挑战性的图像上的检测结果如图5所示。可以发现，CTPN在这些具有挑战性的情况上可以完美的工作，其中一些对于许多以前的方法来说是困难的。它能够有效地处理多尺度和多语言（例如中文和韩文）。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-14f888029b71dcf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 5"></p>
<p>图5：CTPN在几个具有挑战性的图像上的检测结果，包括多尺度和多语言文本行。黄色边界框是真实值。</p>
<p>全面评估是在五个基准数据集上进行的。图像分辨率在不同的数据集中显著不同。我们为SWT和ICDAR 2015设置图像短边为2000，其他三个的短边为600。我们将我们的性能与最近公布的结果[1,28,34]进行了比较。如表1和表2所示，我们的CTPN在所有的五个数据集上都实现了最佳性能。在SWT上，我们的改进对于召回和F-measure都非常重要，并在精确度上取得了很小的收益。我们的检测器在Multilingual上比TextFlow表现更好，表明我们的方法能很好地泛化到各种语言。在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。精确度和召回率都有显著提高，改进分别超过$+5\%$和$+7\%$。此外，我们进一步与[8,11,35]比较了我们的方法，它们是在我们的首次提交后发布的。它始终在F-measure和召回率方面取得重大进展。这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。如图6所示，我们的检测器可以正确地检测到那些具有挑战性的图像，但有些甚至会被真实标签遗漏，这可能会降低我们的评估精度。</p>
<p>表2：ICDAR 2011，2013和2015上的最新结果。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-2dd358bcd114b84e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>我们进一步调查了各种方法的运行时间，在表2中进行了比较。FASText[1]达到0.15s每张图像的CPU时间。我们的方法比它快一点，取得了0.14s每张图像，但是在GPU时间上。尽管直接比较它们是不公平的，但GPU计算已经成为主流，最近在目标检测方面的深度学习方法[25,5,6]上取得了很大成功。无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了$11%$。我们的时间可以通过使用较小的图像尺度来缩短。在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。CTPN直接在卷积映射的一系列细粒度文本提议中检测文本行。我们开发了垂直锚点机制，联合预测每个提议的精确位置和文本/非文本分数，这是实现文本准确定位的关键。我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。这些关键技术的发展带来了检测极具挑战性的文本的强大能力，同时减少了误检。通过在五个基准数据集测试中实现了最佳性能，每张图像运行时间为0.14s，CTPN是有效的。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p>Busta, M., Neumann, L., Matas, J.: Fastext: Efficient unconstrained scene text detector (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Cheng, M., Zhang, Z., Lin, W., Torr, P.: Bing: Binarized normed gradients for objectness estimation at 300fps (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Epshtein, B., Ofek, E., Wexler, Y.: Detecting text in natural scenes with stroke width transform (2010), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Everingham, M., Gool, L.V., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV) 88(2), 303–338 (2010)</p>
</li>
<li><p>Girshick, R.: Fast r-cnn (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Graves, A., Schmidhuber, J.: Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks 18(5), 602–610 (2005)</p>
</li>
<li><p>Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>He,P.,Huang,W.,Qiao,Y.,Loy,C.C.,Tang,X.:Readingscenetextindeepconvo- lutional sequences (2016), in The 30th AAAI Conference on Artificial Intelligence (AAAI-16)</p>
</li>
<li><p>He, T., Huang, W., Qiao, Y., Yao, J.: Accurate text localization in natural image with cascaded convolutional text network (2016), arXiv:1603.09423</p>
</li>
<li><p>He, T., Huang, W., Qiao, Y., Yao, J.: Text-attentional convolutional neural net- works for scene text detection. IEEE Trans. Image Processing (TIP) 25, 2529–2541 (2016)</p>
</li>
<li><p>Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Networks 9(8), 1735–1780 (1997)</p>
</li>
<li><p>Huang, W., Lin, Z., Yang, J., Wang, J.: Text localization in natural images using stroke feature transform and text covariance descriptors (2013), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolutional neural networks induced mser trees (2014), in European Conference on Computer Vision (ECCV)</p>
</li>
<li><p>Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the wild with convolutional neural networks. International Journal of Computer Vision (IJCV) (2016)</p>
</li>
<li><p>Jaderberg, M., Vedaldi, A., Zisserman, A.: Deep features for text spotting (2014), in European Conference on Computer Vision (ECCV)</p>
</li>
<li><p>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding (2014), in ACM International Conference on Multimedia (ACM MM)</p>
</li>
<li><p>Karatzas,D.,Gomez-Bigorda,L.,Nicolaou,A.,Ghosh,S.,Bagdanov,A.,Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S.,Valveny, E.: Icdar 2015 competition on robust reading (2015), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R., Mas, J., Mota, D.F., Almazan, J.A., de las Heras., L.P.: Icdar 2013 robust reading competition (2013), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Mao, J., Li, H., Zhou, W., Yan, S., Tian, Q.: Scale based region growing for scene text detection (2013), in ACM International Conference on Multimedia (ACM MM)</p>
</li>
<li><p>Minetto, R., Thome, N., Cord, M., Fabrizio, J., Marcotegui, B.: Snoopertext: A multiresolution system for text detection in complex visual scenes (2010), in IEEE International Conference on Pattern Recognition (ICIP)</p>
</li>
<li><p>Neumann, L., Matas, J.: Efficient scene text localization and recognition with local character refinement (2015), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Neumann, L., Matas, J.: Real-time lexicon-free scene text localization and recognition. In IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) (2015)</p>
</li>
<li><p>Pan, Y., Hou, X., Liu, C.: Hybrid approach to detect and localize texts in natural scene images. IEEE Trans. Image Processing (TIP) 20, 800–813 (2011)</p>
</li>
<li><p>Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks (2015), in Neural Information Processing Systems (NIPS)</p>
</li>
<li><p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Li, F.: Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV) 115(3), 211–252 (2015)</p>
</li>
<li><p>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2015), in International Conference on Learning Representation (ICLR)</p>
</li>
<li><p>Tian, S., Pan, Y., Huang, C., Lu, S., Yu, K., Tan, C.L.: Text flow: A unified text detection system in natural scene images (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition (2011), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Wolf, C., Jolion, J.: Object count / area graphs for the evaluation of object detection and segmentation algorithms. International Journal of Document Analysis 8, 280–296 (2006)</p>
</li>
<li><p>Yao, C., Bai, X., Liu, W.: A unified framework for multioriented text detection and recognition. IEEE Trans. Image Processing (TIP) 23(11), 4737–4749 (2014)</p>
</li>
<li><p>Yin, X.C., Pei, W.Y., Zhang, J., Hao, H.W.: Multi-orientation scene text detection with adaptive clustering. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 37, 1930–1937 (2015)</p>
</li>
<li><p>Yin, X.C., Yin, X., Huang, K., Hao, H.W.: Robust text detection in natural scene images. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 36, 970–983 (2014)</p>
</li>
<li><p>Zhang, Z., Shen, W., Yao, C., Bai, X.: Symmetry-based text line detection in natural scenes (2015), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Zhang, Z., Zhang, C., Shen, W., Yao, C., Liu, W., Bai, X.: Multi-oriented text de- tection with fully convolutional networks (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——线性回归从零开始</title>
    <link href="noahsnail.com/2018/01/31/2018-01-31-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B/"/>
    <id>noahsnail.com/2018/01/31/2018-01-31-动手学深度学习(一)——线性回归从零开始/</id>
    <published>2018-01-31T10:10:57.000Z</published>
    <updated>2018-02-01T02:04:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><p>ndarray文档<br><a href="https://mxnet.incubator.apache.org/api/python/ndarray.html" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/ndarray.html</a></p>
</li>
<li><p>yield用法<br><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/" target="_blank" rel="external">https://www.ibm.com/developerworks/cn/opensource/os-cn-python-yield/</a></p>
</li>
<li><p>matplotlib用法<br><a href="https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.plot.html" target="_blank" rel="external">https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.plot.html</a><br><a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html" target="_blank" rel="external">https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html</a></p>
</li>
<li><p>指数平滑法<br><a href="http://blog.csdn.net/tz_zs/article/details/78341306" target="_blank" rel="external">http://blog.csdn.net/tz_zs/article/details/78341306</a><br><a href="http://blog.csdn.net/cl1143015961/article/details/41081183" target="_blank" rel="external">http://blog.csdn.net/cl1143015961/article/details/41081183</a></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet的ndarray, autograd</span></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div></pre></td></tr></table></figure>
<h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练数据的维度</span></div><div class="line">num_inputs = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># 训练数据的样本数量</span></div><div class="line">num_examples = <span class="number">1000</span></div><div class="line"></div><div class="line"><span class="comment"># 实际的权重w</span></div><div class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</div><div class="line"></div><div class="line"><span class="comment"># 实际的偏置b</span></div><div class="line">true_b = <span class="number">4.2</span></div><div class="line"></div><div class="line"><span class="comment"># 随机生成均值为0, 方差为1, 服从正态分布的训练数据X, </span></div><div class="line">X = nd.random_normal(shape=(num_examples, num_inputs))</div><div class="line"></div><div class="line"><span class="comment"># 根据X, w, b生成对应的输出y</span></div><div class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_b </div><div class="line"></div><div class="line"><span class="comment"># 给y加上随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div></pre></td></tr></table></figure>
<h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(X[<span class="number">0</span>], y[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<pre><code>(
[ 1.16307867  0.48380461]
&lt;NDArray 2 @cpu(0)&gt;, 
[ 4.87962484]
&lt;NDArray 1 @cpu(0)&gt;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 绘制数据的散点图 </span></div><div class="line">plt.scatter(X[:, <span class="number">1</span>].asnumpy(), y.asnumpy())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-6c96e43fde1069ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="comment"># 训练时的批数据大小</span></div><div class="line">batch_size = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 通过yield进行数据读取</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 产生样本的索引</span></div><div class="line">    idx = list(range(num_examples))</div><div class="line">    <span class="comment"># 将索引随机打乱</span></div><div class="line">    random.shuffle(idx)</div><div class="line">    <span class="comment"># 迭代一个epoch, xrange循环时效率比range更高</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">0</span>, num_examples, batch_size):</div><div class="line">        <span class="comment"># 依次取出样本的索引, 这种实现方式在num_examples/batch_size不能整除时也适用</span></div><div class="line">        j = nd.array(idx[i:min((i + batch_size), num_examples)])</div><div class="line">        <span class="comment"># 根据提供的索引取元素</span></div><div class="line">        <span class="keyword">yield</span> nd.take(X, j), nd.take(y, j)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 查看data_iter是否是generator函数</span></div><div class="line"><span class="keyword">from</span> inspect <span class="keyword">import</span> isgeneratorfunction </div><div class="line"><span class="keyword">print</span> isgeneratorfunction(data_iter)</div><div class="line"></div><div class="line"><span class="comment"># data_iter类似于类的定义, 而data_iter()相当于一个类的实例, 当然是匿名实例</span></div><div class="line"><span class="keyword">import</span> types </div><div class="line"><span class="keyword">print</span> isinstance(data_iter(), types.GeneratorType)</div><div class="line"></div><div class="line"><span class="comment"># 读取数据测试</span></div><div class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter():</div><div class="line">    print(data, label)</div><div class="line">    <span class="keyword">break</span></div></pre></td></tr></table></figure>
<pre><code>True
True
(
[[ 1.18770552 -0.46362698]
 [-3.15577412  2.19352984]
 [-0.45067298 -0.96665388]
 [ 0.05416773 -1.21203637]
 [-1.49418294 -1.61555624]
 [-0.93778831 -1.69338322]
 [ 0.91439158  1.31797135]
 [ 0.82403505  0.33020774]
 [-0.19660901  1.13431609]
 [ 0.15364595  1.01133049]]
&lt;NDArray 10x2 @cpu(0)&gt;, 
[ 8.17057896 -9.57918072  6.58949089  8.41831684  6.69815683  8.08473206
  1.54548573  4.73358202 -0.0632825   1.06603777]
&lt;NDArray 10 @cpu(0)&gt;)
</code></pre><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 随机初始化权重w</span></div><div class="line">w = nd.random_normal(shape=(num_inputs, <span class="number">1</span>))</div><div class="line"><span class="comment"># 偏置b初始化为0</span></div><div class="line">b = nd.zeros((<span class="number">1</span>,))</div><div class="line"><span class="comment"># w, b放入list里</span></div><div class="line">params = [w, b]</div><div class="line"></div><div class="line"><span class="comment"># 需要计算反向传播, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义运算y = w * x + b</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># 向量, 矩阵乘用dot</span></div><div class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</div></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义平方损失</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span><span class="params">(yhat, y)</span>:</span></div><div class="line">    <span class="comment"># 注意这里我们把y变形成yhat的形状来避免矩阵形状的自动转换</span></div><div class="line">    <span class="comment"># loss为预测值减去真实值</span></div><div class="line">    <span class="keyword">return</span> (yhat - y.reshape(yhat.shape)) ** <span class="number">2</span></div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义随机梯度下降法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></div><div class="line">    <span class="comment"># 对参数进行梯度下降</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        <span class="comment"># 这样写不会创建新的param, 而是会写在原来的param里, 新的param没有梯度</span></div><div class="line">        param[:] = param - lr * param.grad</div></pre></td></tr></table></figure>
<h2 id="数据可视化-1"><a href="#数据可视化-1" class="headerlink" title="数据可视化"></a>数据可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">real_fn</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] - true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_b</div><div class="line"></div><div class="line"><span class="comment"># 绘制损失随训练迭代次数变化的折线图，以及预测值和真实值的散点图</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(losses, X, sample_size=<span class="number">100</span>)</span>:</span></div><div class="line">    xs = list(range(len(losses)))</div><div class="line">    <span class="comment"># 绘制两个子图</span></div><div class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">    <span class="comment"># 子图一设置标题</span></div><div class="line">    ax1.set_title(<span class="string">'Loss during training'</span>)</div><div class="line">    <span class="comment"># 绘制loss图像, 蓝色实线</span></div><div class="line">    ax1.plot(xs, losses, <span class="string">'-b'</span>)</div><div class="line">    <span class="comment"># 子图二设置标题</span></div><div class="line">    ax2.set_title(<span class="string">'Estimated vs Real Function'</span>)</div><div class="line">    <span class="comment"># 绘制预测值, 蓝色的小圈</span></div><div class="line">    ax2.plot(X[:sample_size, <span class="number">0</span>].asnumpy(), net(X[:sample_size, :]).asnumpy(), <span class="string">'ob'</span>, label = <span class="string">'Estimated'</span>)</div><div class="line">    <span class="comment"># 绘制实际值, 绿色的星号</span></div><div class="line">    ax2.plot(X[:sample_size, <span class="number">0</span>].asnumpy(), real_fn(X[:sample_size, :]).asnumpy(), <span class="string">'*g'</span>, label = <span class="string">'Real Value'</span>)</div><div class="line">    <span class="comment"># 绘制图例</span></div><div class="line">    ax2.legend()</div><div class="line">    <span class="comment"># 显示图像</span></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练的迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"><span class="comment"># 定义学习率</span></div><div class="line">learning_rate = <span class="number">0.01</span></div><div class="line"><span class="comment"># 迭代次数</span></div><div class="line">niter = <span class="number">0</span></div><div class="line"><span class="comment"># 保存loss</span></div><div class="line">losses = []</div><div class="line"><span class="comment"># 移动平均损失（加权）</span></div><div class="line">moving_loss = <span class="number">0</span></div><div class="line"><span class="comment"># 指数平滑系数</span></div><div class="line">smoothing_constant = <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</div><div class="line">    <span class="comment"># 总的loss</span></div><div class="line">    total_loss = <span class="number">0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter():</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算预测值</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算loss</span></div><div class="line">            loss = square_loss(output, label)</div><div class="line">        <span class="comment"># 根据loss进行反向传播计算梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 使用随机梯度下降求解(BSGD)</span></div><div class="line">        SGD(params, learning_rate)</div><div class="line">        <span class="comment"># 计算总的loss</span></div><div class="line">        total_loss += nd.sum(loss).asscalar()</div><div class="line">        </div><div class="line">        <span class="comment"># 记录每读取一个数据点后，损失的移动平均值的变化</span></div><div class="line">        <span class="comment"># 迭代次数加一</span></div><div class="line">        niter += <span class="number">1</span></div><div class="line">        <span class="comment"># 计算当前损失</span></div><div class="line">        current_loss = nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 计算移动平均损失，指数平滑方法</span></div><div class="line">        moving_loss = (<span class="number">1</span> - smoothing_constant) * moving_loss + smoothing_constant * current_loss</div><div class="line">        <span class="comment"># 计算估计损失</span></div><div class="line">        est_loss = moving_loss / (<span class="number">1</span> - (<span class="number">1</span> - smoothing_constant) ** niter)</div><div class="line">        </div><div class="line">        <span class="comment"># 输出迭代信息</span></div><div class="line">        <span class="keyword">if</span> (niter + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            <span class="comment"># 保存估计损失</span></div><div class="line">            losses.append(est_loss)</div><div class="line">            <span class="keyword">print</span> <span class="string">'Epoch %s, batch %s. Moving average of loss: %s. Average loss: %f'</span> % (epoch, niter, est_loss, total_loss / num_examples)</div><div class="line">            plot(losses, X)</div></pre></td></tr></table></figure>
<pre><code>Epoch 0, batch 99. Moving average of loss: 0.378590331091. Average loss: 0.625015
</code></pre><p><img src="http://upload-images.jianshu.io/upload_images/3232548-fee287ab3b5ee00d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Epoch 0"></p>
<pre><code>Epoch 1, batch 199. Moving average of loss: 0.10108379838. Average loss: 0.000099
</code></pre><p><img src="http://upload-images.jianshu.io/upload_images/3232548-717702568291112c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Epoch 1"></p>
<pre><code>Epoch 2, batch 299. Moving average of loss: 0.033726038259. Average loss: 0.000099
</code></pre><p><img src="http://upload-images.jianshu.io/upload_images/3232548-22f698714a953be8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Epoch 2"></p>
<pre><code>Epoch 3, batch 399. Moving average of loss: 0.0120152144263. Average loss: 0.000099
</code></pre><p><img src="http://upload-images.jianshu.io/upload_images/3232548-69d060709150ed87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Epoch 3"></p>
<pre><code>Epoch 4, batch 499. Moving average of loss: 0.00441111205064. Average loss: 0.000101
</code></pre><p><img src="http://upload-images.jianshu.io/upload_images/3232548-c0e3a8bd94c0537a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Epoch 4"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> w </div><div class="line"><span class="keyword">print</span> true_w</div><div class="line"><span class="keyword">print</span> b</div><div class="line"><span class="keyword">print</span> true_b</div></pre></td></tr></table></figure>
<pre><code>[[ 1.99982905]
 [-3.40232825]]
&lt;NDArray 2x1 @cpu(0)&gt;
[2, -3.4]

[ 4.20024347]
&lt;NDArray 1 @cpu(0)&gt;
4.2
</code></pre><h2 id="其他学习率"><a href="#其他学习率" class="headerlink" title="其他学习率"></a>其他学习率</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">learning_rate = 0.001</div><div class="line"></div><div class="line">Epoch 0, batch 99. Moving average of loss: 4.20676625843. Average loss: 5.549237</div><div class="line">Epoch 1, batch 199. Moving average of loss: 1.1782055765. Average loss: 0.098550</div><div class="line">Epoch 2, batch 299. Moving average of loss: 0.393321947036. Average loss: 0.001857</div><div class="line">Epoch 3, batch 399. Moving average of loss: 0.13944143045. Average loss: 0.000127</div><div class="line">Epoch 4, batch 499. Moving average of loss: 0.0505110244825. Average loss: 0.000096</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">learning_rate = 0.1</div><div class="line"></div><div class="line">Epoch 0, batch 99. Moving average of loss: 3.79341099229e+13. Average loss: 26080307360862.457031</div><div class="line">Epoch 1, batch 199. Moving average of loss: 1.7174457145e+28. Average loss: 15303785876879711197739352064.000000</div><div class="line">Epoch 2, batch 299. Moving average of loss: nan. Average loss: nan</div><div class="line">Epoch 3, batch 399. Moving average of loss: nan. Average loss: nan</div><div class="line">Epoch 4, batch 499. Moving average of loss: nan. Average loss: nan</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">learning_rate = 1</div><div class="line"></div><div class="line">Epoch 0, batch 99. Moving average of loss: nan. Average loss: nan</div><div class="line">Epoch 1, batch 199. Moving average of loss: nan. Average loss: nan</div><div class="line">Epoch 2, batch 299. Moving average of loss: nan. Average loss: nan</div><div class="line">Epoch 3, batch 399. Moving average of loss: nan. Average loss: nan</div><div class="line">Epoch 4, batch 499. Moving average of loss: nan. Average loss: nan</div></pre></td></tr></table></figure>
<h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——线性回归从零开始
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>R-FCN论文翻译——中文版</title>
    <link href="noahsnail.com/2018/01/22/2018-01-22-R-FCN%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2018/01/22/2018-01-22-R-FCN论文翻译——中文版/</id>
    <published>2018-01-22T08:49:02.000Z</published>
    <updated>2018-01-30T02:15:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<h1 id="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks"><a href="#R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks" class="headerlink" title="R-FCN: Object Detection via Region-based Fully Convolutional Networks"></a>R-FCN: Object Detection via Region-based Fully Convolutional Networks</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了基于区域的全卷积网络，以实现准确和高效的目标检测。与先前的基于区域的检测器（如Fast/Faster R-CNN [6，18]）相比，这些检测器应用昂贵的每个区域子网络数百次，我们的基于区域的检测器是全卷积的，几乎所有计算都在整张图像上共享。为了实现这一目标，我们提出了位置敏感分数图，以解决图像分类中的平移不变性与目标检测中的平移变化之间的困境。因此，我们的方法可以自然地采用全卷积图像分类器的主干网络，如最新的残差网络（ResNets）[9]，用于目标检测。我们使用101层ResNet在PASCAL VOC数据集上展示了具有竞争力的结果（例如，2007数据集上$83.6\%$的mAP）。同时，我们的测试结果是以每张图像170ms的测试速度实现的，比Faster R-CNN对应部分速度快2.5-20倍。代码公开发布在：[<a href="https://github.com/daijifeng001/r-fcn](https://github.com/daijifeng001/r-fcn）。" target="_blank" rel="external">https://github.com/daijifeng001/r-fcn](https://github.com/daijifeng001/r-fcn）。</a></p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>流行的目标检测深度网络家族[8，6，18]通过感兴趣区域（RoI）池化层[6]可以划分成两个子网络：（1）独立于RoI的共享“全卷积”子网络，（ii）不共享计算的RoI子网络。这种分解[8]以往是由开创性的分类架构产生的，例如AlexNet[10]和VGG Nets[23]等，在设计上它由两个子网络组成——一个卷积子网络以空间池化层结束，后面是几个全连接（fc）层。因此，图像分类网络中的（最后一个）空间池化层在目标检测网络中[8，6，18]自然地变成了RoI池化层。</p>
<p>但是最近最先进的图像分类网络，如ResNet（ResNets）[9]和GoogLeNets[24，26]是全卷积的。通过类比，在目标检测架构中使用所有卷积层来构建共享的卷积子网络似乎是很自然的，使得RoI的子网络没有隐藏层。然而，在这项工作中通过经验性的调查发现，这个天真的解决方案有相当差的检测精度，不符合网络的优秀分类精度。为了解决这个问题，在ResNet论文[9]中，Faster R-CNN检测器[18]的RoI池层不自然地插入在两组卷积层之间——这创建了更深的RoI子网络，其改善了精度，由于非共享的RoI计算，因此是以更低的速度为代价。</p>
<p>我们认为，前述的非自然设计是由于增加图像分类的变换不变性与目标检测的平移可变性而导致的两难境地。一方面，图像级别的分类任务有利于平移不变性——图像内目标的移动应该是无差别的。因此，深度（全）卷积架构尽可能保持平移不变，这一点可以从ImageNet分类[9，24，26]的主要结果中得到证实。另一方面，目标检测任务的定位表示需要一定程度上的平移可变性。例如，在候选框内目标变换应该产生有意义的响应，用于描述候选框与目标的重叠程度。我们假设图像分类网络中较深的卷积层对平移不太敏感。为了解决这个困境，ResNet论文的检测流程[9]将RoI池化层插入到卷积中——特定区域的操作打破了平移不变性，当在不同区域进行评估时，RoI后卷积层不再是平移不变的。然而，这个设计牺牲了训练和测试效率，因为它引入了大量的区域层（表1）。</p>
<p>表1：使用ResNet-101的基于区域的检测器方法[9]。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-e49d8208168f16ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<p>在本文中，我们开发了一个称为基于区域的全卷积网络（R-FCN）框架来进行目标检测。我们的网络由共享的全卷积架构组成，就像FCN[15]一样。为了将平移可变性并入FCN，我们通过使用一组专门的卷积层作为FCN输出来构建一组位置敏感的分数图。这些分数图中的每一个都对关于相对空间位置（的位置信息进行编码例如，“在目标的左边”）。在这个FCN之上，我们添加了一个位置敏感的RoI池化层，它从这些分数图中获取信息，并且后面没有权重（卷积/fc）层。整个架构是端到端的学习。所有可学习的层都是卷积的，并在整个图像上共享，但对目标检测所需的空间信息进行编码。图1说明了关键思想，表1比较了基于区域的检测器方法。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-6d1b73b6df12510e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1：R-FCN目标检测的主要思想。在这个例子中，由全卷积网络生成了k×k=3×3的位置敏感分数图。对于RoI中的每个k×k组块，仅在$k^2$个映射中的一个上执行池化（用不同的颜色标记）。</p>
<p>使用101层残余网络（ResNet-101）[9]作为主干网络，我们的R-FCN在PASCAL VOC 2007数据集和2012数据集上分别获得了$83.6\%$ mAP和 $82.0\%$ mAP。同时，使用ResNet-101，我们的结果在测试时是以每张图像170ms的速度实现的，比[9]中对应的Faster R-CNN + ResNet-101快了2.5倍到20倍。这些实验表明，我们的方法设法解决平移不变性/可变性和全卷积图像级分类器之间的困境，如ResNet可以有效地转换为全卷积目标检测器。代码公开发布在：<a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="external">https://github.com/daijifeng001/r-fcn</a>。</p>
<h2 id="2-我们的方法"><a href="#2-我们的方法" class="headerlink" title="2. 我们的方法"></a>2. 我们的方法</h2><p><strong>概述</strong>。根据R-CNN[7]，我们采用了流行的两阶段目标检测策略[7，8，6，18，1，22]，其中包括：（i）区域提议和（ii）区域分类。尽管不依赖区域提议的方法确实存在（例如，[17，14]），但是基于区域的系统在几个基准数据集中仍然具有领先的准确性[5，13，20]。我们通过区域提议网络（RPN）提取候选区域[18]，其本身就是一个全卷积架构。在[18]之后，我们在RPN和R-FCN之间的共享特征。图2显示了系统的概述。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-9ee5daf021af0b4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2：R-FCN的总体架构。区域建议网络（RPN）[18]提出了候选RoI，然后将其应用于评分图上。所有可学习的权重层都是卷积的，并在整个图像上计算；每个RoI的计算成本可以忽略不计。</p>
<p>R-FCN以位置敏感的RoI池化层结束。该层聚合最后一个卷积层的输出，并为每个RoI生成分数。与[8，6]不同的是，我们的位置敏感RoI层进行选择性池化，并且$k\times k$个组块中的每一个仅聚合$k\times k$分数图中一个得分图的响应。通过端到端的训练，这个RoI层可以管理最后一个卷积层来学习专门的位置敏感分数图。图1说明了这个想法。图3和图4显示了一个例子。详细介绍如下。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-0bda584afbb25033.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3：行人类别的R-FCN(k×k=3×3)可视化。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-ac5cfc8cd2269840.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>图4：当RoI不能正确重叠目标时的可视化。</p>
<p><strong>主干架构</strong>。本文中典型的R-FCN是基于ResNet-101[9]的，但其他网络[10,23]也适用。ResNet-101有100个卷积层，后面是全局平均池化和1000类的全连接层。我们删除了平均池化层和全连接层，只使用卷积层来计算特征映射。我们使用由[9]的作者发布的ResNet-101，在ImageNet[20]上预训练。ResNet-101中的最后一个卷积块是2048维，我们附加一个随机初始化的1024维的1×1卷积层来降维（准确地说，这增加了表1中的深度）。然后，我们应用$k^2(C+1)$通道卷积层来生成分数图，如下所述。</p>
<p><strong>位置敏感的分数图和位置敏感的RoI池化</strong>。为了将位置信息显式地编码到每个RoI中，我们用规则网格将每个RoI矩形分成$k \times k$的组块。对于大小为$w \times h$的RoI矩形，组块的大小为$\approx \frac{w}{k} \times \frac{h}{k}$[8,6]。在我们的方法中，构建最后的卷积层为每个类别产生$k^2$分数图。在第$(i,j)$个组块（$0 \leq i,j \leq k-1$）中，我们定义了一个位置敏感的RoI池化操作，它只在第$(i,j)$个分数映射中进行池化：$$r_c(i,j ~|~ \Theta) = \sum_{(x,y)\in \text{bin}(i,j)} z_{i,j,c}(x+x_0, y+y_0 ~|~ \Theta)/n. $$ 其中$r_c(i,j)$是第$(i,j)$个组块中第$c$个类别的池化响应，$z_{i,j,c}$是$k^2(C+1)$分数图中的一个分数图，$(x_0, y_0)$表示一个RoI的左上角，$n$是组块中的像素数量，$\Theta$表示网络的所有可学习参数。第$(i,j)$个组块跨越$\lfloor i\frac{w}{k} \rfloor \leq x &lt; \lceil (i+1)\frac{w}{k} \rceil$和$\lfloor j\frac{h}{k} \rfloor \leq y &lt; \lceil (j+1)\frac{h}{k} \rceil$。公式（1）的操作如图1所示，其中颜色表示一对$(i,j)$。方程（1）执行平均池化（正如我们在本文中使用的那样），但是也可以执行最大池化。</p>
<p>$k^2$位置敏感的分数会在RoI上投票。在本文中，我们通过对分数进行平均来简单地投票，为每个RoI产生一个$(C+1)$维向量：$r_c(\Theta)=\sum_{i,j}r_c(i,j ~|~ \Theta)$。然后，我们计算跨类别的softmax响应：$s_c(\Theta)=e^{r_c(\Theta)} / \sum_{c’=0}^C e^{r_{c’}(\Theta)}$。它们被用来评估训练期间的交叉熵损失以及在推断期间的RoI名。</p>
<p>我们以类似的方式进一步解决边界框回归[7，6]。除了上面的$k^2(C+1)$维卷积层，我们在边界框回归上附加了一个$4k^2$维兄弟卷积层。在这组$4k^2$维映射上执行位置敏感的RoI池化，为每个RoI生成一个$4k^2$维的向量。然后通过平均投票聚合到$4$维向量中。这个$4$维向量将边界框参数化为$t=(t_x, t_y, t_w, t_h)$，参见[6]中的参数化。我们注意到为简单起见，我们执行类别不可知的边界框回归，但类别特定的对应部分（即，具有$4k^2C$维输出层）是适用的。</p>
<p>位置敏感分数图的概念部分受到了[3]的启发，它开发了用于实例级语义分割的FCN。我们进一步介绍了位置敏感的RoI池化层，它可以让学习的分数图用于目标检测。RoI层之后没有可学习的层，使得区域计算几乎是零成本的，并加速训练和推断。</p>
<p><strong>训练</strong>。通过预先计算的区域提议，很容易端到端训练R-FCN架构。根据[6]，我们定义的损失函数是每个RoI的交叉熵损失和边界框回归损失的总和：$L(s, t_{x,y,w,h}) = L_{cls}(s_{c^{*}}) + \lambda [c^{*}&gt;0] L_{reg}(t, t^*)$。这里$c^{*}$是RoI的真实标签（$c^{*}=0$表示背景）。$L_{cls}(s_{c^{*}})=-\log(s_{c^{*}})$是分类的交叉熵损失，$L_{reg}$是[6]中定义的边界框回归损失，$t^*$表示真实的边界框。$[c^{*}&gt;0]$是一个指标，如果参数为true，则等于1，否则为0。我们将平衡权重设置为$\lambda=1$，如[6]中所示。我们将正样本定义为与真实边界框重叠的交并比（IoU）至少为0.5的ROI，否则为负样本。</p>
<p>我们的方法很容易在训练期间采用在线难例挖掘（OHEM）[22]。我们可忽略的每个RoI计算使得几乎零成本的样例挖掘成为可能。假设每张图像有$N$个提议，在前向传播中，我们评估所有$N$个提议的损失。然后，我们按损失对所有的RoI（正例和负例）进行分类，并选择具有最高损失的$B$个RoI。反向传播[11]是基于选定的样例进行的。由于我们每个RoI的计算可以忽略不计，所以前向传播时间几乎不受$N$的影响，与[22]中的OHEM Fast R-CNN相比，这可能使训练时间加倍。我们在下一节的表3中提供全面的时间统计。</p>
<p>我们使用0.0005的权重衰减和0.9的动量。默认情况下，我们使用单尺度训练：调整图像的大小，使得尺度（图像的较短边）为600像素[6，18]。每个GPU拥有1张图像，并为反向传播选择$B=128$个RoI。我们用8个GPU来训练模型（所以有效的最小批数据大小是$8\times$）。在VOC上我们对R-FCN进行微调，使用0.001学习率进行2万次迭代和使用0.0001学习率进行1万次迭代。为了使R-FCN与RPN共享特征（图2），我们采用[18]中的四步交替训练，交替训练RPN和R-FCN。</p>
<p><strong>推断</strong>。如图2所示，在RPN和R-FCN之间计算共享的特征映射（在一个单一尺度的图像上）。然后，RPN部分提出RoI，R-FCN部分在其上评估类别分数并回归边界框。在推断过程中，我们评估了300个RoI进行公平比较，如[18]中那样。作为标准实践，使用0.3的IoU阈值[7]，通过非极大值抑制（NMS）对结果进行后处理。</p>
<p><strong>空洞和步长</strong>。我们的全卷积架构享有FCN广泛使用的语义分割的网络修改的好处[15，2]。特别的是，我们将ResNet-101的有效步长从32像素降低到了16像素，增加了分数图的分辨率。conv$4$阶段[9]（stride = 16）之前和之后的所有层都保持不变；第一个conv$5$块中的stride=2操作被修改为stride=1，并且conv$5$阶段的所有卷积滤波器都被“hole algorithm”[15,2]（“Algorithme atrous”[16]）修改来弥补减少的步幅。为了进行公平的比较，RPN是在conv$4$阶段（与R-FCN共享）之上计算的，就像[9]中Faster R-CNN的情况那样，所以RPN不会受空洞行为的影响。下表显示了R-FCN的消融结果（$k\times k = 7\times 7$，没有难例挖掘）。这个空洞行为提高了2.6点的mAP。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-afe80544fef65225.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table"></p>
<p><strong>可视化</strong>。在图3和图4中，当$k × k = 3 × 3$时，我们可视化R-FCN学习的位置敏感分数图。期望这些专门的分数图将在目标特定的相对位置被强烈激活。例如，“顶部中心敏感”分数图大致在目标的顶部中心位置附近呈现高分数。如果一个候选框与一个真实目标精确重叠（图3），则RoI中的大部分$k^2$组块都被强烈地激活，并且他们的投票导致高分。相反，如果一个候选框与一个真实的目标没有正确的重叠（图4），那么RoI中的一些$k^2$组块没有被激活，投票分数也很低。</p>
<h2 id="3-相关工作"><a href="#3-相关工作" class="headerlink" title="3. 相关工作"></a>3. 相关工作</h2><p>R-CNN[7]已经证明了在深度网络中使用区域提议[27，28]的有效性。R-CNN评估裁剪区域和变形区域的卷积网络，计算不在区域之间共享（表1）。SPPnet[8]Fast R-CNN[6]和Faster R-CNN[18]是“半卷积”的，卷积子网络在整张图像上进行共享计算，另一个子网络评估单个区域。</p>
<p>有可以被认为是“全卷积”模型的目标检测器。OverFeat[21]通过在共享卷积特征映射上滑动多尺度窗口来检测目标；同样地，在Fast R-CNN[6]和[12]中，研究了用滑动窗口替代区域提议。在这些情况下，可以将一个单尺度的滑动窗口重新设计为单个卷积层。Faster R-CNN [18]中的RPN组件是一个全卷积检测器，它可以相对于多个尺寸的参考框（锚点）预测边界框。最初的RPN在[18]中是类不可知的，但是它的类特定的对应部分也是适用的（参见[14]），我们在下面进行评估。</p>
<p>另一个目标检测器家族采用全连接（fc）层来在整张图像上生成整体的目标检测结果，如[25，4，17]。</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><p>###4.1 PASCAL VOC上的实验</p>
<p>我们在有20个目标类别的PASCAL VOC[5]上进行实验。我们根据[6]对VOC 2007 trainval和VOC 2012 trainval（“07 + 12”）的联合数据集进行训练，并在VOC 2007测试集上进行评估。目标检测精度通过平均精度均值（mAP）来度量。</p>
<p><strong>与其它全卷积策略的比较</strong></p>
<p>虽然全卷积检测器是可用的，但是实验表明，它们要达到良好的精度是复杂的。我们使用ResNet-101研究以下全卷积策略（或“几乎”全卷积策略，每个RoI只有一个分类器全连接层）：</p>
<p><strong>Naïve Faster R-CNN</strong>。如介绍中所讨论的，可以使用ResNet-101中的所有卷积层来计算共享特征映射，并且在最后的卷积层（conv5之后）之后采用RoI池化。在每个RoI上评估一个廉价的21类全连接层（所以这个变体是“几乎”全卷积的）。空洞窍门是用来进行公平比较的。</p>
<p><strong>类别特定的RPN</strong>。这个RPN按照[18]进行训练，除了两类（是目标或不是）卷积分类器层被替换为21类卷积分类器层。为了公平的比较，对于这个类别特定的RPN，我们使用具有空洞窍门的ResNet-101的conv5层来处理。</p>
<p><strong>没有位置灵敏度的R-FCN</strong>。通过设置$k=1$，我们移除了R-FCN的位置灵敏度。这相当于在每个RoI内进行全局池化。</p>
<p>分析。表2显示了结果。我们注意到在ResNet论文[9]中的标准（非简单）Faster R-CNN与ResNet-101（参见表3）达到了$76.4\%$的mAP，在conv4和conv5之间插入了RoI池化层[9]。相比之下，简单的Faster R-CNN（在conv5之后应用RoI池化）具有$68.9\%$的更低的mAP（表2）。这种比较通过在Faster R-CNN系统的层之间插入RoI池化在经验上证明了尊重空间信息的重要性。在[19]中报道了类似的观测结果。</p>
<p>表2：使用ResNet-101的全卷积（或“几乎”全卷积）策略之间的比较。表中的所有竞争者都使用了空洞窍门。不执行难例挖掘。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8fcdc875fd2a1f5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>表3：使用ResNet-101比较Faster R-CNN和R-FCN。实际是在单个Nvidia K40 GPU上评估的。使用OHEM，在前向传播中计算每张图像的N个RoI，并且选择128个样本用于反向传播。在下面的[18]中使用了300个RoI进行测试。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-272222106febb153.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 3"></p>
<p>类别特定的RPN具有$67.6\%$（表2）的mAP，比标准Faster R-CNN的$76.4\%$低约9个百分点。这个比较符合[6，12]中的观测结果——实际上，类别特定的RPN类似于使用密集滑动窗口作为提议的一种特殊形式的Fast R-CNN[6]，如[6，12]中所报道的较差结果。</p>
<p>另一方面，我们的R-FCN系统具有更好的准确性（表2）。其mAP（$76.6\%$）与标准Faster R-CNN（$76.4%$，表3）相当。这些结果表明，我们的位置敏感策略设法编码有用的空间信息来定位目标，而在RoI池化之后不使用任何可学习的层。</p>
<p>位置灵敏度的重要性通过设置$k=1$来进一步证明，其中R-FCN不能收敛。在这种退化的情况下，在RoI内不能显式捕获空间信息。此外，我们还报告了，如果简单Faster R-CNN的ROI池化输出分辨率为1×1，其能够收敛，但是mAP进一步下降到$61.7\%$（表2）。</p>
<p><strong>与使用ResNet-101的Faster R-CNN的比较</strong></p>
<p>接下来，我们与标准的“Faster R-CNN + ResNet-101”[9]进行比较，它是PASCAL VOC，MS COCO和ImageNet基准测试中最强劲的竞争对手和最佳表现者。我们在下面使用$k×k = 7×7$。表3显示了比较。Faster R-CNN评估了每个区域的10层子网络以达到良好的精度，但是R-FCN每个区域的成本可以忽略不计。在测试时使用300个RoI，Faster R-CNN每张图像花费0.42s，比我们的R-FCN慢了2.5倍，R-FCN每张图像只有0.17s（在K40 GPU上，这个数字在Titan X GPU上是0.11s）。R-FCN的训练速度也快于Faster R-CNN。此外，难例挖掘[22]没有增加R-FCN的训练成本（表3）。当从2000个RoI挖掘时训练R-FCN是可行的，在这种情况下，Faster R-CNN慢了6倍（2.9s vs. 0.46s）。但是实验表明，从更大的候选集（例如2000）中进行挖掘没有好处（表3）。所以我们在本文的其他部分使用了300个RoI来进行训练和推断。</p>
<p>表4显示了更多的比较。在[8]中的多尺度训练之后，我们在每次训练迭代中调整图像大小，使得尺度从{400,500,600,700,800}像素中进行随机地采样。我们仍然测试600像素的单尺度，所以不添加测试时间成本。mAP是$80.5\%$。此外，我们在MS COCO [13]训练验证集上训练我们的模型，然后在PASCAL VOC数据集上对其进行微调。R-FCN达到$83.6\%$mAP（表4），接近也使用ResNet-101的[9]中的“Faster R-CNN +++”系统。我们注意到，我们的竞争结果是在每张图像0.17秒的测试速度下获得的，比花费3.36秒的Faster R-CNN +++快20倍，因为它进一步结合了迭代边界框回归，上下文和多尺度测试[9]。这些比较也可以在PASCAL VOC 2012测试集上观察到（表5）。</p>
<p>表4：使用ResNet-101在PASCAL VOC 2007测试集上的比较。“Faster R-CNN +++”[9]使用迭代边界框回归，上下文和多尺度测试。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-24bd5bfd1d9ccdac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 4"></p>
<p>表5：使用ResNet-101在PASCAL VOC 2012测试集上的比较。“07 ++ 12”[6]表示07训练+测试和12训练的联合数据集。†: <a href="http://host.robots.ox.ac.uk:8080/anonymous/44L5HI.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/44L5HI.html</a> ‡: <a href="http://host.robots.ox.ac.uk:8080/anonymous/MVCM2L.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/MVCM2L.html</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-fc5b5c7fc3e4c4bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 5"></p>
<p><strong>关于深度的影响</strong></p>
<p>下表显示了使用不同深度的ResNets的R-FCN结果[9]。当深度从50增加到101时，我们的检测精度增加了，但是深度达到了152。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-4cd78e7eccdab7a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Depth"></p>
<p><strong>关于区域提议的影响</strong></p>
<p>R-FCN可以很容易地应用于其它的区域提议方法，如选择性搜索（SS）[27]和边缘框（EB）[28]。下表显示了使用不同提议的结果（使用ResNet-101）。R-FCN使用SS或EB运行，竞争性地展示了我们方法的泛化性。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-0cf820328fd6b483.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Region Proposals"></p>
<h3 id="4-2-MS-COCO上的实验"><a href="#4-2-MS-COCO上的实验" class="headerlink" title="4.2 MS COCO上的实验"></a>4.2 MS COCO上的实验</h3><p>接下来，我们评估MS COCO数据集[13]中的80个目标类别。我们的实验包括8万张训练集，4万张验证集和2万张测试开发集。我们将9万次迭代的学习率设为0.001，接下来的3万次迭代的学习率设为0.0001，有效的最小批数据大小为8。我们将交替训练[18]从4步扩展到5步（即在RPN训练步骤后停止），当共享特征时略微提高了在该数据集上的准确性；我们还报告了两步训练足以达到相当好的准确性，但不共享这些特征。</p>
<p>结果如表6所示。我们单尺度训练的R-FCN基准模型的验证结果为$48.9\%/27.6\%$。这与Faster R-CNN的基准模型相当（$48.4\%/27.2\%$），但我们的测试速度是Faster R-CNN的2.5倍。值得注意的是，我们的方法在小尺寸的目标上表现更好（由[13]定义）。我们的多尺度训练（但是仍是单一尺度测试）的R-FCN在验证集上的结果为$49.1\%/27.8\%$，在测试开发集上的结果是$51.5\%/29.2\%$。考虑到COCO广泛的目标尺度，按照[9]我们进一步评估多尺度的测试变种，并使用{200，400，600，800，1000}的测试尺度。mAP是$53.2\%/31.5\%$。这个结果在MS COCO 2015比赛中接近第一名的成绩（Faster R-CNN+++和ResNet-101，$55.7\%/34.9\%$）。尽管如此，我们的方法更简单，并没有添加[9]中所使用的多样功能例如上下文或迭代边界框回归，并且在训练和测试中都更快。</p>
<p>表6：使用ResNet-101在MS COCO数据集上比较。COCO式的AP在IoU∈[0.5，0.95]处评估。AP@0.5是PASCAL式的AP，在IoU=0.5处评估。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-ec3fe15a633c74ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 6"></p>
<h2 id="5-总结和将来的工作"><a href="#5-总结和将来的工作" class="headerlink" title="5. 总结和将来的工作"></a>5. 总结和将来的工作</h2><p>我们提出了基于区域的全卷积网络，这是一个简单但精确且高效的目标检测框架。我们的系统自然地采用了设计为全卷积的最先进的图像分类骨干网络，如ResNet。我们的方法实现了与Faster R-CNN对应网络相比更具竞争力的准确性，但是在训练和推断上都快得多。</p>
<p>我们故意保持R-FCN系统如论文中介绍的那样简单。已经有一系列针对语义分割（例如，参见[2]）开发的FCN的正交扩展，以及用于目标检测的基于区域的方法的扩展（例如参见[9，1，22]）。我们期望我们的系统能够轻松享有这个领域的进步带来的好处。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016.</p>
<p>[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015.</p>
<p>[3] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive fully convolutional networks.arXiv:1603.08678, 2016.</p>
<p>[4] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014.</p>
<p>[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. IJCV, 2010.</p>
<p>[6] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection<br>and semantic segmentation. In CVPR, 2014.</p>
<p>[8] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual<br>recognition. In ECCV. 2014.</p>
<p>[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>[10] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[11] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropa- gation applied to handwritten zip code recognition. Neural computation, 1989.</p>
<p>[12] K. Lenc and A. Vedaldi. R-CNN minus R. In BMVC, 2015.</p>
<p>[13] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[14] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot multibox detector. arXiv:1512.02325v2, 2015.</p>
<p>[15] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[16] S. Mallat. A wavelet tour of signal processing. Academic press, 1999.</p>
<p>[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. In CVPR, 2016.</p>
<p>[18] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[19] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. arXiv:1504.06066, 2015.</p>
<p>[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.</p>
<p>[21] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[22] A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object detectors with online hard example mining. In CVPR, 2016.</p>
<p>[23] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[24] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.</p>
<p>[25] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In NIPS, 2013.</p>
<p>[26] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.</p>
<p>[27] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.</p>
<p>[28] C. L. Zitnick and P. Dollár. Edge boxes: Locating object proposals from edges. In ECCV, 2014.</p>
]]></content>
    
    <summary type="html">
    
      R-FCN论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>R-FCN论文翻译——中英文对照</title>
    <link href="noahsnail.com/2018/01/22/2018-01-22-R-FCN%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2018/01/22/2018-01-22-R-FCN论文翻译——中英文对照/</id>
    <published>2018-01-22T08:48:09.000Z</published>
    <updated>2018-01-29T10:46:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<h1 id="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks"><a href="#R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks" class="headerlink" title="R-FCN: Object Detection via Region-based Fully Convolutional Networks"></a>R-FCN: Object Detection via Region-based Fully Convolutional Networks</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6, 18] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., $83.6\%$ mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20× faster than the Faster R-CNN counterpart. Code is made publicly available at: <a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="external">https://github.com/daijifeng001/r-fcn</a>.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了基于区域的全卷积网络，以实现准确和高效的目标检测。与先前的基于区域的检测器（如Fast/Faster R-CNN [6，18]）相比，这些检测器应用昂贵的每个区域子网络数百次，我们的基于区域的检测器是全卷积的，几乎所有计算都在整张图像上共享。为了实现这一目标，我们提出了位置敏感分数图，以解决图像分类中的平移不变性与目标检测中的平移变化之间的困境。因此，我们的方法可以自然地采用全卷积图像分类器的主干网络，如最新的残差网络（ResNets）[9]，用于目标检测。我们使用101层ResNet在PASCAL VOC数据集上展示了具有竞争力的结果（例如，2007数据集上$83.6\%$的mAP）。同时，我们的测试结果是以每张图像170ms的测试速度实现的，比Faster R-CNN对应部分速度快2.5-20倍。代码公开发布在：[<a href="https://github.com/daijifeng001/r-fcn](https://github.com/daijifeng001/r-fcn）。" target="_blank" rel="external">https://github.com/daijifeng001/r-fcn](https://github.com/daijifeng001/r-fcn）。</a></p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>A prevalent family [8, 6, 18] of deep networks for object detection can be divided into two subnetworks by the Region-of-Interest (RoI) pooling layer [6]: (i) a shared, “fully convolutional” subnetwork independent of RoIs, and (ii) an RoI-wise subnetwork that does not share computation. This decomposition [8] was historically resulted from the pioneering classification architectures, such as AlexNet [10] and VGG Nets [23], that consist of two subnetworks by design —— a convolutional subnetwork ending with a spatial pooling layer, followed by several fully-connected (fc) layers. Thus the (last) spatial pooling layer in image classification networks is naturally turned into the RoI pooling layer in object detection networks [8, 6, 18].</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>流行的目标检测深度网络家族[8，6，18]通过感兴趣区域（RoI）池化层[6]可以划分成两个子网络：（1）独立于RoI的共享“全卷积”子网络，（ii）不共享计算的RoI子网络。这种分解[8]以往是由开创性的分类架构产生的，例如AlexNet[10]和VGG Nets[23]等，在设计上它由两个子网络组成——一个卷积子网络以空间池化层结束，后面是几个全连接（fc）层。因此，图像分类网络中的（最后一个）空间池化层在目标检测网络中[8，6，18]自然地变成了RoI池化层。</p>
<p>But recent state-of-the-art image classification networks such as Residual Nets (ResNets) [9] and GoogLeNets [24, 26] are by design fully convolutional. By analogy, it appears natural to use all convolutional layers to construct the shared, convolutional subnetwork in the object detection architecture, leaving the RoI-wise subnetwork no hidden layer. However, as empirically investigated in this work, this naïve solution turns out to have considerably inferior detection accuracy that does not match the network’s superior classification accuracy. To remedy this issue, in the ResNet paper [9] the RoI pooling layer of the Faster R-CNN detector [18] is unnaturally inserted between two sets of convolutional layers —— this creates a deeper RoI-wise subnetwork that improves accuracy, at the cost of lower speed due to the unshared per-RoI computation.</p>
<p>但是最近最先进的图像分类网络，如ResNet（ResNets）[9]和GoogLeNets[24，26]是全卷积的。通过类比，在目标检测架构中使用所有卷积层来构建共享的卷积子网络似乎是很自然的，使得RoI的子网络没有隐藏层。然而，在这项工作中通过经验性的调查发现，这个天真的解决方案有相当差的检测精度，不符合网络的优秀分类精度。为了解决这个问题，在ResNet论文[9]中，Faster R-CNN检测器[18]的RoI池层不自然地插入在两组卷积层之间——这创建了更深的RoI子网络，其改善了精度，由于非共享的RoI计算，因此是以更低的速度为代价。</p>
<p>We argue that the aforementioned unnatural design is caused by a dilemma of increasing translation invariance for image classification vs. respecting translation variance for object detection. On one hand, the image-level classification task favors translation invariance —— shift of an object inside an image should be indiscriminative. Thus, deep (fully) convolutional architectures that are as translation-invariant as possible are preferable as evidenced by the leading results on ImageNet classification [9, 24, 26]. On the other hand, the object detection task needs localization representations that are translation-variant to an extent. For example, translation of an object inside a candidate box should produce meaningful responses for describing how good the candidate box overlaps the object. We hypothesize that deeper convolutional layers in an image classification network are less sensitive to translation. To address this dilemma, the ResNet paper’s detection pipeline [9] inserts the RoI pooling layer into convolutions —— this region-specific operation breaks down translation invariance, and the post-RoI convolutional layers are no longer translation-invariant when evaluated across different regions. However, this design sacrifices training and testing efficiency since it introduces a considerable number of region-wise layers (Table 1).</p>
<p>Table 1: Methodologies of region-based detectors using ResNet-101 [9].</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-e49d8208168f16ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<p>我们认为，前述的非自然设计是由于增加图像分类的变换不变性与目标检测的平移可变性而导致的两难境地。一方面，图像级别的分类任务有利于平移不变性——图像内目标的移动应该是无差别的。因此，深度（全）卷积架构尽可能保持平移不变，这一点可以从ImageNet分类[9，24，26]的主要结果中得到证实。另一方面，目标检测任务的定位表示需要一定程度上的平移可变性。例如，在候选框内目标变换应该产生有意义的响应，用于描述候选框与目标的重叠程度。我们假设图像分类网络中较深的卷积层对平移不太敏感。为了解决这个困境，ResNet论文的检测流程[9]将RoI池化层插入到卷积中——特定区域的操作打破了平移不变性，当在不同区域进行评估时，RoI后卷积层不再是平移不变的。然而，这个设计牺牲了训练和测试效率，因为它引入了大量的区域层（表1）。</p>
<p>表1：使用ResNet-101的基于区域的检测器方法[9]。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-e49d8208168f16ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<p>In this paper, we develop a framework called Region-based Fully Convolutional Network (R-FCN) for object detection. Our network consists of shared, fully convolutional architectures as is the case of FCN [15]. To incorporate translation variance into FCN, we construct a set of position-sensitive score maps by using a bank of specialized convolutional layers as the FCN output. Each of these score maps encodes the position information with respect to a relative spatial position (e.g., “to the left of an object”). On top of this FCN, we append a position-sensitive RoI pooling layer that shepherds information from these score maps, with no weight (convolutional/fc) layers following. The entire architecture is learned end-to-end. All learnable layers are convolutional and shared on the entire image, yet encode spatial information required for object detection. Figure 1 illustrates the key idea and Table 1 compares the methodologies among region-based detectors.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-6d1b73b6df12510e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>Figure 1: Key idea of R-FCN for object detection. In this illustration, there are k × k = 3 × 3 position-sensitive score maps generated by a fully convolutional network. For each of the k × k bins in an RoI, pooling is only performed on one of the $k^2$ maps (marked by different colors).</p>
<p>在本文中，我们开发了一个称为基于区域的全卷积网络（R-FCN）框架来进行目标检测。我们的网络由共享的全卷积架构组成，就像FCN[15]一样。为了将平移可变性并入FCN，我们通过使用一组专门的卷积层作为FCN输出来构建一组位置敏感的分数图。这些分数图中的每一个都对关于相对空间位置（的位置信息进行编码例如，“在目标的左边”）。在这个FCN之上，我们添加了一个位置敏感的RoI池化层，它从这些分数图中获取信息，并且后面没有权重（卷积/fc）层。整个架构是端到端的学习。所有可学习的层都是卷积的，并在整个图像上共享，但对目标检测所需的空间信息进行编码。图1说明了关键思想，表1比较了基于区域的检测器方法。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-6d1b73b6df12510e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1：R-FCN目标检测的主要思想。在这个例子中，由全卷积网络生成了k×k=3×3的位置敏感分数图。对于RoI中的每个k×k组块，仅在$k^2$个映射中的一个上执行池化（用不同的颜色标记）。</p>
<p>Using the 101-layer Residual Net (ResNet-101) [9] as the backbone, our R-FCN yields competitive results of $83.6\%$ mAP on the PASCAL VOC 2007 set and $82.0\%$ the 2012 set. Meanwhile, our results are achieved at a test-time speed of 170ms per image using ResNet-101, which is 2.5× to 20× faster than the Faster R-CNN + ResNet-101 counterpart in [9]. These experiments demonstrate that our method manages to address the dilemma between invariance/variance on translation, and fully convolutional image-level classifiers such as ResNets can be effectively converted to fully convolutional object detectors. Code is made publicly available at: <a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="external">https://github.com/daijifeng001/r-fcn</a>.</p>
<p>使用101层残余网络（ResNet-101）[9]作为主干网络，我们的R-FCN在PASCAL VOC 2007数据集和2012数据集上分别获得了$83.6\%$ mAP和 $82.0\%$ mAP。同时，使用ResNet-101，我们的结果在测试时是以每张图像170ms的速度实现的，比[9]中对应的Faster R-CNN + ResNet-101快了2.5倍到20倍。这些实验表明，我们的方法设法解决平移不变性/可变性和全卷积图像级分类器之间的困境，如ResNet可以有效地转换为全卷积目标检测器。代码公开发布在：<a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="external">https://github.com/daijifeng001/r-fcn</a>。</p>
<h2 id="2-Our-approach"><a href="#2-Our-approach" class="headerlink" title="2. Our approach"></a>2. Our approach</h2><p><strong>Overview</strong>. Following R-CNN [7], we adopt the popular two-stage object detection strategy [7, 8, 6, 18, 1, 22] that consists of: (i) region proposal, and (ii) region classification. Although methods that do not rely on region proposal do exist (e.g., [17, 14]), region-based systems still possess leading accuracy on several benchmarks [5, 13, 20]. We extract candidate regions by the Region Proposal Network (RPN) [18], which is a fully convolutional architecture in itself. Following [18], we share the features between RPN and R-FCN. Figure 2 shows an overview of the system.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-9ee5daf021af0b4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>Figure 2: Overall architecture of R-FCN. A Region Proposal Network (RPN) [18] proposes candidate RoIs, which are then applied on the score maps. All learnable weight layers are convolutional and are computed on the entire image; the per-RoI computational cost is negligible.</p>
<h2 id="2-我们的方法"><a href="#2-我们的方法" class="headerlink" title="2. 我们的方法"></a>2. 我们的方法</h2><p><strong>概述</strong>。根据R-CNN[7]，我们采用了流行的两阶段目标检测策略[7，8，6，18，1，22]，其中包括：（i）区域提议和（ii）区域分类。尽管不依赖区域提议的方法确实存在（例如，[17，14]），但是基于区域的系统在几个基准数据集中仍然具有领先的准确性[5，13，20]。我们通过区域提议网络（RPN）提取候选区域[18]，其本身就是一个全卷积架构。在[18]之后，我们在RPN和R-FCN之间的共享特征。图2显示了系统的概述。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-9ee5daf021af0b4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2：R-FCN的总体架构。区域建议网络（RPN）[18]提出了候选RoI，然后将其应用于评分图上。所有可学习的权重层都是卷积的，并在整个图像上计算；每个RoI的计算成本可以忽略不计。</p>
<p>Given the proposal regions (RoIs), the R-FCN architecture is designed to classify the RoIs into object categories and background. In R-FCN, all learnable weight layers are convolutional and are computed on the entire image. The last convolutional layer produces a bank of $k^2$ position-sensitive score maps for each category, and thus has a $k^2(C+1)$-channel output layer with $C$ object categories ($+1$ for background). The bank of $k^2$ score maps correspond to a $k\times k$ spatial grid describing relative positions. For example, with $k\times k = 3\times 3$, the 9 score maps encode the cases of {top-left, top-center, top-right, …, bottom-right} of an object category.</p>
<p>鉴于提议区域（RoI），R-FCN架构被设计成将RoI分类为目标类别和背景。在R-FCN中，所有可学习的权重层都是卷积的，并在整个图像上进行计算。最后一个卷积层为每个类别产生一堆大小为$k^2$的位置敏感分数图，从而得到一个具有$C$个目标类别的$k^2(C+1)$通道输出层（$+1$为背景）。一堆$k^2$个分数图对应于描述相对位置的$k\times k$空间网格。例如，对于$k\times k = 3\times 3$，大小为9的分数图编码目标类别在{左上，右上，右上，…，右下}的情况。</p>
<p>R-FCN ends with a position-sensitive RoI pooling layer. This layer aggregates the outputs of the last convolutional layer and generates scores for each RoI. Unlike [8, 6], our position-sensitive RoI layer conducts selective pooling, and each of the $k\times k$ bin aggregates responses from only one score map out of the bank of $k\times k$ score maps. With end-to-end training, this RoI layer shepherds the last convolutional layer to learn specialized position-sensitive score maps. Figure 1 illustrates this idea. Figure 3 and 4 visualize an example. The details are introduced as follows.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-0bda584afbb25033.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>Figure 3: Visualization of R-FCN (k × k = 3 × 3) for the person category.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-ac5cfc8cd2269840.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>Figure 4: Visualization when an RoI does not correctly overlap the object.</p>
<p>R-FCN以位置敏感的RoI池化层结束。该层聚合最后一个卷积层的输出，并为每个RoI生成分数。与[8，6]不同的是，我们的位置敏感RoI层进行选择性池化，并且$k\times k$个组块中的每一个仅聚合$k\times k$分数图中一个得分图的响应。通过端到端的训练，这个RoI层可以管理最后一个卷积层来学习专门的位置敏感分数图。图1说明了这个想法。图3和图4显示了一个例子。详细介绍如下。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-0bda584afbb25033.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3：行人类别的R-FCN(k×k=3×3)可视化。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-ac5cfc8cd2269840.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>图4：当RoI不能正确重叠目标时的可视化。</p>
<p><strong>Backbone architecture</strong>. The incarnation of R-FCN in this paper is based on ResNet-101 [9], though other networks [10, 23] are applicable. ResNet-101 has 100 convolutional layers followed by global average pooling and a 1000-class fc layer. We remove the average pooling layer and the fc layer and only use the convolutional layers to compute feature maps. We use the ResNet-101 released by the authors of [9], pre-trained on ImageNet [20]. The last convolutional block in ResNet-101 is 2048-d, and we attach a randomly initialized 1024-d 1×1 convolutional layer for reducing dimension (to be precise, this increases the depth in Table 1 by 1). Then we apply the $k^2(C + 1)$-channel convolutional layer to generate score maps, as introduced next.</p>
<p><strong>主干架构</strong>。本文中典型的R-FCN是基于ResNet-101[9]的，但其他网络[10,23]也适用。ResNet-101有100个卷积层，后面是全局平均池化和1000类的全连接层。我们删除了平均池化层和全连接层，只使用卷积层来计算特征映射。我们使用由[9]的作者发布的ResNet-101，在ImageNet[20]上预训练。ResNet-101中的最后一个卷积块是2048维，我们附加一个随机初始化的1024维的1×1卷积层来降维（准确地说，这增加了表1中的深度）。然后，我们应用$k^2(C+1)$通道卷积层来生成分数图，如下所述。</p>
<p><strong>Position-sensitive score maps &amp; Position-sensitive RoI pooling</strong>. To explicitly encode position information into each RoI, we divide each RoI rectangle into $k \times k$ bins by a regular grid. For an RoI rectangle of a size $w \times h$, a bin is of a size $\approx \frac{w}{k} \times \frac{h}{k}$ [8, 6]. In our method, the last convolutional layer is constructed to produce $k^2$ score maps for each category. Inside the $(i,j)$-th bin ($0 \leq i,j \leq k-1$), we define a position-sensitive RoI pooling operation that pools only over the $(i,j)$-th score map: $$r_c(i,j ~|~ \Theta) = \sum_{(x,y)\in \text{bin}(i,j)} z_{i,j,c}(x+x_0, y+y_0 ~|~ \Theta)/n. $$ Here $r_c(i,j)$ is the pooled response in the $(i,j)$-th bin for the $c$-th category, $z_{i,j,c}$ is one score map out of the $k^2(C+1)$ score maps, $(x_0, y_0)$ denotes the top-left corner of an RoI, $n$ is the number of pixels in the bin, and $\Theta$ denotes all learnable parameters of the network. The $(i,j)$-th bin spans $\lfloor i\frac{w}{k} \rfloor \leq x &lt; \lceil (i+1)\frac{w}{k} \rceil$ and $\lfloor j\frac{h}{k} \rfloor \leq y &lt; \lceil (j+1)\frac{h}{k} \rceil$. The operation of Eqn.(1) is illustrated in Figure 1, where a color represents a pair of $(i,j)$. Eqn.(1) performs average pooling (as we use throughout this paper), but max pooling can be conducted as well.</p>
<p><strong>位置敏感的分数图和位置敏感的RoI池化</strong>。为了将位置信息显式地编码到每个RoI中，我们用规则网格将每个RoI矩形分成$k \times k$的组块。对于大小为$w \times h$的RoI矩形，组块的大小为$\approx \frac{w}{k} \times \frac{h}{k}$[8,6]。在我们的方法中，构建最后的卷积层为每个类别产生$k^2$分数图。在第$(i,j)$个组块（$0 \leq i,j \leq k-1$）中，我们定义了一个位置敏感的RoI池化操作，它只在第$(i,j)$个分数映射中进行池化：$$r_c(i,j ~|~ \Theta) = \sum_{(x,y)\in \text{bin}(i,j)} z_{i,j,c}(x+x_0, y+y_0 ~|~ \Theta)/n. $$ 其中$r_c(i,j)$是第$(i,j)$个组块中第$c$个类别的池化响应，$z_{i,j,c}$是$k^2(C+1)$分数图中的一个分数图，$(x_0, y_0)$表示一个RoI的左上角，$n$是组块中的像素数量，$\Theta$表示网络的所有可学习参数。第$(i,j)$个组块跨越$\lfloor i\frac{w}{k} \rfloor \leq x &lt; \lceil (i+1)\frac{w}{k} \rceil$和$\lfloor j\frac{h}{k} \rfloor \leq y &lt; \lceil (j+1)\frac{h}{k} \rceil$。公式（1）的操作如图1所示，其中颜色表示一对$(i,j)$。方程（1）执行平均池化（正如我们在本文中使用的那样），但是也可以执行最大池化。</p>
<p>The $k^2$ position-sensitive scores then vote on the RoI. In this paper we simply vote by averaging the scores, producing a $(C+1)$-dimensional vector for each RoI: $r_c(\Theta)=\sum_{i,j}r_c(i,j ~|~ \Theta)$. Then we compute the softmax responses across categories: $s_c(\Theta)=e^{r_c(\Theta)} / \sum_{c’=0}^C e^{r_{c’}(\Theta)}$. They are used for evaluating the cross-entropy loss during training and for ranking the RoIs during inference.</p>
<p>$k^2$位置敏感的分数会在RoI上投票。在本文中，我们通过对分数进行平均来简单地投票，为每个RoI产生一个$(C+1)$维向量：$r_c(\Theta)=\sum_{i,j}r_c(i,j ~|~ \Theta)$。然后，我们计算跨类别的softmax响应：$s_c(\Theta)=e^{r_c(\Theta)} / \sum_{c’=0}^C e^{r_{c’}(\Theta)}$。它们被用来评估训练期间的交叉熵损失以及在推断期间的RoI名。</p>
<p>We further address bounding box regression [7, 6] in a similar way. Aside from the above $k^2(C+1)$-d convolutional layer, we append a sibling $4 k^2$-d convolutional layer for bounding box regression. The position-sensitive RoI pooling is performed on this bank of $4k^2$ maps, producing a $4k^2$-d vector for each RoI. Then it is aggregated into a $4$-d vector by average voting. This $4$-d vector parameterizes a bounding box as $t=(t_x, t_y, t_w, t_h)$ following the parameterization in [6]. We note that we perform class-agnostic bounding box regression for simplicity, but the class-specific counterpart (i.e., with a $4 k^2 C$-d output layer) is applicable.</p>
<p>我们以类似的方式进一步解决边界框回归[7，6]。除了上面的$k^2(C+1)$维卷积层，我们在边界框回归上附加了一个$4k^2$维兄弟卷积层。在这组$4k^2$维映射上执行位置敏感的RoI池化，为每个RoI生成一个$4k^2$维的向量。然后通过平均投票聚合到$4$维向量中。这个$4$维向量将边界框参数化为$t=(t_x, t_y, t_w, t_h)$，参见[6]中的参数化。我们注意到为简单起见，我们执行类别不可知的边界框回归，但类别特定的对应部分（即，具有$4k^2C$维输出层）是适用的。</p>
<p>The concept of position-sensitive score maps is partially inspired by [3] that develops FCNs for instance-level semantic segmentation. We further introduce the position-sensitive RoI pooling layer that shepherds learning of the score maps for object detection. There is no learnable layer after the RoI layer, enabling nearly cost-free region-wise computation and speeding up both training and inference.</p>
<p>位置敏感分数图的概念部分受到了[3]的启发，它开发了用于实例级语义分割的FCN。我们进一步介绍了位置敏感的RoI池化层，它可以让学习的分数图用于目标检测。RoI层之后没有可学习的层，使得区域计算几乎是零成本的，并加速训练和推断。</p>
<p><strong>Training</strong>. With pre-computed region proposals, it is easy to end-to-end train the R-FCN architecture. Following [6], our loss function defined on each RoI is the summation of the cross-entropy loss and the box regression loss: $L(s, t_{x,y,w,h}) = L_{cls}(s_{c^{*}}) + \lambda [c^{*}&gt;0] L_{reg}(t, t^*)$. Here $c^{*}$ is the RoI’s ground-truth label ($c^{*}=0$ means background). $L_{cls}(s_{c^{*}})=-\log(s_{c^{*}})$ is the cross-entropy loss for classification, $L_{reg}$ is the bounding box regression loss as defined in [6], and $t^*$ represents the ground truth box. $[c^{*}&gt;0]$ is an indicator which equals to 1 if the argument is true and 0 otherwise. We set the balance weight $\lambda=1$ as in [6]. We define positive examples as the RoIs that have intersection-over-union (IoU) overlap with a ground-truth box of at least 0.5, and negative otherwise.</p>
<p><strong>训练</strong>。通过预先计算的区域提议，很容易端到端训练R-FCN架构。根据[6]，我们定义的损失函数是每个RoI的交叉熵损失和边界框回归损失的总和：$L(s, t_{x,y,w,h}) = L_{cls}(s_{c^{*}}) + \lambda [c^{*}&gt;0] L_{reg}(t, t^*)$。这里$c^{*}$是RoI的真实标签（$c^{*}=0$表示背景）。$L_{cls}(s_{c^{*}})=-\log(s_{c^{*}})$是分类的交叉熵损失，$L_{reg}$是[6]中定义的边界框回归损失，$t^*$表示真实的边界框。$[c^{*}&gt;0]$是一个指标，如果参数为true，则等于1，否则为0。我们将平衡权重设置为$\lambda=1$，如[6]中所示。我们将正样本定义为与真实边界框重叠的交并比（IoU）至少为0.5的ROI，否则为负样本。</p>
<p>It is easy for our method to adopt online hard example mining (OHEM) [22] during training. Our negligible per-RoI computation enables nearly cost-free example mining. Assuming $N$ proposals per image, in the forward pass, we evaluate the loss of all $N$ proposals. Then we sort all RoIs (positive and negative) by loss and select $B$ RoIs that have the highest loss. Backpropagation [11] is performed based on the selected examples. Because our per-RoI computation is negligible, the forward time is nearly not affected by $N$, in contrast to OHEM Fast R-CNN in [22] that may double training time. We provide comprehensive timing statistics in Table 3 in the next section.</p>
<p>我们的方法很容易在训练期间采用在线难例挖掘（OHEM）[22]。我们可忽略的每个RoI计算使得几乎零成本的样例挖掘成为可能。假设每张图像有$N$个提议，在前向传播中，我们评估所有$N$个提议的损失。然后，我们按损失对所有的RoI（正例和负例）进行分类，并选择具有最高损失的$B$个RoI。反向传播[11]是基于选定的样例进行的。由于我们每个RoI的计算可以忽略不计，所以前向传播时间几乎不受$N$的影响，与[22]中的OHEM Fast R-CNN相比，这可能使训练时间加倍。我们在下一节的表3中提供全面的时间统计。</p>
<p>We use a weight decay of 0.0005 and a momentum of 0.9. By default we use single-scale training: images are resized such that the scale (shorter side of image) is 600 pixels [6, 18]. Each GPU holds 1 image and selects $B=128$ RoIs for backprop. We train the model with 8 GPUs (so the effective mini-batch size is $8\times$). We fine-tune R-FCN using a learning rate of 0.001 for 20k mini-batches and 0.0001 for 10k mini-batches on VOC. To have R-FCN share features with RPN (Figure 2), we adopt the 4-step alternating training in [18], alternating between training RPN and training R-FCN.</p>
<p>我们使用0.0005的权重衰减和0.9的动量。默认情况下，我们使用单尺度训练：调整图像的大小，使得尺度（图像的较短边）为600像素[6，18]。每个GPU拥有1张图像，并为反向传播选择$B=128$个RoI。我们用8个GPU来训练模型（所以有效的最小批数据大小是$8\times$）。在VOC上我们对R-FCN进行微调，使用0.001学习率进行2万次迭代和使用0.0001学习率进行1万次迭代。为了使R-FCN与RPN共享特征（图2），我们采用[18]中的四步交替训练，交替训练RPN和R-FCN。</p>
<p><strong>Inference</strong>. As illustrated in Figure 2, the feature maps shared between RPN and R-FCN are computed (on an image with a single scale of 600). Then the RPN part proposes RoIs, on which the R-FCN part evaluates category-wise scores and regresses bounding boxes. During inference we evaluate 300 RoIs as in [18] for fair comparisons. The results are post-processed by non-maximum suppression (NMS) using a threshold of 0.3 IoU [7], as standard practice.</p>
<p><strong>推断</strong>。如图2所示，在RPN和R-FCN之间计算共享的特征映射（在一个单一尺度的图像上）。然后，RPN部分提出RoI，R-FCN部分在其上评估类别分数并回归边界框。在推断过程中，我们评估了300个RoI进行公平比较，如[18]中那样。作为标准实践，使用0.3的IoU阈值[7]，通过非极大值抑制（NMS）对结果进行后处理。</p>
<p><strong>Atrous and stride</strong>. Our fully convolutional architecture enjoys the benefits of the network modifications that are widely used by FCNs for semantic segmentation [15, 2]. Particularly, we reduce ResNet-101’s effective stride from 32 pixels to 16 pixels, increasing the score map resolution. All layers before and on the conv$4$ stage <a href="stride=16">9</a> are unchanged; the stride=2 operations in the first conv$5$ block is modified to have stride=1, and all convolutional filters on the conv$5$ stage are modified by the “hole algorithm” [15, 2] (“Algorithme atrous”[16]) to compensate for the reduced stride. For fair comparisons, the RPN is computed on top of the conv$4$ stage (that are shared with R-FCN), as is the case in [9] with Faster R-CNN, so the RPN is not affected by the atrous trick. The following table shows the ablation results of R-FCN ($k\times k = 7\times 7$, no hard example mining). The atrous trick improves mAP by 2.6 points.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-afe80544fef65225.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table"></p>
<p><strong>空洞和步长</strong>。我们的全卷积架构享有FCN广泛使用的语义分割的网络修改的好处[15，2]。特别的是，我们将ResNet-101的有效步长从32像素降低到了16像素，增加了分数图的分辨率。conv$4$阶段[9]（stride = 16）之前和之后的所有层都保持不变；第一个conv$5$块中的stride=2操作被修改为stride=1，并且conv$5$阶段的所有卷积滤波器都被“hole algorithm”[15,2]（“Algorithm atrous”[16]）修改来弥补减少的步幅。为了进行公平的比较，RPN是在conv$4$阶段（与R-FCN共享）之上计算的，就像[9]中Faster R-CNN的情况那样，所以RPN不会受空洞行为的影响。下表显示了R-FCN的消融结果（$k\times k = 7\times 7$，没有难例挖掘）。这个空洞窍门提高了2.6点的mAP。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-afe80544fef65225.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table"></p>
<p><strong>Visualization</strong>. In Figure 3 and 4 we visualize the position-sensitive score maps learned by R-FCN when $k × k = 3 × 3$. These specialized maps are expected to be strongly activated at a specific relative position of an object. For example, the “top-center-sensitive” score map exhibits high scores roughly near the top-center position of an object. If a candidate box precisely overlaps with a true object (Figure 3), most of the $k^2$ bins in the RoI are strongly activated, and their voting leads to a high score. On the contrary, if a candidate box does not correctly overlaps with a true object (Figure 4), some of the $k^2$ bins in the RoI are not activated, and the voting score is low.</p>
<p><strong>可视化</strong>。在图3和图4中，当$k × k = 3 × 3$时，我们可视化R-FCN学习的位置敏感分数图。期望这些专门的分数图将在目标特定的相对位置被强烈激活。例如，“顶部中心敏感”分数图大致在目标的顶部中心位置附近呈现高分数。如果一个候选框与一个真实目标精确重叠（图3），则RoI中的大部分$k^2$组块都被强烈地激活，并且他们的投票导致高分。相反，如果一个候选框与一个真实的目标没有正确的重叠（图4），那么RoI中的一些$k^2$组块没有被激活，投票分数也很低。</p>
<h2 id="3-Related-Work"><a href="#3-Related-Work" class="headerlink" title="3. Related Work"></a>3. Related Work</h2><p>R-CNN [7] has demonstrated the effectiveness of using region proposals [27, 28] with deep networks. R-CNN evaluates convolutional networks on cropped and warped regions, and computation is not shared among regions (Table 1). SPPnet [8], Fast R-CNN [6], and Faster R-CNN [18] are “semi-convolutional”, in which a convolutional subnetwork performs shared computation on the entire image and another subnetwork evaluates individual regions.</p>
<h2 id="3-相关工作"><a href="#3-相关工作" class="headerlink" title="3. 相关工作"></a>3. 相关工作</h2><p>R-CNN[7]已经证明了在深度网络中使用区域提议[27，28]的有效性。R-CNN评估裁剪区域和变形区域的卷积网络，计算不在区域之间共享（表1）。SPPnet[8]Fast R-CNN[6]和Faster R-CNN[18]是“半卷积”的，卷积子网络在整张图像上进行共享计算，另一个子网络评估单个区域。</p>
<p>There have been object detectors that can be thought of as “fully convolutional” models. OverFeat [21] detects objects by sliding multi-scale windows on the shared convolutional feature maps; similarly, in Fast R-CNN [6] and [12], sliding windows that replace region proposals are investigated. In these cases, one can recast a sliding window of a single scale as a single convolutional layer. The RPN component in Faster R-CNN [18] is a fully convolutional detector that predicts bounding boxes with respect to reference boxes (anchors) of multiple sizes. The original RPN is class-agnostic in [18], but its class-specific counterpart is applicable (see also [14]) as we evaluate in the following.</p>
<p>有可以被认为是“全卷积”模型的目标检测器。OverFeat[21]通过在共享卷积特征映射上滑动多尺度窗口来检测目标；同样地，在Fast R-CNN[6]和[12]中，研究了用滑动窗口替代区域提议。在这些情况下，可以将一个单尺度的滑动窗口重新设计为单个卷积层。Faster R-CNN [18]中的RPN组件是一个全卷积检测器，它可以相对于多个尺寸的参考框（锚点）预测边界框。最初的RPN在[18]中是类不可知的，但是它的类特定的对应部分也是适用的（参见[14]），我们在下面进行评估。</p>
<p>Another family of object detectors resort to fully-connected (fc) layers for generating holistic object detection results on an entire image, such as [25, 4, 17].</p>
<p>另一个目标检测器家族采用全连接（fc）层来在整张图像上生成整体的目标检测结果，如[25，4，17]。</p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p>###4.1 Experiments on PASCAL VOC</p>
<p>We perform experiments on PASCAL VOC [5] that has 20 object categories. We train the models on the union set of VOC 2007 trainval and VOC 2012 trainval (“07+12”) following [6], and evaluate on VOC 2007 test set. Object detection accuracy is measured by mean Average Precision (mAP).</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><p>###4.1 PASCAL VOC上的实验</p>
<p>我们在有20个目标类别的PASCAL VOC[5]上进行实验。我们根据[6]对VOC 2007 trainval和VOC 2012 trainval（“07 + 12”）的联合数据集进行训练，并在VOC 2007测试集上进行评估。目标检测精度通过平均精度均值（mAP）来度量。</p>
<p><strong>Comparisons with Other Fully Convolutional Strategies</strong></p>
<p>Though fully convolutional detectors are available, experiments show that it is nontrivial for them to achieve good accuracy. We investigate the following fully convolutional strategies (or “almost” fully convolutional strategies that have only one classifier fc layer per RoI), using ResNet-101:</p>
<p><strong>与其它全卷积策略的比较</strong></p>
<p>虽然全卷积检测器是可用的，但是实验表明，它们要达到良好的精度是复杂的。我们使用ResNet-101研究以下全卷积策略（或“几乎”全卷积策略，每个RoI只有一个分类器全连接层）：</p>
<p><strong>Naïve Faster R-CNN</strong>. As discussed in the introduction, one may use all convolutional layers in ResNet-101 to compute the shared feature maps, and adopt RoI pooling after the last convolutional layer (after conv5). An inexpensive 21-class fc layer is evaluated on each RoI (so this variant is “almost” fully convolutional). The àtrous trick is used for fair comparisons.</p>
<p><strong>Naïve Faster R-CNN</strong>。如介绍中所讨论的，可以使用ResNet-101中的所有卷积层来计算共享特征映射，并且在最后的卷积层（conv5之后）之后采用RoI池化。在每个RoI上评估一个廉价的21类全连接层（所以这个变体是“几乎”全卷积的）。空洞窍门是用来进行公平比较的。</p>
<p><strong>Class-specific RPN</strong>. This RPN is trained following [18], except that the 2-class (object or not) convolutional classifier layer is replaced with a 21-class convolutional classifier layer. For fair comparisons, for this class-specific RPN we use ResNet-101’s conv5 layers with the àtrous trick.</p>
<p><strong>类别特定的RPN</strong>。这个RPN按照[18]进行训练，除了两类（是目标或不是）卷积分类器层被替换为21类卷积分类器层。为了公平的比较，对于这个类别特定的RPN，我们使用具有空洞窍门的ResNet-101的conv5层来处理。</p>
<p><strong>R-FCN without position-sensitivity</strong>. By setting $k = 1$ we remove the position-sensitivity of the R-FCN. This is equivalent to global pooling within each RoI.</p>
<p><strong>没有位置灵敏度的R-FCN</strong>。通过设置$k=1$，我们移除了R-FCN的位置灵敏度。这相当于在每个RoI内进行全局池化。</p>
<p>Analysis. Table 2 shows the results. We note that the standard (not naïve) Faster R-CNN in the ResNet paper [9] achieves $76.4\%$ mAP with ResNet-101 (see also Table 3), which inserts the RoI pooling layer between conv4 and conv5 [9]. As a comparison, the naïve Faster R-CNN (that applies RoI pooling after conv5) has a drastically lower mAP of $68.9\%$ (Table 2). This comparison empirically justifies the importance of respecting spatial information by inserting RoI pooling between layers for the Faster R-CNN system. Similar observations are reported in [19].</p>
<p>Table 2: Comparisons among fully convolutional (or “almost” fully convolutional) strategies using ResNet-101. All competitors in this table use the àtrous trick. Hard example mining is not conducted.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8fcdc875fd2a1f5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>Table 3: Comparisons between Faster R-CNN and R-FCN using ResNet-101. Timing is evaluated on a single Nvidia K40 GPU. With OHEM, N RoIs per image are computed in the forward pass, and 128 samples are selected for backpropagation. 300 RoIs are used for testing following [18].</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-272222106febb153.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 3"></p>
<p>分析。表2显示了结果。我们注意到在ResNet论文[9]中的标准（非简单）Faster R-CNN与ResNet-101（参见表3）达到了$76.4\%$的mAP，在conv4和conv5之间插入了RoI池化层[9]。相比之下，简单的Faster R-CNN（在conv5之后应用RoI池化）具有$68.9\%$的更低的mAP（表2）。这种比较通过在Faster R-CNN系统的层之间插入RoI池化在经验上证明了尊重空间信息的重要性。在[19]中报道了类似的观测结果。</p>
<p>表2：使用ResNet-101的全卷积（或“几乎”全卷积）策略之间的比较。表中的所有竞争者都使用了空洞窍门。不执行难例挖掘。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8fcdc875fd2a1f5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>表3：使用ResNet-101比较Faster R-CNN和R-FCN。实际是在单个Nvidia K40 GPU上评估的。使用OHEM，在前向传播中计算每张图像的N个RoI，并且选择128个样本用于反向传播。在下面的[18]中使用了300个RoI进行测试。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-272222106febb153.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 3"></p>
<p>The class-specific RPN has an mAP of $67.6\%$ (Table 2), about 9 points lower than the standard Faster R-CNN’s $76.4\%$. This comparison is in line with the observations in [6, 12] —— in fact, the class-specific RPN is similar to a special form of Fast R-CNN [6] that uses dense sliding windows as proposals, which shows inferior results as reported in [6, 12].</p>
<p>类别特定的RPN具有$67.6\%$（表2）的mAP，比标准Faster R-CNN的$76.4\%$低约9个百分点。这个比较符合[6，12]中的观测结果——实际上，类别特定的RPN类似于使用密集滑动窗口作为提议的一种特殊形式的Fast R-CNN[6]，如[6，12]中所报道的较差结果。</p>
<p>On the other hand, our R-FCN system has significantly better accuracy (Table 2). Its mAP ($76.6\%$) is on par with the standard Faster R-CNN’s ($76.4<br>%$, Table 3). These results indicate that our position-sensitive strategy manages to encode useful spatial information for locating objects, without using any learnable layer after RoI pooling.</p>
<p>另一方面，我们的R-FCN系统具有更好的准确性（表2）。其mAP（$76.6\%$）与标准Faster R-CNN（$76.4%$，表3）相当。这些结果表明，我们的位置敏感策略设法编码有用的空间信息来定位目标，而在RoI池化之后不使用任何可学习的层。</p>
<p>The importance of position-sensitivity is further demonstrated by setting $k = 1$, for which R-FCN is unable to converge. In this degraded case, no spatial information can be explicitly captured within an RoI. Moreover, we report that naïve Faster R-CNN is able to converge if its RoI pooling output resolution is 1 × 1, but the mAP further drops by a large margin to $61.7\%$ (Table 2).</p>
<p>位置灵敏度的重要性通过设置$k=1$来进一步证明，其中R-FCN不能收敛。在这种退化的情况下，在RoI内不能显式捕获空间信息。此外，我们还报告了，如果简单Faster R-CNN的ROI池化输出分辨率为1×1，其能够收敛，但是mAP进一步下降到$61.7\%$（表2）。</p>
<p><strong>Comparisons with Faster R-CNN Using ResNet-101</strong></p>
<p>Next we compare with standard “Faster R-CNN + ResNet-101” [9] which is the strongest competitor and the top-performer on the PASCAL VOC, MS COCO, and ImageNet benchmarks. We use $k × k = 7 × 7$ in the following. Table 3 shows the comparisons. Faster R-CNN evaluates a 10-layer subnetwork for each region to achieve good accuracy, but R-FCN has negligible per-region cost. With 300 RoIs at test time, Faster R-CNN takes 0.42s per image, 2.5× slower than our R-FCN that takes 0.17s per image (on a K40 GPU; this number is 0.11s on a Titan X GPU). R-FCN also trains faster than Faster R-CNN. Moreover, hard example mining [22] adds no cost to R-FCN training (Table 3). It is feasible to train R-FCN when mining from 2000 RoIs, in which case Faster R-CNN is 6× slower (2.9s vs. 0.46s). But experiments show that mining from a larger set of candidates (e.g., 2000) has no benefit (Table 3). So we use 300 RoIs for both training and inference in other parts of this paper.</p>
<p><strong>与使用ResNet-101的Faster R-CNN的比较</strong></p>
<p>接下来，我们与标准的“Faster R-CNN + ResNet-101”[9]进行比较，它是PASCAL VOC，MS COCO和ImageNet基准测试中最强劲的竞争对手和最佳表现者。我们在下面使用$k×k = 7×7$。表3显示了比较。Faster R-CNN评估了每个区域的10层子网络以达到良好的精度，但是R-FCN每个区域的成本可以忽略不计。在测试时使用300个RoI，Faster R-CNN每张图像花费0.42s，比我们的R-FCN慢了2.5倍，R-FCN每张图像只有0.17s（在K40 GPU上，这个数字在Titan X GPU上是0.11s）。R-FCN的训练速度也快于Faster R-CNN。此外，难例挖掘[22]没有增加R-FCN的训练成本（表3）。当从2000个RoI挖掘时训练R-FCN是可行的，在这种情况下，Faster R-CNN慢了6倍（2.9s vs. 0.46s）。但是实验表明，从更大的候选集（例如2000）中进行挖掘没有好处（表3）。所以我们在本文的其他部分使用了300个RoI来进行训练和推断。</p>
<p>Table 4 shows more comparisons. Following the multi-scale training in [8], we resize the image in each training iteration such that the scale is randomly sampled from {400,500,600,700,800} pixels. We still test a single scale of 600 pixels, so add no test-time cost. The mAP is $80.5\%$. In addition, we train our model on the MS COCO [13] trainval set and then fine-tune it on the PASCAL VOC set. R-FCN achieves $83.6\%$ mAP (Table 4), close to the “Faster R-CNN +++” system in [9] that uses ResNet-101 as well. We note that our competitive result is obtained at a test speed of 0.17 seconds per image, 20× faster than Faster R-CNN +++ that takes 3.36 seconds as it further incorporates iterative box regression, context, and multi-scale testing [9]. These comparisons are also observed on the PASCAL VOC 2012 test set (Table 5).</p>
<p>Table 4: Comparisons on PASCAL VOC 2007 test set using ResNet-101. “Faster R-CNN +++” [9] uses iterative box regression, context, and multi-scale testing.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-24bd5bfd1d9ccdac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 4"></p>
<p>Table 5: Comparisons on PASCAL VOC 2012 test set using ResNet-101. “07++12” [6] denotes the union set of 07 trainval+test and 12 trainval. †: <a href="http://host.robots.ox.ac.uk:8080/anonymous/44L5HI.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/44L5HI.html</a> ‡: <a href="http://host.robots.ox.ac.uk:8080/anonymous/MVCM2L.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/MVCM2L.html</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-fc5b5c7fc3e4c4bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 5"></p>
<p>表4显示了更多的比较。在[8]中的多尺度训练之后，我们在每次训练迭代中调整图像大小，使得尺度从{400,500,600,700,800}像素中进行随机地采样。我们仍然测试600像素的单尺度，所以不添加测试时间成本。mAP是$80.5\%$。此外，我们在MS COCO [13]训练验证集上训练我们的模型，然后在PASCAL VOC数据集上对其进行微调。R-FCN达到$83.6\%$mAP（表4），接近也使用ResNet-101的[9]中的“Faster R-CNN +++”系统。我们注意到，我们的竞争结果是在每张图像0.17秒的测试速度下获得的，比花费3.36秒的Faster R-CNN +++快20倍，因为它进一步结合了迭代边界框回归，上下文和多尺度测试[9]。这些比较也可以在PASCAL VOC 2012测试集上观察到（表5）。</p>
<p>表4：使用ResNet-101在PASCAL VOC 2007测试集上的比较。“Faster R-CNN +++”[9]使用迭代边界框回归，上下文和多尺度测试。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-24bd5bfd1d9ccdac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 4"></p>
<p>表5：使用ResNet-101在PASCAL VOC 2012测试集上的比较。“07 ++ 12”[6]表示07训练+测试和12训练的联合数据集。†: <a href="http://host.robots.ox.ac.uk:8080/anonymous/44L5HI.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/44L5HI.html</a> ‡: <a href="http://host.robots.ox.ac.uk:8080/anonymous/MVCM2L.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/MVCM2L.html</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-fc5b5c7fc3e4c4bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 5"></p>
<p><strong>On the Impact of Depth</strong></p>
<p>The following table shows the R-FCN results using ResNets of different depth [9]. Our detection accuracy increases when the depth is increased from 50 to 101, but gets saturated with a depth of 152.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-4cd78e7eccdab7a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Depth"></p>
<p><strong>关于深度的影响</strong></p>
<p>下表显示了使用不同深度的ResNets的R-FCN结果[9]。当深度从50增加到101时，我们的检测精度增加了，但是深度达到了152。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-4cd78e7eccdab7a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Depth"></p>
<p><strong>On the Impact of Region Proposals</strong></p>
<p>R-FCN can be easily applied with other region proposal methods, such as Selective Search (SS) [27] and Edge Boxes (EB) [28]. The following table shows the results (using ResNet-101) with different proposals. R-FCN performs competitively using SS or EB, showing the generality of our method.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-0cf820328fd6b483.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Region Proposals"></p>
<p><strong>关于区域提议的影响</strong></p>
<p>R-FCN可以很容易地应用于其它的区域提议方法，如选择性搜索（SS）[27]和边缘框（EB）[28]。下表显示了使用不同提议的结果（使用ResNet-101）。R-FCN使用SS或EB运行，竞争性地展示了我们方法的泛化性。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-0cf820328fd6b483.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Region Proposals"></p>
<h3 id="4-2-Experiments-on-MS-COCO"><a href="#4-2-Experiments-on-MS-COCO" class="headerlink" title="4.2 Experiments on MS COCO"></a>4.2 Experiments on MS COCO</h3><p>Next we evaluate on the MS COCO dataset [13] that has 80 object categories. Our experiments involve the 80k train set, 40k val set, and 20k test-dev set. We set the learning rate as 0.001 for 90k iterations and 0.0001 for next 30k iterations, with an effective mini-batch size of 8. We extend the alternating training [18] from 4-step to 5-step (i.e., stopping after one more RPN training step), which slightly improves accuracy on this dataset when the features are shared; we also report that 2-step training is sufficient to achieve comparably good accuracy but the features are not shared.</p>
<h3 id="4-2-MS-COCO上的实验"><a href="#4-2-MS-COCO上的实验" class="headerlink" title="4.2 MS COCO上的实验"></a>4.2 MS COCO上的实验</h3><p>接下来，我们评估MS COCO数据集[13]中的80个目标类别。我们的实验包括8万张训练集，4万张验证集和2万张测试开发集。我们将9万次迭代的学习率设为0.001，接下来的3万次迭代的学习率设为0.0001，有效的最小批数据大小为8。我们将交替训练[18]从4步扩展到5步（即在RPN训练步骤后停止），当共享特征时略微提高了在该数据集上的准确性；我们还报告了两步训练足以达到相当好的准确性，但不共享这些特征。</p>
<p>The results are in Table 6. Our single-scale trained R-FCN baseline has a val result of $48.9\%/27.6\%$. This is comparable to the Faster R-CNN baseline ($48.4\%/27.2\%$), but ours is 2.5× faster testing. It is noteworthy that our method performs better on objects of small sizes (defined by [13]). Our multi-scale trained (yet single-scale tested) R-FCN has a result of $49.1\%/27.8\%$ on the val set and $51.5\%/29.2\%$ on the test-dev set. Considering COCO’s wide range of object scales, we further evaluate a multi-scale testing variant following [9], and use testing scales of {200,400,600,800,1000}. The mAP is $53.2\%/31.5\%$. This result is close to the 1st-place result (Faster R-CNN +++ with ResNet-101, $55.7\%/34.9\%$) in the MS COCO 2015 competition. Nevertheless, our method is simpler and adds no bells and whistles such as context or iterative box regression that were used by [9], and is faster for both training and testing.</p>
<p>Table 6: Comparisons on MS COCO dataset using ResNet-101. The COCO-style AP is evaluated @IoU∈[0.5, 0.95]. AP@0.5 is the PASCAL-style AP evaluated @IoU=0.5.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-ec3fe15a633c74ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 6"></p>
<p>结果如表6所示。我们单尺度训练的R-FCN基准模型的验证结果为$48.9\%/27.6\%$。这与Faster R-CNN的基准模型相当（$48.4\%/27.2\%$），但我们的测试速度是Faster R-CNN的2.5倍。值得注意的是，我们的方法在小尺寸的目标上表现更好（由[13]定义）。我们的多尺度训练（但是仍是单一尺度测试）的R-FCN在验证集上的结果为$49.1\%/27.8\%$，在测试开发集上的结果是$51.5\%/29.2\%$。考虑到COCO广泛的目标尺度，按照[9]我们进一步评估多尺度的测试变种，并使用{200，400，600，800，1000}的测试尺度。mAP是$53.2\%/31.5\%$。这个结果在MS COCO 2015比赛中接近第一名的成绩（Faster R-CNN+++和ResNet-101，$55.7\%/34.9\%$）。尽管如此，我们的方法更简单，并没有添加[9]中所使用的多样功能例如上下文或迭代边界框回归，并且在训练和测试中都更快。</p>
<p>表6：使用ResNet-101在MS COCO数据集上比较。COCO式的AP在IoU∈[0.5，0.95]处评估。AP@0.5是PASCAL式的AP，在IoU=0.5处评估。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-ec3fe15a633c74ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 6"></p>
<h2 id="5-Conclusion-and-Future-Work"><a href="#5-Conclusion-and-Future-Work" class="headerlink" title="5. Conclusion and Future Work"></a>5. Conclusion and Future Work</h2><p>We presented Region-based Fully Convolutional Networks, a simple but accurate and efficient framework for object detection. Our system naturally adopts the state-of-the-art image classification backbones, such as ResNets, that are by design fully convolutional. Our method achieves accuracy competitive with the Faster R-CNN counterpart, but is much faster during both training and inference.</p>
<h2 id="5-总结和将来的工作"><a href="#5-总结和将来的工作" class="headerlink" title="5. 总结和将来的工作"></a>5. 总结和将来的工作</h2><p>我们提出了基于区域的全卷积网络，这是一个简单但精确且高效的目标检测框架。我们的系统自然地采用了设计为全卷积的最先进的图像分类骨干网络，如ResNet。我们的方法实现了与Faster R-CNN对应网络相比更具竞争力的准确性，但是在训练和推断上都快得多。</p>
<p>We intentionally keep the R-FCN system presented in the paper simple. There have been a series of orthogonal extensions of FCNs that were developed for semantic segmentation (e.g., see [2]), as well as extensions of region-based methods for object detection (e.g., see [9, 1, 22]). We expect our system will easily enjoy the benefits of the progress in the field.</p>
<p>我们故意保持R-FCN系统如论文中介绍的那样简单。已经有一系列针对语义分割（例如，参见[2]）开发的FCN的正交扩展，以及用于目标检测的基于区域的方法的扩展（例如参见[9，1，22]）。我们期望我们的系统能够轻松享有这个领域的进步带来的好处。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016.</p>
<p>[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015.</p>
<p>[3] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive fully convolutional networks.arXiv:1603.08678, 2016.</p>
<p>[4] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014.</p>
<p>[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. IJCV, 2010.</p>
<p>[6] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. </p>
<p>[8] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.</p>
<p>[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>[10] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[11] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.</p>
<p>[12] K. Lenc and A. Vedaldi. R-CNN minus R. In BMVC, 2015.</p>
<p>[13] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[14] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot multibox detector. arXiv:1512.02325v2, 2015.</p>
<p>[15] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[16] S. Mallat. A wavelet tour of signal processing. Academic press, 1999.</p>
<p>[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. In CVPR, 2016.</p>
<p>[18] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[19] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. arXiv:1504.06066, 2015.</p>
<p>[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.</p>
<p>[21] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[22] A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object detectors with online hard example mining. In CVPR, 2016.</p>
<p>[23] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[24] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.</p>
<p>[25] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In NIPS, 2013.</p>
<p>[26] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.</p>
<p>[27] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.</p>
<p>[28] C. L. Zitnick and P. Dollár. Edge boxes: Locating object proposals from edges. In ECCV, 2014.</p>
]]></content>
    
    <summary type="html">
    
      R-FCN论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux下shell显示用户名和主机名</title>
    <link href="noahsnail.com/2018/01/19/2018-01-19-Linux%E4%B8%8Bshell%E6%98%BE%E7%A4%BA%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E4%B8%BB%E6%9C%BA%E5%90%8D/"/>
    <id>noahsnail.com/2018/01/19/2018-01-19-Linux下shell显示用户名和主机名/</id>
    <published>2018-01-19T02:11:23.000Z</published>
    <updated>2018-01-19T02:34:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>最近在服务器上重新安装了一点东西，结果登录的shell界面变成了显示<code>-bash-4.2$</code>，显得很丑而且使用起来十分不方便，主要是scp时需要用户名和主机名，因此对其进行了修改。</p>
<h2 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2. 解决方案"></a>2. 解决方案</h2><p>修改当前用户的<code>.bash_profile</code>文件，在其中加入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export PS1=&apos;[\u@\h \W]$ &apos;</div></pre></td></tr></table></figure>
<p><code>source .bash_profile</code>，OK，问题解决。</p>
<h2 id="3-解释"><a href="#3-解释" class="headerlink" title="3. 解释"></a>3. 解释</h2><p>PS1是Linux终端用户的一个环境变量，用来说明命令行提示符的设置。<code>\u</code>等是特殊字符，可以通过<code>man bash</code>命令查看，其意义如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">\a     an ASCII bell character (07)</div><div class="line">\d     the date in &quot;Weekday Month Date&quot; format (e.g., &quot;Tue May 26&quot;)</div><div class="line">\D&#123;format&#125;</div><div class="line">       the format is passed to strftime(3) and the result is inserted into the prompt string; an empty format results in a locale-specific time representation.  The braces are required</div><div class="line">\e     an ASCII escape character (033)</div><div class="line">\h     the hostname up to the first `.&apos;</div><div class="line">\H     the hostname</div><div class="line">\j     the number of jobs currently managed by the shell</div><div class="line">\l     the basename of the shell&apos;s terminal device name</div><div class="line">\n     newline</div><div class="line">\r     carriage return</div><div class="line">\s     the name of the shell, the basename of $0 (the portion following the final slash)</div><div class="line">\t     the current time in 24-hour HH:MM:SS format</div><div class="line">\T     the current time in 12-hour HH:MM:SS format</div><div class="line">\@     the current time in 12-hour am/pm format</div><div class="line">\A     the current time in 24-hour HH:MM format</div><div class="line">\u     the username of the current user</div><div class="line">\v     the version of bash (e.g., 2.00)</div><div class="line">\V     the release of bash, version + patch level (e.g., 2.00.0)</div><div class="line">\w     the current working directory, with $HOME abbreviated with a tilde (uses the value of the PROMPT_DIRTRIM variable)</div><div class="line">\W     the basename of the current working directory, with $HOME abbreviated with a tilde</div><div class="line">\!     the history number of this command</div><div class="line">\#     the command number of this command</div><div class="line">\$     if the effective UID is 0, a #, otherwise a $</div><div class="line">\nnn   the character corresponding to the octal number nnn</div><div class="line">\\     a backslash</div><div class="line">\[     begin a sequence of non-printing characters, which could be used to embed a terminal control sequence into the prompt</div><div class="line">\]     end a sequence of non-printing characters</div></pre></td></tr></table></figure>
<p>部分特殊字符解释如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">\u 当前用户的用户名</div><div class="line">\h 用.分开的第一个主机名</div><div class="line">\H 完整的主机名</div><div class="line">\W 当前工作目录的目录名，只显示路径的最后一个目录</div><div class="line">\w 当前工作目录的目录名，显示全路径</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Linux下shell显示用户名和主机名
    
    </summary>
    
      <category term="Linux" scheme="noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="noahsnail.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch socket.error [Errno 111] Connection refused</title>
    <link href="noahsnail.com/2018/01/15/2018-01-15-PyTorch%20socket.error%20%5BErrno%20111%5D%20Connection%20refused/"/>
    <id>noahsnail.com/2018/01/15/2018-01-15-PyTorch socket.error [Errno 111] Connection refused/</id>
    <published>2018-01-15T07:15:49.000Z</published>
    <updated>2018-01-15T07:55:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><p>在nvidia-docker中使用PyTorch训练深度模型时，会碰到程序突然挂掉的情况，主要现象是产生<code>core</code>文件，有时输出错误信息，有时没有错误信息，进程会挂在哪里，错误信息类似于：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">socket.error: [Errno 111] Connection refused</div></pre></td></tr></table></figure>
<p>通常问题会定位在<code>/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py</code>。</p>
<h2 id="2-问题原因"><a href="#2-问题原因" class="headerlink" title="2. 问题原因"></a>2. 问题原因</h2><p>Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with <code>--ipc=host</code> or <code>--shm-size</code> command line options to nvidia-docker run.</p>
<p>主要原因在于PyTorch的数据加载是多线程的，它们使用的是共享内存来共享数据，默认的共享内存是不够的，因此需要增加共享内存。</p>
<h2 id="3-解决方案"><a href="#3-解决方案" class="headerlink" title="3. 解决方案"></a>3. 解决方案</h2><p>主要是在启动docker时加上<code>--ipc=host</code>参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nvidia-docker run --ipc=host -ti -v $(pwd):/workspace docker-image</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://github.com/pytorch/pytorch" target="_blank" rel="external">https://github.com/pytorch/pytorch</a></p>
]]></content>
    
    <summary type="html">
    
      PyTorch socket.error [Errno 111] Connection refused
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="PyTorch" scheme="noahsnail.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Jupyter Notebook的使用</title>
    <link href="noahsnail.com/2018/01/12/2018-01-12-Jupyter%20Notebook%E7%9A%84%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>noahsnail.com/2018/01/12/2018-01-12-Jupyter Notebook的快捷键/</id>
    <published>2018-01-12T06:33:19.000Z</published>
    <updated>2018-01-12T10:08:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>Jupyter Notebook有两种不同的键盘输入模式。编辑模式允许输入代码/文本到一个单元格中，并以绿色单元格边框表示，此时命令模式的快捷键不起作用。命令模式将键盘绑定到计算机级别的操作，并由具有蓝色左边距的灰色单元格边框指示，可以用快捷键命令运行单元格，移动单元格，切换单元格编辑状态等，此时编辑模式下的快捷键不起作用。</p>
<h2 id="2-快捷键"><a href="#2-快捷键" class="headerlink" title="2. 快捷键"></a>2. 快捷键</h2><p>快捷键可以在Jupyter Notebook的顶部<code>Help &gt; Keyboard Shortcuts</code>查看。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-4b1b471b4000769e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Mac键盘"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cd725ab926c6e94c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="命令模式"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-af5158fe2348a7bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="编辑模式"></p>
<h2 id="3-设置自动显示变量值"><a href="#3-设置自动显示变量值" class="headerlink" title="3. 设置自动显示变量值"></a>3. 设置自动显示变量值</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from IPython.core.interactiveshell import InteractiveShell</div><div class="line">InteractiveShell.ast_node_interactivity = &quot;all&quot;</div></pre></td></tr></table></figure>
<h2 id="4-查看文档"><a href="#4-查看文档" class="headerlink" title="4. 查看文档"></a>4. 查看文档</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">?str.split</div></pre></td></tr></table></figure>
<h2 id="5-展示绘制的图像"><a href="#5-展示绘制的图像" class="headerlink" title="5. 展示绘制的图像"></a>5. 展示绘制的图像</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div></pre></td></tr></table></figure>
<h2 id="6-查看魔法命令"><a href="#6-查看魔法命令" class="headerlink" title="6. 查看魔法命令"></a>6. 查看魔法命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%lsmagic</div></pre></td></tr></table></figure>
<p><a href="http://ipython.readthedocs.io/en/stable/interactive/magics.html" target="_blank" rel="external">魔法命令文档</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"># 设置环境变量</div><div class="line">%env TEST = &apos;test&apos;</div><div class="line"></div><div class="line"># 执行其它的ipynb文件</div><div class="line">%run ./a.ipynb</div><div class="line"></div><div class="line"># 导入文件内容</div><div class="line">%load utils.py</div><div class="line"></div><div class="line"></div><div class="line"># 列出所有的全局变量(str类型)</div><div class="line">%who str</div><div class="line"></div><div class="line"># 显示当前代码的执行时间</div><div class="line">%%time</div><div class="line"></div><div class="line"># 执行当前代码100000次，显示最快三次的均值</div><div class="line">%%timeit</div><div class="line"></div><div class="line"># 将当前单元的代码输出到文件中</div><div class="line">%%writefile a.py</div><div class="line"></div><div class="line"># 显示文件内容</div><div class="line">%pycat a.py</div><div class="line"></div><div class="line"># 执行shell命令，以!开头</div><div class="line">!ls</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/" target="_blank" rel="external">https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/</a></p>
]]></content>
    
    <summary type="html">
    
      Jupyter Notebook的使用
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu的apt-file解决依赖问题</title>
    <link href="noahsnail.com/2018/01/12/2018-01-12-Ubuntu%E7%9A%84apt-file%E8%A7%A3%E5%86%B3%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98/"/>
    <id>noahsnail.com/2018/01/12/2018-01-12-Ubuntu的apt-file解决依赖问题/</id>
    <published>2018-01-12T06:00:47.000Z</published>
    <updated>2018-01-12T06:32:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>平常我们在安装应用时，经常会碰到缺少各种文件、依赖等问题，例如缺少<code>.so</code>文件。Ubuntu中提供了一个强大的工具<code>apt-file</code>来查找依赖。最适合的场景是在Docker中使用，因为Docker Image的系统通常是Ubuntu。</p>
<h2 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h2><p><code>apt-file</code>的安装命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># Install</div><div class="line">$ apt-get update</div><div class="line">$ apt-get install apt-file</div><div class="line"></div><div class="line"># Update apt-file</div><div class="line">$ apt-file update</div></pre></td></tr></table></figure>
<h2 id="3-搜索缺少的文件"><a href="#3-搜索缺少的文件" class="headerlink" title="3. 搜索缺少的文件"></a>3. 搜索缺少的文件</h2><p>如果缺少<code>cv.py</code>文件，则搜索<code>cv.py</code>文件，<code>apt-file</code>会列出包含<code>cv.py</code>的包，发现需要的包是<code>python-opencv</code>，然后安装<code>ython-opencv</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"># 命令格式</div><div class="line">$ apt-file seach [filename]</div><div class="line"></div><div class="line"># Demo</div><div class="line">$ apt-file search cv.py</div><div class="line">gnuradio: /usr/lib/python2.7/dist-packages/gnuradio/analog/wfm_rcv.py</div><div class="line">gnuradio: /usr/share/gnuradio/examples/uhd/usrp_am_mw_rcv.py</div><div class="line">gnuradio: /usr/share/gnuradio/examples/uhd/usrp_nbfm_rcv.py</div><div class="line">gnuradio: /usr/share/gnuradio/examples/uhd/usrp_tv_rcv.py</div><div class="line">gnuradio: /usr/share/gnuradio/examples/uhd/usrp_wfm_rcv.py</div><div class="line">gnuradio: /usr/share/gnuradio/examples/uhd/usrp_wxapt_rcv.py</div><div class="line">libqpid-proton2-dev-examples: /usr/share/proton-0.10/examples/python/db_recv.py</div><div class="line">libqpid-proton2-dev-examples: /usr/share/proton-0.10/examples/python/direct_recv.py</div><div class="line">libqpid-proton2-dev-examples: /usr/share/proton-0.10/examples/python/messenger/recv.py</div><div class="line">libqpid-proton2-dev-examples: /usr/share/proton-0.10/examples/python/reactor/recv.py</div><div class="line">libqpid-proton2-dev-examples: /usr/share/proton-0.10/examples/python/selected_recv.py</div><div class="line">libqpid-proton2-dev-examples: /usr/share/proton-0.10/examples/python/simple_recv.py</div><div class="line">libqpid-proton2-dev-examples: /usr/share/proton-0.10/examples/python/tx_recv.py</div><div class="line">lowpan-test-tools: /usr/lib/x86_64-linux-gnu/lowpan-tools/test_recv.py</div><div class="line">python-kivy: /usr/lib/python2.7/dist-packages/kivy/core/camera/camera_opencv.py</div><div class="line">python-mvpa2-doc: /usr/share/doc/python-mvpa2-doc/examples/nested_cv.py</div><div class="line">python-opencv: /usr/lib/python2.7/dist-packages/cv.py</div><div class="line">python-pyavm: /usr/lib/python2.7/dist-packages/pyavm/cv.py</div><div class="line">python-pysnmp4: /usr/lib/python2.7/dist-packages/pysnmp/entity/rfc3413/ntfrcv.py</div><div class="line">python-pysnmp4: /usr/share/pyshared/pysnmp/entity/rfc3413/ntfrcv.py</div><div class="line">python-pysnmp4-doc: /usr/share/doc/python-pysnmp4-doc/examples/v1arch/manager/ntfrcv.py</div><div class="line">python-scapy: /usr/lib/python2.7/dist-packages/scapy/sendrecv.py</div><div class="line">python-scapy: /usr/share/pyshared/scapy/sendrecv.py</div><div class="line">python3-kivy: /usr/lib/python3/dist-packages/kivy/core/camera/camera_opencv.py</div><div class="line">python3-pyavm: /usr/lib/python3/dist-packages/pyavm/cv.py</div><div class="line">python3-pysnmp4: /usr/lib/python3/dist-packages/pysnmp/entity/rfc3413/ntfrcv.py</div></pre></td></tr></table></figure>
<h2 id="4-列出包中的文件"><a href="#4-列出包中的文件" class="headerlink" title="4. 列出包中的文件"></a>4. 列出包中的文件</h2><p>查看<code>python-opencv</code>中的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"># 命令格式</div><div class="line">$ apt-file list [package name]</div><div class="line"></div><div class="line"># Demo</div><div class="line">$ apt-file list python-opencv</div><div class="line">python-opencv: /usr/lib/python2.7/dist-packages/cv.py</div><div class="line">python-opencv: /usr/lib/python2.7/dist-packages/cv2.x86_64-linux-gnu.so</div><div class="line">python-opencv: /usr/share/doc/python-opencv/changelog.Debian.gz</div><div class="line">python-opencv: /usr/share/doc/python-opencv/copyright</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/__init__.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/CamShiftConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/ContourMomentsConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/ConvexHullConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/EdgeDetectionConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/FBackFlowConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/FaceDetectionConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/FindContoursConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/GeneralContoursConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/GoodfeatureTrackConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/HoughCirclesConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/HoughLinesConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/LKFlowConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/PeopleDetectConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/PhaseCorrConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/SegmentObjectsConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/SimpleFlowConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/WatershedSegmentationConfig.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/cfg/__init__.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Circle.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_CircleArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_CircleArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Contour.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_ContourArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_ContourArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Face.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_FaceArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_FaceArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Flow.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_FlowArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_FlowArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_FlowStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Line.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_LineArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_LineArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Moment.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_MomentArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_MomentArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Point2D.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Point2DArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Point2DArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Point2DStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Rect.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_RectArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_RectArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_RotatedRect.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_RotatedRectArray.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_RotatedRectArrayStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_RotatedRectStamped.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/_Size.py</div><div class="line">python-opencv-apps: /usr/lib/python2.7/dist-packages/opencv_apps/msg/__init__.py</div><div class="line">python-opencv-apps: /usr/share/doc/python-opencv-apps/changelog.Debian.gz</div><div class="line">python-opencv-apps: /usr/share/doc/python-opencv-apps/copyright</div></pre></td></tr></table></figure>
<h2 id="5-其它系统"><a href="#5-其它系统" class="headerlink" title="5. 其它系统"></a>5. 其它系统</h2><p>在其它系统中，例如CentOS中，可以用<code>yum whatprovides</code>命令来查询命令所在的包。</p>
]]></content>
    
    <summary type="html">
    
      Ubuntu的apt-file解决依赖问题
    
    </summary>
    
      <category term="Linux" scheme="noahsnail.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="noahsnail.com/tags/Linux/"/>
    
  </entry>
  
</feed>
