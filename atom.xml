<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://noahsnail.com/"/>
  <updated>2018-03-20T10:54:56.000Z</updated>
  <id>http://noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Feature Pyramid Networks for Object Detection论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2018/03/20/2018-03-20-Feature%20Pyramid%20Networks%20for%20Object%20Detection%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2018/03/20/2018-03-20-Feature Pyramid Networks for Object Detection论文翻译——中英文对照/</id>
    <published>2018-03-20T10:30:55.000Z</published>
    <updated>2018-03-20T10:54:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Feature-Pyramid-Networks-for-Object-Detection"><a href="#Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="Feature Pyramid Networks for Object Detection"></a>Feature Pyramid Networks for Object Detection</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>特征金字塔是识别系统中用于检测不同尺度目标的基本组件。但最近的深度学习目标检测器已经避免了金字塔表示，部分原因是它们是计算和内存密集型的。在本文中，我们利用深度卷积网络内在的多尺度、金字塔分级来构造具有很少额外成本的特征金字塔。开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。此外，我们的方法可以在GPU上以6FPS运行，因此是多尺度目标检测的实用和准确的解决方案。代码将公开发布。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Recognizing objects at vastly different scales is a fundamental challenge in computer vision. Feature pyramids built upon image pyramids (for short we call these featurized image pyramids) form the basis of a standard solution [1] (Fig. 1(a)). These pyramids are scale-invariant in the sense that an object’s scale change is offset by shifting its level in the pyramid. Intuitively, this property enables a model to detect objects across a large range of scales by scanning the model over both positions and pyramid levels.</p>
<p>Featurized image pyramids were heavily used in the era of hand-engineered features [5, 25]. They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (e.g., 10 scales per octave). For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (ConvNets) [19, 20]. Aside from being capable of representing higher-level semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)). But even with this robustness, pyramids are still needed to get the most accurate results. All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on featurized image pyramids (e.g., [16, 35]). The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.</p>
<p>Nevertheless, featurizing each level of an image pyramid has obvious limitations. Inference time increases considerably (e.g., by four times [11]), making this approach impractical for real applications. Moreover, training deep networks end-to-end on an image pyramid is infeasible in terms of memory, and so, if exploited, image pyramids are used only at test time [15, 11, 16, 35], which creates an inconsistency between train/test-time inference. For these reasons, Fast and Faster R-CNN [11, 29] opt to not use featurized image pyramids under default settings.</p>
<p>However, image pyramids are not the only way to compute a multi-scale feature representation. A deep ConvNet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape. This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths. The high-resolution maps have low-level features that harm their representational capacity for object recognition.</p>
<p>The Single Shot Detector (SSD) [22] is one of the first attempts at using a ConvNet’s pyramidal feature hierarchy as if it were a featurized image pyramid (Fig. 1(c)). Ideally, the SSD-style pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost. But to avoid using low-level features SSD foregoes reusing already computed layers and instead builds the pyramid starting from high up in the network (e.g., conv4 3 of VGG nets [36]) and then by adding several new layers. Thus it misses the opportunity to reuse the higher-resolution maps of the feature hierarchy. We show that these are important for detecting small objects.</p>
<p>The goal of this paper is to naturally leverage the pyra- midal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales. To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections (Fig. 1(d)). The result is a feature pyramid that has rich semantics at all levels and is built quickly from a single input image scale. In other words, we show how to create in-network feature pyramids that can be used to replace featurized image pyramids with- out sacrificing representational power, speed, or memory.</p>
<p>Similar architectures adopting top-down and skip connections are popular in recent research [28, 17, 8, 26]. Their goals are to produce a single high-level feature map of a fine resolution on which the predictions are to be made (Fig. 2 top). On the contrary, our method leverages the architecture as a feature pyramid where predictions (e.g., object detections) are independently made on each level (Fig. 2 bottom). Our model echoes a featurized image pyramid, which has not been explored in these works.</p>
<p>We evaluate our method, called a Feature Pyramid Net- work (FPN), in various systems for detection and segmentation [11, 29, 27]. Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on FPN and a basic Faster R-CNN detector [29], surpassing all exist- ing heavily-engineered single-model entries of competition winners. In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16]. Our method is also easily extended to mask proposals and improves both instance segmentation AR and speed over state-of-the-art methods that heavily depend on image pyramids.</p>
<p>In addition, our pyramid structure can be trained end-to-end with all scales and is used consistently at train/test time, which would be memory-infeasible using image pyramids. As a result, FPNs are able to achieve higher accuracy than all existing state-of-the-art methods. Moreover, this improvement is achieved without increasing testing time over the single-scale baseline. We believe these advances will facilitate future research and applications. Our code will be made publicly available.</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Hand-engineered features and early neural networks.</strong> SIFT features [25] were originally extracted at scale-space extrema and used for feature point matching. HOG features [5], and later SIFT features as well, were computed densely over entire image pyramids. These HOG and SIFT pyramids have been used in numerous works for image classification, object detection, human pose estimation, and more. There has also been significant interest in computing featurized image pyramids quickly. Dollar et al.[6] demonstrated fast pyramid computation by first computing a sparsely sampled (in scale) pyramid and then interpolating missing levels. Before HOG and SIFT, early work on face detection with ConvNets [38, 32] computed shallow networks over image pyramids to detect faces across scales.</p>
<p><strong>Deep ConvNet object detectors</strong>. With the development of modern deep ConvNets [19], object detectors like OverFeat [34] and R-CNN [12] showed dramatic improvements in accuracy. OverFeat adopted a strategy similar to early neural network face detectors by applying a ConvNet as a sliding window detector on an image pyramid. R-CNN adopted a region proposal-based strategy [37] in which each proposal was scale-normalized before classifying with a ConvNet. SPPnet [15] demonstrated that such region-based detectors could be applied much more efficiently on feature maps extracted on a single image scale. Recent and more accurate detection methods like Fast R-CNN [11] and Faster R-CNN [29] advocate using features computed from a single scale, because it offers a good trade-off between accuracy and speed. Multi-scale detection, however, still performs better, especially for small objects.</p>
<p><strong>Methods using multiple layers</strong>. A number of recent approaches improve detection and segmentation by using different layers in a ConvNet. FCN [24] sums partial scores for each category over multiple scales to compute semantic segmentations. Hypercolumns [13] uses a similar method for object instance segmentation. Several other approaches (HyperNet [18], ParseNet [23], and ION [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features. SSD [22] and MS-CNN [3] predict objects at multiple layers of the feature hierarchy without combining features or scores.</p>
<p>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation. Ghiasi et al. [8] present a Laplacian pyramid presentation for FCNs to progressively refine segmentation. Although these methods adopt architectures with pyramidal shapes, they are unlike featurized image pyramids [5, 7, 34] where predictions are made independently at all levels, see Fig. 2. In fact, for the pyramidal architecture in Fig. 2 (top), image pyramids are still needed to recognize objects across multiple scales [28].</p>
<h2 id="3-Feature-Pyramid-Networks"><a href="#3-Feature-Pyramid-Networks" class="headerlink" title="3. Feature Pyramid Networks"></a>3. Feature Pyramid Networks</h2><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image processing. RCA engineer, 1984.</p>
<p>[2] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016.</p>
<p>[3] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection. In ECCV, 2016.</p>
<p>[4] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive fully convolutional networks. In ECCV, 2016.</p>
<p>[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.</p>
<p>[6] P. Dollar, R. Appel, S. Belongie, and P. Perona. Fast feature pyramids for object detection. TPAMI, 2014.</p>
<p>[7] P.F.Felzenszwalb,R.B.Girshick,D.McAllester,andD.Ramanan. Object detection with discriminatively trained part-based models. TPAMI, 2010.</p>
<p>[8] G.GhiasiandC.C.Fowlkes.Laplacianpyramidreconstruction and refinement for semantic segmentation. In ECCV, 2016.</p>
<p>[9] S. Gidaris and N. Komodakis. Object detection via a multi-region &amp; semantic segmentation-aware CNN model. In ICCV, 2015.</p>
<p>[10] S. Gidaris and N. Komodakis. Attend refine repeat: Active box proposal generation via in-out localization. In BMVC, 2016.</p>
<p>[11] R. Girshick. Fast R-CNN. In ICCV, 2015.</p>
<p>[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.</p>
<p>[13] B.Hariharan,P.Arbelaez,R.Girshick,andJ.Malik.Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015.</p>
<p>[14] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn. arXiv:1703.06870, 2017.</p>
<p>[15] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.</p>
<p>[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>[17] S. Honari, J. Yosinski, P. Vincent, and C. Pal. Recombinator networks: Learning coarse-to-fine feature aggregation. In CVPR, 2016.</p>
<p>[18] T. Kong, A. Yao, Y. Chen, and F. Sun. Hypernet: Towards accurate region proposal generation and joint object detection. In CVPR, 2016.</p>
<p>[19] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.</p>
<p>[20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.</p>
<p>[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.</p>
<p>[22] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot multibox detector. In ECCV, 2016.</p>
<p>[23] W. Liu, A. Rabinovich, and A. C. Berg. ParseNet: Looking wider to see better. In ICLR workshop, 2016.</p>
<p>[24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.</p>
<p>[25] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.</p>
<p>[26] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In ECCV, 2016.</p>
<p>[27] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to segment object candidates. In NIPS, 2015.</p>
<p>[28] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár. Learning to refine object segments. In ECCV, 2016.</p>
<p>[29] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015.</p>
<p>[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. PAMI, 2016.</p>
<p>[31] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image segmentation. In MIC- CAI, 2015.</p>
<p>[32] H. Rowley, S. Baluja, and T. Kanade. Human face detection in visual scenes. Technical Report CMU-CS-95-158R, Carnegie Mellon University, 1995.</p>
<p>[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.</p>
<p>[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.</p>
<p>[35] A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object detectors with online hard example mining. In CVPR, 2016.</p>
<p>[36] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.</p>
<p>[37] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.</p>
<p>[38] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE Proc. on Vision, Image, and Signal Processing, 1994.</p>
<p>[39] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC, 2016.</p>
<p>[40] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chintala, and P. Dollár. A multipath network for object detection. In BMVC, 2016. 10</p>
]]></content>
    
    <summary type="html">
    
      Feature Pyramid Networks for Object Detection论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Feature Pyramid Networks for Object Detection论文翻译——中文版</title>
    <link href="http://noahsnail.com/2018/03/20/2018-03-20-Feature%20Pyramid%20Networks%20for%20Object%20Detection%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2018/03/20/2018-03-20-Feature Pyramid Networks for Object Detection论文翻译——中文版/</id>
    <published>2018-03-20T10:30:15.000Z</published>
    <updated>2018-03-20T10:43:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Feature-Pyramid-Networks-for-Object-Detection"><a href="#Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="Feature Pyramid Networks for Object Detection"></a>Feature Pyramid Networks for Object Detection</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>特征金字塔是识别系统中用于检测不同尺度目标的基本组件。但最近的深度学习目标检测器已经避免了金字塔表示，部分原因是它们是计算和内存密集型的。在本文中，我们利用深度卷积网络内在的多尺度、金字塔分级来构造具有很少额外成本的特征金字塔。开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。此外，我们的方法可以在GPU上以6FPS运行，因此是多尺度目标检测的实用和准确的解决方案。代码将公开发布。</p>
]]></content>
    
    <summary type="html">
    
      Feature Pyramid Networks for Object Detection论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的优化方法</title>
    <link href="http://noahsnail.com/2018/03/19/2018-03-19-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    <id>http://noahsnail.com/2018/03/19/2018-03-19-深度学习中的优化方法/</id>
    <published>2018-03-19T03:33:03.000Z</published>
    <updated>2018-03-20T02:01:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-优化算法"><a href="#1-优化算法" class="headerlink" title="1. 优化算法"></a>1. 优化算法</h2><h2 id="2-SGD"><a href="#2-SGD" class="headerlink" title="2. SGD"></a>2. SGD</h2><h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3. Momentum"></a>3. Momentum</h2><h2 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4. Adam"></a>4. Adam</h2><h2 id="5-Adamgrad"><a href="#5-Adamgrad" class="headerlink" title="5. Adamgrad"></a>5. Adamgrad</h2>]]></content>
    
    <summary type="html">
    
      深度学习中的优化方法
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(三)——丢弃法(gluon)</title>
    <link href="http://noahsnail.com/2018/03/15/2018-03-15-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%89)%E2%80%94%E2%80%94%E4%B8%A2%E5%BC%83%E6%B3%95(gluon)/"/>
    <id>http://noahsnail.com/2018/03/15/2018-03-15-动手学深度学习(三)——丢弃法(gluon)/</id>
    <published>2018-03-15T10:53:32.000Z</published>
    <updated>2018-03-15T10:55:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="定义模型并添加丢弃层"><a href="#定义模型并添加丢弃层" class="headerlink" title="定义模型并添加丢弃层"></a>定义模型并添加丢弃层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义模型</span></div><div class="line">net = nn.Sequential()</div><div class="line"><span class="comment"># 丢弃概率</span></div><div class="line">drop_prob1 = <span class="number">0.2</span></div><div class="line">drop_prob2 = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 添加层</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    <span class="comment"># 将输入数据展开</span></div><div class="line">    net.add(nn.Flatten())</div><div class="line">    <span class="comment"># 第一个全连接层</span></div><div class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</div><div class="line">    <span class="comment"># 添加丢弃层</span></div><div class="line">    net.add(nn.Dropout(drop_prob1))</div><div class="line">    <span class="comment"># 第二个全连接层</span></div><div class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</div><div class="line">    <span class="comment"># 添加丢弃层</span></div><div class="line">    net.add(nn.Dropout(drop_prob2))</div><div class="line">    <span class="comment"># 定义输出层</span></div><div class="line">    net.add(nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 初始化模型参数</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="读取数据并训练"><a href="#读取数据并训练" class="headerlink" title="读取数据并训练"></a>读取数据并训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 加载数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div><div class="line"></div><div class="line"><span class="comment"># 优化</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 梯度反向传播</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 更新梯度</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 记录训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 记录训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 计算测试准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.817475, Train acc 0.697349, Test acc 0.778145
Epoch 1. Loss: 0.515098, Train acc 0.810831, Test acc 0.847456
Epoch 2. Loss: 0.458402, Train acc 0.833450, Test acc 0.823918
Epoch 3. Loss: 0.419452, Train acc 0.846554, Test acc 0.862079
Epoch 4. Loss: 0.396483, Train acc 0.854067, Test acc 0.874499
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(三)——丢弃法(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(三)——丢弃法(从零开始)</title>
    <link href="http://noahsnail.com/2018/03/14/2018-03-14-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%89)%E2%80%94%E2%80%94%E4%B8%A2%E5%BC%83%E6%B3%95(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/03/14/2018-03-14-动手学深度学习(三)——丢弃法(从零开始)/</id>
    <published>2018-03-14T11:02:04.000Z</published>
    <updated>2018-03-15T10:43:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="丢弃法的概念"><a href="#丢弃法的概念" class="headerlink" title="丢弃法的概念"></a>丢弃法的概念</h2><p>在现代神经网络中，我们所指的丢弃法，通常是对输入层或者隐含层做以下操作：</p>
<ul>
<li>随机选择一部分该层的输出作为丢弃元素；</li>
<li>把丢弃元素乘以0；</li>
<li>把非丢弃元素拉伸。</li>
</ul>
<h2 id="丢弃法的实现"><a href="#丢弃法的实现" class="headerlink" title="丢弃法的实现"></a>丢弃法的实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy, SGD</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 实现dropout</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_probability)</span>:</span></div><div class="line">    <span class="comment"># 计算保留数据的比例</span></div><div class="line">    keep_probability = <span class="number">1</span> - drop_probability</div><div class="line">    <span class="comment"># 确保drop_probability的输入合法</span></div><div class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= keep_probability &lt;= <span class="number">1</span></div><div class="line">    <span class="comment"># 丢弃所有元素</span></div><div class="line">    <span class="keyword">if</span> keep_probability == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span> X.zeros_like()</div><div class="line">    <span class="comment"># 随机生成一个相同纬度的矩阵, 根据随机值和keep_probability的对比确定是否丢弃该元素</span></div><div class="line">    mask = nd.random.uniform(<span class="number">0</span>, <span class="number">1.0</span>, X.shape, ctx=X.context) &lt; keep_probability</div><div class="line">    <span class="comment"># 保证 E[dropout(X)] == X, 对剩下的数据进行缩放</span></div><div class="line">    scale = <span class="number">1</span> / keep_probability</div><div class="line">    <span class="keyword">return</span> mask * X * scale</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试dropout</span></div><div class="line">A = nd.arange(<span class="number">20</span>).reshape((<span class="number">5</span>,<span class="number">4</span>))</div><div class="line">dropout(A, <span class="number">0.0</span>)</div></pre></td></tr></table></figure>
<pre><code>[[  0.   1.   2.   3.]
 [  4.   5.   6.   7.]
 [  8.   9.  10.  11.]
 [ 12.  13.  14.  15.]
 [ 16.  17.  18.  19.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dropout(A, <span class="number">1.0</span>)</div></pre></td></tr></table></figure>
<pre><code>[[ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dropout(A, <span class="number">0.5</span>)</div></pre></td></tr></table></figure>
<pre><code>[[  0.   2.   4.   0.]
 [  8.   0.  12.   0.]
 [ 16.  18.   0.   0.]
 [  0.   0.   0.  30.]
 [  0.  34.  36.   0.]]
&lt;NDArray 5x4 @cpu(0)&gt;
</code></pre><h2 id="丢弃法的本质"><a href="#丢弃法的本质" class="headerlink" title="丢弃法的本质"></a>丢弃法的本质</h2><p>一般来说，在集成学习里，我们可以对训练数据集有放回地采样若干次并分别训练若干个不同的分类器；测试时，把这些分类器的结果集成一下作为最终分类结果。事实上，丢弃法在模拟集成学习。丢弃法实质上是对每一个这样的数据集分别训练一个原神经网络子集的分类器。与一般的集成学习不同，这里每个原神经网络子集的分类器用的是同一套参数。因此丢弃法只是在模拟集成学习。使用丢弃法的神经网络实质上是对输入层和隐含层的参数做了正则化：学到的参数使得原神经网络不同子集在训练数据上都尽可能表现良好。</p>
<h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 加载数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="含两个隐藏层的多层感知机"><a href="#含两个隐藏层的多层感知机" class="headerlink" title="含两个隐藏层的多层感知机"></a>含两个隐藏层的多层感知机</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型输入大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"></div><div class="line"><span class="comment"># 模型输出大小</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 第一个隐藏层节点数量</span></div><div class="line">num_hidden1 = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 第二个隐藏层节点数量</span></div><div class="line">num_hidden2 = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 随机数据时的标准差</span></div><div class="line">weight_scale = <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 第一个隐藏层权重</span></div><div class="line">W1 = nd.random_normal(shape=(num_inputs, num_hidden1), scale=weight_scale)</div><div class="line"><span class="comment"># 第一个隐藏层偏置</span></div><div class="line">b1 = nd.zeros(num_hidden1)</div><div class="line"></div><div class="line"><span class="comment"># 第二个隐藏层权重</span></div><div class="line">W2 = nd.random_normal(shape=(num_hidden1, num_hidden2), scale=weight_scale)</div><div class="line"><span class="comment"># 第二个隐藏层偏置</span></div><div class="line">b2 = nd.zeros(num_hidden2)</div><div class="line"></div><div class="line"><span class="comment"># 输出层权重</span></div><div class="line">W3 = nd.random_normal(shape=(num_hidden2, num_outputs), scale=weight_scale)</div><div class="line"><span class="comment"># 输出层偏置</span></div><div class="line">b3 = nd.zeros(num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W1, b1, W2, b2, W3, b3]</div><div class="line"></div><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="定义包含丢弃层的模型"><a href="#定义包含丢弃层的模型" class="headerlink" title="定义包含丢弃层的模型"></a>定义包含丢弃层的模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 第一个隐藏层的丢弃概率</span></div><div class="line">drop_prob1 = <span class="number">0.2</span></div><div class="line"><span class="comment"># 第二个隐藏层的丢弃概率</span></div><div class="line">drop_prob2 = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</div><div class="line">    <span class="comment"># 第一层全连接</span></div><div class="line">    h1 = nd.relu(nd.dot(X, W1) + b1)</div><div class="line">    <span class="comment"># 在第一层全连接后添加丢弃层</span></div><div class="line">    h1 = dropout(h1, drop_prob1)</div><div class="line">    <span class="comment"># 第二层全连接</span></div><div class="line">    h2 = nd.relu(nd.dot(h1, W2) + b2)</div><div class="line">    <span class="comment"># 在第二层全连接后添加丢弃层</span></div><div class="line">    h2 = dropout(h2, drop_prob2)</div><div class="line">    <span class="comment"># 返回输出</span></div><div class="line">    <span class="keyword">return</span> nd.dot(h2, W3) + b3</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># 定义学习率</span></div><div class="line">learning_rate = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 梯度反向传播</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># SGD更新梯度</span></div><div class="line">        SGD(params, learning_rate / batch_size)</div><div class="line">        <span class="comment"># 记录训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 记录训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 计算测试准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 1.221062, Train acc 0.528746, Test acc 0.754006
Epoch 1. Loss: 0.598503, Train acc 0.774890, Test acc 0.813101
Epoch 2. Loss: 0.499490, Train acc 0.818493, Test acc 0.840244
Epoch 3. Loss: 0.457343, Train acc 0.832699, Test acc 0.835036
Epoch 4. Loss: 0.426575, Train acc 0.846070, Test acc 0.849159
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(三)——丢弃法(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——正则化(gluon)</title>
    <link href="http://noahsnail.com/2018/03/09/2018-03-09-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96(gluon)/"/>
    <id>http://noahsnail.com/2018/03/09/2018-03-09-动手学深度学习(二)——正则化(gluon)/</id>
    <published>2018-03-09T10:07:25.000Z</published>
    <updated>2018-03-09T10:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="高维线性回归数据集"><a href="#高维线性回归数据集" class="headerlink" title="高维线性回归数据集"></a>高维线性回归数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">random.seed(<span class="number">2</span>)</div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># 输入数据特征维度</span></div><div class="line">num_inputs = <span class="number">200</span></div><div class="line"></div><div class="line"><span class="comment"># 实际权重</span></div><div class="line">true_w = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 实际偏置</span></div><div class="line">true_b = <span class="number">0.05</span></div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">X = nd.random_normal(shape=(num_train + num_test, num_inputs))</div><div class="line">y = nd.dot(X, true_w) + true_b</div><div class="line"></div><div class="line"><span class="comment"># 添加随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据和测试数据</span></div><div class="line">X_train, X_test = X[:num_train, :], X[num_train:, :]</div><div class="line">y_train, y_test = y[:num_train], y[num_train:]</div></pre></td></tr></table></figure>
<h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># 创建数据集</span></div><div class="line">dataset_train = gluon.data.ArrayDataset(X_train, y_train)</div><div class="line"></div><div class="line"><span class="comment"># 读取数据</span></div><div class="line">data_iter = gluon.data.DataLoader(dataset_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 损失函数</span></div><div class="line">square_loss = gluon.loss.L2Loss()</div><div class="line"></div><div class="line"><span class="comment"># 测试</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(net, X, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> square_loss(net(X), y).mean().asscalar()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(weight_decay)</span>:</span></div><div class="line">    <span class="comment"># 定义训练的迭代周期</span></div><div class="line">    epochs = <span class="number">10</span></div><div class="line">    <span class="comment"># 定义学习率</span></div><div class="line">    learning_rate = <span class="number">0.005</span></div><div class="line">    <span class="comment"># 定义网络</span></div><div class="line">    net = gluon.nn.Sequential()</div><div class="line">    <span class="keyword">with</span> net.name_scope():</div><div class="line">        net.add(gluon.nn.Dense(<span class="number">1</span>))</div><div class="line">    <span class="comment">#net.collect_params().initialize(mx.init.Normal(sigma=1))</span></div><div class="line">    <span class="comment"># 初始化网络参数</span></div><div class="line">    net.initialize(mx.init.Normal(sigma=<span class="number">1</span>))</div><div class="line">    <span class="comment"># SGD训练, 使用权重衰减代替L2正则化</span></div><div class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: learning_rate, <span class="string">'wd'</span>: weight_decay&#125;)</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">            <span class="comment"># 记录梯度</span></div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                <span class="comment"># 计算预测值</span></div><div class="line">                output = net(data)</div><div class="line">                <span class="comment"># 计算损失</span></div><div class="line">                loss = square_loss(output, label)</div><div class="line">            <span class="comment"># 反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新权重</span></div><div class="line">            trainer.step(batch_size)</div><div class="line">            <span class="comment"># 训练损失</span></div><div class="line">            train_loss.append(test(net, X_train, y_train))</div><div class="line">            <span class="comment"># 测试损失</span></div><div class="line">            test_loss.append(test(net, X_test, y_test))</div><div class="line">    <span class="comment"># 绘制图像</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>,<span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> (<span class="string">'learned w[:10]:'</span>, net[<span class="number">0</span>].weight.data()[:,:<span class="number">10</span>], <span class="string">'\nlearned b:'</span>, net[<span class="number">0</span>].bias.data())</div></pre></td></tr></table></figure>
<h2 id="训练模型并观察过拟合"><a href="#训练模型并观察过拟合" class="headerlink" title="训练模型并观察过拟合"></a>训练模型并观察过拟合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-bd038b77c2ef488a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Overfitting"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 1.04817235 -0.02568591  0.86764944  0.29322273  0.01006198 -0.56152564
    0.38436413 -0.3084037  -2.32450151  0.03733355]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;\nlearned b:&apos;, 
 [ 0.79914868]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="使用Gluon的正则化"><a href="#使用Gluon的正则化" class="headerlink" title="使用Gluon的正则化"></a>使用Gluon的正则化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">5</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8d88c219f34f10ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Normal"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 0.00107634 -0.00052574  0.00450234 -0.00110544 -0.00683913 -0.00181657
   -0.00530634  0.00512847 -0.00742552 -0.00058494]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;\nlearned b:&apos;, 
 [ 0.00449433]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="可用权重衰减代替L2正则化的原因"><a href="#可用权重衰减代替L2正则化的原因" class="headerlink" title="可用权重衰减代替L2正则化的原因"></a>可用权重衰减代替L2正则化的原因</h2><p><img src="http://upload-images.jianshu.io/upload_images/3232548-c9da036502d1949d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="推导"></p>
<p>注：图片来自Gluon社区。</p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——正则化(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——正则化(从零开始)</title>
    <link href="http://noahsnail.com/2018/03/08/2018-03-08-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/03/08/2018-03-08-动手学深度学习(二)——正则化(从零开始)/</id>
    <published>2018-03-08T11:09:49.000Z</published>
    <updated>2018-03-08T11:12:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="高维线性回归"><a href="#高维线性回归" class="headerlink" title="高维线性回归"></a>高维线性回归</h2><p>使用线性函数$y = 0.05 + \sum_{i = 1}^p 0.01x_i +  \text{noise}$生成数据样本，噪音服从均值0和标准差为0.01的正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">random.seed(<span class="number">2</span>)</div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div></pre></td></tr></table></figure>
<h2 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># 输入数据特征维度</span></div><div class="line">num_inputs = <span class="number">200</span></div><div class="line"></div><div class="line"><span class="comment"># 实际权重</span></div><div class="line">true_w = nd.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 实际偏置</span></div><div class="line">true_b = <span class="number">0.05</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成数据</span></div><div class="line">X = nd.random_normal(shape=(num_train + num_test, num_inputs))</div><div class="line">y = nd.dot(X, true_w) + true_b</div><div class="line"></div><div class="line"><span class="comment"># 添加随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据和测试数据</span></div><div class="line">X_train, X_test = X[:num_train, :], X[num_train:, :]</div><div class="line">y_train, y_test = y[:num_train], y[num_train:]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># 通过yield进行数据读取</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(num_examples)</span>:</span></div><div class="line">    <span class="comment"># 产生样本的索引</span></div><div class="line">    idx = list(range(num_examples))</div><div class="line">    <span class="comment"># 将索引随机打乱</span></div><div class="line">    random.shuffle(idx)</div><div class="line">    <span class="comment"># 迭代一个epoch</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</div><div class="line">        <span class="comment"># 依次取出样本的索引, 这种实现方式在num_examples/batch_size不能整除时也适用</span></div><div class="line">        j = nd.array(idx[i:min((i + batch_size), num_examples)])</div><div class="line">        <span class="comment"># 根据提供的索引取元素</span></div><div class="line">        <span class="keyword">yield</span> nd.take(X, j), nd.take(y, j)</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># 随机初始化权重w</span></div><div class="line">    w = nd.random_normal(shape=(num_inputs, <span class="number">1</span>))</div><div class="line">    <span class="comment"># 偏置b初始化为0</span></div><div class="line">    b = nd.zeros((<span class="number">1</span>,))</div><div class="line">    <span class="comment"># w, b放入list里</span></div><div class="line">    params = [w, b]</div><div class="line"></div><div class="line">    <span class="comment"># 需要计算反向传播, 添加自动求导</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        param.attach_grad()</div><div class="line">    <span class="keyword">return</span> params</div></pre></td></tr></table></figure>
<h2 id="L-2-范数正则化"><a href="#L-2-范数正则化" class="headerlink" title="$L_2$ 范数正则化"></a>$L_2$ 范数正则化</h2><p>在训练时最小化函数为：$\text{loss} + \lambda \sum_{p \in \textrm{params}}|p|_2^2。$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># L2范数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2_penalty</span><span class="params">(w, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> ((w**<span class="number">2</span>).sum() + b ** <span class="number">2</span>) / <span class="number">2</span></div></pre></td></tr></table></figure>
<h2 id="定义训练和测试"><a href="#定义训练和测试" class="headerlink" title="定义训练和测试"></a>定义训练和测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X, w, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.dot(X, w) + b</div><div class="line"></div><div class="line"><span class="comment"># 损失函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_loss</span><span class="params">(yhat, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> (yhat - y.reshape(yhat.shape)) ** <span class="number">2</span> / <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># 梯度下降</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        param[:] = param - lr * param.grad / batch_size</div><div class="line"></div><div class="line"><span class="comment"># 测试</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(net, params, X, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> square_loss(net(X, *params), y).mean().asscalar()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(_lambda)</span>:</span></div><div class="line">    <span class="comment"># 定义训练的迭代周期</span></div><div class="line">    epochs = <span class="number">10</span></div><div class="line">    <span class="comment"># 定义学习率</span></div><div class="line">    learning_rate = <span class="number">0.005</span></div><div class="line">    <span class="comment"># 初始化参数</span></div><div class="line">    w, b = params = init_params()</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter(num_train):</div><div class="line">            <span class="comment"># 记录梯度</span></div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                 <span class="comment"># 计算预测值</span></div><div class="line">                output = net(data, *params)</div><div class="line">                <span class="comment"># 计算loss</span></div><div class="line">                loss = square_loss(output, label) + _lambda * L2_penalty(*params)</div><div class="line">            <span class="comment"># 反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新梯度</span></div><div class="line">            sgd(params, learning_rate, batch_size)</div><div class="line">            <span class="comment"># 训练损失</span></div><div class="line">            train_loss.append(test(net, params, X_train, y_train))</div><div class="line">            <span class="comment"># 测试损失</span></div><div class="line">            test_loss.append(test(net, params, X_test, y_test))</div><div class="line">            </div><div class="line">    <span class="comment"># 绘制损失图像</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> <span class="string">'learned w[:10]:'</span>, w[:<span class="number">10</span>].T, <span class="string">'learned b:'</span>, b</div></pre></td></tr></table></figure>
<h1 id="观察过拟合"><a href="#观察过拟合" class="headerlink" title="观察过拟合"></a>观察过拟合</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-bd038b77c2ef488a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Overfitting"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 1.04817176 -0.02568593  0.86764956  0.29322267  0.01006179 -0.56152576
    0.38436398 -0.30840367 -2.32450151  0.03733355]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;learned b:&apos;, 
 [ 0.79914856]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="使用正则化"><a href="#使用正则化" class="headerlink" title="使用正则化"></a>使用正则化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(<span class="number">5</span>)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8d88c219f34f10ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Normal"></p>
<pre><code>(&apos;learned w[:10]:&apos;, 
 [[ 0.00107633 -0.00052574  0.00450233 -0.00110545 -0.0068391  -0.00181657
   -0.00530632  0.00512845 -0.00742549 -0.00058495]]
 &lt;NDArray 1x10 @cpu(0)&gt;, &apos;learned b:&apos;, 
 [ 0.00449432]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——正则化(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——欠拟合和过拟合</title>
    <link href="http://noahsnail.com/2018/03/01/2018-03-01-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>http://noahsnail.com/2018/03/01/2018-03-01-动手学深度学习(二)——欠拟合和过拟合/</id>
    <published>2018-03-01T10:49:47.000Z</published>
    <updated>2018-03-08T11:09:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h1 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h1><h3 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h3><p>机器学习模型在训练数据集上表现出的误差叫做训练误差，在任意一个测试数据样本上表现出的误差的期望值叫做泛化误差。</p>
<p>统计学习理论的一个假设是：训练数据集和测试数据集里的每一个数据样本都是从同一个概率分布中相互独立地生成出的（独立同分布假设）。</p>
<p>一个重要结论是：训练误差的降低不一定意味着泛化误差的降低。机器学习既需要降低训练误差，又需要降低泛化误差。</p>
<h3 id="欠拟合和过拟合-1"><a href="#欠拟合和过拟合-1" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h3><ul>
<li>欠拟合：机器学习模型无法得到较低训练误差。</li>
<li>过拟合：机器学习模型的训练误差远小于其在测试数据集上的误差。</li>
</ul>
<h3 id="模型的选择"><a href="#模型的选择" class="headerlink" title="模型的选择"></a>模型的选择</h3><p>模型拟合能力和误差之间的关系如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f50a8a6737e3927b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="error_model_complexity.png"></p>
<h3 id="训练数据集的大小"><a href="#训练数据集的大小" class="headerlink" title="训练数据集的大小"></a>训练数据集的大小</h3><p>一般来说，如果训练数据集过小，特别是比模型参数数量更小时，过拟合更容易发生。除此之外，泛化误差不会随训练数据集里样本数量增加而增大。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-7e957140a2d8242c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="model_vs_data.png"></p>
<h3 id="多项式拟合"><a href="#多项式拟合" class="headerlink" title="多项式拟合"></a>多项式拟合</h3><p>给定一个<strong>标量</strong>数据点集合<code>x</code>和对应的标量目标值<code>y</code>，多项式拟合的目标是找一个K阶多项式，其由向量<code>w</code>和位移<code>b</code>组成，来最好地近似每个样本<code>x</code>和<code>y</code>。用数学符号来表示就是我们将学<code>w</code>和<code>b</code>来预测</p>
<p>$$\hat{y} = b + \sum_{k=1}^K x^k w_k$$</p>
<p>并以平方误差为损失函数，一阶多项式拟合又叫线性拟合。</p>
<h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><p>使用二阶多项式来生成每一个数据样本，$y=1.2x−3.4x^2+5.6x^3+5.0+noise$，噪音服从均值0和标准差为0.1的正态分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练数据数量</span></div><div class="line">num_train = <span class="number">100</span></div><div class="line"><span class="comment"># 测试数据数量</span></div><div class="line">num_test = <span class="number">100</span></div><div class="line"><span class="comment"># 多项式权重</span></div><div class="line">true_w = [<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>]</div><div class="line"><span class="comment"># 多项式偏置</span></div><div class="line">true_b = <span class="number">5.0</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成随机数据x</span></div><div class="line">x = nd.random.normal(shape=(num_train + num_test, <span class="number">1</span>))</div><div class="line"><span class="comment"># 计算x的多项式值</span></div><div class="line">X = nd.concat(x, nd.power(x, <span class="number">2</span>), nd.power(x, <span class="number">3</span>))</div><div class="line"><span class="comment"># 计算y</span></div><div class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_w[<span class="number">2</span>] * X[:, <span class="number">2</span>] + true_b</div><div class="line"><span class="comment"># 查看数据</span></div><div class="line">(<span class="string">'x:'</span>, x[:<span class="number">5</span>], <span class="string">'X:'</span>, X[:<span class="number">5</span>], <span class="string">'y:'</span>, y[:<span class="number">5</span>])</div></pre></td></tr></table></figure>
<pre><code>(200L,)
</code></pre><h2 id="定义训练和测试步骤"><a href="#定义训练和测试步骤" class="headerlink" title="定义训练和测试步骤"></a>定义训练和测试步骤</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</div><div class="line">mpl.rcParams[<span class="string">'figure.dpi'</span>]= <span class="number">120</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义训练过程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X_train, X_test, y_train, y_test)</span>:</span></div><div class="line">    <span class="comment"># 定义线性回归模型</span></div><div class="line">    net = gluon.nn.Sequential()</div><div class="line">    <span class="keyword">with</span> net.name_scope():</div><div class="line">        net.add(gluon.nn.Dense(<span class="number">1</span>))</div><div class="line">    <span class="comment"># 权重初始化</span></div><div class="line">    net.initialize()</div><div class="line">    <span class="comment"># 学习率</span></div><div class="line">    learning_rate = <span class="number">0.01</span></div><div class="line">    <span class="comment"># 迭代周期</span></div><div class="line">    epochs = <span class="number">100</span></div><div class="line">    <span class="comment"># 训练的批数据大小</span></div><div class="line">    batch_size = min(<span class="number">10</span>, y_train.shape[<span class="number">0</span>])</div><div class="line">    <span class="comment"># 创建训练数据集</span></div><div class="line">    dataset_train = gluon.data.ArrayDataset(X_train, y_train)</div><div class="line">    <span class="comment"># 读取数据</span></div><div class="line">    data_iter_train = gluon.data.DataLoader(dataset_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 训练方法SGD</span></div><div class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: learning_rate&#125;)</div><div class="line">    <span class="comment"># 定义损失函数</span></div><div class="line">    square_loss = gluon.loss.L2Loss()</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = []</div><div class="line">    <span class="comment"># 测试损失</span></div><div class="line">    test_loss = []</div><div class="line">    <span class="comment"># 进行训练</span></div><div class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</div><div class="line">        <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter_train:</div><div class="line">            <span class="keyword">with</span> autograd.record():</div><div class="line">                <span class="comment"># 进行预测</span></div><div class="line">                output = net(data)</div><div class="line">                <span class="comment"># 计算预测值与实际值之间的损失</span></div><div class="line">                loss = square_loss(output, label)</div><div class="line">            <span class="comment"># 损失进行反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新权重</span></div><div class="line">            trainer.step(batch_size)</div><div class="line">        <span class="comment"># 保存训练损失</span></div><div class="line">        train_loss.append(square_loss(net(X_train), y_train).mean().asscalar())</div><div class="line">        <span class="comment"># 保存测试损失</span></div><div class="line">        test_loss.append(square_loss(net(X_test), y_test).mean().asscalar())</div><div class="line">    <span class="comment"># 绘制损失</span></div><div class="line">    plt.plot(train_loss)</div><div class="line">    plt.plot(test_loss)</div><div class="line">    plt.legend([<span class="string">'train'</span>,<span class="string">'test'</span>])</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span> (<span class="string">'learned weight'</span>, net[<span class="number">0</span>].weight.data(), <span class="string">'learned bias'</span>, net[<span class="number">0</span>].bias.data())</div></pre></td></tr></table></figure>
<h2 id="三阶多项式拟合（正常）"><a href="#三阶多项式拟合（正常）" class="headerlink" title="三阶多项式拟合（正常）"></a>三阶多项式拟合（正常）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(X[:num_train, :], X[num_train:, :], y[:num_train], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-4500dba51617c3ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 1.22117233 -3.39606118  5.59531116]]
 &lt;NDArray 1x3 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [ 4.98550272]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="线性拟合（欠拟合）"><a href="#线性拟合（欠拟合）" class="headerlink" title="线性拟合（欠拟合）"></a>线性拟合（欠拟合）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(x[:num_train, :], x[num_train:, :], y[:num_train], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cac456292c149670.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 19.74101448]]
 &lt;NDArray 1x1 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [-0.23861444]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="训练量不足（过拟合）"><a href="#训练量不足（过拟合）" class="headerlink" title="训练量不足（过拟合）"></a>训练量不足（过拟合）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(X[<span class="number">0</span>:<span class="number">2</span>, :], X[num_train:, :], y[<span class="number">0</span>:<span class="number">2</span>], y[num_train:])</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-cc94f0d732ebd573.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<pre><code>(&apos;learned weight&apos;, 
 [[ 3.10832024 -0.740421    4.85165691]]
 &lt;NDArray 1x3 @cpu(0)&gt;, &apos;learned bias&apos;, 
 [ 0.29450524]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>训练误差的降低并不一定意味着泛化误差的降低。</li>
<li>欠拟合和过拟合都是需要尽量避免的。我们要注意模型的选择和训练量的大小。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——欠拟合和过拟合
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——多层感知机(gluon)</title>
    <link href="http://noahsnail.com/2018/02/28/2018-02-28-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(gluon)/"/>
    <id>http://noahsnail.com/2018/02/28/2018-02-28-动手学深度学习(二)——多层感知机(gluon)/</id>
    <published>2018-02-28T08:42:14.000Z</published>
    <updated>2018-02-28T08:42:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 按顺序堆叠网络层</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># name_scope作用, 方便管理参数名称</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    net.add(gluon.nn.Flatten())</div><div class="line">    net.add(gluon.nn.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</div><div class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 参数初始化</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练器和优化方法</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.819048, Train acc 0.728666, Test acc 0.768530
Epoch 1. Loss: 0.550646, Train acc 0.808644, Test acc 0.823618
Epoch 2. Loss: 0.488554, Train acc 0.829210, Test acc 0.845553
Epoch 3. Loss: 0.457407, Train acc 0.839493, Test acc 0.842448
Epoch 4. Loss: 0.438059, Train acc 0.845486, Test acc 0.852063
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——多层感知机(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(二)——多层感知机(从零开始)</title>
    <link href="http://noahsnail.com/2018/02/27/2018-02-27-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%BA%8C)%E2%80%94%E2%80%94%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/02/27/2018-02-27-动手学深度学习(二)——多层感知机(从零开始)/</id>
    <published>2018-02-27T10:35:46.000Z</published>
    <updated>2018-02-27T10:35:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy, SGD</div></pre></td></tr></table></figure>
<h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输入数据大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"><span class="comment"># 输出数据大小, 分为10类</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 隐藏单元个数</span></div><div class="line">num_hidden = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 正态分布的标准差</span></div><div class="line">weight_scale = <span class="number">0.01</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化输入层权重</span></div><div class="line">W1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale)</div><div class="line">b1 = nd.zeros(num_hidden)</div><div class="line"></div><div class="line"><span class="comment"># 随机初始化隐藏层权重</span></div><div class="line">W2 = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale)</div><div class="line">b2 = nd.zeros(num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W1, b1, W2, b2]</div><div class="line"></div><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 激活函数使用ReLU, relu(x)=max(x,0)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.maximum(X, <span class="number">0</span>)</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># 输入数据重排</span></div><div class="line">    X = X.reshape((<span class="number">-1</span>, num_inputs))</div><div class="line">    <span class="comment"># 计算激活值</span></div><div class="line">    h1 = relu(nd.dot(X, W1) + b1)</div><div class="line">    <span class="comment"># 计算输出</span></div><div class="line">    output = nd.dot(h1, W2) + b2</div><div class="line">    <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment">## 定义学习率</span></div><div class="line">learning_rate = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        SGD(params, learning_rate/batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 1.042064, Train acc 0.630976, Test acc 0.776142
Epoch 1. Loss: 0.601578, Train acc 0.788862, Test acc 0.815204
Epoch 2. Loss: 0.525148, Train acc 0.816556, Test acc 0.835136
Epoch 3. Loss: 0.486619, Train acc 0.829427, Test acc 0.833033
Epoch 4. Loss: 0.459395, Train acc 0.836104, Test acc 0.835136
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(二)——多层感知机(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——逻辑回归(gluon)</title>
    <link href="http://noahsnail.com/2018/02/22/2018-02-22-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(gluon)/"/>
    <id>http://noahsnail.com/2018/02/22/2018-02-22-动手学深度学习(一)——逻辑回归(gluon)/</id>
    <published>2018-02-22T09:58:07.000Z</published>
    <updated>2018-02-22T09:58:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</div></pre></td></tr></table></figure>
<h2 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data_fashion_mnist, accuracy, evaluate_accuracy</div></pre></td></tr></table></figure>
<h2 id="获取和读取数据"><a href="#获取和读取数据" class="headerlink" title="获取和读取数据"></a>获取和读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 获取训练数据和测试数据</span></div><div class="line">train_data, test_data = load_data_fashion_mnist(batch_size)</div></pre></td></tr></table></figure>
<h2 id="定义和初始化模型"><a href="#定义和初始化模型" class="headerlink" title="定义和初始化模型"></a>定义和初始化模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个空的模型</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># name_scope作用, 方便管理参数命名</span></div><div class="line"><span class="keyword">with</span> net.name_scope():</div><div class="line">    <span class="comment"># 加入一个平铺层, 其会将输入数据平铺为batch_size*?维</span></div><div class="line">    net.add(gluon.nn.Flatten())</div><div class="line">    <span class="comment"># 加入一个全连接层, 输出为10类</span></div><div class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 参数初始化</span></div><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="Softmax和交叉熵损失函数"><a href="#Softmax和交叉熵损失函数" class="headerlink" title="Softmax和交叉熵损失函数"></a>Softmax和交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练器和优化方法</span></div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代训练</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = softmax_cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 梯度下降</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 0.793821, Train acc 0.744107, Test acc 0.786659
Epoch 1. Loss: 0.575076, Train acc 0.809879, Test acc 0.820112
Epoch 2. Loss: 0.530560, Train acc 0.822583, Test acc 0.831731
Epoch 3. Loss: 0.506161, Train acc 0.829728, Test acc 0.835837
Epoch 4. Loss: 0.488752, Train acc 0.834769, Test acc 0.834135
</code></pre>]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——逻辑回归(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——逻辑回归(从零开始)</title>
    <link href="http://noahsnail.com/2018/02/09/2018-02-09-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B)/"/>
    <id>http://noahsnail.com/2018/02/09/2018-02-09-动手学深度学习(一)——逻辑回归(从零开始)/</id>
    <published>2018-02-09T06:23:27.000Z</published>
    <updated>2018-02-09T07:18:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<p>注意：mxnet随机种子设为1时，loss一直为nan，经测试，种子为2时，jupyter-notebook有时会出现nan，但在命令行执行python文件多次都不会出现nan。</p>
<h2 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 数据预处理</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label)</span>:</span></div><div class="line">    <span class="keyword">return</span> data.astype(<span class="string">'float32'</span>) / <span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 加载训练数据</span></div><div class="line">mnist_train = gluon.data.vision.FashionMNIST(train=<span class="keyword">True</span>, transform=transform)</div><div class="line"></div><div class="line"><span class="comment"># 加载测试数据</span></div><div class="line">mnist_test = gluon.data.vision.FashionMNIST(train=<span class="keyword">False</span>, transform=transform)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 取出单条数据</span></div><div class="line">data, label = mnist_train[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="comment"># 查看数据</span></div><div class="line">(<span class="string">'example shape: '</span>, data.shape, <span class="string">'label:'</span>, label)</div></pre></td></tr></table></figure>
<pre><code>(&apos;example shape: &apos;, (28L, 28L, 1L), &apos;label:&apos;, 2.0)
</code></pre><h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 显示图像</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span><span class="params">(images)</span>:</span></div><div class="line">    <span class="comment"># 获得图像的数量</span></div><div class="line">    n = images.shape[<span class="number">0</span>]</div><div class="line">    <span class="comment"># 绘制子图</span></div><div class="line">    _, figs = plt.subplots(<span class="number">1</span>, n, figsize=(<span class="number">15</span>, <span class="number">15</span>))</div><div class="line">    <span class="comment"># 遍历绘制图像</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(n):</div><div class="line">        <span class="comment"># 显示图像</span></div><div class="line">        figs[i].imshow(images[i].reshape((<span class="number">28</span>, <span class="number">28</span>)).asnumpy())</div><div class="line">        </div><div class="line">        <span class="comment"># 显示灰度图</span></div><div class="line">        <span class="comment">#figs[i].imshow(images[i].reshape((28, 28)).asnumpy(), cmap="gray")</span></div><div class="line">        </div><div class="line">        <span class="comment"># 不显示坐标轴</span></div><div class="line">        figs[i].axes.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">        figs[i].axes.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    plt.show()</div><div class="line">    </div><div class="line"><span class="comment"># 获取图像对应的文本标签</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text_labels</span><span class="params">(labels)</span>:</span></div><div class="line">    <span class="comment"># 图像标签对应的文本</span></div><div class="line">    text_labels = [</div><div class="line">        <span class="string">'t-shirt'</span>, <span class="string">'trouser'</span>, <span class="string">'pullover'</span>, <span class="string">'dress,'</span>, <span class="string">'coat'</span>,</div><div class="line">        <span class="string">'sandal'</span>, <span class="string">'shirt'</span>, <span class="string">'sneaker'</span>, <span class="string">'bag'</span>, <span class="string">'ankle boot'</span></div><div class="line">    ]</div><div class="line">    <span class="comment"># 返回图像标签对应的文本</span></div><div class="line">    <span class="keyword">return</span> [text_labels[int(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</div><div class="line"></div><div class="line"><span class="comment"># 取出训练集的前9条数据</span></div><div class="line">data, label = mnist_train[<span class="number">0</span>:<span class="number">9</span>]</div><div class="line"></div><div class="line"><span class="comment"># 显示数据</span></div><div class="line">show_images(data)</div><div class="line"><span class="comment"># 显示标签</span></div><div class="line"><span class="keyword">print</span> get_text_labels(label)</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-8cb1872a2eba78be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure"></p>
<pre><code>[&apos;pullover&apos;, &apos;ankle boot&apos;, &apos;shirt&apos;, &apos;t-shirt&apos;, &apos;dress,&apos;, &apos;coat&apos;, &apos;coat&apos;, &apos;sandal&apos;, &apos;coat&apos;]
</code></pre><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 批数据大小</span></div><div class="line">batch_size = <span class="number">256</span></div><div class="line"></div><div class="line"><span class="comment"># 读取训练数据</span></div><div class="line">train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># 读取测试数据</span></div><div class="line">test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输入数据大小</span></div><div class="line">num_inputs = <span class="number">28</span> * <span class="number">28</span></div><div class="line"><span class="comment"># 输出数据大小, 分为10类</span></div><div class="line">num_outputs = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化权重</span></div><div class="line">W = nd.random_normal(shape=(num_inputs, num_outputs))</div><div class="line"><span class="comment"># 随机初始化偏置</span></div><div class="line">b = nd.random_normal(shape=num_outputs)</div><div class="line"></div><div class="line"><span class="comment"># 参数数组</span></div><div class="line">params = [W, b]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 需要计算梯度, 添加自动求导</span></div><div class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">    param.attach_grad()</div></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义softmax</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># 计算e^x</span></div><div class="line">    exp = nd.exp(X)</div><div class="line">    <span class="comment"># 假设exp是矩阵，这里对行进行求和，并要求保留axis 1, 就是返回 (nrows, 1) 形状的矩阵</span></div><div class="line">    partition = exp.sum(axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 对exp进行归一化</span></div><div class="line">    <span class="keyword">return</span> exp / partition</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试softmax</span></div><div class="line"></div><div class="line"><span class="comment"># 随机初始化数据</span></div><div class="line">X = nd.random_normal(shape=(<span class="number">2</span>, <span class="number">5</span>))</div><div class="line"><span class="comment"># 求softmax</span></div><div class="line">X_prob = softmax(X)</div><div class="line"><span class="comment"># 输出结果</span></div><div class="line"><span class="keyword">print</span> X_prob</div><div class="line"><span class="comment"># 对每行概率求和, 如果和为1, 说明没问题</span></div><div class="line"><span class="keyword">print</span> X_prob.sum(axis=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<pre><code>[[ 0.06774449  0.5180918   0.1474141   0.11459844  0.1521512 ]
 [ 0.23102701  0.47666225  0.10536087  0.09706162  0.08988826]]
&lt;NDArray 2x5 @cpu(0)&gt;

[ 1.  1.]
&lt;NDArray 2 @cpu(0)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义模型</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> softmax(nd.dot(X.reshape((<span class="number">-1</span>, num_inputs)), W) + b)</div></pre></td></tr></table></figure>
<h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵损失</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(yhat, y)</span>:</span></div><div class="line">    <span class="keyword">return</span> -nd.pick(nd.log(yhat), y)</div></pre></td></tr></table></figure>
<h2 id="计算精度"><a href="#计算精度" class="headerlink" title="计算精度"></a>计算精度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算分类准确率</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span></div><div class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>) == label).asscalar()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 评估整个数据集的分类精度</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net)</span>:</span></div><div class="line">    <span class="comment"># 最终的准确率</span></div><div class="line">    acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 遍历测试数据集</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</div><div class="line">        output = net(data)</div><div class="line">        <span class="comment"># 累加准确率</span></div><div class="line">        acc += accuracy(output, label)</div><div class="line">    <span class="comment"># 返回平均准确率</span></div><div class="line">    <span class="keyword">return</span> acc / len(data_iterator)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 测试随机初始化参数的分类准确率</span></div><div class="line">evaluate_accuracy(test_data, net)</div></pre></td></tr></table></figure>
<pre><code>0.15156249999999999
</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义SGD</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(params, lr)</span>:</span></div><div class="line">    <span class="comment"># 对参数进行梯度下降</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</div><div class="line">        <span class="comment"># 这样写不会创建新的param, 而是会写在原来的param里, 新的param没有梯度</span></div><div class="line">        param[:] = param - lr * param.grad</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 学习率</span></div><div class="line">learning_rate = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># 定义迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    <span class="comment"># 训练损失</span></div><div class="line">    train_loss = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 训练集准确率</span></div><div class="line">    train_acc = <span class="number">0.0</span></div><div class="line">    <span class="comment"># 迭代巡礼</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算输出</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = cross_entropy(output, label)</div><div class="line">        <span class="comment"># 反向传播求梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 将梯度做平均，这样学习率会对batch size不那么敏感, 避免学习率与batch_size耦合</span></div><div class="line">        SGD(params, learning_rate / batch_size)</div><div class="line">        <span class="comment"># 总的训练损失</span></div><div class="line">        train_loss += nd.mean(loss).asscalar()</div><div class="line">        <span class="comment"># 总的训练准确率</span></div><div class="line">        train_acc += accuracy(output, label)</div><div class="line">    </div><div class="line">    <span class="comment"># 测试集的准确率</span></div><div class="line">    test_acc = evaluate_accuracy(test_data, net)</div><div class="line">    </div><div class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</div><div class="line">        epoch, train_loss / len(train_data), train_acc / len(train_data), test_acc))</div></pre></td></tr></table></figure>
<pre><code>Epoch 0. Loss: 3.725043, Train acc 0.487916, Test acc 0.602344
Epoch 1. Loss: 1.965692, Train acc 0.632812, Test acc 0.666895
Epoch 2. Loss: 1.630362, Train acc 0.677349, Test acc 0.691016
Epoch 3. Loss: 1.450728, Train acc 0.701690, Test acc 0.710352
Epoch 4. Loss: 1.329035, Train acc 0.717996, Test acc 0.720410
</code></pre><ul>
<li>learning_rate = 1</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Epoch 0. Loss: 18.803541, Train acc 0.081366, Test acc 0.085840</div><div class="line">Epoch 1. Loss: 18.806381, Train acc 0.081394, Test acc 0.085840</div><div class="line">Epoch 2. Loss: 18.795504, Train acc 0.081449, Test acc 0.085840</div><div class="line">Epoch 3. Loss: 18.798995, Train acc 0.081505, Test acc 0.085840</div><div class="line">Epoch 4. Loss: 18.794157, Train acc 0.081505, Test acc 0.085840</div></pre></td></tr></table></figure>
<ul>
<li>learning_rate = 0.01</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Epoch 0. Loss: 10.406917, Train acc 0.115165, Test acc 0.169629</div><div class="line">Epoch 1. Loss: 5.512807, Train acc 0.246349, Test acc 0.304590</div><div class="line">Epoch 2. Loss: 3.911078, Train acc 0.373232, Test acc 0.412695</div><div class="line">Epoch 3. Loss: 3.218969, Train acc 0.454189, Test acc 0.471191</div><div class="line">Epoch 4. Loss: 2.855459, Train acc 0.502371, Test acc 0.509082</div></pre></td></tr></table></figure>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 从测试集中取数据</span></div><div class="line">data, label = mnist_test[<span class="number">0</span>:<span class="number">9</span>]</div><div class="line"><span class="comment"># 显示图片及真实标签</span></div><div class="line">show_images(data)</div><div class="line"><span class="keyword">print</span> <span class="string">'True labels: '</span></div><div class="line"><span class="keyword">print</span> get_text_labels(label)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 预测标签</span></div><div class="line">predicted_labels = net(data).argmax(axis=<span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">'Predicted labels: '</span></div><div class="line"><span class="keyword">print</span> get_text_labels(predicted_labels.asnumpy())</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3d41ebd81c95b691.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure"></p>
<pre><code>True labels: 
[&apos;t-shirt&apos;, &apos;trouser&apos;, &apos;pullover&apos;, &apos;pullover&apos;, &apos;dress,&apos;, &apos;pullover&apos;, &apos;bag&apos;, &apos;shirt&apos;, &apos;sandal&apos;]
Predicted labels: 
[&apos;shirt&apos;, &apos;trouser&apos;, &apos;pullover&apos;, &apos;pullover&apos;, &apos;dress,&apos;, &apos;bag&apos;, &apos;bag&apos;, &apos;shirt&apos;, &apos;sandal&apos;]
</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>FashionMNIST<br><a href="https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/vision.html#FashionMNIST" target="_blank" rel="external">https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/vision.html#FashionMNIST</a>  </li>
</ul>
<h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——逻辑回归(从零开始)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python中的__all__</title>
    <link href="http://noahsnail.com/2018/02/08/2018-02-08-Python%E4%B8%AD%E7%9A%84__all__/"/>
    <id>http://noahsnail.com/2018/02/08/2018-02-08-Python中的__all__/</id>
    <published>2018-02-08T08:19:15.000Z</published>
    <updated>2018-02-08T08:43:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-动机"><a href="#1-动机" class="headerlink" title="1. 动机"></a>1. 动机</h2><p>今天看MXNet的gluon源码时发现<code>mxnet.gluon.data.vision</code>有<code>__all__</code>如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">__all__ = [&apos;MNIST&apos;, &apos;FashionMNIST&apos;, &apos;CIFAR10&apos;, &apos;CIFAR100&apos;,</div><div class="line">           &apos;ImageRecordDataset&apos;, &apos;ImageFolderDataset&apos;]</div></pre></td></tr></table></figure>
<h2 id="2-作用"><a href="#2-作用" class="headerlink" title="2. 作用"></a>2. 作用</h2><p><code>__all__</code>是一个字符串list，用来定义模块中对于<code>from XXX import *</code>时要对外导出的符号，即要暴露的借口，但它只对<code>import *</code>起作用，对<code>from XXX import XXX</code>不起作用。</p>
<h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3. 测试"></a>3. 测试</h2><p><code>all.py</code>文件时要导出的模块，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">__all__ = [&apos;x&apos;, &apos;y&apos;, &apos;test&apos;]</div><div class="line"></div><div class="line">x = 2</div><div class="line">y = 3</div><div class="line">z = 4</div><div class="line"></div><div class="line">def test():</div><div class="line">	print(&apos;test&apos;)</div></pre></td></tr></table></figure>
<ul>
<li>测试文件一</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from foo import *</div><div class="line">print(&apos;x: &apos;, x)</div><div class="line"></div><div class="line">print(&apos;y: &apos;, y)</div><div class="line">print(&apos;z: &apos;, z)</div><div class="line"></div><div class="line">test()</div></pre></td></tr></table></figure>
<ul>
<li>运行结果：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x:  2</div><div class="line">y:  3</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;test.py&quot;, line 6, in &lt;module&gt;</div><div class="line">    print(&apos;z: &apos;, z)</div><div class="line">NameError: name &apos;z&apos; is not defined</div></pre></td></tr></table></figure>
<ul>
<li>测试文件二</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from foo import *</div><div class="line">from foo import z</div><div class="line">print(&apos;x: &apos;, x)</div><div class="line"></div><div class="line">print(&apos;y: &apos;, y)</div><div class="line">print(&apos;z: &apos;, z)</div><div class="line"></div><div class="line">test()</div></pre></td></tr></table></figure>
<ul>
<li>运行结果</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x:  2</div><div class="line">y:  3</div><div class="line">z:  4</div><div class="line">test</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python" target="_blank" rel="external">https://stackoverflow.com/questions/44834/can-someone-explain-all-in-python</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python中的__all__
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>使用Docker搭建Anaconda Python3.6的练习环境</title>
    <link href="http://noahsnail.com/2018/02/08/2018-02-08-%E4%BD%BF%E7%94%A8Docker%E6%90%AD%E5%BB%BAAnaconda%20Python3.6%E7%9A%84%E7%BB%83%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    <id>http://noahsnail.com/2018/02/08/2018-02-08-使用Docker搭建Anaconda Python3.6的练习环境/</id>
    <published>2018-02-08T07:34:43.000Z</published>
    <updated>2018-02-08T08:18:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>最近在看Python 3的相关内容，由于电脑里已经装了Anaconda 2.7，因此就在Docker里搭建了一个Anaconda Python3.6的练习环境。Dockerfile如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">FROM nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04</div><div class="line">MAINTAINER Tyan &lt;tyan.liu.git@gmail.com&gt;</div><div class="line"></div><div class="line"></div><div class="line"># Install basic dependencies</div><div class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</div><div class="line">        build-essential \</div><div class="line">        cmake \</div><div class="line">        git \</div><div class="line">        wget \</div><div class="line">        libopencv-dev \</div><div class="line">        libsnappy-dev \</div><div class="line">        python-dev \</div><div class="line">        python-pip \</div><div class="line">        tzdata \</div><div class="line">        vim</div><div class="line"></div><div class="line"></div><div class="line"># Install anaconda for python 3.6</div><div class="line">RUN wget --quiet https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh -O ~/anaconda.sh &amp;&amp; \</div><div class="line">    /bin/bash ~/anaconda.sh -b -p /opt/conda &amp;&amp; \</div><div class="line">    rm ~/anaconda.sh &amp;&amp; \</div><div class="line">    echo &quot;export PATH=/opt/conda/bin:$PATH&quot; &gt;&gt; ~/.bashrc</div><div class="line"></div><div class="line"></div><div class="line"># Set timezone</div><div class="line">RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</div><div class="line"></div><div class="line"></div><div class="line"># Set locale</div><div class="line">ENV LANG C.UTF-8 LC_ALL=C.UTF-8</div><div class="line"></div><div class="line"></div><div class="line"># Initialize workspace</div><div class="line">RUN mkdir /workspace</div><div class="line">WORKDIR /workspace</div></pre></td></tr></table></figure>
<p>构建docker image命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker build -t python:3.6 .</div></pre></td></tr></table></figure>
<p>运行docker image命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -ti --rm python:3.6</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      使用Docker搭建Anaconda Python3.6的练习环境
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>nohup python缓存问题</title>
    <link href="http://noahsnail.com/2018/02/07/2018-02-07-nohup%20python%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98/"/>
    <id>http://noahsnail.com/2018/02/07/2018-02-07-nohup python缓存问题/</id>
    <published>2018-02-07T09:08:12.000Z</published>
    <updated>2018-02-08T07:28:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>深度学习用python跑数据时，经常会用到<code>nohup</code>命令，通常的命令格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup python [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<p>如果没有指定输出文件，<code>nohup</code>会将输出放到<code>nohup.out</code>文件中，但在程序运行过程中<code>nohup.out</code>文件中不能实时的看到python的输出，原因是python的输出有缓冲。</p>
<p>解决方案如下：</p>
<ul>
<li>方案一</li>
</ul>
<p>使用<code>-u</code>参数，使python输出不进行缓冲，命令格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nohup python -u [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<ul>
<li>方案二</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export PYTHONUNBUFFERED=1</div><div class="line">nohup python [python source file] (&gt; [log file]) 2&gt;&amp;1 &amp;</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://stackoverflow.com/questions/12919980/nohup-is-not-writing-log-to-output-file" target="_blank" rel="external">https://stackoverflow.com/questions/12919980/nohup-is-not-writing-log-to-output-file</a></p>
]]></content>
    
    <summary type="html">
    
      nohup python缓存问题
    
    </summary>
    
      <category term="Python" scheme="http://noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习(一)——线性回归(gluon)</title>
    <link href="http://noahsnail.com/2018/02/06/2018-02-06-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E4%B8%80)%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(gluon)/"/>
    <id>http://noahsnail.com/2018/02/06/2018-02-06-动手学深度学习(一)——线性回归(gluon)/</id>
    <published>2018-02-06T10:59:38.000Z</published>
    <updated>2018-02-09T06:31:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>注：本文为李沐大神的《动手学深度学习》的课程笔记！</strong></p>
<h2 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入mxnet</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment"># 导入mxnet的gluon, ndarray, autograd</span></div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"></div><div class="line"><span class="comment"># 设置随机种子</span></div><div class="line">mx.random.seed(<span class="number">1</span>)</div><div class="line">random.seed(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 训练数据的维度</span></div><div class="line">num_inputs = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># 训练数据的样本数量</span></div><div class="line">num_examples = <span class="number">1000</span></div><div class="line"></div><div class="line"><span class="comment"># 实际的权重w</span></div><div class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</div><div class="line"></div><div class="line"><span class="comment"># 实际的偏置b</span></div><div class="line">true_b = <span class="number">4.2</span></div><div class="line"></div><div class="line"><span class="comment"># 随机生成均值为0, 方差为1, 服从正态分布的训练数据X, </span></div><div class="line">X = nd.random_normal(shape=(num_examples, num_inputs))</div><div class="line"></div><div class="line"><span class="comment"># 根据X, w, b生成对应的输出y</span></div><div class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_b </div><div class="line"></div><div class="line"><span class="comment"># 给y加上随机噪声</span></div><div class="line">y += <span class="number">0.01</span> * nd.random_normal(shape=y.shape)</div></pre></td></tr></table></figure>
<h2 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 绘制数据的散点图 </span></div><div class="line">plt.scatter(X[:, <span class="number">1</span>].asnumpy(), y.asnumpy())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1ddbc52187302d98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="数据展示"></p>
<h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 训练时的批数据大小</span></div><div class="line">batch_size = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># 创建数据集</span></div><div class="line">dataset = gluon.data.ArrayDataset(X, y)</div><div class="line"></div><div class="line"><span class="comment"># 读取数据</span></div><div class="line">data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 查看数据</span></div><div class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">    <span class="keyword">print</span> data, label</div><div class="line">    <span class="keyword">break</span></div></pre></td></tr></table></figure>
<pre><code>[[-2.11255503  0.61242002]
 [ 2.18546367 -0.48856559]
 [ 0.91085583  0.38985687]
 [-0.56097323  1.44421673]
 [ 0.31765923 -1.75729597]
 [-0.57738042  2.03963804]
 [-0.91808975  0.64181799]
 [-0.20269176  0.21012937]
 [-0.22549874  0.19895147]
 [ 1.42844415  0.06982213]]
&lt;NDArray 10x2 @cpu(0)&gt; 
[ -2.11691356  10.22533131   4.70613146  -1.82755637  10.82125568
  -3.88111711   0.17608714   3.07074499   3.06542921   6.82972908]
&lt;NDArray 10 @cpu(0)&gt;
</code></pre><h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个空的模型</span></div><div class="line">net = gluon.nn.Sequential()</div><div class="line"></div><div class="line"><span class="comment"># 加入一个Dense层</span></div><div class="line">net.add(gluon.nn.Dense(<span class="number">1</span>))</div></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net.initialize()</div></pre></td></tr></table></figure>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">square_loss = gluon.loss.L2Loss()</div></pre></td></tr></table></figure>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.01</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义训练的迭代周期</span></div><div class="line">epochs = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(epochs):</div><div class="line">    <span class="comment"># 总的loss</span></div><div class="line">    total_loss = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</div><div class="line">        <span class="comment"># 记录梯度</span></div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            <span class="comment"># 计算预测值</span></div><div class="line">            output = net(data)</div><div class="line">            <span class="comment"># 计算loss</span></div><div class="line">            loss = square_loss(output, label)</div><div class="line">        <span class="comment"># 根据loss进行反向传播计算梯度</span></div><div class="line">        loss.backward()</div><div class="line">        <span class="comment"># 更新权重, batch_size用来进行梯度平均</span></div><div class="line">        trainer.step(batch_size)</div><div class="line">        <span class="comment"># 计算总的loss</span></div><div class="line">        total_loss += nd.sum(loss).asscalar()</div><div class="line">        </div><div class="line">    <span class="keyword">print</span> <span class="string">"Epoch %d, average loss: %f"</span> % (epoch, total_loss/num_examples)</div></pre></td></tr></table></figure>
<pre><code>Epoch 0, average loss: 7.403182
Epoch 1, average loss: 0.854247
Epoch 2, average loss: 0.099864
Epoch 3, average loss: 0.011887
Epoch 4, average loss: 0.001479
</code></pre><h2 id="代码地址"><a href="#代码地址" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><p>ArrayDataset<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.ArrayDataset" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.ArrayDataset</a>  </p>
</li>
<li><p>DataLoader<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataload#mxnet.gluon.data.DataLoader" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataload#mxnet.gluon.data.DataLoader</a>  </p>
</li>
<li><p>Sequential<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=gluon.nn.sequential#mxnet.gluon.nn.Sequential" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=gluon.nn.sequential#mxnet.gluon.nn.Sequential</a>  </p>
</li>
<li><p>L2Loss<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html?highlight=l2loss#mxnet.gluon.loss.L2Loss" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/loss.html?highlight=l2loss#mxnet.gluon.loss.L2Loss</a>  </p>
</li>
<li><p>Trainer<br><a href="https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=trainer#mxnet.gluon.Trainer" target="_blank" rel="external">https://mxnet.incubator.apache.org/api/python/gluon/gluon.html?highlight=trainer#mxnet.gluon.Trainer</a>  </p>
</li>
</ul>
<h2 id="代码地址-1"><a href="#代码地址-1" class="headerlink" title="代码地址"></a>代码地址</h2><p><a href="https://github.com/SnailTyan/gluon-practice-code" target="_blank" rel="external">https://github.com/SnailTyan/gluon-practice-code</a></p>
]]></content>
    
    <summary type="html">
    
      动手学深度学习(一)——线性回归(gluon)
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Caffe Model Zoo</title>
    <link href="http://noahsnail.com/2018/02/02/2018-02-02-Caffe%20Model%20Zoo/"/>
    <id>http://noahsnail.com/2018/02/02/2018-02-02-Caffe Model Zoo/</id>
    <published>2018-02-02T10:59:55.000Z</published>
    <updated>2018-02-02T10:59:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Caffe有许多分类的预训练模型及网络结构，我自己训练过的模型总结在Github上，基本上涵盖了大部分的分类模型，包括AlexNet，VGG，GoogLeNet，Inception系列，ResNet，SENet，DenseNet，SqueezeNet。</p>
<p>其中会碰到不少坑，例如VGG给的结构已经太旧了，需要根据新版本Caffe的进行修改，DenseNet训练有些地方需要修改等。鉴于以上原因，我自己整理了一个Caffe Model Zoo，都是已经使用Caffe训练过模型的。</p>
<p>Github地址：<a href="https://github.com/SnailTyan/caffe-model-zoo" target="_blank" rel="external">https://github.com/SnailTyan/caffe-model-zoo</a></p>
]]></content>
    
    <summary type="html">
    
      Caffe Model Zoo
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习模型训练流程</title>
    <link href="http://noahsnail.com/2018/02/02/2018-02-02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/"/>
    <id>http://noahsnail.com/2018/02/02/2018-02-02-深度学习模型训练流程/</id>
    <published>2018-02-02T08:37:32.000Z</published>
    <updated>2018-02-04T14:02:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>工作中训练了很多的深度学习模型，目前到了上升到方法论的角度来看了。日常工作中有的人可能已经在遵循方法论做事，可能自己没有注意，有的人可能没有遵循方法论在做事，虽然可能最后的结果差不多，但花费的时间和精力应该会差别很大，当然这是我自己的感受。我们不必完全按照方法论来做，但基本流程跟方法论应该一致。</p>
<p>下面的具体步骤以图像分类，识别图像中的猫为例。</p>
<h2 id="1-问题定义"><a href="#1-问题定义" class="headerlink" title="1. 问题定义"></a>1. 问题定义</h2><p>问题定义，“what，how，why”中的what，首先要弄清楚自己要干什么，然后调研相关的技术，确定解决方案。例如这一步中我的工作是进行图像分类，问题定义是图像分类——识别猫，相关的技术包括各种分类模型，各种深度学习框架。我选择的是BN-Inception + Caffe。</p>
<h2 id="2-确定评估标准"><a href="#2-确定评估标准" class="headerlink" title="2. 确定评估标准"></a>2. 确定评估标准</h2><p>根据问题定义，确定了相关技术之后，不要着急动手去做，先确定评估标准，怎么评价模型的好坏，例如分类猫可以通过准确率（Precision）、召回率（Recall）、F1、ROC曲线、AUC面积等。确定了评估标准之后，评估数据集也要准备好。</p>
<h2 id="3-确定Baseline和Target"><a href="#3-确定Baseline和Target" class="headerlink" title="3. 确定Baseline和Target"></a>3. 确定Baseline和Target</h2><ul>
<li>Baseline</li>
</ul>
<p>有了评估标准后，需要确定一个Baseline，例如可以简单快速的训练一个模型或已经有一个Pretrained Model，在评估数据集上进行评估，得到一个指标作为Baseline，然后在Baseline的基础上进行提高，确定Baseline类似于敏捷开发中的快速原型开发。</p>
<ul>
<li>Target</li>
</ul>
<p>有了Baseline之后，可以确定一个目标，但这个目标不能是拍脑袋出来的，如果你的业务与别人的业务类似，例如色情识别，可以使用大厂（BAT)的模型先在评估数据集上得出一个结果，目标定为达到他们的水平或超过他们的水平。如果不跟别人的业务类似，那么需要根据具体的业务需求确定一个目标。目标还是要有的，起码确定一个方向。</p>
<h2 id="4-模型训练"><a href="#4-模型训练" class="headerlink" title="4. 模型训练"></a>4. 模型训练</h2><p>模型训练这部分就没太多说的了，深度学习工程师的基本功。</p>
<h2 id="5-模型评估"><a href="#5-模型评估" class="headerlink" title="5. 模型评估"></a>5. 模型评估</h2><p>将训练的模型在评估数据集上进行评估，分析评估结果，与上一次的模型结果以及Target进行对比。将错误的数据取出来，分析存在的问题，讨论调整的方向，记录实验结果。</p>
<h2 id="6-模型再训练"><a href="#6-模型再训练" class="headerlink" title="6. 模型再训练"></a>6. 模型再训练</h2><p>重复步骤4、5，直至达到目标。如果模型还不错，可以将模型放到Beta环境测试，分析线上的结果，重复步骤4、5。</p>
<h2 id="7-服务部署"><a href="#7-服务部署" class="headerlink" title="7. 服务部署"></a>7. 服务部署</h2><p>如果模型在Beta环境也不错，则可以进行线上测试，继续重复步骤4、5，因为有的模型需要不断的进行迭代更新。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://towardsdatascience.com/machine-learning-in-practice-what-are-the-steps-a4b15ee18546" target="_blank" rel="external">https://towardsdatascience.com/machine-learning-in-practice-what-are-the-steps-a4b15ee18546</a></p>
]]></content>
    
    <summary type="html">
    
      深度学习模型训练流程
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中英文对照</title>
    <link href="http://noahsnail.com/2018/02/02/2018-02-02-Detecting%20Text%20in%20Natural%20Image%20with%20Connectionist%20Text%20Proposal%20Network%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>http://noahsnail.com/2018/02/02/2018-02-02-Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中英文对照/</id>
    <published>2018-02-02T07:15:37.000Z</published>
    <updated>2018-03-19T10:26:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network"><a href="#Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network" class="headerlink" title="Detecting Text in Natural Image with Connectionist Text Proposal Network"></a>Detecting Text in Natural Image with Connectionist Text Proposal Network</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8,35] by a large margin. The CTPN is computationally efficient with 0.14s/image, by using the very deep VGG16 model [27]. Online demo is available at: <a href="http://textdet.com/" target="_blank" rel="external">http://textdet.com/</a>.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。CTPN直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。我们开发了一个垂直锚点机制，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。这使得CTPN可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。CTPN在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。在线演示获取地址：<a href="http://textdet.com/" target="_blank" rel="external">http://textdet.com/</a>。</p>
<h2 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h2><p>Scene text detection, convolutional network, recurrent neural network, anchor mechanism</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>场景文本检测；卷积网络；循环神经网络；锚点机制</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Reading text in natural image has recently attracted increasing attention in computer vision [8,14,15,10,35,11,9,1,28,32]. This is due to its numerous practical applications such as image OCR, multi-language translation, image retrieval, etc. It includes two sub tasks: text detection and recognition. This work focus on the detection task [14,1,28,32], which is more challenging than recognition task carried out on a well-cropped word image [15,9]. Large variance of text patterns and highly cluttered background pose main challenge of accurate text localization.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在自然图像中阅读文本最近在计算机视觉中引起越来越多的关注[8，14，15，10，35，11，9，1，28，32]。这是由于它的许多实际应用，如图像OCR，多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。这项工作的重点是检测任务[14，1，28，32]，这是比在一个良好的裁剪字图像[15，9]进行的识别任务更具有挑战性。文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。</p>
<p>Current approaches for text detection mostly employ a bottom-up pipeline [28,1,14,32,33]. They commonly start from low-level character or stroke detection, which is typically followed by a number of subsequent steps: non-text component filtering, text line construction and text line verification. These multi-step bottom-up approaches are generally complicated with less robustness and reliability. Their performance heavily rely on the results of character detection, and connected-components methods or sliding-window methods have been proposed. These methods commonly explore low-level features (e.g., based on SWT [3,13], MSER [14,33,23], or HoG [28]) to distinguish text candidates from background. However, they are not robust by identifying individual strokes or characters separately, without context information. For example, it is more confident for people to identify a sequence of characters than an individual one, especially when a character is extremely ambiguous. These limitations often result in a large number of non-text components in character detection, causing main difficulties for handling them in following steps. Furthermore, these false detections are easily accumulated sequentially in bottom-up pipeline, as pointed out in [28]. To address these problems, we exploit strong deep features for detecting text information directly in convolutional maps. We develop text anchor mechanism that accurately predicts text locations in fine scale. Then, an in-network recurrent architecture is proposed to connect these fine-scale text proposals in sequences, allowing them to encode rich context information.</p>
<p>目前的文本检测方法大多采用自下而上的流程[28，1，14，32，33]。它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。它们的性能很大程度上依赖于字符检测的结果，并且已经提出了连接组件方法或滑动窗口方法。这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。但是，如果没有上下文信息，他们不能鲁棒的单独识别各个笔划或字符。例如，相比单个字符人们更信任一个字符序列，特别是当一个字符非常模糊时。这些限制在字符检测中通常会导致大量非文本组件，在后续步骤中的主要困难是处理它们。此外，正如[28]所指出的，这些误检很容易在自下而上的过程中连续累积。为了解决这些问题，我们利用强大的深度特征直接在卷积映射中检测文本信息。我们开发的文本锚点机制能在细粒度上精确预测文本位置。然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</p>
<p>Deep Convolutional Neural Networks (CNN) have recently advanced general object detection substantially [25,5,6]. The state-of-the-art method is Faster Region-CNN (R-CNN) system [25] where a Region Proposal Network (RPN) is proposed to generate high-quality class-agnostic object proposals directly from convolutional feature maps. Then the RPN proposals are fed into a Fast R-CNN [5] model for further classification and refinement, leading to the state-of-the-art performance on generic object detection. However, it is difficult to apply these general object detection systems directly to scene text detection, which generally requires a higher localization accuracy. In generic object detection, each object has a well-defined closed boundary [2], while such a well-defined boundary may not exist in text, since a text line or word is composed of a number of separate characters or strokes. For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (e.g., the PASCAL standard [4]), since people can recognize an object easily from major part of it. By contrast, reading text comprehensively is a fine-grained recognition task which requires a correct detection that covers a full region of a text line or word. Therefore, text detection generally requires a more accurate localization, leading to a different evaluation standard, e.g., the Wolf’s standard [30] which is commonly employed by text benchmarks [19,21].</p>
<p>深度卷积神经网络（CNN）最近已经基本实现了一般物体检测[25，5，6]。最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。然而，很难将这些通用目标检测系统直接应用于场景文本检测，这通常需要更高的定位精度。在通用目标检测中，每个目标都有一个明确的封闭边界[2]，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠&gt;0.5，因为人们可以容易地从目标的主要部分识别它。相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的Wolf标准[19，21]。</p>
<p>In this work, we fill this gap by extending the RPN architecture [25] to accurate text line localization. We present several technical developments that tailor generic object detection model elegantly towards our problem. We strive for a further step by proposing an in-network recurrent mechanism that allows our model to detect text sequence directly in the convolutional maps, avoiding further post-processing by an additional costly CNN detection model.</p>
<p>在这项工作中，我们通过将RPN架构[25]扩展到准确的文本行定义来填补这个空白。我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的CNN检测模型进行进一步的后处理。</p>
<h3 id="1-1-Contributions"><a href="#1-1-Contributions" class="headerlink" title="1.1 Contributions"></a>1.1 Contributions</h3><p>We propose a novel Connectionist Text Proposal Network (CTPN) that directly localizes text sequences in convolutional layers. This overcomes a number of main limitations raised by previous bottom-up approaches building on character detection. We leverage the advantages of strong deep convolutional features and sharing computation mechanism, and propose the CTPN architecture which is described in Fig. 1. It makes the following major contributions:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3c885b0226d1b773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>Fig. 1: (a) Architecture of the Connectionist Text Proposal Network (CTPN). We densely slide a 3×3 spatial window through the last convolutional maps (conv5 ) of the VGG16 model [27]. The sequential windows in each row are recurrently connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs). The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which jointly predicts text/non-text scores, y-axis coordinates and side-refinement offsets of $k$ anchors. (b) The CTPN outputs sequential fixed-width fine-scale text proposals. Color of each box indicates the text/non-text score. Only the boxes with positive scores are presented.</p>
<h3 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1 贡献"></a>1.1 贡献</h3><p>我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。这克服了以前的建立在字符检测基础上的自下而上方法带来的一些主要限制。我们利用强深度卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。主要贡献如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3c885b0226d1b773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1：（a）连接文本提议网络（CTPN）的架构。我们通过VGG16模型[27]的最后一个卷积映射（conv5）密集地滑动3×3空间窗口。每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。RNN层连接到512维的全连接层，接着是输出层，联合预测$k$个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。（b）CTPN输出连续的固定宽度细粒度文本提议。每个框的颜色表示文本/非文本分数。只显示文本框正例的分数。</p>
<p>First, we cast the problem of text detection into localizing a sequence of fine-scale text proposals. We develop an anchor regression mechanism that jointly predicts vertical location and text/non-text score of each text proposal, resulting in an excellent localization accuracy. This departs from the RPN prediction of a whole object, which is difficult to provide a satisfied localization accuracy.</p>
<p>首先，我们将文本检测的问题转化为一系列细粒度的文本提议。我们开发了一个锚点回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。这背离了整个目标的RPN预测，RPN预测难以提供令人满意的定位精度。</p>
<p>Second, we propose an in-network recurrence mechanism that elegantly connects sequential text proposals in the convolutional feature maps. This connection allows our detector to explore meaningful context information of text line, making it powerful to detect extremely challenging text reliably.</p>
<p>其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。通过这种连接，我们的检测器可以探索文本行有意义的上下文信息，使其能够可靠地检测极具挑战性的文本。</p>
<p>Third, both methods are integrated seamlessly to meet the nature of text sequence, resulting in a unified end-to-end trainable model. Our method is able to handle multi-scale and multi-lingual text in a single process, avoiding further post filtering or refinement.</p>
<p>第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</p>
<p>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015). Furthermore, it is computationally efficient, resulting in a 0.14s/image running time (on the ICDAR 2013) by using the very deep VGG16 model [27].</p>
<p>第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Text detection.</strong> Past works in scene text detection have been dominated by bottom-up approaches which are generally built on stroke or character detection. They can be roughly grouped into two categories, connected-components (CCs) based approaches and sliding-window based methods. The CCs based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are greedily grouped into stroke or character candidates, by using low-level properties, e.g., intensity, color, gradient, etc. [33,14,32,13,3]. The sliding-window based methods detect character candidates by densely moving a multi-scale window through an image. The character or non-character window is discriminated by a pre-trained classifier, by using manually-designed features [28,29], or recent CNN features [16]. However, both groups of methods commonly suffer from poor performance of character detection, causing accumulated errors in following component filtering and text line construction steps. Furthermore, robustly filtering out non-character components or confidently verifying detected text lines are even difficult themselves [1,33,14]. Another limitation is that the sliding-window methods are computationally expensive, by running a classifier on a huge number of the sliding windows.</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>文本检测</strong>。过去在场景文本检测中的工作一直以自下而上的方法为主，一般建立在笔画或字符检测上。它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。基于滑动窗口的方法通过在图像中密集地滑动多尺度窗口来检测候选字符。字符或非字符窗口通过预先训练的分类器，使用手动设计的特征[28，29]或最近的CNN特征[16]进行区分。然而，这两种方法通常都会受到较差的字符检测性能的影响，导致在接下来的组件过滤和文本行构建步骤中出现累积的错误。此外，强大地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难[1，33，14]。另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。</p>
<p><strong>Object detection.</strong> Convolutional Neural Networks (CNN) have recently advanced general object detection substantially [25,5,6]. A common strategy is to generate a number of object proposals by employing inexpensive low-level features, and then a strong CNN classifier is applied to further classify and refine the generated proposals. Selective Search (SS) [4] which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5]. Recently, Ren et al. [25] proposed a Faster R-CNN system for object detection. They proposed a Region Proposal Network (RPN) that generates high-quality class-agnostic object proposals directly from the convolutional feature maps. The RPN is fast by sharing convolutional computation. However, the RPN proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5]. More importantly, text is different significantly from general objects, making it difficult to directly apply general object detection system to this highly domain-specific task.</p>
<p><strong>目标检测。</strong>卷积神经网络（CNN）近来在通用目标检测[25，5，6]上已经取得了实质的进步。一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强CNN分类器来进一步对生成的提议进行分类和细化。生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。最近，Ren等人[25]提出了Faster R-CNN目标检测系统。他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。通过共享卷积计算RPN是快速的。然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。</p>
<h2 id="3-Connectionist-Text-Proposal-Network"><a href="#3-Connectionist-Text-Proposal-Network" class="headerlink" title="3. Connectionist Text Proposal Network"></a>3. Connectionist Text Proposal Network</h2><p>This section presents details of the Connectionist Text Proposal Network (CTPN). It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, recurrent connectionist text proposals, and side-refinement.</p>
<h2 id="3-连接文本提议网络"><a href="#3-连接文本提议网络" class="headerlink" title="3. 连接文本提议网络"></a>3. 连接文本提议网络</h2><p>本节介绍连接文本提议网络（CTPN）的细节。它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</p>
<h3 id="3-1-Detecting-Text-in-Fine-scale-Proposals"><a href="#3-1-Detecting-Text-in-Fine-scale-Proposals" class="headerlink" title="3.1 Detecting Text in Fine-scale Proposals"></a>3.1 Detecting Text in Fine-scale Proposals</h3><p>Similar to Region Proposal Network (RPN) [25], the CTPN is essentially a fully convolutional network that allows an input image of arbitrary size. It detects a text line by densely sliding a small window in the convolutional feature maps, and outputs a sequence of fine-scale (e.g., fixed 16-pixel width) text proposals, as shown in Fig. 1 (b).</p>
<h3 id="3-1-在细粒度提议中检测文本"><a href="#3-1-在细粒度提议中检测文本" class="headerlink" title="3.1 在细粒度提议中检测文本"></a>3.1 在细粒度提议中检测文本</h3><p>类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的16个像素）文本提议，如图1（b）所示。</p>
<p>We take the very deep 16-layer vggNet (VGG16) [27] as an example to describe our approach, which is readily applicable to other deep models. Architecture of the CTPN is presented in Fig. 1 (a). We use a small spatial window, 3×3, to slide the feature maps of last convolutional layer (e.g., the conv5 of the VGG16). The size of conv5 feature maps is determined by the size of input image, while the total stride and receptive field are fixed as 16 and 228 pixels, respectively. Both the total stride and receptive field are fixed by the network architecture. Using a sliding window in the convolutional layer allows it to share convolutional computation, which is the key to reduce computation of the costly sliding-window based methods.</p>
<p>我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。CTPN的架构如图1（a）所示。我们使用一个小的空间窗口3×3来滑动最后的卷积层特征映射（例如，VGG16的conv5）。conv5特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为16个和228个像素。网络架构决定总步长和感受野。在卷积层中使用滑动窗口允许它共享卷积计算，这是减少昂贵的基于滑动窗口的方法的计算量的关键。</p>
<p>Generally, sliding-window methods adopt multi-scale windows to detect objects of different sizes, where one window scale is fixed to objects of similar size. In [25], Ren et al. proposed an efficient anchor regression mechanism that allows the RPN to detect multi-scale objects with a single-scale window. The key insight is that a single window is able to predict objects in a wide range of scales and aspect ratios, by using a number of flexible anchors. We wish to extend this efficient anchor mechanism to our text task. However, text differs from generic objects substantially, which generally have a well-defined enclosed boundary and center, allowing inferring whole object from even a part of it [2]. Text is a sequence which does not have an obvious closed boundary. It may include multi-level components, such as stroke, character, word, text line and text region, which are not distinguished clearly between each other. Text detection is defined in word or text line level, so that it may be easy to make an incorrect detection by defining it as a single object, e.g., detecting part of a word. Therefore, directly predicting the location of a text line or word may be difficult or unreliable, making it hard to get a satisfied accuracy. An example is shown in Fig. 2, where the RPN is directly trained for localizing text lines in an image.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1d1b210f78d93fed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>Fig. 2: Left: RPN proposals. Right: Fine-scale text proposals.</p>
<p>通常，滑动窗口方法采用多尺度窗口来检测不同尺寸的目标，其中一个窗口尺度被固定到与目标的尺寸相似。在[25]中，Ren等人提出了一种有效的锚点回归机制，允许RPN使用单尺度窗口检测多尺度目标。关键的洞察力是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和长宽比的目标。我们希望将这种有效的锚点机制扩展到我们的文本任务。然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标[2]。文本是一个没有明显封闭边界的序列。它可能包含多层次的组件，如笔划，字符，单词，文本行和文本区域等，这些组件之间没有明确区分。文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。因此，直接预测文本行或单词的位置可能很难或不可靠，因此很难获得令人满意的准确性。一个例子如图2所示，其中RPN直接被训练用于定位图像中的文本行。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1d1b210f78d93fed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2：左：RPN提议。右：细粒度的文本提议。</p>
<p>We look for a unique property of text that is able to generalize well to text components in all levels. We observed that word detection by the RPN is difficult to accurately predict the horizontal sides of words, since each character within a word is isolated or separated, making it confused to find the start and end locations of a word. Obviously, a text line is a sequence which is the main difference between text and generic objects. It is natural to consider a text line as a sequence of fine-scale text proposals, where each proposal generally represents a small part of a text line, e.g., a text piece with 16-pixel width. Each proposal may include a single or multiple strokes, a part of a character, a single or multiple characters, etc. We believe that it would be more accurate to just predict the vertical location of each proposal, by fixing its horizontal location which may be more difficult to predict. This reduces the search space, compared to the RPN which predicts 4 coordinates of an object. We develop a vertical anchor mechanism that simultaneously predicts a text/non-text score and y-axis location of each fine-scale proposal. It is also more reliable to detect a general fixed-width text proposal than identifying an isolate character, which is easily confused with part of a character or multiple characters. Furthermore, detecting a text line in a sequence of fixed-width text proposals also works reliably on text of multiple scales and multiple aspect ratios.</p>
<p>我们寻找文本的独特属性，能够很好地概括各个层次的文本组件。我们观察到由RPN进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。显然，文本行是一个序列，它是文本和通用目标之间的主要区别。将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16个像素的文本块。每个提议可能包含单个或多个笔划，字符的一部分，单个或多个字符等。我们认为，通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测。与预测目标4个坐标的RPN相比，这减少了搜索空间。我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。检测一般固定宽度的文本提议比识别分隔的字符更可靠，分隔字符容易与字符或多个字符的一部分混淆。此外，检测一系列固定宽度文本提议中的文本行也可以在多个尺度和多个长宽比的文本上可靠地工作。</p>
<p>To this end, we design the fine-scale text proposal as follow. Our detector investigates each spatial location in the <em>conv5</em> densely. A text proposal is defined to have a fixed width of 16 pixels (in the input image). This is equal to move the detector densely through the <em>conv5</em> maps, where the total stride is exactly 16 pixels. Then we design $k$ vertical anchors to  predict  $y$-coordinates for each proposal. The $k$ anchors have a same horizontal location with a fixed width of 16 pixels, but their vertical locations are varied in $k$ different heights. In our experiments, we use ten anchors for each proposal, $k=10$, whose heights are varied from 11 to 273 pixels (by $\div 0.7$ each time) in the input image. The explicit vertical coordinates are measured by the height and $y$-axis center of a proposal bounding box. We compute relative predicted vertical coordinates ($\textbf{v}$) with respect to the bounding box location of an anchor as, $$v_c=(c_y-c_y^a)/h^a, \qquad v_h=\log (h/h^a) \tag{1} $$$$v^*_c=(c^*_y-c_y^a)/h^a, \qquad v^*_h=\log (h^*/h^a)\tag{2}$$ where $\textbf{v}=\lbrace v_c,v_h \rbrace$ and $\textbf{v}^*=\lbrace v^*_c,v^*_h\rbrace$ are the relative predicted coordinates and ground truth coordinates, respectively. $c_y^a$ and $h^a$ are the center ($y$-axis) and height of the anchor box, which can be pre-computed from an input image. $c_y$ and $h$ are the predicted $y$-axis coordinates in the input image, while $c^*_y$ and $h^*$ are the ground truth coordinates. Therefore, each predicted text proposal has a bounding box with size of $h\times 16$ (in the input image), as shown in Fig. 1 (b) and Fig. 2 (right). Generally, an text proposal is largely smaller than its effective receptive field which is 228$\times$228.</p>
<p>为此，我们设计如下的细粒度文本提议。我们的检测器密集地调查了<em>conv5</em>中的每个空间位置。文本提议被定义为具有16个像素的固定宽度（在输入图像中）。这相当于在<em>conv5</em>的映射上密集地移动检测器，其中总步长恰好为16个像素。然后，我们设计$k$个垂直锚点来预测每个提议的$y$坐标。$k$个锚点具有相同的水平位置，固定宽度为16个像素，但其垂直位置在$k$个不同的高度变化。在我们的实验中，我们对每个提议使用十个锚点，$k=10$，其高度在输入图像中从11个像素变化到273个像素（每次$\div 0.7$）。明确的垂直坐标是通过提议边界框的高度和$y$轴中心来度量的。我们计算相对于锚点的边界框位置的相对预测的垂直坐标（$\textbf{v}$），如下所示：$$v_c=(c_y-c_y^a)/h^a, \qquad v_h=\log (h/h^a) \tag{1} $$$$v^*_c=(c^*_y-c_y^a)/h^a, \qquad v^*_h=\log (h^*/h^a)\tag{2}$$ 其中$\textbf{v}=\lbrace v_c,v_h \rbrace$和$\textbf{v}^*=\lbrace v^*_c,v^*_h\rbrace$分别是相对于预测坐标和实际坐标。$c_y^a$和$h^a$是锚盒的中心（$y$轴）和高度，可以从输入图像预先计算。$c_y$和$h$是输入图像中预测的$y$轴坐标，而$c^*_y$和$h^*$是实际坐标。因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为$h\times 16$的边界框（在输入图像中）。一般来说，文本提议在很大程度上要比它的有效感受野228$\times$228要小。</p>
<p>The detection processing is summarised as follow. Given an input image, we have $W \times H \times C$  <em>conv5</em> features maps (by using the VGG16 model), where $C$ is the number of feature maps or channels, and $W \times H$ is the spatial arrangement. When our detector is sliding a 3$\times$3 window densely through the conv5, each sliding-window takes a convolutional feature of $3 \times 3 \times C$ for producing the prediction. For each prediction, the horizontal location ($x$-coordinates) and $k$-anchor locations are fixed, which can be pre-computed by mapping the spatial window location in the <em>conv5</em> onto the input image. Our detector outputs the text/non-text scores and the predicted $y$-coordinates ($\textbf{v}$) for $k$ anchors at each window location. The detected text proposals are generated from the anchors having a text/non-text score of $&gt;0.7$  (with non-maximum suppression). By the designed vertical anchor and fine-scale detection strategy, our detector is able to handle text lines in a wide range of scales and aspect ratios by using a single-scale image. This further reduces its computation, and at the same time, predicting accurate localizations of the text lines. Compared to the RPN or Faster R-CNN system [25], our fine-scale detection provides more detailed supervised information that naturally leads to a more accurate detection.</p>
<p>检测处理总结如下。给定输入图像，我们有$W \times H \times C$ <em>conv5</em>特征映射（通过使用VGG16模型），其中$C$是特征映射或通道的数目，并且$W \times H$是空间布置。当我们的检测器通过conv5密集地滑动3$\times$3窗口时，每个滑动窗口使用$3 \times 3 \times C$的卷积特征来产生预测。对于每个预测，水平位置（$x$轴坐标）和$k$个锚点位置是固定的，可以通过将<em>conv5</em>中的空间窗口位置映射到输入图像上来预先计算。我们的检测器在每个窗口位置输出$k$个锚点的文本/非文本分数和预测的$y$轴坐标（$\textbf{v}$）。检测到的文本提议是从具有$&gt; 0.7 $（具有非极大值抑制）的文本/非文本分数的锚点生成的。通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。这进一步减少了计算量，同时预测了文本行的准确位置。与RPN或Faster R-CNN系统[25]相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</p>
<h3 id="3-2-Recurrent-Connectionist-Text-Proposals"><a href="#3-2-Recurrent-Connectionist-Text-Proposals" class="headerlink" title="3.2 Recurrent Connectionist Text Proposals"></a>3.2 Recurrent Connectionist Text Proposals</h3><p>To improve localization accuracy, we split a text line into a sequence of fine-scale text proposals, and predict each of them separately. Obviously, it is not robust to regard each isolated proposal independently. This may lead to a number of false detections on non-text objects which have a similar structure as text patterns, such as windows, bricks, leaves, etc. (referred as text-like outliers in [13]). It is also possible to discard some ambiguous patterns which contain weak text information. Several examples are presented in Fig. 3 (top). Text have strong sequential characteristics where the sequential context information is crucial to make a reliable decision. This has been verified by recent work [9] where a recurrent neural network (RNN) is applied to encode this context information for text recognition. Their results have shown that the sequential context information is greatly facilitate the recognition task on cropped word images.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f0023bd7bb92ba91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>Fig. 3: Top: CTPN without RNN. Bottom: CTPN with RNN connection.</p>
<h3 id="3-2-循环连接文本提议"><a href="#3-2-循环连接文本提议" class="headerlink" title="3.2 循环连接文本提议"></a>3.2 循环连接文本提议</h3><p>为了提高定位精度，我们将文本行分成一系列细粒度的文本提议，并分别预测每个文本提议。显然，将每个孤立的提议独立考虑并不鲁棒。这可能会导致对与文本模式类似的非文本目标的误检，如窗口，砖块，树叶等（在文献[13]中称为类文本异常值）。还可以丢弃一些含有弱文本信息的模糊模式。图3给出了几个例子（上）。文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。最近的工作已经证实了这一点[9]，其中应用递归神经网络（RNN）来编码用于文本识别的上下文信息。他们的结果表明，序列上下文信息极大地促进了对裁剪的单词图像的识别任务。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f0023bd7bb92ba91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3：上：没有RNN的CTPN。下：有RNN连接的CTPN。</p>
<p>Motivated from this work, we believe that this context information may also be of importance for our detection task. Our detector should be able to explore this important context information to make a more reliable decision, when it works on each individual proposal. Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and seamless in-network connection of the fine-scale text proposals. RNN provides a natural choice for encoding this information recurrently using its hidden layers. To this end, we propose to design a RNN layer upon the conv5, which takes the convolutional feature of each window as sequential inputs, and updates its internal state recurrently in the hidden layer, $H_t$, $$H_{t}=\varphi(H_{t-1}, X_t),  \qquad t=1,2,…,W \tag{3}$$ where $X_t \in R^{3\times 3 \times C}$ is the input <em>conv5</em> feature from $t$-th  sliding-window (3$\times$3). The sliding-window moves densely from left to right, resulting in $t=1,2,…,W$ sequential features for each row.  $W$ is the width of the <em>conv5</em>. $H_t$ is a recurrent internal state that is computed jointly from  both current input ($X_t$) and previous states encoded in $H_{t-1}$. The recurrence is computed by using a non-linear function $\varphi$, which defines exact form of the recurrent model. We exploit the long short-term memory (LSTM) architecture [12] for our RNN layer. The LSTM was proposed specially to address vanishing gradient problem, by introducing three additional multiplicative gates: the <em>input gate</em>, <em>forget gate</em> and <em>output gate</em>. Details can be found in [12]. Hence the internal state in RNN hidden layer accesses the sequential context information scanned by all previous windows through the recurrent connection.  We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the recurrent context in both directions, so that the connectionist receipt field is able to cover the whole image width, e.g., 228 $\times$ width. We use a 128D hidden layer for each LSTM, resulting in a 256D RNN hidden layer, $H_t \in R^{256}$.</p>
<p>受到这项工作的启发，我们认为这种上下文信息对于我们的检测任务也很重要。我们的检测器应该能够探索这些重要的上下文信息，以便在每个单独的提议中都可以做出更可靠的决策。此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。RNN提供了一种自然选择，使用其隐藏层对这些信息进行循环编码。为此，我们提出在conv5上设计一个RNN层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，$$H_{t}=\varphi(H_{t-1}, X_t),  \qquad t=1,2,…,W \tag{3}$$其中$X_t \in R^{3\times 3 \times C}$是第$t$个滑动窗口(3$\times$3)的输入<em>conv5</em>特征。滑动窗口从左向右密集移动，导致每行的$t=1,2,…,W$序列特征。$W$是<em>conv5</em>的宽度。$H_t$是从当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算的循环内部状态。递归是通过使用非线性函数$\varphi$来计算的，它定义了循环模型的确切形式。我们利用长短时记忆（LSTM）架构[12]作为我们的RNN层。通过引入三个附加乘法门：<em>输入门</em>，<em>忘记门</em>和<em>输出门</em>，专门提出了LSTM以解决梯度消失问题。细节可以在[12]中找到。因此，RNN隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如228$\times$width。我们对每个LSTM使用一个128维的隐藏层，从而产生256维的RNN隐藏层$H_t \in R^{256}$。</p>
<p>The internal state in $H_t$ is mapped to the following FC layer, and output layer for computing the predictions of the t-th proposal. Therefore, our integration with the RNN layer is elegant, resulting in an efficient model that is end-to-end trainable without additional cost. The efficiency of the RNN connection is demonstrated in Fig. 3. Obviously, it reduces false detections considerably, and at the same time, recovers many missed text proposals which contain very weak text information.</p>
<p>$H_t$中的内部状态被映射到后面的FC层，并且输出层用于计算第$t$个提议的预测。因此，我们与RNN层的集成非常优雅，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。RNN连接的功效如图3所示。显然，它大大减少了错误检测，同时还能够恢复很多包含非常弱的文本信息的遗漏文本提议。</p>
<h3 id="3-3-Side-refinement"><a href="#3-3-Side-refinement" class="headerlink" title="3.3 Side-refinement"></a>3.3 Side-refinement</h3><p>The fine-scale text proposals are detected accurately and reliably by our CTPN. Text line construction is straightforward by connecting continuous text proposals whose text/non-text score is $&gt;0.7$. Text lines are constructed as follow. First, we define a paired neighbour ($B_j$) for a proposal $B_i$ as $B_j-&gt;B_i$, when (i) $B_j$ is the nearest horizontal distance to $B_i$, and (ii) this distance is less than 50 pixels, and (iii) their vertical overlap is $&gt;0.7$. Second, two proposals are grouped into a pair, if $B_j-&gt;B_i$ and $B_i-&gt;B_j$. Then a text line is constructed by sequentially connecting the pairs having a same proposal.</p>
<h3 id="3-3-边缘细化"><a href="#3-3-边缘细化" class="headerlink" title="3.3 边缘细化"></a>3.3 边缘细化</h3><p>我们的CTPN能够准确可靠地检测细粒度的文本提议。通过连接其文本/非文本分数为$&gt;0.7$的连续文本提议，文本行的构建非常简单。文本行构建如下。首先，我们为提议$B_i$定义一个配对邻居（$B_j$）作为$B_j-&gt;B_i$，当（i）$B_j$是最接近$B_i$的水平距离，（ii）该距离小于50像素，并且（iii）它们的垂直重叠是$&gt;0.7$时。其次，如果$B_j-&gt;B_i$和$B_i-&gt;B_j$，则将两个提议分组为一对。然后通过顺序连接具有相同提议的对来构建文本行。</p>
<p>The fine-scale detection and RNN connection are able to predict accurate localizations in vertical direction. In horizontal direction, the image is divided into a sequence of equal 16-pixel width proposals. This may lead to an inaccurate localization when the text proposals in both horizontal sides are not exactly covered by a ground truth text line area, or some side proposals are discarded (e.g., having a low text score), as shown in Fig. 4. This inaccuracy may be not crucial in generic object detection, but should not be ignored in text detection, particularly for those small-scale text lines or words. To address this problem, we propose a side-refinement approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as side-anchor or side-proposal). Similar to the y-coordinate prediction, we compute relative offset as, $$o=(x_{side}-c_x^a)/w^a, \quad o^*=(x^*_{side}-c_x^a)/w^a$$ where $x_{side}$ is the predicted $x$-coordinate of the nearest horizontal side (e.g., left or right side) to current anchor. $x^*_{side}$ is the ground truth (GT) side coordinate in $x$-axis, which is pre-computed from the GT bounding box and anchor location. $c_x^a$ is the center of anchor in $x$-axis. $w^a$ is the width of anchor, which is fixed, $w^a=16$ . The side-proposals are defined as the start and end proposals when we connect a sequence of detected fine-scale text proposals into a text line. We only use the offsets of the side-proposals to refine the final text line bounding box. Several detection examples improved by side-refinement are presented in Fig. 4. The side-refinement further improves the localization accuracy, leading to about $2\%$ performance improvements on the SWT and Multi-Lingual datasets. Notice that the offset for side-refinement is predicted simultaneously by our model, as shown in Fig. 1. It is not computed from an additional post-processing step.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-af7aa618291f03f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>Fig.4: CTPN detection with (red box) and without (yellow dashed box) the side-refinement. Color of fine-scale proposal box indicate a text/non-text score.</p>
<p>细粒度的检测和RNN连接可以预测垂直方向的精确位置。在水平方向上，图像被分成一系列相等的宽度为16个像素的提议。如图4所示，当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。与y坐标预测类似，我们计算相对偏移为：$$o=(x_{side}-c_x^a)/w^a, \quad o^*=(x^*_{side}-c_x^a)/w^a$$，其中$x_{side}$是最接近水平边（例如，左边或右边）到当前锚点的预测的$x$坐标。$x^*_{side}$是$x$轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。$c_x^a$是$x$轴的锚点的中心。$w^a$是固定的锚点宽度，$w^a=16$。当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。我们只使用边缘提议的偏移量来优化最终的文本行边界框。通过边缘细化改进的几个检测示例如图4所示。边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约$2\%$。请注意，我们的模型同时预测了边缘细化的偏移量，如图1所示。它不是通过额外的后处理步骤计算的。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-af7aa618291f03f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。细粒度提议边界框的颜色表示文本/非文本分数。</p>
<h3 id="3-4-Model-Outputs-and-Loss-Functions"><a href="#3-4-Model-Outputs-and-Loss-Functions" class="headerlink" title="3.4 Model Outputs and Loss Functions"></a>3.4 Model Outputs and Loss Functions</h3><p>The proposed CTPN has three outputs which are jointly connected to the last FC layer, as shown in Fig. 1 (a). The three outputs simultaneously predict text/non-text scores ($\textbf{s}$), vertical coordinates ($\textbf{v}=\lbrace v_c, v_h\rbrace$) in E.q. (2) and side-refinement offset ($\textbf{o}$). We explore $k$ anchors to predict them on each spatial location in the <em>conv5</em>,  resulting in $2k$, $2k$ and $k$ parameters in the output layer, respectively.</p>
<h3 id="3-4-模型输出与损失函数"><a href="#3-4-模型输出与损失函数" class="headerlink" title="3.4 模型输出与损失函数"></a>3.4 模型输出与损失函数</h3><p>提出的CTPN有三个输出共同连接到最后的FC层，如图1（a）所示。这三个输出同时预测公式（2）中的文本/非文本分数（$ \ textbf {s} $），垂直坐标（$\textbf{v}=\lbrace v_c, v_h\rbrace$）和边缘细化偏移（$\textbf{o}$）。我们将探索$k$个锚点来预测它们在<em>conv5</em>中的每个空间位置，从而在输出层分别得到$2k$，$2k$和$k$个参数。</p>
<p>We employ multi-task learning to jointly optimize model parameters. We introduce three loss functions, $L^{cl}_s$, $L^{re}_v$ and $l^{re}_o$, which compute errors of text/non-text score, coordinate and side-refinement, respectively. With these considerations, we follow the multi-task loss applied in [5,25], and minimize an overall objective function ($L$) for an image as, $$L(\textbf{s}_i, \textbf{v}_j, \textbf{o}_k) =\frac1{N_{s}}\sum_iL^{cl}_{s}(\textbf{s}_i, \textbf{s}_i^*) +\frac{\lambda_1}{N_v}\sum_j L^{re}_v(\textbf{v}_j, \textbf{v}_j^*) +\frac{\lambda_2}{N_o}\sum_k L^{re}_o(\textbf{o}_k, \textbf{o}_k^*) \tag{5}$$ where each anchor is a training sample, and $i$ is the index of an anchor in a mini-batch. $\textbf{s}_i$ is the predicted probability of anchor $i$ being a true text. $\textbf{s}_i^*=\lbrace 0,1\rbrace$ is the ground truth. $j$ is the index of an anchor in the set of valid anchors for $y$-coordinates regression, which are defined as follow. A valid anchor is a defined positive anchor ($\textbf{s}_j^*=1$, described below), or has an Intersection-over-Union (IoU) $&gt;0.5$ overlap  with a ground truth text proposal. $\textbf{v}_j$ and $\textbf{v}_j^*$ are the prediction and ground truth $y$-coordinates associated with the $j$-{th} anchor. $k$ is the index of a side-anchor, which is defined as a set of anchors within a horizontal distance (e.g.,  32-pixel) to the left or right side of a ground truth text line bounding box. $\textbf{o}_k$ and $\textbf{o}_k^*$ are the predicted and ground truth offsets in $x$-axis associated to the $k$-{th} anchor. $L^{cl}_s$ is the classification loss which we use Softmax loss to distinguish text and non-text. $L^{re}_v$ and $L^{re}_o$ are the regression loss. We follow previous work by using the smooth $L_1$ function to compute them [5, 25]. $\lambda_1$ and $\lambda_2$ are loss weights to balance different tasks, which are empirically set to 1.0 and 2.0. $N_{s}$ $N_{v}$ and $N_{o}$ are normalization parameters, denoting the total number of anchors used by $L^{cl}_s$, $L^{re}_v$ and $L^{re}_o$, respectively.</p>
<p>我们采用多任务学习来联合优化模型参数。我们引入了三种损失函数：$L^{cl}_s$，$L^{re}_v$和$l^{re}_o$，其分别计算文本/非文本分数，坐标和边缘细化。考虑到这些因素，我们遵循[5，25]中应用的多任务损失，并最小化图像的总体目标函数（$L$）最小化：$$L(\textbf{s}_i, \textbf{v}_j, \textbf{o}_k) =\frac1{N_{s}}\sum_iL^{cl}_{s}(\textbf{s}_i, \textbf{s}_i^*) +\frac{\lambda_1}{N_v}\sum_j L^{re}_v(\textbf{v}_j, \textbf{v}_j^*) +\frac{\lambda_2}{N_o}\sum_k L^{re}_o(\textbf{o}_k, \textbf{o}_k^*) \tag{5}$$，其中每个锚点都是一个训练样本，$i$是一个小批量数据中一个锚点的索引。$\textbf{s}_i$是预测的锚点$i$作为实际文本的预测概率。$\textbf{s}_i^*=\lbrace 0,1\rbrace$是真实值。$j$是$y$坐标回归中有效锚点集合中锚点的索引，定义如下。有效的锚点是定义的正锚点（$\textbf{s}_j^*=1$，如下所述），或者与实际文本提议重叠的交并比（IoU）$&gt;0.5$。$\textbf{v}_j$和$\textbf{v}_j^*$是与第$j$个锚点关联的预测的和真实的$y$坐标。$k$是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第$k$个锚点关联的$x$轴的预测和实际偏移量。$L^{cl}_s$是我们使用Softmax损失区分文本和非文本的分类损失。$L^{re}_v$和$L^{re}_o$是回归损失。我们遵循以前的工作，使用平滑$L_1$函数来计算它们[5，25]。$\lambda_1$和$\lambda_2$是损失权重，用来平衡不同的任务，将它们经验地设置为1.0和2.0。$N_{s}$ $N_{v}$和$N_{o}$是标准化参数，表示$L^{cl}_s$，$L^{re}_v$，$L^{re}_o$分别使用的锚点总数。</p>
<h3 id="3-5-Training-and-Implementation-Details"><a href="#3-5-Training-and-Implementation-Details" class="headerlink" title="3.5 Training and Implementation Details"></a>3.5 Training and Implementation Details</h3><p>The CTPN can be trained end-to-end by using the standard back-propagation and stochastic gradient descent (SGD). Similar to RPN [25], training samples are the anchors, whose locations can be pre computed in input image, so that the training labels of each anchor can be computed from corresponding GT box.</p>
<h3 id="3-5-训练和实现细节"><a href="#3-5-训练和实现细节" class="headerlink" title="3.5 训练和实现细节"></a>3.5 训练和实现细节</h3><p>通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。与RPN[25]类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</p>
<p><strong>Training labels.</strong> For text/non-text classification, a binary label is assigned to each positive (text) or negative (non-text) anchor. It is defined by computing the IoU overlap with the GT bounding box (divided by anchor location). A positive anchor is defined as : (i) an anchor that has an $&gt; 0.7$ IoU overlap with any GT box; or (ii) the anchor with the highest IoU overlap with a GT box. <em>By the condition (ii), even a very small text pattern can assign a positive anchor. This is crucial to detect small-scale text patterns, which is one of key advantages of the CTPN.</em> This is different from generic object detection where the impact of condition (ii) may be not significant. The negative anchors are defined as $&lt;0.5$ IoU overlap with all GT boxes. The training labels for the $y$-coordinate regression ($\textbf{v}^*$) and offset regression ($\textbf{o}^*$) are computed as E.q. (2) and (4) respectively.</p>
<p><strong>训练标签</strong>。对于文本/非文本分类，二值标签分配给每个正（文本）锚点或负（非文本）锚点。它通过计算与实际边界框的IoU重叠（除以锚点位置）来定义。正锚点被定义为：（i）与任何实际边界框具有$&gt;0.7$的IoU重叠；或者（ii）与实际边界框具有最高IoU重叠。<em>通过条件（ii），即使是非常小的文本模式也可以分为正锚点。这对于检测小规模文本模式至关重要，这是CTPN的主要优势之一。</em>这不同于通用目标检测，通用目标检测中条件（ii）的影响可能不显著。负锚点定义为与所有实际边界框具有$&lt;0.5$的IoU重叠。$y$坐标回归（$\textbf{v}^*$）和偏移回归（$\textbf{o}^*$）的训练标签分别按公式（2）和（4）计算。</p>
<p><strong>Training data.</strong> In the training process, each mini-batch samples are collected randomly from a single image. The number of anchors for each mini-batch is fixed to  $N_s=128$, with 1:1 ratio for positive and negative samples. A mini-patch is pad with negative samples if the number of positive ones is fewer than 64. Our model was trained on 3,000 natural images, including 229 images from the ICDAR 2013 training set. We collected the other images ourselves and manually labelled them with text line bounding boxes. All self-collected training images are not overlapped with any test image in all benchmarks.  The input image is resized by setting its short side to 600 for training, while keeping its original aspect ratio.</p>
<p><strong>训练数据</strong>。在训练过程中，每个小批量样本从单张图像中随机收集。每个小批量数据的锚点数量固定为$N_s=128$，正负样本的比例为1：1。如果正样本的数量少于64，则会用小图像块填充负样本。我们的模型在3000张自然图像上训练，其中包括来自ICDAR 2013训练集的229张图像。我们自己收集了其他图像，并用文本行边界框进行了手工标注。在所有基准测试集中，所有自我收集的训练图像都不与任何测试图像重叠。为了训练，通过将输入图像的短边设置为600来调整输入图像的大小，同时保持其原始长宽比。</p>
<p><strong>Implementation Details.</strong> We follow the standard practice, and explore the very deep VGG16 model [27] pre-trained on the ImageNet data [26]. We initialize the new layers (e.g., the RNN and output layers) by using random weights with Gaussian distribution of 0 mean and 0.01 standard deviation. The model was trained end-to-end by fixing the parameters in the first two convolutional layers. We used 0.9 momentum and 0.0005 weight decay. The learning rate was set to 0.001 in the first 16K iterations, followed by another 4K iterations with 0.0001 learning rate. Our model was implemented in Caffe framework [17].</p>
<p><strong>实现细节。</strong>我们遵循标准实践，并在ImageNet数据[26]上探索预先训练的非常深的VGG16模型[27]。我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。该模型通过固定前两个卷积层中的参数进行端对端的训练。我们使用0.9的动量和0.0005的重量衰减。在前16K次迭代中，学习率被设置为0.001，随后以0.0001的学习率再进行4K次迭代。我们的模型在Caffe框架[17]中实现。</p>
<h2 id="4-Experimental-Results-and-Discussions"><a href="#4-Experimental-Results-and-Discussions" class="headerlink" title="4. Experimental Results and Discussions"></a>4. Experimental Results and Discussions</h2><p>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24]. In our experiments, we first verify the efficiency of each proposed component individually, e.g., the fine-scale text proposal detection or in-network recurrent connection. The ICDAR 2013 is used for this component evaluation.</p>
<h2 id="4-实验结果和讨论"><a href="#4-实验结果和讨论" class="headerlink" title="4. 实验结果和讨论"></a>4. 实验结果和讨论</h2><p>我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。ICDAR 2013用于该组件的评估。</p>
<h3 id="4-1-Benchmarks-and-Evaluation-Metric"><a href="#4-1-Benchmarks-and-Evaluation-Metric" class="headerlink" title="4.1 Benchmarks and Evaluation Metric"></a>4.1 Benchmarks and Evaluation Metric</h3><p>The ICDAR 2011 dataset [21] consists of 229 training images and 255 testing ones, where the images are labelled in word level. The ICDAR 2013 [19] is similar as the ICDAR 2011, and has in total 462 images, including 229 images and 233 images for training and testing, respectively. The ICDAR 2015 (Incidental Scene Text - Challenge 4) [18] includes 1,500 images which were collected by using the Google Glass. The training set has 1,000 images, and the remained 500 images are used for test. This dataset is more challenging than previous ones by including arbitrary orientation, very small-scale and low resolution text. The Multilingual scene text dataset is collected by [24]. It contains 248 images for training and 239 for testing. The images include multi-languages text, and the ground truth is labelled in text line level. Epshtein et al. [3] introduced the SWT dataset containing 307 images which include many extremely small-scale text.</p>
<h3 id="4-1-基准数据集和评估标准"><a href="#4-1-基准数据集和评估标准" class="headerlink" title="4.1 基准数据集和评估标准"></a>4.1 基准数据集和评估标准</h3><p>ICDAR 2011数据集[21]由229张训练图像和255张测试图像组成，图像以字级别标记。ICDAR 2013[19]与ICDAR 2011类似，共有462张图像，其中包括229张训练图像和233张测试图像。ICDAR 2015年（Incidental Scene Text —— Challenge 4）[18]包括使用Google Glass收集的1500张图像。训练集有1000张图像，剩余的500张图像用于测试。这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。Multilingual场景文本数据集由[24]收集。它包含248张训练图像和239张测试图像。图像包含多种语言的文字，并且真实值以文本行级别标注。Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</p>
<p>We follow previous work by using standard evaluation protocols which are provided by the dataset creators or competition organizers. For the ICDAR 2011 we use the standard protocol proposed by [30], the evaluation on the ICDAR 2013 follows the standard in [19]. For the ICDAR 2015, we used the online evaluation system provided by the organizers as in [18]. The evaluations on the SWT and Multilingual datasets follow the protocols defined in [3] and [24] respectively.</p>
<p>我们遵循以前的工作，使用由数据集创建者或竞赛组织者提供的标准评估协议。对于ICDAR 2011，我们使用[30]提出的标准协议，对ICDAR 2013的评估遵循[19]中的标准。对于ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。SWT和Multilingual数据集的评估分别遵循[3]和[24]中定义的协议。</p>
<h3 id="4-2-Fine-Scale-Text-Proposal-Network-with-Faster-R-CNN"><a href="#4-2-Fine-Scale-Text-Proposal-Network-with-Faster-R-CNN" class="headerlink" title="4.2 Fine-Scale Text Proposal Network with Faster R-CNN"></a>4.2 Fine-Scale Text Proposal Network with Faster R-CNN</h3><p>We first discuss our fine-scale detection strategy against the RPN and Faster R-CNN system [25]. As can be found in Table 1 (left), the individual RPN is difficult to perform accurate text localization, by generating a large amount of false detections (low precision). By refining the RPN proposals with a Fast R-CNN detection model [5], the Faster R-CNN system improves localization accuracy considerably, with a F-measure of 0.75. One observation is that the Faster R-CNN also increases the recall of original RPN. This may benefit from joint bounding box regression mechanism of the Fast R-CNN, which improves the accuracy of a predicted bounding box. The RPN proposals may roughly localize a major part of a text line or word, but they are not accurate enough by the ICDAR 2013 standard. Obviously, the proposed fine-scale text proposal network (FTPN) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.</p>
<p>Table 1: Component evaluation on the ICDAR 2013, and State-of-the-art results on the SWT and MULTILINGUAL.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-a4d735190f65caf2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<h3 id="4-2-具有Faster-R-CNN的细粒度文本提议网络"><a href="#4-2-具有Faster-R-CNN的细粒度文本提议网络" class="headerlink" title="4.2 具有Faster R-CNN的细粒度文本提议网络"></a>4.2 具有Faster R-CNN的细粒度文本提议网络</h3><p>我们首先讨论我们关于RPN和Faster R-CNN系统[25]的细粒度检测策略。如表1（左）所示，通过产生大量的错误检测（低精度），单独的RPN难以执行准确的文本定位。通过使用Fast R-CNN检测模型[5]完善RPN提议，Faster R-CNN系统显著提高了定位精度，其F-measure为0.75。一个观察结果是Faster R-CNN也增加了原始RPN的召回率。这可能受益于Fast R-CNN的联合边界框回归机制，其提高了预测边界框的准确性。RPN提议可以粗略定位文本行或文字的主要部分，但根据ICDAR 2013的标准这不够准确。显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</p>
<p>表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-a4d735190f65caf2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<h3 id="4-3-Recurrent-Connectionist-Text-Proposals"><a href="#4-3-Recurrent-Connectionist-Text-Proposals" class="headerlink" title="4.3 Recurrent Connectionist Text Proposals"></a>4.3 Recurrent Connectionist Text Proposals</h3><p>We discuss impact of recurrent connection on our CTPN. As shown in Fig. 3, the context information is greatly helpful to reduce false detections, such as text-like outliers. It is of great importance for recovering highly ambiguous text (e.g., extremely small-scale ones), which is one of main advantages of our CTPN, as demonstrated in Fig. 6. These appealing properties result in a significant performance boost. As shown in Table 1 (left), with our recurrent connection, the CTPN improves the FTPN substantially from a F-measure of 0.80 to 0.88.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-d36a956d9c67e6f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 6"></p>
<p>Fig.6: CTPN detection results on extremely small-scale cases (in red boxes), where some ground truth boxes are missed. Yellow boxes are the ground truth.</p>
<h3 id="4-3-循环连接文本提议"><a href="#4-3-循环连接文本提议" class="headerlink" title="4.3 循环连接文本提议"></a>4.3 循环连接文本提议</h3><p>我们讨论循环连接对CTPN的影响。如图3所示，上下文信息对于减少误检非常有用，例如类似文本的异常值。对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。这些吸引人的属性可显著提升性能。如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-d36a956d9c67e6f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 6"></p>
<p>图6：在极小尺度的情况下（红色框内）CTPN检测结果，其中一些真实边界框被遗漏。黄色边界箱是真实值。</p>
<p><strong>Running time</strong>. The implementation time of our CTPN (for whole detection processing) is about 0.14s per image with a fixed short side of 600, by using a single GPU. The CTPN without the RNN connection takes about 0.13s/image GPU time. Therefore, the proposed in-network recurrent mechanism increase model computation marginally, with considerable performance gain obtained.</p>
<p><strong>运行时间</strong>。通过使用单个GPU，我们的CTPN（用于整个检测处理）的执行时间为每张图像大约0.14s，固定短边为600。没有RNN连接的CTPN每张图像GPU时间大约需要0.13s。因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</p>
<h3 id="4-4-Comparisons-with-state-of-the-art-results"><a href="#4-4-Comparisons-with-state-of-the-art-results" class="headerlink" title="4.4 Comparisons with state-of-the-art results"></a>4.4 Comparisons with state-of-the-art results</h3><p>Our detection results on several challenging images are presented in Fig. 5. As can be found, the CTPN works perfectly on these challenging cases, some of which are difficult for many previous methods. It is able to handle multi-scale and multi-language efficiently (e.g., Chinese and Korean).</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-14f888029b71dcf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 5"></p>
<p>Fig. 5: CTPN detection results several challenging images, including multi-scale and multi-language text lines. Yellow boxes are the ground truth.</p>
<h3 id="4-4-与最新结果的比较"><a href="#4-4-与最新结果的比较" class="headerlink" title="4.4 与最新结果的比较"></a>4.4 与最新结果的比较</h3><p>我们在几个具有挑战性的图像上的检测结果如图5所示。可以发现，CTPN在这些具有挑战性的情况上可以完美的工作，其中一些对于许多以前的方法来说是困难的。它能够有效地处理多尺度和多语言（例如中文和韩文）。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-14f888029b71dcf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 5"></p>
<p>图5：CTPN在几个具有挑战性的图像上的检测结果，包括多尺度和多语言文本行。黄色边界框是真实值。</p>
<p>The full evaluation was conducted on five benchmarks. Image resolution is varied significantly in different datasets. We set short side of images to 2000 for the SWT and ICDAR 2015, and 600 for the other three. We compare our performance against recently published results in [1,28,34]. As shown in Table 1 and 2, our CTPN achieves the best performance on all five datasets. On the SWT, our improvements are significant on both recall and F-measure, with marginal gain on precision. Our detector performs favourably against the TextFlow on the Multilingual, suggesting that our method generalize well to various languages. On the ICDAR 2013, it outperforms recent TextFlow [28] and FASText [1] remarkably by improving the F-measure from 0.80 to 0.88. The gains are considerable in both precision and recall, with more than $+5\%$ and $+7\%$ improvements, respectively. In addition, we further compare our method against [8,11,35], which were published after our initial submission. It consistently obtains substantial improvements on F-measure and recall. This may due to strong capability of CTPN for detecting extremely challenging text, e.g., very small-scale ones, some of which are even difficult for human. As shown in Fig. 6, those challenging ones are detected correctly by our detector, but some of them are even missed by the GT labelling, which may reduce our precision in evaluation.</p>
<p>Table 2: State-of-the-art results on the ICDAR 2011, 2013 and 2015.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-2dd358bcd114b84e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>全面评估是在五个基准数据集上进行的。图像分辨率在不同的数据集中显著不同。我们为SWT和ICDAR 2015设置图像短边为2000，其他三个的短边为600。我们将我们的性能与最近公布的结果[1,28,34]进行了比较。如表1和表2所示，我们的CTPN在所有的五个数据集上都实现了最佳性能。在SWT上，我们的改进对于召回和F-measure都非常重要，并在精确度上取得了很小的收益。我们的检测器在Multilingual上比TextFlow表现更好，表明我们的方法能很好地泛化到各种语言。在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。精确度和召回率都有显著提高，改进分别超过$+5\%$和$+7\%$。此外，我们进一步与[8,11,35]比较了我们的方法，它们是在我们的首次提交后发布的。它始终在F-measure和召回率方面取得重大进展。这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。如图6所示，我们的检测器可以正确地检测到那些具有挑战性的图像，但有些甚至会被真实标签遗漏，这可能会降低我们的评估精度。</p>
<p>表2：ICDAR 2011，2013和2015上的最新结果。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-2dd358bcd114b84e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>We further investigate running time of various methods, as compared in Table 2. FASText [1] achieves 0.15s/image CPU time. Our method is slightly faster than it by obtaining 0.14s/image, but in GPU time. Though it is not fair to compare them directly, the GPU computation has become mainstream with recent great success of deep learning approaches on object detection [25,5,6]. Regardless of running time, our method outperforms the FASText substantially with $11\%$ improvement on F-measure. Our time can be reduced by using a smaller image scale. By using the scale of 450, it is reduced to 0.09s/image, while obtaining P/R/F of 0.92/0.77/0.84 on the ICDAR 2013, which are compared competitively against Gupta et al.’s approach [8] using 0.07s/image with GPU.</p>
<p>我们进一步调查了各种方法的运行时间，在表2中进行了比较。FASText[1]达到0.15s每张图像的CPU时间。我们的方法比它快一点，取得了0.14s每张图像，但是在GPU时间上。尽管直接比较它们是不公平的，但GPU计算已经成为主流，最近在目标检测方面的深度学习方法[25,5,6]上取得了很大成功。无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了$11%$。我们的时间可以通过使用较小的图像尺度来缩短。在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</p>
<h2 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5. Conclusions"></a>5. Conclusions</h2><p>We have presented a Connectionist Text Proposal Network (CTPN) —— an efficient text detector that is end-to-end trainable. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional maps. We develop vertical anchor mechanism that jointly predicts precise location and text/non-text score for each proposal, which is the key to realize accurate localization of text. We propose an in-network RNN layer that connects sequential text proposals elegantly, allowing it to explore meaningful context information. These key technical developments result in a powerful ability to detect highly challenging text, with less false detections. The CTPN is efficient by achieving new state-of-the-art performance on five benchmarks, with 0.14s/image running time.</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。CTPN直接在卷积映射的一系列细粒度文本提议中检测文本行。我们开发了垂直锚点机制，联合预测每个提议的精确位置和文本/非文本分数，这是实现文本准确定位的关键。我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。这些关键技术的发展带来了检测极具挑战性的文本的强大能力，同时减少了误检。通过在五个基准数据集测试中实现了最佳性能，每张图像运行时间为0.14s，CTPN是有效的。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p>Busta, M., Neumann, L., Matas, J.: Fastext: Efficient unconstrained scene text detector (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Cheng, M., Zhang, Z., Lin, W., Torr, P.: Bing: Binarized normed gradients for objectness estimation at 300fps (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Epshtein, B., Ofek, E., Wexler, Y.: Detecting text in natural scenes with stroke width transform (2010), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Everingham, M., Gool, L.V., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV) 88(2), 303–338 (2010)</p>
</li>
<li><p>Girshick, R.: Fast r-cnn (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Graves, A., Schmidhuber, J.: Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks 18(5), 602–610 (2005)</p>
</li>
<li><p>Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>He,P.,Huang,W.,Qiao,Y.,Loy,C.C.,Tang,X.:Readingscenetextindeepconvo- lutional sequences (2016), in The 30th AAAI Conference on Artificial Intelligence (AAAI-16)</p>
</li>
<li><p>He, T., Huang, W., Qiao, Y., Yao, J.: Accurate text localization in natural image with cascaded convolutional text network (2016), arXiv:1603.09423</p>
</li>
<li><p>He, T., Huang, W., Qiao, Y., Yao, J.: Text-attentional convolutional neural net- works for scene text detection. IEEE Trans. Image Processing (TIP) 25, 2529–2541 (2016)</p>
</li>
<li><p>Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Networks 9(8), 1735–1780 (1997)</p>
</li>
<li><p>Huang, W., Lin, Z., Yang, J., Wang, J.: Text localization in natural images using stroke feature transform and text covariance descriptors (2013), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolutional neural networks induced mser trees (2014), in European Conference on Computer Vision (ECCV)</p>
</li>
<li><p>Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the wild with convolutional neural networks. International Journal of Computer Vision (IJCV) (2016)</p>
</li>
<li><p>Jaderberg, M., Vedaldi, A., Zisserman, A.: Deep features for text spotting (2014), in European Conference on Computer Vision (ECCV)</p>
</li>
<li><p>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding (2014), in ACM International Conference on Multimedia (ACM MM)</p>
</li>
<li><p>Karatzas,D.,Gomez-Bigorda,L.,Nicolaou,A.,Ghosh,S.,Bagdanov,A.,Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S.,Valveny, E.: Icdar 2015 competition on robust reading (2015), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R., Mas, J., Mota, D.F., Almazan, J.A., de las Heras., L.P.: Icdar 2013 robust reading competition (2013), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Mao, J., Li, H., Zhou, W., Yan, S., Tian, Q.: Scale based region growing for scene text detection (2013), in ACM International Conference on Multimedia (ACM MM)</p>
</li>
<li><p>Minetto, R., Thome, N., Cord, M., Fabrizio, J., Marcotegui, B.: Snoopertext: A multiresolution system for text detection in complex visual scenes (2010), in IEEE International Conference on Pattern Recognition (ICIP)</p>
</li>
<li><p>Neumann, L., Matas, J.: Efficient scene text localization and recognition with local character refinement (2015), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Neumann, L., Matas, J.: Real-time lexicon-free scene text localization and recognition. In IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) (2015)</p>
</li>
<li><p>Pan, Y., Hou, X., Liu, C.: Hybrid approach to detect and localize texts in natural scene images. IEEE Trans. Image Processing (TIP) 20, 800–813 (2011)</p>
</li>
<li><p>Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks (2015), in Neural Information Processing Systems (NIPS)</p>
</li>
<li><p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Li, F.: Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV) 115(3), 211–252 (2015)</p>
</li>
<li><p>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2015), in International Conference on Learning Representation (ICLR)</p>
</li>
<li><p>Tian, S., Pan, Y., Huang, C., Lu, S., Yu, K., Tan, C.L.: Text flow: A unified text detection system in natural scene images (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition (2011), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Wolf, C., Jolion, J.: Object count / area graphs for the evaluation of object detection and segmentation algorithms. International Journal of Document Analysis 8, 280–296 (2006)</p>
</li>
<li><p>Yao, C., Bai, X., Liu, W.: A unified framework for multioriented text detection and recognition. IEEE Trans. Image Processing (TIP) 23(11), 4737–4749 (2014)</p>
</li>
<li><p>Yin, X.C., Pei, W.Y., Zhang, J., Hao, H.W.: Multi-orientation scene text detection with adaptive clustering. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 37, 1930–1937 (2015)</p>
</li>
<li><p>Yin, X.C., Yin, X., Huang, K., Hao, H.W.: Robust text detection in natural scene images. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 36, 970–983 (2014)</p>
</li>
<li><p>Zhang, Z., Shen, W., Yao, C., Bai, X.: Symmetry-based text line detection in natural scenes (2015), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Zhang, Z., Zhang, C., Shen, W., Yao, C., Liu, W., Bai, X.: Multi-oriented text de- tection with fully convolutional networks (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中文版</title>
    <link href="http://noahsnail.com/2018/02/02/2018-02-02-Detecting%20Text%20in%20Natural%20Image%20with%20Connectionist%20Text%20Proposal%20Network%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>http://noahsnail.com/2018/02/02/2018-02-02-Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中文版/</id>
    <published>2018-02-02T07:14:50.000Z</published>
    <updated>2018-03-19T10:26:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network"><a href="#Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network" class="headerlink" title="Detecting Text in Natural Image with Connectionist Text Proposal Network"></a>Detecting Text in Natural Image with Connectionist Text Proposal Network</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。CTPN直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。我们开发了一个垂直锚点机制，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。这使得CTPN可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。CTPN在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。在线演示获取地址：<a href="http://textdet.com/" target="_blank" rel="external">http://textdet.com/</a>。</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>场景文本检测；卷积网络；循环神经网络；锚点机制</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在自然图像中阅读文本最近在计算机视觉中引起越来越多的关注[8，14，15，10，35，11，9，1，28，32]。这是由于它的许多实际应用，如图像OCR，多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。这项工作的重点是检测任务[14，1，28，32]，这是比在一个良好的裁剪字图像[15，9]进行的识别任务更具有挑战性。文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。</p>
<p>目前的文本检测方法大多采用自下而上的流程[28，1，14，32，33]。它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。它们的性能很大程度上依赖于字符检测的结果，并且已经提出了连接组件方法或滑动窗口方法。这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。但是，如果没有上下文信息，他们不能鲁棒的单独识别各个笔划或字符。例如，相比单个字符人们更信任一个字符序列，特别是当一个字符非常模糊时。这些限制在字符检测中通常会导致大量非文本组件，在后续步骤中的主要困难是处理它们。此外，正如[28]所指出的，这些误检很容易在自下而上的过程中连续累积。为了解决这些问题，我们利用强大的深度特征直接在卷积映射中检测文本信息。我们开发的文本锚点机制能在细粒度上精确预测文本位置。然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</p>
<p>深度卷积神经网络（CNN）最近已经基本实现了一般物体检测[25，5，6]。最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。然而，很难将这些通用目标检测系统直接应用于场景文本检测，这通常需要更高的定位精度。在通用目标检测中，每个目标都有一个明确的封闭边界[2]，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠&gt;0.5，因为人们可以容易地从目标的主要部分识别它。相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的Wolf标准[19，21]。</p>
<p>在这项工作中，我们通过将RPN架构[25]扩展到准确的文本行定义来填补这个空白。我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的CNN检测模型进行进一步的后处理。</p>
<h3 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1 贡献"></a>1.1 贡献</h3><p>我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。这克服了以前的建立在字符检测基础上的自下而上方法带来的一些主要限制。我们利用强深度卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。主要贡献如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-3c885b0226d1b773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 1"></p>
<p>图1：（a）连接文本提议网络（CTPN）的架构。我们通过VGG16模型[27]的最后一个卷积映射（conv5）密集地滑动3×3空间窗口。每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。RNN层连接到512维的全连接层，接着是输出层，联合预测$k$个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。（b）CTPN输出连续的固定宽度细粒度文本提议。每个框的颜色表示文本/非文本分数。只显示文本框正例的分数。</p>
<p>首先，我们将文本检测的问题转化为一系列细粒度的文本提议。我们开发了一个锚点回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。这背离了整个目标的RPN预测，RPN预测难以提供令人满意的定位精度。</p>
<p>其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。通过这种连接，我们的检测器可以探索文本行有意义的上下文信息，使其能够可靠地检测极具挑战性的文本。</p>
<p>第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</p>
<p>第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>文本检测</strong>。过去在场景文本检测中的工作一直以自下而上的方法为主，一般建立在笔画或字符检测上。它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。基于滑动窗口的方法通过在图像中密集地滑动多尺度窗口来检测候选字符。字符或非字符窗口通过预先训练的分类器，使用手动设计的特征[28，29]或最近的CNN特征[16]进行区分。然而，这两种方法通常都会受到较差的字符检测性能的影响，导致在接下来的组件过滤和文本行构建步骤中出现累积的错误。此外，强大地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难[1，33，14]。另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。</p>
<p><strong>目标检测。</strong>卷积神经网络（CNN）近来在通用目标检测[25，5，6]上已经取得了实质的进步。一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强CNN分类器来进一步对生成的提议进行分类和细化。生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。最近，Ren等人[25]提出了Faster R-CNN目标检测系统。他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。通过共享卷积计算RPN是快速的。然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。</p>
<h2 id="3-连接文本提议网络"><a href="#3-连接文本提议网络" class="headerlink" title="3. 连接文本提议网络"></a>3. 连接文本提议网络</h2><p>本节介绍连接文本提议网络（CTPN）的细节。它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</p>
<h3 id="3-1-在细粒度提议中检测文本"><a href="#3-1-在细粒度提议中检测文本" class="headerlink" title="3.1 在细粒度提议中检测文本"></a>3.1 在细粒度提议中检测文本</h3><p>类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的16个像素）文本提议，如图1（b）所示。</p>
<p>我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。CTPN的架构如图1（a）所示。我们使用一个小的空间窗口3×3来滑动最后的卷积层特征映射（例如，VGG16的conv5）。conv5特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为16个和228个像素。网络架构决定总步长和感受野。在卷积层中使用滑动窗口允许它共享卷积计算，这是减少昂贵的基于滑动窗口的方法的计算量的关键。</p>
<p>通常，滑动窗口方法采用多尺度窗口来检测不同尺寸的目标，其中一个窗口尺度被固定到与目标的尺寸相似。在[25]中，Ren等人提出了一种有效的锚点回归机制，允许RPN使用单尺度窗口检测多尺度目标。关键的洞察力是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和长宽比的目标。我们希望将这种有效的锚点机制扩展到我们的文本任务。然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标[2]。文本是一个没有明显封闭边界的序列。它可能包含多层次的组件，如笔划，字符，单词，文本行和文本区域等，这些组件之间没有明确区分。文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。因此，直接预测文本行或单词的位置可能很难或不可靠，因此很难获得令人满意的准确性。一个例子如图2所示，其中RPN直接被训练用于定位图像中的文本行。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-1d1b210f78d93fed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 2"></p>
<p>图2：左：RPN提议。右：细粒度的文本提议。</p>
<p>我们寻找文本的独特属性，能够很好地概括各个层次的文本组件。我们观察到由RPN进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。显然，文本行是一个序列，它是文本和通用目标之间的主要区别。将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16个像素的文本块。每个提议可能包含单个或多个笔划，字符的一部分，单个或多个字符等。我们认为，通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测。与预测目标4个坐标的RPN相比，这减少了搜索空间。我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。检测一般固定宽度的文本提议比识别分隔的字符更可靠，分隔字符容易与字符或多个字符的一部分混淆。此外，检测一系列固定宽度文本提议中的文本行也可以在多个尺度和多个长宽比的文本上可靠地工作。</p>
<p>为此，我们设计如下的细粒度文本提议。我们的检测器密集地调查了<em>conv5</em>中的每个空间位置。文本提议被定义为具有16个像素的固定宽度（在输入图像中）。这相当于在<em>conv5</em>的映射上密集地移动检测器，其中总步长恰好为16个像素。然后，我们设计$k$个垂直锚点来预测每个提议的$y$坐标。$k$个锚点具有相同的水平位置，固定宽度为16个像素，但其垂直位置在$k$个不同的高度变化。在我们的实验中，我们对每个提议使用十个锚点，$k=10$，其高度在输入图像中从11个像素变化到273个像素（每次$\div 0.7$）。明确的垂直坐标是通过提议边界框的高度和$y$轴中心来度量的。我们计算相对于锚点的边界框位置的相对预测的垂直坐标（$\textbf{v}$），如下所示：$$v_c=(c_y-c_y^a)/h^a, \qquad v_h=\log (h/h^a) \tag{1} $$$$v^*_c=(c^*_y-c_y^a)/h^a, \qquad v^*_h=\log (h^*/h^a)\tag{2}$$ 其中$\textbf{v}=\lbrace v_c,v_h \rbrace$和$\textbf{v}^*=\lbrace v^*_c,v^*_h\rbrace$分别是相对于预测坐标和实际坐标。$c_y^a$和$h^a$是锚盒的中心（$y$轴）和高度，可以从输入图像预先计算。$c_y$和$h$是输入图像中预测的$y$轴坐标，而$c^*_y$和$h^*$是实际坐标。因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为$h\times 16$的边界框（在输入图像中）。一般来说，文本提议在很大程度上要比它的有效感受野228$\times$228要小。</p>
<p>检测处理总结如下。给定输入图像，我们有$W \times H \times C$ <em>conv5</em>特征映射（通过使用VGG16模型），其中$C$是特征映射或通道的数目，并且$W \times H$是空间布置。当我们的检测器通过conv5密集地滑动3$\times$3窗口时，每个滑动窗口使用$3 \times 3 \times C$的卷积特征来产生预测。对于每个预测，水平位置（$x$轴坐标）和$k$个锚点位置是固定的，可以通过将<em>conv5</em>中的空间窗口位置映射到输入图像上来预先计算。我们的检测器在每个窗口位置输出$k$个锚点的文本/非文本分数和预测的$y$轴坐标（$\textbf{v}$）。检测到的文本提议是从具有$&gt; 0.7 $（具有非极大值抑制）的文本/非文本分数的锚点生成的。通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。这进一步减少了计算量，同时预测了文本行的准确位置。与RPN或Faster R-CNN系统[25]相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</p>
<h3 id="3-2-循环连接文本提议"><a href="#3-2-循环连接文本提议" class="headerlink" title="3.2 循环连接文本提议"></a>3.2 循环连接文本提议</h3><p>为了提高定位精度，我们将文本行分成一系列细粒度的文本提议，并分别预测每个文本提议。显然，将每个孤立的提议独立考虑并不鲁棒。这可能会导致对与文本模式类似的非文本目标的误检，如窗口，砖块，树叶等（在文献[13]中称为类文本异常值）。还可以丢弃一些含有弱文本信息的模糊模式。图3给出了几个例子（上）。文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。最近的工作已经证实了这一点[9]，其中应用递归神经网络（RNN）来编码用于文本识别的上下文信息。他们的结果表明，序列上下文信息极大地促进了对裁剪的单词图像的识别任务。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-f0023bd7bb92ba91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 3"></p>
<p>图3：上：没有RNN的CTPN。下：有RNN连接的CTPN。</p>
<p>受到这项工作的启发，我们认为这种上下文信息对于我们的检测任务也很重要。我们的检测器应该能够探索这些重要的上下文信息，以便在每个单独的提议中都可以做出更可靠的决策。此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。RNN提供了一种自然选择，使用其隐藏层对这些信息进行循环编码。为此，我们提出在conv5上设计一个RNN层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，$$H_{t}=\varphi(H_{t-1}, X_t),  \qquad t=1,2,…,W \tag{3}$$其中$X_t \in R^{3\times 3 \times C}$是第$t$个滑动窗口(3$\times$3)的输入<em>conv5</em>特征。滑动窗口从左向右密集移动，导致每行的$t=1,2,…,W$序列特征。$W$是<em>conv5</em>的宽度。$H_t$是从当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算的循环内部状态。递归是通过使用非线性函数$\varphi$来计算的，它定义了循环模型的确切形式。我们利用长短时记忆（LSTM）架构[12]作为我们的RNN层。通过引入三个附加乘法门：<em>输入门</em>，<em>忘记门</em>和<em>输出门</em>，专门提出了LSTM以解决梯度消失问题。细节可以在[12]中找到。因此，RNN隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如228$\times$width。我们对每个LSTM使用一个128维的隐藏层，从而产生256维的RNN隐藏层$H_t \in R^{256}$。</p>
<p>$H_t$中的内部状态被映射到后面的FC层，并且输出层用于计算第$t$个提议的预测。因此，我们与RNN层的集成非常优雅，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。RNN连接的功效如图3所示。显然，它大大减少了错误检测，同时还能够恢复很多包含非常弱的文本信息的遗漏文本提议。</p>
<h3 id="3-3-边缘细化"><a href="#3-3-边缘细化" class="headerlink" title="3.3 边缘细化"></a>3.3 边缘细化</h3><p>我们的CTPN能够准确可靠地检测细粒度的文本提议。通过连接其文本/非文本分数为$&gt;0.7$的连续文本提议，文本行的构建非常简单。文本行构建如下。首先，我们为提议$B_i$定义一个配对邻居（$B_j$）作为$B_j-&gt;B_i$，当（i）$B_j$是最接近$B_i$的水平距离，（ii）该距离小于50像素，并且（iii）它们的垂直重叠是$&gt;0.7$时。其次，如果$B_j-&gt;B_i$和$B_i-&gt;B_j$，则将两个提议分组为一对。然后通过顺序连接具有相同提议的对来构建文本行。</p>
<p>细粒度的检测和RNN连接可以预测垂直方向的精确位置。在水平方向上，图像被分成一系列相等的宽度为16个像素的提议。如图4所示，当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。与y坐标预测类似，我们计算相对偏移为：$$o=(x_{side}-c_x^a)/w^a, \quad o^*=(x^*_{side}-c_x^a)/w^a$$，其中$x_{side}$是最接近水平边（例如，左边或右边）到当前锚点的预测的$x$坐标。$x^*_{side}$是$x$轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。$c_x^a$是$x$轴的锚点的中心。$w^a$是固定的锚点宽度，$w^a=16$。当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。我们只使用边缘提议的偏移量来优化最终的文本行边界框。通过边缘细化改进的几个检测示例如图4所示。边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约$2\%$。请注意，我们的模型同时预测了边缘细化的偏移量，如图1所示。它不是通过额外的后处理步骤计算的。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-af7aa618291f03f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 4"></p>
<p>图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。细粒度提议边界框的颜色表示文本/非文本分数。</p>
<h3 id="3-4-模型输出与损失函数"><a href="#3-4-模型输出与损失函数" class="headerlink" title="3.4 模型输出与损失函数"></a>3.4 模型输出与损失函数</h3><p>提出的CTPN有三个输出共同连接到最后的FC层，如图1（a）所示。这三个输出同时预测公式（2）中的文本/非文本分数（$ \ textbf {s} $），垂直坐标（$\textbf{v}=\lbrace v_c, v_h\rbrace$）和边缘细化偏移（$\textbf{o}$）。我们将探索$k$个锚点来预测它们在<em>conv5</em>中的每个空间位置，从而在输出层分别得到$2k$，$2k$和$k$个参数。</p>
<p>我们采用多任务学习来联合优化模型参数。我们引入了三种损失函数：$L^{cl}_s$，$L^{re}_v$和$l^{re}_o$，其分别计算文本/非文本分数，坐标和边缘细化。考虑到这些因素，我们遵循[5，25]中应用的多任务损失，并最小化图像的总体目标函数（$L$）最小化：$$L(\textbf{s}_i, \textbf{v}_j, \textbf{o}_k) =\frac1{N_{s}}\sum_iL^{cl}_{s}(\textbf{s}_i, \textbf{s}_i^*) +\frac{\lambda_1}{N_v}\sum_j L^{re}_v(\textbf{v}_j, \textbf{v}_j^*) +\frac{\lambda_2}{N_o}\sum_k L^{re}_o(\textbf{o}_k, \textbf{o}_k^*) $$，其中每个锚点都是一个训练样本，$i$是一个小批量数据中一个锚点的索引。$\textbf{s}_i$是预测的锚点$i$作为实际文本的预测概率。$\textbf{s}_i^*=\lbrace 0,1\rbrace$是真实值。$j$是$y$坐标回归中有效锚点集合中锚点的索引，定义如下。有效的锚点是定义的正锚点（$\textbf{s}_j^*=1$，如下所述），或者与实际文本提议重叠的交并比（IoU）$&gt;0.5$。$\textbf{v}_j$和$\textbf{v}_j^*$是与第$j$个锚点关联的预测的和真实的$y$坐标。$k$是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第$k$个锚点关联的$x$轴的预测和实际偏移量。$L^{cl}_s$是我们使用Softmax损失区分文本和非文本的分类损失。$L^{re}_v$和$L^{re}_o$是回归损失。我们遵循以前的工作，使用平滑$L_1$函数来计算它们[5，25]。$\lambda_1$和$\lambda_2$是损失权重，用来平衡不同的任务，将它们经验地设置为1.0和2.0。$N_{s}$ $N_{v}$和$N_{o}$是标准化参数，表示$L^{cl}_s$，$L^{re}_v$，$L^{re}_o$分别使用的锚点总数。</p>
<h3 id="3-5-训练和实现细节"><a href="#3-5-训练和实现细节" class="headerlink" title="3.5 训练和实现细节"></a>3.5 训练和实现细节</h3><p>通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。与RPN[25]类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</p>
<p><strong>训练标签</strong>。对于文本/非文本分类，二值标签分配给每个正（文本）锚点或负（非文本）锚点。它通过计算与实际边界框的IoU重叠（除以锚点位置）来定义。正锚点被定义为：（i）与任何实际边界框具有$&gt;0.7$的IoU重叠；或者（ii）与实际边界框具有最高IoU重叠。<em>通过条件（ii），即使是非常小的文本模式也可以分为正锚点。这对于检测小规模文本模式至关重要，这是CTPN的主要优势之一。</em>这不同于通用目标检测，通用目标检测中条件（ii）的影响可能不显著。负锚点定义为与所有实际边界框具有$&lt;0.5$的IoU重叠。$y$坐标回归（$\textbf{v}^*$）和偏移回归（$\textbf{o}^*$）的训练标签分别按公式（2）和（4）计算。</p>
<p><strong>训练数据</strong>。在训练过程中，每个小批量样本从单张图像中随机收集。每个小批量数据的锚点数量固定为$N_s=128$，正负样本的比例为1：1。如果正样本的数量少于64，则会用小图像块填充负样本。我们的模型在3000张自然图像上训练，其中包括来自ICDAR 2013训练集的229张图像。我们自己收集了其他图像，并用文本行边界框进行了手工标注。在所有基准测试集中，所有自我收集的训练图像都不与任何测试图像重叠。为了训练，将输入图像的短边设置为600来调整输入图像的大小，同时保持其原始长宽比。</p>
<p><strong>实现细节。</strong>我们遵循标准实践，并在ImageNet数据[26]上探索预先训练的非常深的VGG16模型[27]。我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。该模型通过固定前两个卷积层中的参数进行端对端的训练。我们使用0.9的动量和0.0005的重量衰减。在前16K次迭代中，学习率被设置为0.001，随后以0.0001的学习率再进行4K次迭代。我们的模型在Caffe框架[17]中实现。</p>
<h2 id="4-实验结果和讨论"><a href="#4-实验结果和讨论" class="headerlink" title="4. 实验结果和讨论"></a>4. 实验结果和讨论</h2><p>我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。ICDAR 2013用于该组件的评估。</p>
<h3 id="4-1-基准数据集和评估标准"><a href="#4-1-基准数据集和评估标准" class="headerlink" title="4.1 基准数据集和评估标准"></a>4.1 基准数据集和评估标准</h3><p>ICDAR 2011数据集[21]由229张训练图像和255张测试图像组成，图像以字级别标记。ICDAR 2013[19]与ICDAR 2011类似，共有462张图像，其中包括229张训练图像和233张测试图像。ICDAR 2015年（Incidental Scene Text —— Challenge 4）[18]包括使用Google Glass收集的1500张图像。训练集有1000张图像，剩余的500张图像用于测试。这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。Multilingual场景文本数据集由[24]收集。它包含248张训练图像和239张测试图像。图像包含多种语言的文字，并且真实值以文本行级别标注。Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</p>
<p>我们遵循以前的工作，使用由数据集创建者或竞赛组织者提供的标准评估协议。对于ICDAR 2011，我们使用[30]提出的标准协议，对ICDAR 2013的评估遵循[19]中的标准。对于ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。SWT和Multilingual数据集的评估分别遵循[3]和[24]中定义的协议。</p>
<h3 id="4-2-具有Faster-R-CNN的细粒度文本提议网络"><a href="#4-2-具有Faster-R-CNN的细粒度文本提议网络" class="headerlink" title="4.2 具有Faster R-CNN的细粒度文本提议网络"></a>4.2 具有Faster R-CNN的细粒度文本提议网络</h3><p>我们首先讨论我们关于RPN和Faster R-CNN系统[25]的细粒度检测策略。如表1（左）所示，通过产生大量的错误检测（低精度），单独的RPN难以执行准确的文本定位。通过使用Fast R-CNN检测模型[5]完善RPN提议，Faster R-CNN系统显著提高了定位精度，其F-measure为0.75。一个观察结果是Faster R-CNN也增加了原始RPN的召回率。这可能受益于Fast R-CNN的联合边界框回归机制，其提高了预测边界框的准确性。RPN提议可以粗略定位文本行或文字的主要部分，但根据ICDAR 2013的标准这不够准确。显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</p>
<p>表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-a4d735190f65caf2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 1"></p>
<h3 id="4-3-循环连接文本提议"><a href="#4-3-循环连接文本提议" class="headerlink" title="4.3 循环连接文本提议"></a>4.3 循环连接文本提议</h3><p>我们讨论循环连接对CTPN的影响。如图3所示，上下文信息对于减少误检非常有用，例如类似文本的异常值。对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。这些吸引人的属性可显著提升性能。如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-d36a956d9c67e6f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 6"></p>
<p>图6：在极小尺度的情况下（红色框内）CTPN检测结果，其中一些真实边界框被遗漏。黄色边界箱是真实值。</p>
<p><strong>运行时间</strong>。通过使用单个GPU，我们的CTPN（用于整个检测处理）的执行时间为每张图像大约0.14s，固定短边为600。没有RNN连接的CTPN每张图像GPU时间大约需要0.13s。因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</p>
<h3 id="4-4-与最新结果的比较"><a href="#4-4-与最新结果的比较" class="headerlink" title="4.4 与最新结果的比较"></a>4.4 与最新结果的比较</h3><p>我们在几个具有挑战性的图像上的检测结果如图5所示。可以发现，CTPN在这些具有挑战性的情况上可以完美的工作，其中一些对于许多以前的方法来说是困难的。它能够有效地处理多尺度和多语言（例如中文和韩文）。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-14f888029b71dcf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Figure 5"></p>
<p>图5：CTPN在几个具有挑战性的图像上的检测结果，包括多尺度和多语言文本行。黄色边界框是真实值。</p>
<p>全面评估是在五个基准数据集上进行的。图像分辨率在不同的数据集中显著不同。我们为SWT和ICDAR 2015设置图像短边为2000，其他三个的短边为600。我们将我们的性能与最近公布的结果[1,28,34]进行了比较。如表1和表2所示，我们的CTPN在所有的五个数据集上都实现了最佳性能。在SWT上，我们的改进对于召回和F-measure都非常重要，并在精确度上取得了很小的收益。我们的检测器在Multilingual上比TextFlow表现更好，表明我们的方法能很好地泛化到各种语言。在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。精确度和召回率都有显著提高，改进分别超过$+5\%$和$+7\%$。此外，我们进一步与[8,11,35]比较了我们的方法，它们是在我们的首次提交后发布的。它始终在F-measure和召回率方面取得重大进展。这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。如图6所示，我们的检测器可以正确地检测到那些具有挑战性的图像，但有些甚至会被真实标签遗漏，这可能会降低我们的评估精度。</p>
<p>表2：ICDAR 2011，2013和2015上的最新结果。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/3232548-2dd358bcd114b84e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Table 2"></p>
<p>我们进一步调查了各种方法的运行时间，在表2中进行了比较。FASText[1]达到0.15s每张图像的CPU时间。我们的方法比它快一点，取得了0.14s每张图像，但是在GPU时间上。尽管直接比较它们是不公平的，但GPU计算已经成为主流，最近在目标检测方面的深度学习方法[25,5,6]上取得了很大成功。无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了$11%$。我们的时间可以通过使用较小的图像尺度来缩短。在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。CTPN直接在卷积映射的一系列细粒度文本提议中检测文本行。我们开发了垂直锚点机制，联合预测每个提议的精确位置和文本/非文本分数，这是实现文本准确定位的关键。我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。这些关键技术的发展带来了检测极具挑战性的文本的强大能力，同时减少了误检。通过在五个基准数据集测试中实现了最佳性能，每张图像运行时间为0.14s，CTPN是有效的。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><p>Busta, M., Neumann, L., Matas, J.: Fastext: Efficient unconstrained scene text detector (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Cheng, M., Zhang, Z., Lin, W., Torr, P.: Bing: Binarized normed gradients for objectness estimation at 300fps (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Epshtein, B., Ofek, E., Wexler, Y.: Detecting text in natural scenes with stroke width transform (2010), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Everingham, M., Gool, L.V., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV) 88(2), 303–338 (2010)</p>
</li>
<li><p>Girshick, R.: Fast r-cnn (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Graves, A., Schmidhuber, J.: Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks 18(5), 602–610 (2005)</p>
</li>
<li><p>Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>He,P.,Huang,W.,Qiao,Y.,Loy,C.C.,Tang,X.:Readingscenetextindeepconvo- lutional sequences (2016), in The 30th AAAI Conference on Artificial Intelligence (AAAI-16)</p>
</li>
<li><p>He, T., Huang, W., Qiao, Y., Yao, J.: Accurate text localization in natural image with cascaded convolutional text network (2016), arXiv:1603.09423</p>
</li>
<li><p>He, T., Huang, W., Qiao, Y., Yao, J.: Text-attentional convolutional neural net- works for scene text detection. IEEE Trans. Image Processing (TIP) 25, 2529–2541 (2016)</p>
</li>
<li><p>Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Networks 9(8), 1735–1780 (1997)</p>
</li>
<li><p>Huang, W., Lin, Z., Yang, J., Wang, J.: Text localization in natural images using stroke feature transform and text covariance descriptors (2013), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolutional neural networks induced mser trees (2014), in European Conference on Computer Vision (ECCV)</p>
</li>
<li><p>Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the wild with convolutional neural networks. International Journal of Computer Vision (IJCV) (2016)</p>
</li>
<li><p>Jaderberg, M., Vedaldi, A., Zisserman, A.: Deep features for text spotting (2014), in European Conference on Computer Vision (ECCV)</p>
</li>
<li><p>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding (2014), in ACM International Conference on Multimedia (ACM MM)</p>
</li>
<li><p>Karatzas,D.,Gomez-Bigorda,L.,Nicolaou,A.,Ghosh,S.,Bagdanov,A.,Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S.,Valveny, E.: Icdar 2015 competition on robust reading (2015), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R., Mas, J., Mota, D.F., Almazan, J.A., de las Heras., L.P.: Icdar 2013 robust reading competition (2013), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Mao, J., Li, H., Zhou, W., Yan, S., Tian, Q.: Scale based region growing for scene text detection (2013), in ACM International Conference on Multimedia (ACM MM)</p>
</li>
<li><p>Minetto, R., Thome, N., Cord, M., Fabrizio, J., Marcotegui, B.: Snoopertext: A multiresolution system for text detection in complex visual scenes (2010), in IEEE International Conference on Pattern Recognition (ICIP)</p>
</li>
<li><p>Neumann, L., Matas, J.: Efficient scene text localization and recognition with local character refinement (2015), in International Conference on Document Analysis and Recognition (ICDAR)</p>
</li>
<li><p>Neumann, L., Matas, J.: Real-time lexicon-free scene text localization and recognition. In IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) (2015)</p>
</li>
<li><p>Pan, Y., Hou, X., Liu, C.: Hybrid approach to detect and localize texts in natural scene images. IEEE Trans. Image Processing (TIP) 20, 800–813 (2011)</p>
</li>
<li><p>Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks (2015), in Neural Information Processing Systems (NIPS)</p>
</li>
<li><p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Li, F.: Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV) 115(3), 211–252 (2015)</p>
</li>
<li><p>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2015), in International Conference on Learning Representation (ICLR)</p>
</li>
<li><p>Tian, S., Pan, Y., Huang, C., Lu, S., Yu, K., Tan, C.L.: Text flow: A unified text detection system in natural scene images (2015), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition (2011), in IEEE International Conference on Computer Vision (ICCV)</p>
</li>
<li><p>Wolf, C., Jolion, J.: Object count / area graphs for the evaluation of object detection and segmentation algorithms. International Journal of Document Analysis 8, 280–296 (2006)</p>
</li>
<li><p>Yao, C., Bai, X., Liu, W.: A unified framework for multioriented text detection and recognition. IEEE Trans. Image Processing (TIP) 23(11), 4737–4749 (2014)</p>
</li>
<li><p>Yin, X.C., Pei, W.Y., Zhang, J., Hao, H.W.: Multi-orientation scene text detection with adaptive clustering. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 37, 1930–1937 (2015)</p>
</li>
<li><p>Yin, X.C., Yin, X., Huang, K., Hao, H.W.: Robust text detection in natural scene images. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 36, 970–983 (2014)</p>
</li>
<li><p>Zhang, Z., Shen, W., Yao, C., Bai, X.: Symmetry-based text line detection in natural scenes (2015), in IEEE Computer Vision and Pattern Recognition (CVPR)</p>
</li>
<li><p>Zhang, Z., Zhang, C., Shen, W., Yao, C., Liu, W., Bai, X.: Multi-oriented text detection with fully convolutional networks (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="http://noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
