<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SnailTyan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="noahsnail.com/"/>
  <updated>2017-09-24T06:32:07.000Z</updated>
  <id>noahsnail.com/</id>
  
  <author>
    <name>Tyan</name>
    <email>Tyan.Liu.Git@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Structuring Machine Learning Projects学习笔记(一)</title>
    <link href="noahsnail.com/2017/09/24/2017-9-24-Structuring%20Machine%20Learning%20Projects%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
    <id>noahsnail.com/2017/09/24/2017-9-24-Structuring Machine Learning Projects学习笔记(一)/</id>
    <published>2017-09-24T05:39:05.000Z</published>
    <updated>2017-09-24T06:32:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-Introduction-to-ML-Strategy"><a href="#1-Introduction-to-ML-Strategy" class="headerlink" title="1. Introduction to ML Strategy"></a>1. Introduction to ML Strategy</h2><h4 id="1-1-Why-ML-Strategy"><a href="#1-1-Why-ML-Strategy" class="headerlink" title="1.1 Why ML Strategy?"></a>1.1 Why ML Strategy?</h4><p>Teach you ways of analyzing a machine learning problem that will point you in the direction of the most promising things to try.</p>
<h4 id="1-2-Orthogonalization"><a href="#1-2-Orthogonalization" class="headerlink" title="1.2 Orthogonalization"></a>1.2 Orthogonalization</h4><p>Chain of assumptions in ML</p>
<ul>
<li>Fit training set well on cost function</li>
<li>Fit dev set well on cost function</li>
<li>Fit test set well on cost function</li>
<li>Performs well in real world</li>
</ul>
<p>Orthogonalization or orthogonality is a system design property that assures that modifying an instruction or a component of an algorithm will not create or propagate side effects to other components of the system. It becomes easier to verify the algorithms independently from one another, it reduces testing and development time.</p>
<p>When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal.</p>
<ol>
<li>Fit training set well in cost function<br>- If it doesn’t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.</li>
<li>Fit development set well on cost function<br>- If it doesn’t fit well, regularization or using bigger training set might help.</li>
<li>Fit test set well on cost function<br>- If it doesn’t fit well, the use of a bigger development set might help</li>
<li>Performs well in real world<br>- If it doesn’t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing.</li>
</ol>
<h2 id="2-Setting-up-your-goal"><a href="#2-Setting-up-your-goal" class="headerlink" title="2. Setting up your goal"></a>2. Setting up your goal</h2><h4 id="2-1-Single-number-evaluation-metric"><a href="#2-1-Single-number-evaluation-metric" class="headerlink" title="2.1 Single number evaluation metric"></a>2.1 Single number evaluation metric</h4><h4 id="2-2-Satisficing-and-Optimiziong-metric"><a href="#2-2-Satisficing-and-Optimiziong-metric" class="headerlink" title="2.2 Satisficing and Optimiziong metric"></a>2.2 Satisficing and Optimiziong metric</h4><h4 id="2-3-Train-dev-test-distribution"><a href="#2-3-Train-dev-test-distribution" class="headerlink" title="2.3 Train/dev/test distribution"></a>2.3 Train/dev/test distribution</h4><h4 id="2-4-Size-of-the-dev-and-test-sets"><a href="#2-4-Size-of-the-dev-and-test-sets" class="headerlink" title="2.4 Size of the dev and test sets"></a>2.4 Size of the dev and test sets</h4><h4 id="2-5-When-to-change-dev-test-sets-and-metrics"><a href="#2-5-When-to-change-dev-test-sets-and-metrics" class="headerlink" title="2.5 When to change dev/test sets and metrics"></a>2.5 When to change dev/test sets and metrics</h4>]]></content>
    
    <summary type="html">
    
      Structuring Machine Learning Projects学习笔记(一)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(十)——卷积神经网络</title>
    <link href="noahsnail.com/2017/09/22/2017-9-22-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%8D%81)%E2%80%94%E2%80%94%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>noahsnail.com/2017/09/22/2017-9-22-PyTorch基本用法(十)——卷积神经网络/</id>
    <published>2017-09-22T12:24:41.000Z</published>
    <updated>2017-09-22T12:26:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torchvision</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 超参数定义</span></div><div class="line">EPOCH = <span class="number">1</span></div><div class="line">LR = <span class="number">0.01</span></div><div class="line">BATCH_SIZE = <span class="number">64</span></div><div class="line"></div><div class="line"><span class="comment"># 下载MNIST数据集</span></div><div class="line">train_data = torchvision.datasets.MNIST(</div><div class="line">    root = <span class="string">'./mnist/'</span>,</div><div class="line">    <span class="comment"># 是否是训练数据</span></div><div class="line">    train = <span class="keyword">True</span>,</div><div class="line">    <span class="comment"># 数据变换(0, 255) -&gt; (0, 1)</span></div><div class="line">    transform = torchvision.transforms.ToTensor(),</div><div class="line">    <span class="comment"># 是否下载MNIST数据</span></div><div class="line">    download = <span class="keyword">True</span></div><div class="line">)</div><div class="line"></div><div class="line">test_data = torchvision.datasets.MNIST(</div><div class="line">    root = <span class="string">'./mnist/'</span>,</div><div class="line">    <span class="comment"># 是否是训练数据</span></div><div class="line">    train = <span class="keyword">False</span>,</div><div class="line">    <span class="comment"># 数据变换(0, 255) -&gt; (0, 1)</span></div><div class="line">    transform = torchvision.transforms.ToTensor(),</div><div class="line">    <span class="comment"># 是否下载MNIST数据</span></div><div class="line">    download = <span class="keyword">True</span></div><div class="line">)</div><div class="line"></div><div class="line"><span class="keyword">print</span> train_data.train_data.size()</div><div class="line"><span class="keyword">print</span> train_data.train_labels.size()</div><div class="line"><span class="keyword">print</span> test_data.test_data.size()</div><div class="line"><span class="keyword">print</span> test_data.test_labels.size()</div></pre></td></tr></table></figure>
<pre><code>torch.Size([60000, 28, 28])
torch.Size([60000])
torch.Size([10000, 28, 28])
torch.Size([10000])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 查看图像</span></div><div class="line">plt.imshow(train_data.train_data[<span class="number">0</span>].numpy(), cmap = <span class="string">'gray'</span>)</div><div class="line">plt.title(<span class="string">'%i'</span> % train_data.train_labels[<span class="number">0</span>])</div><div class="line">plt.show()</div><div class="line"></div><div class="line">plt.imshow(test_data.test_data[<span class="number">0</span>].numpy(), cmap = <span class="string">'gray'</span>)</div><div class="line">plt.title(<span class="string">'%i'</span> % test_data.test_labels[<span class="number">0</span>])</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/cnn_1_0.png" alt="png"></p>
<p><img src="http://ocs628urt.bkt.clouddn.com/cnn_1_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 数据加载</span></div><div class="line">train_loader = Data.DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = <span class="keyword">True</span>, num_workers = <span class="number">2</span>)</div><div class="line">test_loader = Data.DataLoader(dataset = test_data, batch_size = BATCH_SIZE, shuffle = <span class="keyword">False</span>, num_workers = <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义卷积神经网络</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(CNN, self).__init__()</div><div class="line">        self.conv1 = nn.Sequential(</div><div class="line">            nn.Conv2d(</div><div class="line">                in_channels = <span class="number">1</span>,</div><div class="line">                out_channels = <span class="number">16</span>,</div><div class="line">                kernel_size = <span class="number">5</span>,</div><div class="line">                stride = <span class="number">1</span>,</div><div class="line">                padding = <span class="number">2</span></div><div class="line">            ),</div><div class="line">            nn.ReLU(),</div><div class="line">            nn.MaxPool2d(kernel_size = <span class="number">2</span>)</div><div class="line">        )</div><div class="line">        <span class="comment"># conv1输出为(16, 14, 14)</span></div><div class="line">        self.conv2 = nn.Sequential(</div><div class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</div><div class="line">            nn.ReLU(),</div><div class="line">            nn.MaxPool2d(<span class="number">2</span>)</div><div class="line">        )</div><div class="line">        <span class="comment"># conv2输出为(32, 7, 7)</span></div><div class="line">        self.output = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.conv1(x)</div><div class="line">        x = self.conv2(x)</div><div class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        prediction = self.output(x)</div><div class="line">        <span class="keyword">return</span> prediction</div><div class="line"></div><div class="line">cnn = CNN()</div><div class="line"><span class="keyword">print</span> cnn</div></pre></td></tr></table></figure>
<pre><code>CNN (
  (conv1): Sequential (
    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU ()
    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  )
  (conv2): Sequential (
    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU ()
    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))
  )
  (output): Linear (1568 -&gt; 10)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义优化器</span></div><div class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr = LR, betas= (<span class="number">0.9</span>, <span class="number">0.999</span>))</div><div class="line"></div><div class="line"><span class="comment"># 定义损失函数</span></div><div class="line">loss_func = nn.CrossEntropyLoss()</div><div class="line"></div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(EPOCH):</div><div class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(train_loader):</div><div class="line">        x_var = Variable(x)</div><div class="line">        y_var = Variable(y)</div><div class="line">        prediction = cnn(x_var)</div><div class="line">        loss = loss_func(prediction, y_var)</div><div class="line">        optimizer.zero_grad()</div><div class="line">        loss.backward()</div><div class="line">        optimizer.step()</div><div class="line">        </div><div class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            correct = <span class="number">0.0</span></div><div class="line">            <span class="keyword">for</span> step_test, (test_x, test_y) <span class="keyword">in</span> enumerate(test_loader):</div><div class="line">                test_x = Variable(test_x)</div><div class="line">                test_output = cnn(test_x)</div><div class="line">                pred_y = torch.max(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.squeeze()</div><div class="line">                correct += sum(pred_y == test_y)</div><div class="line">            accuracy = correct / test_data.test_data.size(<span class="number">0</span>)</div><div class="line">            <span class="keyword">print</span> <span class="string">'Epoch: '</span>, epoch, <span class="string">'| train loss: %.4f'</span> % loss.data[<span class="number">0</span>], <span class="string">'| accuracy: '</span>, accuracy</div></pre></td></tr></table></figure>
<pre><code>Epoch:  0 | train loss: 2.2787 | accuracy:  0.0982
Epoch:  0 | train loss: 0.0788 | accuracy:  0.9592
Epoch:  0 | train loss: 0.0587 | accuracy:  0.9626
Epoch:  0 | train loss: 0.0188 | accuracy:  0.9745
Epoch:  0 | train loss: 0.0707 | accuracy:  0.9759
Epoch:  0 | train loss: 0.0564 | accuracy:  0.9775
Epoch:  0 | train loss: 0.0489 | accuracy:  0.9779
Epoch:  0 | train loss: 0.0925 | accuracy:  0.9791
Epoch:  0 | train loss: 0.0566 | accuracy:  0.9834
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(十)——卷积神经网络
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improving Deep Neural Networks学习笔记(三)</title>
    <link href="noahsnail.com/2017/09/22/2017-9-22-Improving%20Deep%20Neural%20Networks%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%89)/"/>
    <id>noahsnail.com/2017/09/22/2017-9-22-Improving Deep Neural Networks学习笔记(三)/</id>
    <published>2017-09-21T23:59:41.000Z</published>
    <updated>2017-09-25T08:13:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="5-Hyperparameter-tuning"><a href="#5-Hyperparameter-tuning" class="headerlink" title="5. Hyperparameter tuning"></a>5. Hyperparameter tuning</h2><h4 id="5-1-Tuning-process"><a href="#5-1-Tuning-process" class="headerlink" title="5.1 Tuning process"></a>5.1 Tuning process</h4><p>Hyperparameters:</p>
<p>$\alpha$, $\beta$, $\beta_1,\beta_2, \epsilon$, layers, hidden units, learning rate decay, mini-batch size.</p>
<p>The learning rate is the most important hyperparameter to tune. $\beta$, mini-batch size and hidden units is second in importance to tune.</p>
<p>Try random values: Don’t use a grid. Corarse to fine.</p>
<h4 id="5-2-Using-an-appropriate-scale-to-pick-hyperparameters"><a href="#5-2-Using-an-appropriate-scale-to-pick-hyperparameters" class="headerlink" title="5.2 Using an appropriate scale to pick hyperparameters"></a>5.2 Using an appropriate scale to pick hyperparameters</h4><p>Appropriate scale to hyperparameters:</p>
<p>$\alpha = [0.0001, 1]$, r = -4 * np.random.rand(), $\alpha = 10^r$.</p>
<p>If $\alpha = [10^a, 10^b]$, random pick from [a, b] uniformly, and set $\alpha = 10^r$.</p>
<p>Hyperparameters for exponentially weighted average</p>
<p>$\beta = [0.9, 0.999]$, don’t random pick from $[0.9, 0.999]$. Use $1-\beta = [0.001, 0.1]$, use similar method lik $\alpha$.</p>
<p>Why don’t use linear pick? Because when $\beta$ is close one, even if a little change, it will have a huge impact on algorithm.</p>
<h4 id="5-3-Hyperparameters-tuning-in-practice-Pandas-vs-Caviar"><a href="#5-3-Hyperparameters-tuning-in-practice-Pandas-vs-Caviar" class="headerlink" title="5.3 Hyperparameters tuning in practice: Pandas vs Caviar"></a>5.3 Hyperparameters tuning in practice: Pandas vs Caviar</h4><ul>
<li><p>Re-test hyperparamters occasionally</p>
</li>
<li><p>Babysitting one model(Pandas)</p>
</li>
<li><p>Training many models in parallel(Caviar)</p>
</li>
</ul>
<h2 id="6-Batch-Normalization"><a href="#6-Batch-Normalization" class="headerlink" title="6. Batch Normalization"></a>6. Batch Normalization</h2><h4 id="6-1-Normalizing-activations-in-a-network"><a href="#6-1-Normalizing-activations-in-a-network" class="headerlink" title="6.1 Normalizing activations in a network"></a>6.1 Normalizing activations in a network</h4><p>In logistic regression, normalizing inputs to speed up learning.</p>
<ol>
<li>compute means$\mu = \frac {1} {m} \sum_{i=1}^n x^{(i)}$</li>
<li>subtract off the means from training set $x = x - \mu$\</li>
<li>compute the variances $\sigma ^2 = \frac {1} {m} \sum_{i=1}^n {x^{(i)}}^2$</li>
<li>normalize training set $X = \frac {X} {\sigma ^2}$</li>
</ol>
<p>Similarly, in order to speed up training neural network, we can normalize intermediate values in layers（<code>z</code> in hidden layer）, it is called Batch Normalization or Batch Norm.</p>
<p>Implementing Batch Norm</p>
<ol>
<li>Given some intermediate value in neural network, $z^{(1)}, z^{(2)},…,z^{(m)}$</li>
<li>compute means $\mu = \frac {1} {m} \sum_{i=1} z^{(i)}$</li>
<li>compute the variances $\sigma ^2 = \frac {1} {m} \sum_{i=1} (z^{(i)} - \mu)^2$</li>
<li>normalize $z$, $z^{(i)} = \frac {z^{(i)} - \mu} {\sqrt {(\sigma ^2 + \epsilon)}}$</li>
<li>compute $\hat z$, $\hat z = \gamma z^{(i)} + \beta$.</li>
</ol>
<p>Now we have normalized Z to have mean zero and standard unit variance. But maybe it makes sense for hidden units to have a different distribution. So we use $\hat z$ instead of $z$, $\gamma$ and $\beta$ are learnable parameters of your model.</p>
<h4 id="6-2-Fitting-Batch-Norm-into-a-neural-network"><a href="#6-2-Fitting-Batch-Norm-into-a-neural-network" class="headerlink" title="6.2 Fitting Batch Norm into a neural network"></a>6.2 Fitting Batch Norm into a neural network</h4><p>Add Batch Norm to a network</p>
<p>$X \rightarrow Z^{[1]} \rightarrow {\hat Z^{[1]}} \rightarrow {a^{[1]}} \rightarrow Z^{[2]} \rightarrow {\hat Z^{[2]}} \rightarrow {a^{[2]}}…$</p>
<p>Parameters:<br>$W^{[1]}, b^{[1]}$, $W^{[2]}, b^{[2]}…$<br>$\gamma^{[1]}, \beta^{[1]}$, $\gamma^{[2]}, \beta^{[2]}…$</p>
<p>If you use Batch Norm, you need to computing means and subtracting means, so $b^{[i]}$ is useless, so we can set $b^{[i]} = 0$ permanently.</p>
<h4 id="6-3-Why-does-Batch-Norm-work"><a href="#6-3-Why-does-Batch-Norm-work" class="headerlink" title="6.3 Why does Batch Norm work?"></a>6.3 Why does Batch Norm work?</h4><p>Covariate Shift: You have learned a function from $x \rightarrow y$, it works well. If the distribution of $x$ changes, you need to learn a new function to make it work well.</p>
<p>Hidden unit values change all the time, and so it’s suffering from the problem of covariate.</p>
<p>Batch Norm as regularization</p>
<ul>
<li>Each mini-batch is scaled by the mean/variance computed on just that mini-batch.</li>
<li>This adds some noise to the values $z^{[l]}$ within that mini-batch. So similar to dropout, it adds some noise to each hidden layer’s activations.</li>
<li>This has a slight regularization effect.</li>
</ul>
<h4 id="6-4-Batch-Norm-at-test-time"><a href="#6-4-Batch-Norm-at-test-time" class="headerlink" title="6.4 Batch Norm at test time"></a>6.4 Batch Norm at test time</h4><p>In order to apply neural network at test time, come up with some seperate estimate of mu and sigma squared.</p>
<h2 id="7-Multi-class-classification"><a href="#7-Multi-class-classification" class="headerlink" title="7. Multi-class classification"></a>7. Multi-class classification</h2><h4 id="7-1-Softmax-regression"><a href="#7-1-Softmax-regression" class="headerlink" title="7.1 Softmax regression"></a>7.1 Softmax regression</h4><h4 id="7-2-Training-a-softmax-classifier"><a href="#7-2-Training-a-softmax-classifier" class="headerlink" title="7.2 Training a softmax classifier"></a>7.2 Training a softmax classifier</h4><p>Hard max. </p>
<p>Loss function.</p>
<p>Gradient descent with softmax.</p>
<h2 id="8-Programming-Frameworks"><a href="#8-Programming-Frameworks" class="headerlink" title="8. Programming Frameworks"></a>8. Programming Frameworks</h2><h4 id="8-1-Deep-Learning-frameworks"><a href="#8-1-Deep-Learning-frameworks" class="headerlink" title="8.1 Deep Learning frameworks"></a>8.1 Deep Learning frameworks</h4><ul>
<li>Caffe/Caffe2</li>
<li>TensorFlow</li>
<li>Torch</li>
<li>Theano</li>
<li>mxnet</li>
<li>PaddlePaddle</li>
<li>Keras</li>
<li>CNTK</li>
</ul>
<p>Choosing deep learning frameworks</p>
<ul>
<li>Ease of programming (development and deployment)</li>
<li>Running speed</li>
<li>Truly open (open source with good governance)</li>
</ul>
<h4 id="8-2-TensorFlow"><a href="#8-2-TensorFlow" class="headerlink" title="8.2 TensorFlow"></a>8.2 TensorFlow</h4><p>…</p>
]]></content>
    
    <summary type="html">
    
      Improving Deep Neural Networks学习笔记(三)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(九)——优化器</title>
    <link href="noahsnail.com/2017/09/21/2017-9-21-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B9%9D)%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    <id>noahsnail.com/2017/09/21/2017-9-21-PyTorch基本用法(九)——优化器/</id>
    <published>2017-09-21T11:32:17.000Z</published>
    <updated>2017-09-21T11:34:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义超参数</span></div><div class="line">LR = <span class="number">0.01</span></div><div class="line">BATCH_SIZE = <span class="number">32</span></div><div class="line">EPOCH = <span class="number">10</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1000</span>), dim = <span class="number">1</span>)</div><div class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.1</span>  * torch.normal(torch.zeros(x.size()))</div><div class="line"></div><div class="line"><span class="comment"># 绘制数据图像</span></div><div class="line">plt.scatter(x.numpy(), y.numpy())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/opt_0_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义数据库</span></div><div class="line">dataset = Data.TensorDataset(data_tensor = x, target_tensor = y)</div><div class="line"></div><div class="line"><span class="comment"># 定义数据加载器</span></div><div class="line">loader = Data.DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = <span class="keyword">True</span>, num_workers = <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义pytorch网络</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, n_hidden, n_output)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.hidden = torch.nn.Linear(n_features, n_hidden)</div><div class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.relu(self.hidden(x))</div><div class="line">        y = self.predict(x)</div><div class="line">        <span class="keyword">return</span> y</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义不同的优化器网络</span></div><div class="line">net_SGD = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line">net_Momentum = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line">net_RMSprop = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line">net_Adam = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 选择不同的优化方法</span></div><div class="line">opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr = LR)</div><div class="line">opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr = LR, momentum = <span class="number">0.9</span>)</div><div class="line">opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr = LR, alpha = <span class="number">0.9</span>)</div><div class="line">opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr = LR, betas= (<span class="number">0.9</span>, <span class="number">0.99</span>))</div><div class="line"></div><div class="line">nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]</div><div class="line">optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</div><div class="line"></div><div class="line"><span class="comment"># 选择损失函数</span></div><div class="line">loss_func = torch.nn.MSELoss()</div><div class="line"></div><div class="line"><span class="comment"># 不同方法的loss</span></div><div class="line">loss_SGD = []</div><div class="line">loss_Momentum = []</div><div class="line">loss_RMSprop =[]</div><div class="line">loss_Adam = []</div><div class="line"></div><div class="line"><span class="comment"># 保存所有loss</span></div><div class="line">losses = [loss_SGD, loss_Momentum, loss_RMSprop, loss_Adam]</div><div class="line"></div><div class="line"><span class="comment"># 执行训练</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(EPOCH):</div><div class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> enumerate(loader):</div><div class="line">        var_x = Variable(batch_x)</div><div class="line">        var_y = Variable(batch_y)</div><div class="line">        <span class="keyword">for</span> net, optimizer, loss_history <span class="keyword">in</span> zip(nets, optimizers, losses):</div><div class="line">            <span class="comment"># 对x进行预测</span></div><div class="line">            prediction = net(var_x)</div><div class="line">            <span class="comment"># 计算损失</span></div><div class="line">            loss = loss_func(prediction, var_y)</div><div class="line">            <span class="comment"># 每次迭代清空上一次的梯度</span></div><div class="line">            optimizer.zero_grad()</div><div class="line">            <span class="comment"># 反向传播</span></div><div class="line">            loss.backward()</div><div class="line">            <span class="comment"># 更新梯度</span></div><div class="line">            optimizer.step()</div><div class="line">            <span class="comment"># 保存loss记录</span></div><div class="line">            loss_history.append(loss.data[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 画图</span></div><div class="line">labels = [<span class="string">'SGD'</span>, <span class="string">'Momentum'</span>, <span class="string">'RMSprop'</span>, <span class="string">'Adam'</span>]</div><div class="line"><span class="keyword">for</span> i, loss_history <span class="keyword">in</span> enumerate(losses):</div><div class="line">    plt.plot(loss_history, label = labels[i])</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line">plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">plt.ylabel(<span class="string">'Loss'</span>)</div><div class="line">plt.ylim((<span class="number">0</span>, <span class="number">0.2</span>))</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/opt_3_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(九)——优化器
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(八)——批训练</title>
    <link href="noahsnail.com/2017/09/20/2017-9-20-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AB)%E2%80%94%E2%80%94%E6%89%B9%E8%AE%AD%E7%BB%83/"/>
    <id>noahsnail.com/2017/09/20/2017-9-20-PyTorch基本用法(八)——批训练/</id>
    <published>2017-09-20T13:30:52.000Z</published>
    <updated>2017-09-20T13:31:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 定义batch size</span></div><div class="line">BATCH_SIZE = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># 定义数据</span></div><div class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>)</div><div class="line">y = torch.linspace(<span class="number">10</span>, <span class="number">1</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> x.numpy()</div><div class="line"><span class="keyword">print</span> y.numpy()</div></pre></td></tr></table></figure>
<pre><code>[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]
[ 10.   9.   8.   7.   6.   5.   4.   3.   2.   1.]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义数据库</span></div><div class="line">dataset = Data.TensorDataset(data_tensor = x, target_tensor = y)</div><div class="line"></div><div class="line"><span class="comment"># 定义数据加载器</span></div><div class="line">loader = Data.DataLoader(dataset = dataset, batch_size = BATCH_SIZE, shuffle = <span class="keyword">True</span>, num_workers = <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 训练过程</span></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(<span class="number">5</span>):</div><div class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> enumerate(loader):</div><div class="line">        <span class="comment"># 训练过程</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'Epoch: '</span>, epoch, <span class="string">'| Step: '</span>, step, <span class="string">'| batch x: '</span>, batch_x.numpy(), <span class="string">'| betch y: '</span>, batch_y.numpy()</div></pre></td></tr></table></figure>
<pre><code>Epoch:  0 | Step:  0 | batch x:  [ 7.  4.  8.  5.  2.] | betch y:  [ 4.  7.  3.  6.  9.]
Epoch:  0 | Step:  1 | batch x:  [ 10.   6.   3.   1.   9.] | betch y:  [  1.   5.   8.  10.   2.]
Epoch:  1 | Step:  0 | batch x:  [  6.   7.  10.   1.   3.] | betch y:  [  5.   4.   1.  10.   8.]
Epoch:  1 | Step:  1 | batch x:  [ 9.  4.  5.  8.  2.] | betch y:  [ 2.  7.  6.  3.  9.]
Epoch:  2 | Step:  0 | batch x:  [ 5.  4.  7.  3.  8.] | betch y:  [ 6.  7.  4.  8.  3.]
Epoch:  2 | Step:  1 | batch x:  [  6.   9.   2.  10.   1.] | betch y:  [  5.   2.   9.   1.  10.]
Epoch:  3 | Step:  0 | batch x:  [  9.   1.   5.   3.  10.] | betch y:  [  2.  10.   6.   8.   1.]
Epoch:  3 | Step:  1 | batch x:  [ 8.  6.  4.  2.  7.] | betch y:  [ 3.  5.  7.  9.  4.]
Epoch:  4 | Step:  0 | batch x:  [ 10.   5.   9.   7.   3.] | betch y:  [ 1.  6.  2.  4.  8.]
Epoch:  4 | Step:  1 | batch x:  [ 6.  8.  2.  4.  1.] | betch y:  [  5.   3.   9.   7.  10.]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(八)——批训练
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(七)——模型的保存与加载</title>
    <link href="noahsnail.com/2017/09/20/2017-9-20-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%83)%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD/"/>
    <id>noahsnail.com/2017/09/20/2017-9-20-PyTorch基本用法(七)——模型的保存与加载/</id>
    <published>2017-09-20T11:42:59.000Z</published>
    <updated>2017-09-20T12:24:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim = <span class="number">1</span>)</div><div class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())</div><div class="line"></div><div class="line"><span class="comment"># 变为Variable</span></div><div class="line">x, y = Variable(x), Variable(y)</div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line">net = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</div><div class="line">)</div><div class="line"></div><div class="line"><span class="keyword">print</span> net</div></pre></td></tr></table></figure>
<pre><code>Sequential (
  (0): Linear (1 -&gt; 10)
  (1): ReLU ()
  (2): Linear (10 -&gt; 1)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 选择优化方法</span></div><div class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># 选择损失函数</span></div><div class="line">loss_func = torch.nn.MSELoss()</div><div class="line"></div><div class="line"><span class="comment"># 训练网络</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</div><div class="line">    <span class="comment"># 对x进行预测</span></div><div class="line">    prediction = net(x)</div><div class="line">    <span class="comment"># 计算损失</span></div><div class="line">    loss = loss_func(prediction, y)</div><div class="line">    <span class="comment"># 每次迭代清空上一次的梯度</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    <span class="comment"># 反向传播</span></div><div class="line">    loss.backward()</div><div class="line">    <span class="comment"># 更新梯度</span></div><div class="line">    optimizer.step()</div><div class="line"></div><div class="line">plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.4f'</span> % loss.data[<span class="number">0</span>], fontdict=&#123;<span class="string">'size'</span>: <span class="number">10</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/model_save_1_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 保存训练的模型</span></div><div class="line"></div><div class="line"><span class="comment"># 保存整个网络和参数</span></div><div class="line">torch.save(net, <span class="string">'net.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 重新加载模型</span></div><div class="line">net = torch.load(<span class="string">'net.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 用新加载的模型进行预测</span></div><div class="line">prediction = net(x)</div><div class="line">plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/model_save_2_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 只保存网络的参数, 官方推荐的方式</span></div><div class="line">torch.save(net.state_dict(), <span class="string">'net_params.pkl'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义网络</span></div><div class="line">net = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># 加载网络参数</span></div><div class="line">net.load_state_dict(torch.load(<span class="string">'net_params.pkl'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 用新加载的参数进行预测</span></div><div class="line">prediction = net(x)</div><div class="line">plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/model_save_3_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(七)——模型的保存与加载
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(六)——快速搭建网络</title>
    <link href="noahsnail.com/2017/09/20/2017-9-20PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%85%AD)%E2%80%94%E2%80%94%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C/"/>
    <id>noahsnail.com/2017/09/20/2017-9-20PyTorch基本用法(六)——快速搭建网络/</id>
    <published>2017-09-20T11:19:11.000Z</published>
    <updated>2017-09-20T11:23:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 许多没解释的东西可以去查文档, 文档中都有, 已查过</span></div><div class="line"><span class="comment"># pytorch文档: http://pytorch.org/docs/master/index.html</span></div><div class="line"><span class="comment"># matplotlib文档: https://matplotlib.org/</span></div><div class="line"></div><div class="line"><span class="comment"># 随机算法的生成种子</span></div><div class="line">torch.manual_seed(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 类别一的数据</span></div><div class="line">x0 = torch.normal(<span class="number">2</span> * n_data, <span class="number">1</span>)</div><div class="line"><span class="comment"># 类别一的标签</span></div><div class="line">y0 = torch.zeros(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># 类别二的数据</span></div><div class="line">x1 = torch.normal(<span class="number">-2</span> * n_data, <span class="number">1</span>)</div><div class="line"><span class="comment"># 类别二的标签</span></div><div class="line">y1 = torch.ones(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># x0, x1连接起来, 按维度0连接, 并指定数据的类型</span></div><div class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.FloatTensor)</div><div class="line"><span class="comment"># y0, y1连接, 由于只有一维, 因此没有指定维度, torch中标签类型必须为LongTensor</span></div><div class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># x,y 转为变量, torch只支持变量的训练, 因为Variable中有grad</span></div><div class="line">x, y = Variable(x), Variable(y)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制数据散点图</span></div><div class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c = y.data.numpy(), s = <span class="number">100</span>, lw = <span class="number">0</span>, cmap = <span class="string">'RdYlGn'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/classification_1_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 快速搭建分类网络</span></div><div class="line">net = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(<span class="number">2</span>, <span class="number">10</span>),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">2</span>))</div><div class="line">print(net)</div></pre></td></tr></table></figure>
<pre><code>Sequential (
  (0): Linear (2 -&gt; 10)
  (1): ReLU ()
  (2): Linear (10 -&gt; 2)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义优化方法</span></div><div class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.02</span>)</div><div class="line"><span class="comment"># 定义损失函数</span></div><div class="line">loss_func = torch.nn.CrossEntropyLoss()</div><div class="line"></div><div class="line">plt.ion()</div><div class="line"></div><div class="line"><span class="comment"># 训练过程</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">100</span>):</div><div class="line">    prediction = net(x)</div><div class="line">    loss = loss_func(prediction, y)</div><div class="line"></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</div><div class="line">        plt.cla()</div><div class="line">        <span class="comment"># 获取概率最大的类别的索引</span></div><div class="line">        prediction = torch.max(F.softmax(prediction), <span class="number">1</span>)[<span class="number">1</span>]</div><div class="line">        <span class="comment"># 将输出结果变为一维</span></div><div class="line">        pred_y = prediction.data.numpy().squeeze()</div><div class="line">        target_y = y.data.numpy()</div><div class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c = pred_y, s = <span class="number">100</span>, lw = <span class="number">0</span>, cmap = <span class="string">'RdYlGn'</span>)</div><div class="line">        <span class="comment"># 计算准确率</span></div><div class="line">        accuracy = sum(pred_y == target_y) / <span class="number">200.0</span></div><div class="line">        plt.text(<span class="number">1.5</span>, <span class="number">-4</span>, <span class="string">'Accuracy=%.2f'</span> % accuracy, fontdict = &#123;<span class="string">'size'</span>: <span class="number">10</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</div><div class="line">        plt.pause(<span class="number">0.1</span>)</div><div class="line"></div><div class="line">plt.ioff()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/classification_4_0.png" alt="png"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(六)——快速搭建网络
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(五)——分类</title>
    <link href="noahsnail.com/2017/09/19/2017-9-19-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%94)%E2%80%94%E2%80%94%E5%88%86%E7%B1%BB/"/>
    <id>noahsnail.com/2017/09/19/2017-9-19-PyTorch基本用法(五)——分类/</id>
    <published>2017-09-19T12:43:42.000Z</published>
    <updated>2017-09-19T12:46:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"></div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 许多没解释的东西可以去查文档, 文档中都有, 已查过</span></div><div class="line"><span class="comment"># pytorch文档: http://pytorch.org/docs/master/index.html</span></div><div class="line"><span class="comment"># matplotlib文档: https://matplotlib.org/</span></div><div class="line"></div><div class="line"><span class="comment"># 随机算法的生成种子</span></div><div class="line">torch.manual_seed(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 类别一的数据</span></div><div class="line">x0 = torch.normal(<span class="number">2</span> * n_data, <span class="number">1</span>)</div><div class="line"><span class="comment"># 类别一的标签</span></div><div class="line">y0 = torch.zeros(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># 类别二的数据</span></div><div class="line">x1 = torch.normal(<span class="number">-2</span> * n_data, <span class="number">1</span>)</div><div class="line"><span class="comment"># 类别二的标签</span></div><div class="line">y1 = torch.ones(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># x0, x1连接起来, 按维度0连接, 并指定数据的类型</span></div><div class="line">x = torch.cat((x0, x1), <span class="number">0</span>).type(torch.FloatTensor)</div><div class="line"><span class="comment"># y0, y1连接, 由于只有一维, 因此没有指定维度, torch中标签类型必须为LongTensor</span></div><div class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># x,y 转为变量, torch只支持变量的训练, 因为Variable中有grad</span></div><div class="line">x, y = Variable(x), Variable(y)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制数据散点图</span></div><div class="line">plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c = y.data.numpy(), s = <span class="number">100</span>, lw = <span class="number">0</span>, cmap = <span class="string">'RdYlGn'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/classification_1_0.png" alt="png"></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 定义分类网络</div><div class="line">class Net(torch.nn.Module):</div><div class="line">    </div><div class="line">    def __init__(self, n_feature, n_hidden, n_output):</div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</div><div class="line">        self.prediction = torch.nn.Linear(n_hidden, n_output)</div><div class="line"></div><div class="line">    def forward(self, x)</div><div class="line">        x = F.relu(self.hidden(x))</div><div class="line">        x = self.prediction(x)</div><div class="line">        return x</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义网络</span></div><div class="line">net = Net(n_feature = <span class="number">2</span>, n_hidden = <span class="number">10</span>, n_output = <span class="number">2</span>)</div><div class="line">print(net)</div></pre></td></tr></table></figure>
<pre><code>Net (
  (hidden): Linear (2 -&gt; 10)
  (prediction): Linear (10 -&gt; 2)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义优化方法</span></div><div class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.02</span>)</div><div class="line"><span class="comment"># 定义损失函数</span></div><div class="line">loss_func = torch.nn.CrossEntropyLoss()</div><div class="line"></div><div class="line">plt.ion()</div><div class="line"></div><div class="line"><span class="comment"># 训练过程</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">100</span>):</div><div class="line">    prediction = net(x)</div><div class="line">    loss = loss_func(prediction, y)</div><div class="line"></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</div><div class="line">        plt.cla()</div><div class="line">        <span class="comment"># 获取概率最大的类别的索引</span></div><div class="line">        prediction = torch.max(F.softmax(prediction), <span class="number">1</span>)[<span class="number">1</span>]</div><div class="line">        <span class="comment"># 将输出结果变为一维</span></div><div class="line">        pred_y = prediction.data.numpy().squeeze()</div><div class="line">        target_y = y.data.numpy()</div><div class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c = pred_y, s = <span class="number">100</span>, lw = <span class="number">0</span>, cmap = <span class="string">'RdYlGn'</span>)</div><div class="line">        <span class="comment"># 计算准确率</span></div><div class="line">        accuracy = sum(pred_y == target_y) / <span class="number">200.0</span></div><div class="line">        plt.text(<span class="number">1.5</span>, <span class="number">-4</span>, <span class="string">'Accuracy=%.2f'</span> % accuracy, fontdict = &#123;<span class="string">'size'</span>: <span class="number">10</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</div><div class="line">        plt.pause(<span class="number">0.1</span>)</div><div class="line"></div><div class="line">plt.ioff()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/classification_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># torch.max用法</span></div><div class="line">a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</div><div class="line"><span class="keyword">print</span> a</div><div class="line"><span class="keyword">print</span> torch.max(a, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<pre><code>-1.8524 -1.0491  0.5382 -0.5129
 0.1233 -0.1821  2.1519 -1.4547
-1.0267  0.2644 -0.8832 -0.2647
 0.3944 -1.2512 -0.1158  0.5071
[torch.FloatTensor of size 4x4]

(
 0.5382
 2.1519
 0.2644
 0.5071
[torch.FloatTensor of size 4]
, 
 2
 2
 1
 3
[torch.LongTensor of size 4]
)
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(五)——分类
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(四)——回归</title>
    <link href="noahsnail.com/2017/09/19/2017-9-19-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E5%9B%9B)%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/"/>
    <id>noahsnail.com/2017/09/19/2017-9-19-PyTorch基本用法(四)——回归/</id>
    <published>2017-09-19T07:47:28.000Z</published>
    <updated>2017-09-19T07:49:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># 生成数据</span></div><div class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim = <span class="number">1</span>)</div><div class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())</div><div class="line"></div><div class="line"><span class="comment"># 变为Variable</span></div><div class="line">x, y = Variable(x), Variable(y)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制数据图像</span></div><div class="line">plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/output_1_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义pytorch网络</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_features, n_hidden, n_output)</span>:</span></div><div class="line">        super(Net, self).__init__()</div><div class="line">        self.hidden = torch.nn.Linear(n_features, n_hidden)</div><div class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.relu(self.hidden(x))</div><div class="line">        y = self.predict(x)</div><div class="line">        <span class="keyword">return</span> y</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 构建网络</span></div><div class="line">net = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> net</div></pre></td></tr></table></figure>
<pre><code>Net (
  (hidden): Linear (1 -&gt; 10)
  (predict): Linear (10 -&gt; 1)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 选择优化方法</span></div><div class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># 选择损失函数</span></div><div class="line">loss_func = torch.nn.MSELoss()</div><div class="line"></div><div class="line">plt.ion()</div><div class="line"><span class="comment"># 训练</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">100</span>):</div><div class="line">    <span class="comment"># 对x进行预测</span></div><div class="line">    prediction = net(x)</div><div class="line">    <span class="comment"># 计算损失</span></div><div class="line">    loss = loss_func(prediction, y)</div><div class="line">    <span class="comment"># 每次迭代清空上一次的梯度</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    <span class="comment"># 反向传播</span></div><div class="line">    loss.backward()</div><div class="line">    <span class="comment"># 更新梯度</span></div><div class="line">    optimizer.step()</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</div><div class="line">        plt.cla()</div><div class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</div><div class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">'r-'</span>, lw = <span class="number">5</span>)</div><div class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">'Loss=%.4f'</span> % loss.data[<span class="number">0</span>], fontdict=&#123;<span class="string">'size'</span>: <span class="number">10</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;)</div><div class="line">        plt.pause(<span class="number">0.1</span>)</div><div class="line">plt.ioff()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://ocs628urt.bkt.clouddn.com/output_4_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># unsqueeze用法, 一维变二维</span></div><div class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</div><div class="line"><span class="keyword">print</span> x</div><div class="line"><span class="keyword">print</span> torch.unsqueeze(x, <span class="number">0</span>)</div><div class="line"><span class="keyword">print</span> torch.unsqueeze(x, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<pre><code> 1
 2
 3
 4
[torch.FloatTensor of size 4]


 1  2  3  4
[torch.FloatTensor of size 1x4]


 1
 2
 3
 4
[torch.FloatTensor of size 4x1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># rand用法, rand返回的是[0,1)之间的均匀分布</span></div><div class="line"><span class="keyword">print</span> torch.rand(<span class="number">4</span>)</div><div class="line"><span class="keyword">print</span> torch.rand(<span class="number">2</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure>
<pre><code> 0.8473
 0.2252
 0.0599
 0.0777
[torch.FloatTensor of size 4]


 0.2864  0.1693  0.1261
 0.9013  0.2009  0.9854
[torch.FloatTensor of size 2x3]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(四)——回归
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improving Deep Neural Networks学习笔记(二)</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-Improving%20Deep%20Neural%20Networks%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%BA%8C)/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-Improving Deep Neural Networks学习笔记(二)/</id>
    <published>2017-09-18T13:53:18.000Z</published>
    <updated>2017-09-21T14:32:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="4-Optimization-algorithms"><a href="#4-Optimization-algorithms" class="headerlink" title="4. Optimization algorithms"></a>4. Optimization algorithms</h2><h4 id="4-1-Mini-batch-gradient-descent"><a href="#4-1-Mini-batch-gradient-descent" class="headerlink" title="4.1 Mini-batch gradient descent"></a>4.1 Mini-batch gradient descent</h4><p>$x^{\{t\}}$，$y^{\{t\}}$ is used to index into different mini batches. $x^{[t]}$，$y^{[t]}$ is used to index into different layer. $x^{(t)}$，$y^{(t)}$ is used to index into different examples.</p>
<p>Batch gradient descent is to process entire training set at the same time. Mini-batch gradient descent is to process single mini batch $x^{\{t\}}$，$y^{\{t\}}$ at the same time.</p>
<p>Run forward propagation and back propagation once on mini batch is called one iteration.</p>
<p>Mini-batch gradient descent runs much faster than batch gradient descent.</p>
<h4 id="4-2-Understanding-mini-batch-gradient-descent"><a href="#4-2-Understanding-mini-batch-gradient-descent" class="headerlink" title="4.2 Understanding mini-batch gradient descent"></a>4.2 Understanding mini-batch gradient descent</h4><p>If mini-batch size = m, it’s batch gradient descend.<br>If mini-batch size = 1, it’s stochastic gradient descend.<br>In pracice, mini-batch size between 1 and m.</p>
<p>Batch gradient descend: too long per iteration.<br>Stochastic gradient descend: lose speed up from vectorization.<br>Mini-batch gradient descend: Faster learning, 1. vectorization 2. Make progress without needing to wait.</p>
<p>Choosing mini-batch size:</p>
<p>If small training set(m &lt;= 2000), use batch gradient descend.<br>Typical mini-batch size: 64, 128, 256, 512, 1024(rare).</p>
<h4 id="4-3-Exponentially-weighted-averages"><a href="#4-3-Exponentially-weighted-averages" class="headerlink" title="4.3 Exponentially weighted averages"></a>4.3 Exponentially weighted averages</h4><p>$$V_t = \beta V_{t-1} + (1-\beta)\theta_t$$</p>
<p>View $V_t$ as approximately averaging over $\frac {1} {1 - \beta}$.</p>
<p>It’s called moving average in the statistics literature.</p>
<p>$\beta = 0.9$：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/ewa_1.png" alt="Figure 1"></p>
<p>$\beta = 0.9(red)$，$\beta = 0.98(green)$，$\beta = 0.5(yellow)$：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/ewa_2.png" alt="Figure 2"></p>
<h4 id="4-4-Understanding-exponentially-weighted-averages"><a href="#4-4-Understanding-exponentially-weighted-averages" class="headerlink" title="4.4 Understanding exponentially weighted averages"></a>4.4 Understanding exponentially weighted averages</h4><p>$\theta$ is the temperature of the day.</p>
<p>$$v_{100} = 0.9v_{99} + 0.1 \theta_{100}$$$$v_{99} = 0.9v_{98} + 0.1 \theta_{99}$$$$…$$</p>
<p>So $$v_{100} = 0.1 * \theta _{100} + 0.1 * 0.9 * \theta _{99} + … + 0.1 * 0.9^{i} * \theta _{100-i} + …$$</p>
<p>Th coefficients is $$0.1 + 0.1 * 0.9 + 0.1 * 0.9^2 + …$$</p>
<p>All of these coefficients, add up to one or add up to very close to one. It is called bias correction.</p>
<p>$$(1 - \epsilon)^{\frac {1} {\epsilon}} \approx \frac {1} {e}$$ $$\frac {1} {e} \approx 0.3679$$</p>
<p>Implement exponentially weighted average:</p>
<p>$$v_0 = 0$$$$v_1 = \beta v_0 + (1- \beta) \theta _1$$$$v_2 = \beta v_1 + (1- \beta) \theta _2$$$$…$$</p>
<p>Exponentially weighted average takes very low memory.</p>
<h4 id="4-5-Bias-correction-in-exponentially-weighted-averages"><a href="#4-5-Bias-correction-in-exponentially-weighted-averages" class="headerlink" title="4.5 Bias correction in exponentially weighted averages"></a>4.5 Bias correction in exponentially weighted averages</h4><p>It’s not a very good estimate of the first several day’s temperature. Bias correction is used to mofity this estimate that makes it much better. The formula is: $$\frac {v_t} {1 - \beta^t} = \beta v_{t-1} + (1- \beta) \theta _t.$$</p>
<h4 id="4-6-Gradient-descent-with-momentum"><a href="#4-6-Gradient-descent-with-momentum" class="headerlink" title="4.6 Gradient descent with momentum"></a>4.6 Gradient descent with momentum</h4><p>Gradient descent with momentum almost always works faster than the standard gradient descent algorithm. The basic idea is to compute an exponentially weighted average of gradients, and then use that gradient to update weights instead.</p>
<p>On iteration t:</p>
<ol>
<li>compute $dw$, db on current mini-batch.</li>
<li>compute $v_{dw}$, $v_{db}$<br>$$v_{dw} = \beta v_{dw} + (1 - \beta)dw$$$$v_{db} = \beta v_{db} + (1 - \beta)db$$</li>
<li>update dw, db<br>$$w = w - \alpha v_{dw}$$$$b = b - \alpha v_{db}$$</li>
</ol>
<p>There are two hyperparameters, the most common value for $\beta$ is 0.9.</p>
<p>Another formula is $v_{dw} = \beta v_{dw} + dw$, you need to modify corresponding $\alpha$.</p>
<h4 id="4-7-RMSprop"><a href="#4-7-RMSprop" class="headerlink" title="4.7 RMSprop"></a>4.7 RMSprop</h4><p>RMSprop stands for root mean square prop, that can also speed up gradient descent.</p>
<p>On iteration t:</p>
<ol>
<li>compute $dw$, db on current mini-batch.</li>
<li>compute $s_{dw}$, $s_{db}$<br>$$s_{dw} = \beta s_{dw} + (1 - \beta){dw}^2$$$$s_{db} = \beta s_{db} + (1 - \beta){db}^2$$</li>
<li>update dw, db<br>$$w = w - \alpha \frac {dw} {\sqrt {s_{dw}}}$$$$b = b - \alpha \frac {db} {\sqrt {s_{db}}}$$</li>
</ol>
<p>In practice, in order to avoid $\sqrt {s_{dw}}$ being very close zero:</p>
<p>$$w = w - \alpha \frac {dw} {\sqrt {s_{dw}} + \epsilon}$$$$b = b - \alpha \frac {db} {\sqrt {s_{db}} + \epsilon}$$</p>
<p>Usually $$\epsilon = 10^{-8}$$</p>
<h4 id="4-8-Adam-optimization-algorithm"><a href="#4-8-Adam-optimization-algorithm" class="headerlink" title="4.8 Adam optimization algorithm"></a>4.8 Adam optimization algorithm</h4><p>$$v_{dw}=0, s_{dw}=0,v_{db},s_{db}=0$$</p>
<p>On iteration t:</p>
<p>$$v_{dw} = \beta_1 v_{dw} + (1 - \beta_1)dw$$$$v_{db} = \beta_1 v_{db} + (1 - \beta_1)db$$</p>
<p>$$s_{dw} = \beta_2 s_{dw} + (1 - \beta_2){dw}^2$$$$s_{db} = \beta_2 s_{db} + (1 - \beta_2){db}^2$$</p>
<p>Bias correction:</p>
<p>$$v_{dw}^{bc} = \frac {v_{dw}} {1 - \beta_1^t}, v_{db}^{bc} = \frac {v_{db}} {1 - \beta_1^t}$$$$s_{dw}^{bc} = \frac {s_{dw}} {1 - \beta_2^t}, s_{db}^{bc} = \frac {s_{db}} {1 - \beta_2^t}$$</p>
<p>Update weight:</p>
<p>$$w = w - \alpha \frac {v_{dw}^{bc}} {\sqrt {s_{dw}^{bc}} + \epsilon}$$$$b = b - \alpha \frac {v_{db}^{bc}} {\sqrt {s_{db}^{bc}} + \epsilon}$$</p>
<p>Adam combines the effect of gradient descent with momentum together with gradient descent with RMSprop. It’s a commonly used learning algorithm that is proven to be very effective for many different neural networks of a very wide variety of architectures.\</p>
<p>$\alpha$ needs to be tuned. $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$.</p>
<p>Adam stands for Adaptive Moment Estimation.</p>
<h4 id="4-9-Learning-rate-decay"><a href="#4-9-Learning-rate-decay" class="headerlink" title="4.9 Learning rate decay"></a>4.9 Learning rate decay</h4><p>Learning rate decay is slowly reduce the learning rate.</p>
<p>$$\alpha = \frac {1} {1 + {decay rate} * epochs} \alpha_0$$</p>
<p>$\alpha_0$ is the initial learning rate.</p>
<p>Other learning rate decay methods:</p>
<p>$\alpha = 0.95^{epochs}\alpha_0$, this is called exponentially decay.</p>
<p>$\alpha = \frac {k} {\sqrt {epochs} } \alpha_0$, $\alpha = \frac {k} {\sqrt t} \alpha_0$.</p>
<p>$\alpha = {\frac {1} {2}}^{epochs} \alpha _0$, this is called a discrete staircase.</p>
<h4 id="4-10-The-problem-of-local-optima"><a href="#4-10-The-problem-of-local-optima" class="headerlink" title="4.10 The problem of local optima"></a>4.10 The problem of local optima</h4><p>In very high-dimensional spaces you’re actually much more likely to run into a saddle point, rather than local optimum.</p>
<ul>
<li>Unlikely to get stuck in a bad local optima.</li>
<li>Plateaus can make learning slow.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Improving Deep Neural Networks学习笔记(二)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(三)——激活函数</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%89)%E2%80%94%E2%80%94%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(三)——激活函数/</id>
    <published>2017-09-18T12:25:07.000Z</published>
    <updated>2017-09-18T13:15:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是关于PyTorch的激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> func</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义数据x</span></div><div class="line">x = torch.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)</div><div class="line">x = Variable(x)</div><div class="line">np_x = x.data.numpy()</div><div class="line"></div><div class="line"><span class="comment"># 通过激活函数处理x</span></div><div class="line">y_relu = func.relu(x).data.numpy()</div><div class="line">y_sigmoid = func.sigmoid(x).data.numpy()</div><div class="line">y_tanh = func.tanh(x).data.numpy()</div><div class="line">y_softmax = func.softplus(x).data.numpy()</div><div class="line"></div><div class="line"><span class="comment"># 绘制激活函数图</span></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">221</span>)</div><div class="line">plt.plot(np_x, y_relu, c = <span class="string">'red'</span>, label = <span class="string">'relu'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">222</span>)</div><div class="line">plt.plot(np_x, y_sigmoid, c = <span class="string">'red'</span>, label = <span class="string">'sigmoid'</span>)</div><div class="line">plt.ylim((<span class="number">0</span>, <span class="number">1</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">223</span>)</div><div class="line">plt.plot(np_x, y_tanh, c = <span class="string">'red'</span>, label = <span class="string">'tanh'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">plt.subplot(<span class="number">224</span>)</div><div class="line">plt.plot(np_x, y_softmax, c = <span class="string">'red'</span>, label = <span class="string">'softmax'</span>)</div><div class="line">plt.ylim((<span class="number">-1</span>, <span class="number">5</span>))</div><div class="line">plt.legend(loc = <span class="string">'best'</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/ac_func.png" alt="Figure"></p>
]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(三)——激活函数
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(二)——Variable</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%BA%8C)%E2%80%94%E2%80%94Variable/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(二)——Variable/</id>
    <published>2017-09-18T10:46:11.000Z</published>
    <updated>2017-09-18T12:24:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是PyTorch中Variable变量的一些用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">tensor = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</div><div class="line"></div><div class="line"><span class="comment"># 定义Variable, requires_grad用来指定是否需要计算梯度</span></div><div class="line">variable = Variable(tensor, requires_grad = <span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="keyword">print</span> tensor</div><div class="line"><span class="keyword">print</span> variable</div></pre></td></tr></table></figure>
<pre><code> 1  2
 3  4
[torch.FloatTensor of size 2x2]

Variable containing:
 1  2
 3  4
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算x^2的均值</span></div><div class="line">tensor_mean = torch.mean(tensor * tensor)</div><div class="line">variable_mean = torch.mean(variable * variable)</div><div class="line"><span class="keyword">print</span> tensor_mean</div><div class="line"><span class="keyword">print</span> variable_mean</div></pre></td></tr></table></figure>
<pre><code>7.5
Variable containing:
 7.5000
[torch.FloatTensor of size 1]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># variable进行反向传播</span></div><div class="line"><span class="comment"># 梯度计算如下：</span></div><div class="line"><span class="comment"># variable_mean = 1/4 * sum(variable * variable)</span></div><div class="line"><span class="comment"># d(variable_mean)/d(variable) = 1/4 * 2 * variable = 1/2 * variable</span></div><div class="line">variable_mean.backward()</div><div class="line"></div><div class="line"><span class="comment"># 输出variable中的梯度</span></div><div class="line"><span class="keyword">print</span> variable.grad</div></pre></td></tr></table></figure>
<pre><code>Variable containing:
 0.5000  1.0000
 1.5000  2.0000
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># *表示逐元素点乘,不是矩阵乘法</span></div><div class="line"><span class="keyword">print</span> tensor * tensor</div><div class="line"><span class="keyword">print</span> variable * variable</div></pre></td></tr></table></figure>
<pre><code>  1   4
  9  16
[torch.FloatTensor of size 2x2]

Variable containing:
  1   4
  9  16
[torch.FloatTensor of size 2x2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 输出variable中的data, data是tensor</span></div><div class="line"><span class="keyword">print</span> variable.data</div></pre></td></tr></table></figure>
<pre><code> 1  2
 3  4
[torch.FloatTensor of size 2x2]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(二)——Variable
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python性能优化</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-Python%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-Python性能优化/</id>
    <published>2017-09-18T02:15:08.000Z</published>
    <updated>2017-09-18T10:40:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Python使用非常方便、灵活，因此很受欢迎。但正因为如此，导致实现同一功能时，Python代码有很多写法，但不同的写法有不同的性能。因此写Python代码要有良好的习惯，多写高性能的代码。作者原来平常写Python代码也很随意，直到某天处理大量数据时半天看不到结果，究其原因，是Python代码的性能问题导致的。</p>
<h2 id="1-列表解析与列表重建"><a href="#1-列表解析与列表重建" class="headerlink" title="1. 列表解析与列表重建"></a>1. 列表解析与列表重建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"><span class="comment"># _*_ coding: utf-8 _*_</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">fr = open(<span class="string">'words.txt'</span>)</div><div class="line">t1 = time.time()</div><div class="line">word_list = fr.readlines()</div><div class="line">t2 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'read file time: '</span>, t2 -t1</div><div class="line">fr.close()</div><div class="line"></div><div class="line"><span class="comment"># for循环构建列表</span></div><div class="line">keywords = []</div><div class="line">t1 = time.time()</div><div class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_list:</div><div class="line">    word = word.strip()</div><div class="line">    keywords.append(word)</div><div class="line">t2 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'for loop time: '</span>, t2 - t1</div><div class="line"></div><div class="line"><span class="comment"># 列表解析</span></div><div class="line">t3 = time.time()</div><div class="line">keywords = [word.strip() <span class="keyword">for</span> word <span class="keyword">in</span> word_list]</div><div class="line">t4 = time.time()</div><div class="line"><span class="keyword">print</span> <span class="string">'list pars time: '</span>, t4 - t3</div><div class="line"></div><div class="line">fr = open(<span class="string">'words.txt'</span>)</div><div class="line">t5 = time.time()</div><div class="line">keywords = [word.strip() <span class="keyword">for</span> word <span class="keyword">in</span> fr.readlines()]</div><div class="line">t6 = time.time()</div><div class="line">fr.close()</div><div class="line"><span class="keyword">print</span> <span class="string">'read file and list parse time: '</span>, t6 - t5</div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'list length: '</span>, len(word_list)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">read file time:  0.0318450927734</div><div class="line">for loop time:  0.137716054916</div><div class="line">list pars time:  0.0910630226135</div><div class="line">read file and list parse time:  0.124923944473</div><div class="line">list length:  441669</div></pre></td></tr></table></figure>
<p>结论：本次测试中，列表解析时间是for循环时间的<code>2/3</code>。</p>
<h2 id="2-字符串拼接"><a href="#2-字符串拼接" class="headerlink" title="2. 字符串拼接"></a>2. 字符串拼接</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import time</div><div class="line"></div><div class="line">fr = open(&apos;words.txt&apos;)</div><div class="line">keywords = [word.strip() for word in fr.readlines()]</div><div class="line">fr.close()</div><div class="line"></div><div class="line"># 加号拼接字符串</div><div class="line">t1 = time.time()</div><div class="line">str1 = &apos;&apos;</div><div class="line">for word in keywords:</div><div class="line">    str1 += word</div><div class="line">t2 = time.time()</div><div class="line">print &apos;string concat time: &apos;, t2 - t1</div><div class="line"></div><div class="line"># join拼接字符串</div><div class="line">t1 = time.time()</div><div class="line">str2 = &apos;&apos;.join(keywords)</div><div class="line">t2 = time.time()</div><div class="line">print &apos;string join time: &apos;, t2 - t1</div><div class="line"></div><div class="line">print &apos;list length: &apos;, len(keywords)</div></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">string concat time:  0.0814869403839</div><div class="line">string join time:  0.0123951435089</div><div class="line">list length:  441669</div></pre></td></tr></table></figure>
<p>结论：本次测试中，<code>join</code>函数拼接字符串比<code>+=</code>拼接字符串快<code>6倍多</code>。</p>
<h2 id="3-range与xrange"><a href="#3-range与xrange" class="headerlink" title="3. range与xrange"></a>3. range与xrange</h2><ul>
<li>range</li>
</ul>
<p>python中range会直接生成一个list对象。</p>
<ul>
<li>xrange</li>
</ul>
<p>用法与range完全相同，所不同的是生成的不是一个数组，而是一个生成器，它的类型为<code>xrange</code>。在生成非常大的数字序列时，xrange不会马上开辟很大的一块内存空间。如果不是需要返回列表，则尽可能使用<code>xrange</code>。</p>
<p>测试代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import time</div><div class="line"></div><div class="line">t1 = time.time()</div><div class="line">for i in range(1000000):</div><div class="line">    pass</div><div class="line">t2 = time.time()</div><div class="line">print &apos;range time: &apos;, t2 -t1</div><div class="line"></div><div class="line">t1 = time.time()</div><div class="line">for i in xrange(1000000):</div><div class="line">    pass</div><div class="line">t2 = time.time()</div><div class="line">print &apos;xrange time: &apos;, t2 -t1</div></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">range time:  0.0680990219116</div><div class="line">xrange time:  0.0329170227051</div></pre></td></tr></table></figure>
<p>结论：本次测试中，<code>xrange</code>比<code>range</code>快一倍多。</p>
<h2 id="4-待续。"><a href="#4-待续。" class="headerlink" title="4. 待续。"></a>4. 待续。</h2>]]></content>
    
    <summary type="html">
    
      Python性能优化
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch基本用法(一)——Numpy，Torch对比</title>
    <link href="noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94Numpy%EF%BC%8CTorch%E5%AF%B9%E6%AF%94/"/>
    <id>noahsnail.com/2017/09/18/2017-9-18-PyTorch基本用法(一)——Numpy，Torch对比/</id>
    <published>2017-09-18T01:08:35.000Z</published>
    <updated>2017-09-18T03:47:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是对比Torch与Numpy的一些操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># numpy的array与torch的tensor的转换</span></div><div class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</div><div class="line">torch_data = torch.from_numpy(np_data)</div><div class="line">tensor2array = torch_data.numpy() </div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'numpy data: '</span>, np_data</div><div class="line"><span class="keyword">print</span> <span class="string">'torch data: '</span>, torch_data</div><div class="line"><span class="keyword">print</span> <span class="string">'tensor2array: '</span>, tensor2array</div></pre></td></tr></table></figure>
<pre><code>numpy data:  [[0 1 2]
 [3 4 5]]
torch data:  
 0  1  2
 3  4  5
[torch.LongTensor of size 2x3]

tensor2array:  [[0 1 2]
 [3 4 5]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Tensor的文档：http://pytorch.org/docs/master/tensors.html</span></div><div class="line">data = [<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</div><div class="line">float_data = torch.FloatTensor(data)</div><div class="line"><span class="keyword">print</span> float_data</div></pre></td></tr></table></figure>
<pre><code>-2
-1
 0
 1
 2
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># abs操作</span></div><div class="line"><span class="keyword">print</span> np.abs(data)</div><div class="line"><span class="keyword">print</span> torch.abs(float_data)</div></pre></td></tr></table></figure>
<pre><code>[2 1 0 1 2]

 2
 1
 0
 1
 2
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># sin操作</span></div><div class="line"><span class="keyword">print</span> np.sin(data)</div><div class="line"><span class="keyword">print</span> torch.sin(float_data)</div></pre></td></tr></table></figure>
<pre><code>[-0.90929743 -0.84147098  0.          0.84147098  0.90929743]

-0.9093
-0.8415
 0.0000
 0.8415
 0.9093
[torch.FloatTensor of size 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># mean操作</span></div><div class="line"><span class="keyword">print</span> np.mean(data)</div><div class="line"><span class="keyword">print</span> torch.mean(float_data)</div></pre></td></tr></table></figure>
<pre><code>0.0
0.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 矩阵相乘</span></div><div class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</div><div class="line">tensor = torch.FloatTensor(data)</div><div class="line"></div><div class="line"><span class="keyword">print</span> np.matmul(data, data)</div><div class="line"><span class="comment"># torch.mm不支持广播形式</span></div><div class="line"><span class="keyword">print</span> torch.mm(tensor, tensor)</div><div class="line"><span class="comment"># torch.matmul支持广播形式</span></div><div class="line"><span class="keyword">print</span> torch.matmul(tensor, tensor)</div></pre></td></tr></table></figure>
<pre><code>[[ 7 10]
 [15 22]]

  7  10
 15  22
[torch.FloatTensor of size 2x2]


  7  10
 15  22
[torch.FloatTensor of size 2x2]
</code></pre>]]></content>
    
    <summary type="html">
    
      PyTorch基本用法(一)——Numpy，Torch对比
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习——第一课笔记(上)</title>
    <link href="noahsnail.com/2017/09/17/2017-9-17-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E8%AF%BE/"/>
    <id>noahsnail.com/2017/09/17/2017-9-17-动手学深度学习——第一课/</id>
    <published>2017-09-17T10:09:47.000Z</published>
    <updated>2017-09-17T14:22:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>本文主要是学习李沐直播课程的笔记。视频及内容的具体地址可参考：<a href="https://zhuanlan.zhihu.com/p/29125290" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29125290</a>。</p>
<h2 id="第一课：从上手到多类分类"><a href="#第一课：从上手到多类分类" class="headerlink" title="第一课：从上手到多类分类"></a>第一课：从上手到多类分类</h2><p>课程首先介绍了深度学习的很多应用：例如增强学习、物体识别、语音识别、机器翻译、推荐系统、广告点击预测等。</p>
<p>课程目的：通过动手实现来理解深度学习，跟工业界应用相比，主要只是数据规模和模型复杂度的区别。</p>
<p>深度学习的轮子很多，例如Caffe，TensorFlow，mxnet，PyTorch，CNTK等。它们之间的主要区别在于：1.便利的开发；2.方便的部署。</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/chapter1_1.png" alt="Figure 1"></p>
<p>mxnet之上的一个package是Gluon，主要目的是一次解决开发和部署。课程主要分为以下三个部分：</p>
<p><img src="http://ocs628urt.bkt.clouddn.com/chapter1_2.png" alt="Figure 2"></p>
<h3 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1. 环境配置"></a>1. 环境配置</h3><p>我的配置环境是Mac，Linux平台类似。</p>
<p>mxnet安装命令如下，前提是已经安装好了Anaconda，Anaconda的安装可以参考官网：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install mxnet</div></pre></td></tr></table></figure>
<p>测试mxnet：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import mxnet</div><div class="line">&gt;&gt;&gt; print mxnet.__version__</div><div class="line">0.11.0</div></pre></td></tr></table></figure>
<p>然后安装notedown，运行Jupyter并加载notedown插件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pip install https://github.com/mli/notedown/tarball/master</div><div class="line">jupyter notebook --NotebookApp.contents_manager_class=&apos;notedown.NotedownContentsManager&apos;</div></pre></td></tr></table></figure>
<p>通过ExecutionTime插件来对每个cell的运行计时，国内使用豆瓣源。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pip install jupyter_contrib_nbextensions -i https://pypi.douban.com/simple</div><div class="line">jupyter contrib nbextension install --user</div><div class="line">jupyter nbextension enable execute_time/ExecuteTime</div></pre></td></tr></table></figure>
<h3 id="2-NDArray"><a href="#2-NDArray" class="headerlink" title="2. NDArray"></a>2. NDArray</h3><p>NDArray是MXNet储存和变换数据的主要工具，它与numpy非常类似。NDArray提供了CPU和GPU的异步计算，还提供了自动求导。NDArray的基本用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</div><div class="line"></div><div class="line"><span class="comment"># 创建矩阵</span></div><div class="line">nd.zeros((<span class="number">3</span>, <span class="number">4</span>))</div><div class="line">x = nd.ones((<span class="number">3</span>, <span class="number">4</span>))</div><div class="line">nd.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">3</span>]])</div><div class="line">y = nd.random_normal(<span class="number">0</span>, <span class="number">1</span>, shape=(<span class="number">3</span>, <span class="number">4</span>))</div><div class="line"></div><div class="line"><span class="comment"># 查看矩阵大小</span></div><div class="line">y.shape</div><div class="line"></div><div class="line"><span class="comment"># 查看矩阵元素个数</span></div><div class="line">y.size</div><div class="line"></div><div class="line"><span class="comment"># 矩阵加法</span></div><div class="line">x + y</div><div class="line"></div><div class="line"><span class="comment"># 矩阵乘法</span></div><div class="line">x * y</div><div class="line"></div><div class="line"><span class="comment"># 指数运算</span></div><div class="line">nd.exp(y)</div><div class="line"></div><div class="line"><span class="comment"># 矩阵乘法</span></div><div class="line">nd.dot(x, y.T)</div><div class="line"></div><div class="line"><span class="comment"># 广播操作</span></div><div class="line">a = nd.arange(<span class="number">3</span>).reshape((<span class="number">3</span>,<span class="number">1</span>))</div><div class="line">b = nd.arange(<span class="number">2</span>).reshape((<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">print(<span class="string">'a:'</span>, a)</div><div class="line">print(<span class="string">'b:'</span>, b)</div><div class="line">print(<span class="string">'a+b:'</span>, a+b)</div><div class="line"></div><div class="line"><span class="comment"># NDArray与Numpy的转换</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.ones((<span class="number">2</span>,<span class="number">3</span>))</div><div class="line">y = nd.array(x)  <span class="comment"># numpy -&gt; mxnet</span></div><div class="line">z = y.asnumpy()  <span class="comment"># mxnet -&gt; numpy</span></div><div class="line">print([z, y])</div></pre></td></tr></table></figure>
<p>NDArray的自动求导：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet.ndarray <span class="keyword">as</span> nd</div><div class="line"><span class="keyword">import</span> mxnet.autograd <span class="keyword">as</span> ag</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 定义矩阵</span></div><div class="line">x = nd.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</div><div class="line"></div><div class="line"><span class="comment"># 添加自动求导</span></div><div class="line">x.attach_grad()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 记录x的变化</span></div><div class="line"><span class="keyword">with</span> ag.record():</div><div class="line">    y = x * <span class="number">2</span></div><div class="line">    z = y * x</div><div class="line"></div><div class="line"><span class="comment"># 求导</span></div><div class="line">z.backward()</div><div class="line"></div><div class="line"><span class="comment"># 判断导数是否相等</span></div><div class="line">x.grad == <span class="number">4</span>*x</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      动手学深度学习——第一课笔记(上)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Improving Deep Neural Networks学习笔记(一)</title>
    <link href="noahsnail.com/2017/09/16/2017-9-16-Improving%20Deep%20Neural%20Networks%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"/>
    <id>noahsnail.com/2017/09/16/2017-9-16-Improving Deep Neural Networks学习笔记(一)/</id>
    <published>2017-09-16T01:21:06.000Z</published>
    <updated>2017-09-16T14:21:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="1-Setting-up-your-Machine-Learning-Application"><a href="#1-Setting-up-your-Machine-Learning-Application" class="headerlink" title="1. Setting up your Machine Learning Application"></a>1. Setting up your Machine Learning Application</h2><h4 id="1-1-Train-Dev-Test-sets"><a href="#1-1-Train-Dev-Test-sets" class="headerlink" title="1.1 Train/Dev/Test sets"></a>1.1 Train/Dev/Test sets</h4><p>Make sure that the dev and test sets come from the same distribution。</p>
<p>Not having a test set might be okay.(Only dev set.)</p>
<p>So having set up a train dev and test set will allow you to integrate more quickly. It will also allow you to more efficiently measure the bias and variance of your algorithm, so you can more efficiently select ways to improve your algorithm.</p>
<h4 id="1-2-Bias-Variance"><a href="#1-2-Bias-Variance" class="headerlink" title="1.2 Bias/Variance"></a>1.2 Bias/Variance</h4><p>High Bias: underfitting<br>High Variance: overfitting</p>
<p>Assumption——human: 0% (Optimal/Bayes error), train set and dev set are drawn from the same distribution.</p>
<table>
<thead>
<tr>
<th>Train set error</th>
<th>Dev set error</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>1%</td>
<td>11%</td>
<td>high variance</td>
</tr>
<tr>
<td>15%</td>
<td>16%</td>
<td>high bias</td>
</tr>
<tr>
<td>15%</td>
<td>30%</td>
<td>high bias and high variance</td>
</tr>
<tr>
<td>0.5%</td>
<td>1%</td>
<td>low bias and low variance</td>
</tr>
</tbody>
</table>
<h4 id="1-3-Basic-Recipe-for-Machine-Learning"><a href="#1-3-Basic-Recipe-for-Machine-Learning" class="headerlink" title="1.3 Basic Recipe for Machine Learning"></a>1.3 Basic Recipe for Machine Learning</h4><p>High bias –&gt; Bigger network, Training longer, Advanced optimization algorithms, Try different netword.</p>
<p>High variance –&gt; More data, Try regularization, Find a more appropriate neural network architecture.</p>
<h2 id="2-Regularizing-your-neural-network"><a href="#2-Regularizing-your-neural-network" class="headerlink" title="2. Regularizing your neural network"></a>2. Regularizing your neural network</h2><h4 id="2-1-Regularization"><a href="#2-1-Regularization" class="headerlink" title="2.1 Regularization"></a>2.1 Regularization</h4><p>In logistic regression, $$w \in R^{n_x}, b \in R$$$$J(w, b) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} ||w||_2^2$$$$||w||_2^2 = \sum _{j=1} ^{n_x} w_j^2 = w^Tw$$<br>This is called L2 regularization.</p>
<p>$$J(w, b) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} ||w||_1$$<br>This is called L1 regularization. <code>w</code> will end up being sparse. $\lambda$ is called regularization parameter.</p>
<p>In neural network, the formula is $$J(w^{[1]},b^{[1]},…,w^{[L]},b^{[L]}) = \frac {1} {m} \sum _{i=1} ^m L(\hat y^{(i)}, y^{(i)}) + \frac {\lambda} {2m} \sum _{l=1}^L ||w^{[l]}||^2$$$$||w^{[l]}||^2 = \sum_{i=1}^{n^{[l-1]}}\sum _{j=1}^{n^{[l]}} (w_{ij}^{[l]})^2, w:(n^{[l-1]}, n^{[l]})$$</p>
<p>This matrix norm, it turns out is called the <code>Frobenius Norm</code> of the matrix, denoted with a <code>F</code> in the subscript.</p>
<p>L2 norm regularization is also called <code>weight decay</code>.</p>
<h4 id="2-2-Why-regularization-reduces-overfitting"><a href="#2-2-Why-regularization-reduces-overfitting" class="headerlink" title="2.2 Why regularization reduces overfitting?"></a>2.2 Why regularization reduces overfitting?</h4><p>If $\lambda$ is set too large, matrices <code>W</code> is set to be reasonabley close to zero, and it will zero out the impact of these hidden units. And that’s the case, then this much simplified neural network becomes a much smaller neural network. It will take you from overfitting to underfitting, but there is a <code>just right case</code> in the middle.</p>
<h4 id="2-3-Dropout-regularization"><a href="#2-3-Dropout-regularization" class="headerlink" title="2.3 Dropout regularization"></a>2.3 Dropout regularization</h4><p>Dropout will go through each of the layers of the network, and set some probability of eliminating a node in neural network. By far the most common implementation of dropouts today is inverted dropouts.</p>
<p>Inverted dropout, <code>kp</code> stands for <code>keep-prob</code>:</p>
<p>$$z^{[i + 1]} = w^{[i + 1]} a^{[i]} + b^{[i + 1]}$$$$a^{[i]} = a^{[i]} / kp$$</p>
<p>In test phase, we don’t use dropout and <code>keep-prob</code>.</p>
<h4 id="2-4-Understanding-dropout"><a href="#2-4-Understanding-dropout" class="headerlink" title="2.4 Understanding dropout"></a>2.4 Understanding dropout</h4><p>Why does dropout workd? Intuition: Can’t rely on any one feature, so have to spread out weights.</p>
<p>By spreading all the weights, this will tend to have an effect of shrinking the squared norm of the weights.</p>
<h4 id="2-5-Other-regularization-methods"><a href="#2-5-Other-regularization-methods" class="headerlink" title="2.5 Other regularization methods"></a>2.5 Other regularization methods</h4><ul>
<li>Data augmentation.</li>
<li>Early stopping</li>
</ul>
<h2 id="3-Setting-up-your-optimization-problem"><a href="#3-Setting-up-your-optimization-problem" class="headerlink" title="3. Setting up your optimization problem"></a>3. Setting up your optimization problem</h2><h4 id="3-1-Normalizing-inputs"><a href="#3-1-Normalizing-inputs" class="headerlink" title="3.1 Normalizing inputs"></a>3.1 Normalizing inputs</h4><p>Normalizing inputs can speed up training. Normalizing inputs corresponds to two steps. The first is to subtract out or to zero out the mean. And then the second step is to normalize the variances.</p>
<h4 id="3-2-Vanishing-Exploding-gradients"><a href="#3-2-Vanishing-Exploding-gradients" class="headerlink" title="3.2 Vanishing/Exploding gradients"></a>3.2 Vanishing/Exploding gradients</h4><p>If the network is very deeper, deep network suffer from the problems of vanishing or exploding gradients.</p>
<h4 id="3-3-Weight-initialization-for-deep-networks"><a href="#3-3-Weight-initialization-for-deep-networks" class="headerlink" title="3.3 Weight initialization for deep networks"></a>3.3 Weight initialization for deep networks</h4><p>If activation function is <code>ReLU</code> or <code>tanh</code>, <code>w</code> initialization is: $$w^{[l]} = np.random.randn(shape) * np.sqrt(\frac {2} {n^{[l-1]}}).$$ This is called Xavier initalization. </p>
<p>Another formula is $$w^{[l]} = np.random.randn(shape) * np.sqrt(\frac {2} {n^{[l-1]} + n^{[l]}}).$$</p>
<h4 id="3-4-Numberical-approximation-of-gradients"><a href="#3-4-Numberical-approximation-of-gradients" class="headerlink" title="3.4 Numberical approximation of gradients"></a>3.4 Numberical approximation of gradients</h4><p>In order to build up to gradient checking, you need to numerically approximate computatiions of gradients.</p>
<p>$$g(\theta) \approx \frac {f(\theta + \epsilon) - f(\theta - \epsilon)} {2 \epsilon}$$</p>
<h4 id="3-5-Gradient-checking"><a href="#3-5-Gradient-checking" class="headerlink" title="3.5 Gradient checking"></a>3.5 Gradient checking</h4><p>Take matrix <code>W</code>, vector <code>b</code> and reshape them into vectors, and then concatenate them, you have a giant vector $\theta$. For each <code>i</code>:</p>
<p>$$d\theta _{approx}[i]= \frac {J(\theta_1,…,\theta_i + \epsilon,…)-J(\theta_1,…,\theta_i - \epsilon,…)} {2\epsilon} \approx d\theta_i=\frac {\partial J} {\partial \theta_i}$$</p>
<p>If $$\frac {||d\theta_{approx} - d\theta ||_2} {||d\theta_{approx}||_2 + ||\theta||_2} \approx 10^{-7}$$, that’s great. If $\approx 10^{-5}$, you need to do double check, if $\approx 10^{-5}$, there may be a bug.</p>
<h4 id="3-6-Gradient-checking-implementation-notes"><a href="#3-6-Gradient-checking-implementation-notes" class="headerlink" title="3.6 Gradient checking implementation notes"></a>3.6 Gradient checking implementation notes</h4><ul>
<li>Don’t use gradient check in training, only to debug.</li>
<li>If algorithm fails gradient check, look at components to try to identify bug.</li>
<li>Remember regularization.</li>
<li>Doesn’t work with dropout.</li>
<li>Run at random initialization; perhaps again after some training.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Improving Deep Neural Networks学习笔记(一)
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Python的命令行参数解析</title>
    <link href="noahsnail.com/2017/09/13/2017-9-13-Python%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/"/>
    <id>noahsnail.com/2017/09/13/2017-9-13-Python的命令行参数解析/</id>
    <published>2017-09-13T02:22:00.000Z</published>
    <updated>2017-09-13T02:59:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>命令行参数解析在编程语言中基本都会碰到，Python中内置了一个用于命令项选项与参数解析的模块<code>argparse</code>。下面主要介绍两种解析Python命令行参数的方式。</p>
<h2 id="1-sys-argv"><a href="#1-sys-argv" class="headerlink" title="1. sys.argv"></a>1. sys.argv</h2><p>解析Python中命令行参数的最传统的方法是通过<code>sys.argv</code>。Demo如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#!/usr/env/python python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import sys</div><div class="line"></div><div class="line">param1 = sys.argv[1]</div><div class="line">param2 = sys.argv[2]</div><div class="line"></div><div class="line">print sys.argv</div><div class="line">print param1</div><div class="line">print param2</div><div class="line">print type(param1)</div><div class="line">print type(param2)</div></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ python test.py 1 2</div><div class="line">[&apos;test.py&apos;, &apos;1&apos;, &apos;2&apos;]</div><div class="line">1</div><div class="line">2</div></pre></td></tr></table></figure>
<p>这种方法比较古老，灵活性很差，同时解析出来的参数都是<code>str</code>类型。但在编写简单脚本，参数较少且固定时比较方便。</p>
<h2 id="2-argparse"><a href="#2-argparse" class="headerlink" title="2. argparse"></a>2. argparse</h2><p><code>argparse</code>模块是Python内置的参数解析模块，使用起来比较简单且功能强大。Demo如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">#!/usr/env/python python</div><div class="line"># _*_ coding: utf-8 _*_</div><div class="line"></div><div class="line">import argparse</div><div class="line"></div><div class="line"># Create ArgumentParser() object</div><div class="line">parser = argparse.ArgumentParser()</div><div class="line"></div><div class="line"># Add argument</div><div class="line">parser.add_argument(&apos;--train&apos;, required=True, help=&apos;path to dataset&apos;)</div><div class="line">parser.add_argument(&apos;--val&apos;, required=True, help=&apos;path to dataset&apos;)</div><div class="line">parser.add_argument(&apos;--total&apos;, type=int, help=&apos;number of dataset&apos;, default=100)</div><div class="line">parser.add_argument(&apos;--lr&apos;, type=float, default=0.01, help=&apos;learning rate&apos;)</div><div class="line"></div><div class="line"># Print usage</div><div class="line">parser.print_help()</div><div class="line"></div><div class="line"># Parse argument</div><div class="line">args = parser.parse_args()</div><div class="line"></div><div class="line"># Print args</div><div class="line">print args</div><div class="line"></div><div class="line">print args.train</div><div class="line">print type(args.train)</div><div class="line">print args.val</div><div class="line">print type(args.val)</div><div class="line">print args.total</div><div class="line">print type(args.total)</div><div class="line">print args.lr</div><div class="line">print type(args.lr)</div></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"># Test 1</div><div class="line">python test.py --train train_lmdb --val val_lmdb --total 10000 --lr 0.001</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.001, total=10000, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">10000</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.001</div><div class="line">&lt;type &apos;float&apos;&gt;</div><div class="line"></div><div class="line"># Test 2</div><div class="line">python test.py --train train_lmdb --val val_lmdb</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.01, total=100, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">100</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.01</div><div class="line">&lt;type &apos;float&apos;&gt;</div><div class="line"></div><div class="line"></div><div class="line"># Test 3</div><div class="line">python test.py --val val_lmdb --train train_lmdb</div><div class="line">usage: test.py [-h] --train TRAIN --val VAL [--total TOTAL] [--lr LR]</div><div class="line"></div><div class="line">optional arguments:</div><div class="line">  -h, --help     show this help message and exit</div><div class="line">  --train TRAIN  path to dataset</div><div class="line">  --val VAL      path to dataset</div><div class="line">  --total TOTAL  number of dataset</div><div class="line">  --lr LR        learning rate</div><div class="line">Namespace(lr=0.01, total=100, train=&apos;train_lmdb&apos;, val=&apos;val_lmdb&apos;)</div><div class="line">train_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">val_lmdb</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">100</div><div class="line">&lt;type &apos;int&apos;&gt;</div><div class="line">0.01</div><div class="line">&lt;type &apos;float&apos;&gt;</div></pre></td></tr></table></figure>
<p><code>ArgumentParser</code>类创建时的参数如下：</p>
<ul>
<li>prog - 程序的名字（默认：sys.argv[0]）</li>
<li>usage - 描述程序用法的字符串（默认：从解析器的参数生成）</li>
<li>description - 参数帮助信息之前的文本（默认：空）</li>
<li>epilog - 参数帮助信息之后的文本（默认：空）</li>
<li>parents - ArgumentParser 对象的一个列表，这些对象的参数应该包括进去</li>
<li>formatter_class - 定制化帮助信息的类</li>
<li>prefix_chars - 可选参数的前缀字符集（默认：‘-‘）</li>
<li>fromfile_prefix_chars - 额外的参数应该读取的文件的前缀字符集（默认：None）</li>
<li>argument_default - 参数的全局默认值（默认：None）</li>
<li>conflict_handler - 解决冲突的可选参数的策略（通常没有必要）</li>
<li>add_help - 给解析器添加-h/–help 选项（默认：True）</li>
</ul>
<p><code>add_argument</code>函数的参数如下：</p>
<ul>
<li>name or flags - 选项字符串的名字或者列表，例如foo 或者-f, –foo。</li>
<li>action - 在命令行遇到该参数时采取的基本动作类型。</li>
<li>nargs - 应该读取的命令行参数数目。</li>
<li>const - 某些action和nargs选项要求的常数值。</li>
<li>default - 如果命令行中没有出现该参数时的默认值。</li>
<li>type - 命令行参数应该被转换成的类型。</li>
<li>choices - 参数可允许的值的一个容器。</li>
<li>required - 该命令行选项是否可以省略（只针对可选参数）。</li>
<li>help - 参数的简短描述。</li>
<li>metavar - 参数在帮助信息中的名字。</li>
<li>dest - 给parse_args()返回的对象要添加的属性名称。</li>
</ul>
<p>参考资料：</p>
<ol>
<li><a href="http://python.usyiyi.cn/translate/python_278/library/argparse.html" target="_blank" rel="external">http://python.usyiyi.cn/translate/python_278/library/argparse.html</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/explore-python/Standard-Modules/argparse.html" target="_blank" rel="external">http://wiki.jikexueyuan.com/project/explore-python/Standard-Modules/argparse.html</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      Python的命令行参数解析
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python中的编码</title>
    <link href="noahsnail.com/2017/09/07/2017-9-7-Python%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E7%A0%81/"/>
    <id>noahsnail.com/2017/09/07/2017-9-7-Python中的字符串编码/</id>
    <published>2017-09-07T03:26:27.000Z</published>
    <updated>2017-09-07T08:48:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>Python处理字符串，写文件时会碰到许多的编码问题，特别是涉及到中文的时候，非常烦人，但又不得不学。下面主要记录工作过程中碰到的Python编码问题。</p>
<h2 id="1-字符串编码"><a href="#1-字符串编码" class="headerlink" title="1. 字符串编码"></a>1. 字符串编码</h2><p>Python的字符串类型为<code>str</code>，可以通过<code>type</code>函数查看返回的类型。Python中字符串默认的编码方式需要通过<code>sys.getfilesystemencoding()</code>查看，通常是<code>utf-8</code>。<code>u&#39;中文&#39;</code>构造出来的是<code>unicode</code>类型，不是<code>str</code>类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># 查看字符串编码方式</div><div class="line">&gt;&gt;&gt; import sys</div><div class="line">&gt;&gt;&gt; print sys.getfilesystemencoding()</div><div class="line">utf-8</div><div class="line"></div><div class="line">&gt;&gt;&gt; s1 = &apos;中国&apos;</div><div class="line">&gt;&gt;&gt; s2 = u&apos;中国&apos;</div><div class="line">&gt;&gt;&gt; type(s1)</div><div class="line">&lt;type &apos;str&apos;&gt;</div><div class="line">&gt;&gt;&gt; type(s2)</div><div class="line">&lt;type &apos;unicode&apos;&gt;</div></pre></td></tr></table></figure>
<p><code>str</code>类型和<code>unicode</code>类型分别有<code>decode</code>和<code>encode</code>函数。<code>str.decode</code>用来将<code>str</code>转为<code>unicode</code>，<code>unicode.encode</code>用来将<code>unicdoe</code>转为<code>str</code>。用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># decode</div><div class="line">&gt;&gt;&gt; s1.decode(&apos;utf8&apos;)</div><div class="line">u&apos;\u4e2d\u56fd&apos;</div><div class="line">&gt;&gt;&gt; type(s1.decode(&apos;utf8&apos;))</div><div class="line">&lt;type &apos;unicode&apos;&gt;</div><div class="line"></div><div class="line"># encode</div><div class="line">&gt;&gt;&gt; s2.encode(&apos;utf8&apos;)</div><div class="line">&apos;\xe4\xb8\xad\xe5\x9b\xbd&apos;</div><div class="line">&gt;&gt;&gt; type(s2.encode(&apos;utf8&apos;))</div><div class="line">&lt;type &apos;str&apos;&gt;</div></pre></td></tr></table></figure>
<h2 id="2-代码文件编码"><a href="#2-代码文件编码" class="headerlink" title="2. 代码文件编码"></a>2. 代码文件编码</h2><p><code>py</code>文件默认的编码是ASCII编码，中文显示时会进行ASCII编码到系统默认编码的转换，在运行Python文件时经常会报错。因此需要设置<code>py</code>文件的编码为<code>utf-8</code>。设置方式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># _*_ coding: utf-8 _*_</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      Python中的编码
    
    </summary>
    
      <category term="Python" scheme="noahsnail.com/categories/Python/"/>
    
    
      <category term="Python" scheme="noahsnail.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization论文翻译——中英文对照</title>
    <link href="noahsnail.com/2017/09/04/2017-9-4-Batch%20Normalization%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/"/>
    <id>noahsnail.com/2017/09/04/2017-9-4-Batch Normalization论文翻译——中英文对照/</id>
    <published>2017-09-04T02:02:52.000Z</published>
    <updated>2017-09-25T08:13:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h1 id="Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift"><a href="#Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift" class="headerlink" title="Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"></a>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as <em>internal covariate shift</em>, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization <em>for each training mini-batch</em>. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout.  Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching <code>4.9% top-5</code> validation error (and <code>4.8%</code> test error), exceeding the accuracy of human raters.</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>训练深度神经网络的复杂性在于，每层输入的分布在训练过程中会发生变化，因为前面的层的参数会发生变化。通过要求较低的学习率和仔细的参数初始化减慢了训练，并且使具有饱和非线性的模型训练起来非常困难。我们将这种现象称为<em>内部协变量转移</em>，并通过规范化层输入来解决这个问题。我们的方法力图使规范化成为模型架构的一部分，并为<em>每个训练小批量数据</em>执行规范化。批量规范化使我们能够使用更高的学习率，并且不用太注意初始化。它也作为一个正则化项，在某些情况下不需要Dropout。将批量规范化应用到最先进的图像分类模型上，批量规范化在取得相同的精度的情况下，减少了14倍的训练步骤，并以显著的差距击败了原始模型。使用批规范化网络的组合，我们改进了在ImageNet分类上公布的最佳结果：达到了<code>4.9％ top-5</code>的验证误差（和<code>4.8％</code>测试误差），超过了人类评估者的准确性。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Deep learning has dramatically advanced the state of the art in vision, speech, and many other areas. Stochastic gradient descent (SGD) has proved to be an effective way of training deep networks, and SGD variants such as momentum (Sutskever et al., 2013) and Adagrad (Duchi et al., 2011) have been used to achieve state of the art performance. SGD optimizes the parameters $\Theta$ of the network, so as to minimize the loss </p>
<p>$$\Theta = \arg \min_\Theta \frac{1}{N}\sum_{i=1}^N \ell(x_i, \Theta)$$</p>
<p>where $x_{1\ldots N}$ is the training data set. With SGD, the training proceeds in steps, and at each step we consider a <em>mini-batch</em> $x_{1\ldots m}$ of size $m$. The mini-batch is used to approximate the gradient of the loss function with respect to the parameters, by computing $\frac {1} {m} \sum _{i=1} ^m \frac {\partial \ell(x_i, \Theta)} {\partial \Theta}$. Using mini-batches of examples, as opposed to one example at a time, is helpful in several ways. First, the gradient of the loss over a mini-batch is an estimate of the gradient over the training set, whose quality improves as the batch size increases. Second, computation over a batch can be much more efficient than $m$ computations for individual examples, due to the parallelism afforded by the modern computing platforms.</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>深度学习在视觉、语音等诸多方面显著提高了现有技术的水平。随机梯度下降（SGD）已经被证明是训练深度网络的有效方式，并且已经使用诸如动量（Sutskever等，2013）和Adagrad（Duchi等人，2011）等SGD变种取得了最先进的性能。SGD优化网络参数$\Theta$，以最小化损失</p>
<p>$$\Theta = \arg \min_\Theta \frac{1}{N}\sum_{i=1}^N \ell(x_i, \Theta)$$</p>
<p>$x_{1\ldots N}$是训练数据集。使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为$m$的<em>小批量数据</em>$x_{1 \ldots m}$。通过计算$\frac {1} {m} \sum _{i=1} ^m \frac {\partial \ell(x_i, \Theta)} {\partial \Theta}$，使用小批量数据来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而改善。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算$m$次效率更高。</p>
<p>While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers —— so that small changes to the network parameters amplify as the network becomes deeper.</p>
<p>虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。</p>
<p>The change in the distributions of layers’ inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience <em>covariate shift</em> (Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply to its parts, such as a sub-network or a layer. Consider a network computing $$\ell = F_2(F_1(u, \Theta_1), \Theta_2)$$ where $F_1$ and $F_2$ are arbitrary transformations, and the parameters $\Theta_1, \Theta_2$ are to be learned so as to minimize the loss $\ell$.  Learning $\Theta_2$ can be viewed as if the inputs $x=F_1(u,\Theta_1)$ are fed into the sub-network $$\ell = F_2(x, \Theta_2).$$</p>
<p>层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。当学习系统的输入分布发生变化时，据说会经历<em>协变量转移</em>（Shimodaira，2000）。这通常是通过域适应（Jiang，2008）来处理的。然而，协变量漂移的概念可以扩展到整个学习系统之外，应用到学习系统的一部分，例如子网络或一层。考虑网络计算$$\ell = F_2(F_1(u, \Theta_1), \Theta_2)$$ $F_1$和$F_2$是任意变换，学习参数$\Theta_1，\Theta_2$以便最小化损失$\ell$。学习$\Theta_2$可以看作输入$x=F_1(u,\Theta_1)$送入到子网络$$\ell = F_2(x, \Theta_2)。$$</p>
<p>For example, a gradient descent step $$\Theta_2\leftarrow \Theta_2 - \frac {\alpha} {m} \sum_{i=1}^m \frac {\partial F_2(x_i,\Theta_2)} {\partial \Theta_2}$$ (for batch size $m$ and learning rate $\alpha$) is exactly equivalent to that for a stand-alone network $F_2$ with input $x$.  Therefore, the input distribution properties that make training more efficient —— such as having the same distribution between the training and test data —— apply to training the sub-network as well.  As such it is advantageous for the distribution of $x$ to remain fixed over time. Then, $\Theta_2$ does not have to readjust to compensate for the change in the distribution of $x$.</p>
<p>例如，梯度下降步骤$$\Theta_2\leftarrow \Theta_2 - \frac {\alpha} {m} \sum_{i=1}^m \frac {\partial F_2(x_i,\Theta_2)} {\partial \Theta_2}$$（对于批大小$m$和学习率$\alpha$）与输入为$x$的单独网络$F_2$完全等价。因此，输入分布特性使训练更有效——例如训练数据和测试数据之间有相同的分布——也适用于训练子网络。因此$x$的分布在时间上保持固定是有利的。然后，$\Theta_2$不必重新调整来补偿$x$分布的变化。</p>
<p>Fixed distribution of inputs to a sub-network would have positive consequences for the layers <em>outside</em> the sub-network, as well. Consider a layer with a sigmoid activation function $z = g(Wu+b)$ where $u$ is the layer input, the weight matrix $W$ and bias vector $b$ are the layer parameters to be learned, and $g(x) = \frac{1}{1+\exp(-x)}$. As $|x|$ increases, $g’(x)$ tends to zero. This means that for all dimensions of $x=Wu+b$ except those with small absolute values, the gradient flowing down to $u$ will vanish and the model will train slowly. However, since $x$ is affected by $W, b$ and the parameters of all the layers below, changes to those parameters during training will likely move many dimensions of $x$ into the saturated regime of the nonlinearity and slow down the convergence. This effect is amplified as the network depth increases. In practice, the saturation problem and the resulting vanishing gradients are usually addressed by using Rectified Linear Units (Nair &amp; Hinton, 2010) $ReLU(x)=\max(x,0)$, careful initialization (Bengio &amp; Glorot, 2010; Saxe et al., 2013), and small learning rates.  If, however, we could ensure that the distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate.</p>
<p>子网络输入的固定分布对于子网络外的层也有积极的影响。考虑一个激活函数为$g(x) = \frac{1}{1+\exp(-x)}$的层，$u$是层输入，权重矩阵$W$和偏置向量$b$是要学习的层参数，$g(x) = \frac{1}{1+\exp(-x)}$。随着$|x|$的增加，$g’(x)$趋向于0。这意味着对于$x=Wu+b$的所有维度，除了那些具有小的绝对值之外，流向$u$的梯度将会消失，模型将缓慢的进行训练。然而，由于$x$受$W,b$和下面所有层的参数的影响，训练期间那些参数的改变可能会将$x$的许多维度移动到非线性的饱和状态并减慢收敛。这个影响随着网络深度的增加而放大。在实践中，饱和问题和由此产生的梯度消失通常通过使用修正线性单元(Nair &amp; Hinton, 2010) $ReLU(x)=\max(x,0)$，仔细的初始化(Bengio &amp; Glorot, 2010; Saxe et al., 2013)和小的学习率来解决。然而，如果我们能保证非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，训练将加速。</p>
<p>We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as <em>Internal Covariate Shift</em>. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call <em>Batch Normalization</em>, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for Dropout (Srivastava et al., 2014). Finally, Batch Normalization makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes.</p>
<p>我们把训练过程中深度网络内部结点的分布变化称为<em>内部协变量转移</em>。消除它可以保证更快的训练。我们提出了一种新的机制，我们称为为<em>批规范化</em>，它是减少内部协变量转移的一个步骤，这样做可以显著加速深度神经网络的训练。它通过规范化步骤来实现，规范化步骤修正了层输入的均值和方差。批规范化减少了梯度对参数或它们的初始值尺度上的依赖，对通过网络的梯度流动有有益的影响。这允许我们使用更高的学习率而没有发散的风险。此外，批规范化使模型正则化并减少了对Dropout(Srivastava et al., 2014)的需求。最后，批规范化通过阻止网络陷入饱和模式让使用饱和非线性成为可能。</p>
<p>In Sec. 4.2, we apply Batch Normalization to the best-performing ImageNet classification network, and show that we can match its performance using only 7% of the training steps, and can further exceed its accuracy by a substantial margin.  Using an ensemble of such networks trained with Batch Normalization, we achieve the top-5 error rate that improves upon the best known results on ImageNet classification.</p>
<p>在4.2小节，我们将批规范化应用到性能最好的ImageNet分类网络上，并且表明我们可以使用仅7％的训练步骤来匹配其性能，并且可以进一步超过其准确性一大截。通过使用批规范化训练的网络的集合，我们取得了top-5错误率，其改进了ImageNet分类上已知的最佳结果。</p>
<h2 id="2-Towards-Reducing-Internal-Covariate-Shift"><a href="#2-Towards-Reducing-Internal-Covariate-Shift" class="headerlink" title="2. Towards Reducing Internal Covariate Shift"></a>2. Towards Reducing Internal Covariate Shift</h2><p>We define <em>Internal Covariate Shift</em> as the change in the distribution of network activations due to the change in network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By fixing the distribution of the layer inputs $x$ as the training progresses, we expect to improve the training speed. It has been long known (LeCun et al., 1998b; Wiesler &amp; Ney, 2011) that the network training converges faster if its inputs are whitened – i.e., linearly transformed to have zero means and unit variances, and decorrelated. As each layer observes the inputs produced by the layers below, it would be advantageous to achieve the same whitening of the inputs of each layer. By whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.</p>
<h2 id="2-减少内部协变量转变"><a href="#2-减少内部协变量转变" class="headerlink" title="2. 减少内部协变量转变"></a>2. 减少内部协变量转变</h2><p>由于训练过程中网络参数的变化，我们将<em>内部协变量转移</em>定义为网络激活分布的变化。为了改善训练，我们寻求减少内部协变量转移。随着训练的进行，通过固定层输入$x$的分布，我们期望提高训练速度。众所周知(LeCun et al., 1998b; Wiesler &amp; Ney, 2011)如果对网络的输入进行白化，网络训练将会收敛的更快——即输入线性变换为具有零均值和单位方差，并去相关。当每一层观察下面的层产生的输入时，实现每一层输入进行相同的白化将是有利的。通过白化每一层的输入，我们将采取措施实现输入的固定分布，消除内部协变量转移的不良影响。</p>
<p>We could consider whitening activations at every training step or at some interval, either by modifying the network directly or by changing the parameters of the optimization algorithm to depend on the network activation values (Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins &amp; Kavukcuoglu). However, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires the normalization to be updated, which reduces the effect of the gradient step. For example, consider a layer with the input $u$ that adds the learned bias $b$, and normalizes the result by subtracting the mean of the activation computed over the training data: $\hat x=x - E[x]$ where $x = u+b$, $X={x_{1\ldots N}}$ is the set of values of $x$ over the training set, and $E[x] = \frac{1}{N}\sum_{i=1}^N x_i$. If a gradient descent step ignores the dependence of $E[x]$  on $b$, then it will update $b\leftarrow b+\Delta b$, where $\Delta b\propto -\partial{\ell}/\partial{\hat x}$. Then  $u+(b+\Delta b) -E[u+(b+\Delta b)] = u+b-E[u+b]$. Thus, the combination of the update to $b$ and subsequent change in normalization led to no change in the output of the layer nor, consequently, the loss. As the training continues, $b$ will grow indefinitely while the loss remains fixed. This problem can get worse if the normalization not only centers but also scales the activations. We have observed this empirically in initial experiments, where the model blows up when the normalization parameters are computed outside the gradient descent step.</p>
<p>我们考虑在每个训练步骤或在某些间隔来白化激活值，通过直接修改网络或根据网络激活值来更改优化方法的参数(Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins &amp; Kavukcuoglu)。然而，如果这些修改分散在优化步骤中，那么梯度下降步骤可能会试图以要求规范化进行更新的方式来更新参数，这会降低梯度下降步骤的影响。例如，考虑一个层，其输入$u$加上学习到的偏置$b$，通过减去在训练集上计算的激活值的均值对结果进行归一化：$\hat x=x - E[x]$，$x = u+b$, $X={x_{1\ldots N}}$是训练集上$x$值的集合，$E[x] = \frac{1}{N}\sum_{i=1}^N x_i$。如果梯度下降步骤忽略了$E[x]$对$b$的依赖，那它将更新$b\leftarrow b+\Delta b$，其中$\Delta b\propto -\partial{\ell}/\partial{\hat x}$。然后$u+(b+\Delta b) -E[u+(b+\Delta b)] = u+b-E[u+b]$。因此，结合$b$的更新和接下来标准化中的改变会导致层的输出没有变化，从而导致损失没有变化。随着训练的继续，$b$将无限增长而损失保持不变。如果标准化不仅中心化而且缩放了激活值，问题会变得更糟糕。我们在最初的实验中已经观察到了这一点，当标准化参数在梯度下降步骤之外计算时，模型会爆炸。</p>
<p>The issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place. To address this issue, we would like to ensure that, for any parameter values, the network <em>always</em> produces activations with the desired distribution. Doing so would allow the gradient of the loss with respect to the model parameters to account for the normalization, and for its dependence on the model parameters $\Theta$. Let again $x$ be a layer input, treated as a vector, and $\cal X$ be the set of these inputs over the training data set. The normalization can then be written as a transformation $$\hat x=Norm(x, \cal X)$$ which depends not only on the given training example $x$ but on all examples $\cal X$ – each of which depends on $\Theta$ if $x$ is generated by another layer. For backpropagation, we would need to compute the Jacobians $\frac {\partial Norm(x,\cal X)} {\partial x}$ and $\frac {\partial Norm(x,\cal X)} {\partial \cal X}$; ignoring the latter term would lead to the explosion described above. Within this framework, whitening the layer inputs is expensive, as it requires computing the covariance matrix $Cov[x]=E_{x\in \cal X}[x x^T]- E[x]E[x]^T$ and its inverse square root, to produce the whitened activations $Cov[x]^{-1/2}(x-E[x])$, as well as the derivatives of these transforms for backpropagation. This motivates us to seek an alternative that performs input normalization in a way that is differentiable and does not require the analysis of the entire training set after every parameter update.</p>
<p>上述方法的问题是梯度下降优化没有考虑到标准化中发生的事实。为了解决这个问题，我们希望确保对于任何参数值，网络<em>总是</em>产生具有所需分布的激活值。这样做将允许关于模型参数损失的梯度来解释标准化，以及它对模型参数$\Theta$的依赖。设$x$为层的输入，将其看作向量，$\cal X$是这些输入在训练集上的集合。标准化可以写为变换$$\hat x=Norm(x, \cal X)$$它不仅依赖于给定的训练样本$x$而且依赖于所有样本$\cal X$——它们中的每一个都依赖于$\Theta$，如果$x$是由另一层生成的。对于反向传播，我们将需要计算Jacobians$\frac {\partial Norm(x,\cal X)} {\partial x}$和$\frac {\partial Norm(x,\cal X)} {\partial \cal X}$；忽略后一项会导致上面描述的爆炸。在这个框架中，白化层输入是昂贵的，因为它要求计算协方差矩阵$Cov[x]=E_{x\in \cal X}[x x^T]- E[x]E[x]^T$和它的平方根倒数，从而生成白化的激活$Cov[x]^{-1/2}(x-E[x])$和这些变换进行反向传播的偏导数。这促使我们寻求一种替代方案，以可微分的方式执行输入标准化，并且在每次参数更新后不需要对整个训练集进行分析。</p>
<p>Some of the previous approaches (e.g. (Lyu &amp; Simoncelli, 2008)) use statistics computed over a single training example, or, in the case of image networks, over different feature maps at a given location. However, this changes the representation ability of a network by discarding the absolute scale of activations. We want to a preserve the information in the network, by normalizing the activations in a training example relative to the statistics of the entire training data.</p>
<p>以前的一些方法（例如（Lyu＆Simoncelli，2008））使用通过单个训练样本计算的统计信息，或者在图像网络的情况下，使用给定位置处不同特征图上的统计。然而，通过丢弃激活值绝对尺度改变了网络的表示能力。我们希望通过对相对于整个训练数据统计信息的单个训练样本的激活值进行归一化来保留网络中的信息。</p>
<h2 id="3-Normalization-via-Mini-Batch-Statistics"><a href="#3-Normalization-via-Mini-Batch-Statistics" class="headerlink" title="3. Normalization via Mini-Batch Statistics"></a>3. Normalization via Mini-Batch Statistics</h2><p>Since the full whitening of each layer’s inputs is costly and not everywhere differentiable, we make two necessary simplifications. The first is that instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature independently, by making it have the mean of zero and unit variance. For a layer with $d$-dimensional input $x = (x^{(1)}\ldots x^{(d)})$, we will normalize each dimension $$\hat x^{(k)} = \frac{x^{(k)} - E[x^{(k)}]} {\sqrt {Var[x^{(k)}]}}$$ where the expectation and variance are computed over the training data set. As shown in (LeCun et al., 1998b), such normalization speeds up convergence, even when the features are not decorrelated.</p>
<h2 id="3-通过Mini-Batch统计进行标准化"><a href="#3-通过Mini-Batch统计进行标准化" class="headerlink" title="3. 通过Mini-Batch统计进行标准化"></a>3. 通过Mini-Batch统计进行标准化</h2><p>由于每一层输入的整个白化是代价昂贵的并且不是到处可微分的，因此我们做了两个必要的简化。首先是我们将单独标准化每个标量特征，从而代替在层输入输出对特征进行共同白化，使其具有零均值和单位方差。对于具有$d$维输入$x = (x^{(1)}\ldots x^{(d)})$的层，我们将标准化每一维$$\hat x^{(k)} = \frac{x^{(k)} - E[x^{(k)}]} {\sqrt {Var[x^{(k)}]}}$$其中期望和方差在整个训练数据集上计算。如(LeCun et al., 1998b)中所示，这种标准化加速了收敛，即使特征没有去相关。</p>
<p>Note that simply normalizing each input of a layer may change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address this, we make sure that <em>the transformation inserted in the network can represent the identity transform</em>. To accomplish this, we introduce, for each activation $x^{(k)}$, a pair of parameters $\gamma^{(k)}, \beta^{(k)}$, which scale and shift the normalized value: $$y^{(k)} = \gamma^{(k)}\hat x^{(k)} + \beta^{(k)}.$$ These parameters are learned along with the original model parameters, and restore the representation power of the network. Indeed, by setting $\gamma^{(k)} = \sqrt{Var[x^{(k)}]}$ and $\beta^{(k)} = E[x^{(k)}]$, we could recover the original activations, if that were the optimal thing to do.</p>
<p>注意简单标准化层的每一个输入可能会改变层可以表示什么。例如，标准化sigmoid的输入会将它们约束到非线性的线性状态。为了解决这个问题，我们要确保<em>插入到网络中的变换可以表示恒等变换</em>。为了实现这个，对于每一个激活值$x^{(k)}$，我们引入成对的参数$\gamma^{(k)}，\beta^{(k)}$，它们会归一化和移动标准化值：$$y^{(k)} = \gamma^{(k)}\hat x^{(k)} + \beta^{(k)}.$$</p>
<p>In the batch setting where each training step is based on the entire training set, we would use the whole set to normalize activations. However, this is impractical when using stochastic optimization. Therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, <em>each mini-batch produces estimates of the mean and variance</em> of each activation. This way, the statistics used for normalization can fully participate in the gradient backpropagation. Note that the use of mini-batches is enabled by computation of per-dimension variances rather than joint covariances; in the joint case, regularization would be required since the mini-batch size is likely to be smaller than the number of activations being whitened, resulting in singular covariance matrices.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Bengio, Yoshua and Glorot, Xavier. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of AISTATS 2010, volume 9, pp. 249–256, May 2010.</p>
<p>Dean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai, Devin, Matthieu, Le, Quoc V., Mao, Mark Z., Ranzato, Marc’Aurelio, Senior, Andrew, Tucker, Paul, Yang, Ke, and Ng, Andrew Y. Large scale distributed deep networks. In NIPS, 2012.</p>
<p>Desjardins, Guillaume and Kavukcuoglu, Koray. Natural neural networks. (unpublished).</p>
<p>Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July 2011. ISSN 1532-4435.</p>
<p>Gu ̈lc ̧ehre, C ̧ aglar and Bengio, Yoshua. Knowledge matters: Importance of prior information for optimization. CoRR, abs/1301.4083, 2013.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. ArXiv e-prints, February 2015.</p>
<p>Hyva ̈rinen, A. and Oja, E. Independent component analysis: Algorithms and applications. Neural Netw., 13(4-5): 411–430, May 2000.<br>Jiang, Jing. A literature survey on domain adaptation of statistical classifiers, 2008.</p>
<p>LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998a.</p>
<p>LeCun, Y., Bottou, L., Orr, G., and Muller, K. Efficient backprop. In Orr, G. and K., Muller (eds.), Neural Networks: Tricks of the trade. Springer, 1998b.</p>
<p>Lyu, S and Simoncelli, E P. Nonlinear image representation using divisive normalization. In Proc. Computer Vision and Pattern Recognition, pp. 1–8. IEEE Computer Society, Jun 23-28 2008. doi: 10.1109/CVPR.2008.4587821.</p>
<p>Nair, Vinod and Hinton, Geoffrey E. Rectified linear units improve restricted boltzmann machines. In ICML, pp. 807–814. Omnipress, 2010.</p>
<p>Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1310–1318, 2013.</p>
<p>Povey, Daniel, Zhang, Xiaohui, and Khudanpur, Sanjeev. Parallel training of deep neural networks with natural gradient and parameter averaging. CoRR, abs/1410.7455, 2014.</p>
<p>Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep learning made easier by linear transformations in perceptrons. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 924–932, 2012.</p>
<p>Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Fei-Fei, Li. ImageNet Large Scale Visual Recognition Challenge, 2014.</p>
<p>Saxe, Andrew M., McClelland, James L., and Ganguli, Surya. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013.</p>
<p>Shimodaira, Hidetoshi. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90 (2):227–244, October 2000.</p>
<p>Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1):1929–1958, January 2014.</p>
<p>Sutskever, Ilya, Martens, James, Dahl, George E., and Hinton, Geoffrey E. On the importance of initialization and momentum in deep learning. In ICML (3), volume 28 of JMLR Proceedings, pp. 1139–1147. JMLR.org, 2013.</p>
<p>Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.</p>
<p>Wiesler, Simon and Ney, Hermann. A convergence analysis of log-linear training. In Shawe-Taylor, J., Zemel, R.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 24, pp. 657–665, Granada, Spain, December 2011.</p>
<p>Wiesler, Simon, Richard, Alexander, Schlu ̈ter, Ralf, and Ney, Hermann. Mean-normalized stochastic gradient for large-scale deep learning. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 180–184, Florence, Italy, May 2014.</p>
<p>Wu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and Sun, Gang. Deep image: Scaling up image recognition, 2015.</p>
]]></content>
    
    <summary type="html">
    
      Batch Normalization论文翻译——中英文对照
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization论文翻译——中文版</title>
    <link href="noahsnail.com/2017/09/04/2017-9-4-Batch%20Normalization%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/"/>
    <id>noahsnail.com/2017/09/04/2017-9-4-Batch Normalization论文翻译——中文版/</id>
    <published>2017-09-04T02:02:23.000Z</published>
    <updated>2017-09-25T07:18:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>训练深度神经网络的复杂性在于，每层输入的分布在训练过程中会发生变化，因为前面的层的参数会发生变化。通过要求较低的学习率和仔细的参数初始化减慢了训练，并且使具有饱和非线性的模型训练起来非常困难。我们将这种现象称为<em>内部协变量转移</em>，并通过归一化层输入来解决这个问题。我们的方法力图使归一化成为模型架构的一部分，并为<em>每个训练小批量数据</em>执行归一化。批量归一化使我们能够使用更高的学习率，并且不用太注意初始化。它也作为一个正则化项，在某些情况下不需要Dropout。将批量归一化应用到最先进的图像分类模型上，批量归一化在取得相同的精度的情况下，减少了14倍的训练步骤，并以显著的差距击败了原始模型。使用批量归一化网络的组合，我们改进了在ImageNet分类上公布的最佳结果：达到了<code>4.9％ top-5</code>的验证误差（和<code>4.8％</code>测试误差），超过了人类评估者的准确性。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>深度学习在视觉、语音等诸多方面显著提高了现有技术的水平。随机梯度下降（SGD）已经被证明是训练深度网络的有效方式，并且已经使用诸如动量（Sutskever等，2013）和Adagrad（Duchi等人，2011）等SGD变种取得了最先进的性能。SGD优化网络参数$\Theta$，以最小化损失</p>
<p>$$\Theta = \arg \min_\Theta \frac{1}{N}\sum_{i=1}^N \ell(x_i, \Theta)$$</p>
<p>$x_{1\ldots N}$是训练数据集。使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为$m$的<em>小批量数据</em>$x_{1 \ldots m}$。通过计算$\frac {1} {m} \sum _{i=1} ^m \frac {\partial \ell(x_i, \Theta)} {\partial \Theta}$，使用小批量数据来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而改善。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算$m$次效率更高。</p>
<p>虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。</p>
<p>层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。当学习系统的输入分布发生变化时，据说会经历<em>协变量转移</em>（Shimodaira，2000）。这通常是通过域适应（Jiang，2008）来处理的。然而，协变量漂移的概念可以扩展到整个学习系统之外，应用到学习系统的一部分，例如子网络或一层。考虑网络计算$$\ell = F_2(F_1(u, \Theta_1), \Theta_2)$$ $F_1$和$F_2$是任意变换，学习参数$\Theta_1，\Theta_2$以便最小化损失$\ell$。学习$\Theta_2$可以看作输入$x=F_1(u,\Theta_1)$送入到子网络$$\ell = F_2(x, \Theta_2)。$$</p>
<p>例如，梯度下降步骤$$\Theta_2\leftarrow \Theta_2 - \frac {\alpha} {m} \sum_{i=1}^m \frac {\partial F_2(x_i,\Theta_2)} {\partial \Theta_2}$$（对于批大小$m$和学习率$\alpha$）与输入为$x$的单独网络$F_2$完全等价。因此，输入分布特性使训练更有效——例如训练数据和测试数据之间有相同的分布——也适用于训练子网络。因此$x$的分布在时间上保持固定是有利的。然后，$\Theta_2$不必重新调整来补偿$x$分布的变化。</p>
<p>子网络输入的固定分布对于子网络外的层也有积极的影响。考虑一个激活函数为$g(x) = \frac{1}{1+\exp(-x)}$的层，$u$是层输入，权重矩阵$W$和偏置向量$b$是要学习的层参数，$g(x) = \frac{1}{1+\exp(-x)}$。随着$|x|$的增加，$g’(x)$趋向于0。这意味着对于$x=Wu+b$的所有维度，除了那些具有小的绝对值之外，流向$u$的梯度将会消失，模型将缓慢的进行训练。然而，由于$x$受$W,b$和下面所有层的参数的影响，训练期间那些参数的改变可能会将$x$的许多维度移动到非线性的饱和状态并减慢收敛。这个影响随着网络深度的增加而放大。在实践中，饱和问题和由此产生的梯度消失通常通过使用修正线性单元(Nair &amp; Hinton, 2010) $ReLU(x)=\max(x,0)$，仔细的初始化(Bengio &amp; Glorot, 2010; Saxe et al., 2013)和小的学习率来解决。然而，如果我们能保证非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，训练将加速。</p>
<p>我们把训练过程中深度网络内部结点的分布变化称为<em>内部协变量转移</em>。消除它可以保证更快的训练。我们提出了一种新的机制，我们称为为<em>批规范化</em>，它是减少内部协变量转移的一个步骤，这样做可以显著加速深度神经网络的训练。它通过规范化步骤来实现，规范化步骤修正了层输入的均值和方差。批规范化减少了梯度对参数或它们的初始值尺度上的依赖，对通过网络的梯度流动有有益的影响。这允许我们使用更高的学习率而没有发散的风险。此外，批规范化使模型正则化并减少了对Dropout(Srivastava et al., 2014)的需求。最后，批规范化通过阻止网络陷入饱和模式让使用饱和非线性成为可能。</p>
<p>在4.2小节，我们将批规范化应用到性能最好的ImageNet分类网络上，并且表明我们可以使用仅7％的训练步骤来匹配其性能，并且可以进一步超过其准确性一大截。通过使用批规范化训练的网络的集合，我们取得了top-5错误率，其改进了ImageNet分类上已知的最佳结果。</p>
<h2 id="2-减少内部协变量转变"><a href="#2-减少内部协变量转变" class="headerlink" title="2. 减少内部协变量转变"></a>2. 减少内部协变量转变</h2><p>由于训练过程中网络参数的变化，我们将<em>内部协变量转移</em>定义为网络激活分布的变化。为了改善训练，我们寻求减少内部协变量转移。随着训练的进行，通过固定层输入$x$的分布，我们期望提高训练速度。众所周知(LeCun et al., 1998b; Wiesler &amp; Ney, 2011)如果对网络的输入进行白化，网络训练将会收敛的更快——即输入线性变换为具有零均值和单位方差，并去相关。当每一层观察下面的层产生的输入时，实现每一层输入进行相同的白化将是有利的。通过白化每一层的输入，我们将采取措施实现输入的固定分布，消除内部协变量转移的不良影响。</p>
<p>我们考虑在每个训练步骤或在某些间隔来白化激活值，通过直接修改网络或根据网络激活值来更改优化方法的参数(Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins &amp; Kavukcuoglu)。然而，如果这些修改分散在优化步骤中，那么梯度下降步骤可能会试图以要求规范化进行更新的方式来更新参数，这会降低梯度下降步骤的影响。例如，考虑一个层，其输入$u$加上学习到的偏置$b$，通过减去在训练集上计算的激活值的均值对结果进行归一化：$\hat x=x - E[x]$，$x = u+b$, $X={x_{1\ldots N}}$是训练集上$x$值的集合，$E[x] = \frac{1}{N}\sum_{i=1}^N x_i$。如果梯度下降步骤忽略了$E[x]$对$b$的依赖，那它将更新$b\leftarrow b+\Delta b$，其中$\Delta b\propto -\partial{\ell}/\partial{\hat x}$。然后$u+(b+\Delta b) -E[u+(b+\Delta b)] = u+b-E[u+b]$。因此，结合$b$的更新和接下来标准化中的改变会导致层的输出没有变化，从而导致损失没有变化。随着训练的继续，$b$将无限增长而损失保持不变。如果标准化不仅中心化而且缩放了激活值，问题会变得更糟糕。我们在最初的实验中已经观察到了这一点，当标准化参数在梯度下降步骤之外计算时，模型会爆炸。</p>
<p>上述方法的问题是梯度下降优化没有考虑到标准化中发生的事实。为了解决这个问题，我们希望确保对于任何参数值，网络<em>总是</em>产生具有所需分布的激活值。这样做将允许关于模型参数损失的梯度来解释标准化，以及它对模型参数$\Theta$的依赖。设$x$为层的输入，将其看作向量，$\cal X$是这些输入在训练集上的集合。标准化可以写为变换$$\hat x=Norm(x, \cal X)$$它不仅依赖于给定的训练样本$x$而且依赖于所有样本$\cal X$——它们中的每一个都依赖于$\Theta$，如果$x$是由另一层生成的。对于反向传播，我们将需要计算Jacobians$\frac {\partial Norm(x,\cal X)} {\partial x}$和$\frac {\partial Norm(x,\cal X)} {\partial \cal X}$；忽略后一项会导致上面描述的爆炸。在这个框架中，白化层输入是昂贵的，因为它要求计算协方差矩阵$Cov[x]=E_{x\in \cal X}[x x^T]- E[x]E[x]^T$和它的平方根倒数，从而生成白化的激活$Cov[x]^{-1/2}(x-E[x])$和这些变换进行反向传播的偏导数。这促使我们寻求一种替代方案，以可微分的方式执行输入标准化，并且在每次参数更新后不需要对整个训练集进行分析。</p>
<p>以前的一些方法（例如（Lyu＆Simoncelli，2008））使用通过单个训练样本计算的统计信息，或者在图像网络的情况下，使用给定位置处不同特征图上的统计。然而，通过丢弃激活值绝对尺度改变了网络的表示能力。我们希望通过对相对于整个训练数据统计信息的单个训练样本的激活值进行归一化来保留网络中的信息。</p>
<h2 id="3-通过Mini-Batch统计进行标准化"><a href="#3-通过Mini-Batch统计进行标准化" class="headerlink" title="3. 通过Mini-Batch统计进行标准化"></a>3. 通过Mini-Batch统计进行标准化</h2><p>由于每一层输入的整个白化是代价昂贵的并且不是到处可微分的，因此我们做了两个必要的简化。首先是我们将单独标准化每个标量特征，从而代替在层输入输出对特征进行共同白化，使其具有零均值和单位方差。对于具有$d$维输入$x = (x^{(1)}\ldots x^{(d)})$的层，我们将标准化每一维$$\hat x^{(k)} = \frac{x^{(k)} - E[x^{(k)}]} {\sqrt {Var[x^{(k)}]}}$$其中期望和方差在整个训练数据集上计算。如(LeCun et al., 1998b)中所示，这种标准化加速了收敛，即使特征没有去相关。</p>
]]></content>
    
    <summary type="html">
    
      Batch Normalization论文翻译——中文版
    
    </summary>
    
      <category term="深度学习" scheme="noahsnail.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="noahsnail.com/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
