<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Deep Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Faster R-CNN论文翻译——中文版">
<meta property="og:type" content="article">
<meta property="og:title" content="Faster R-CNN论文翻译——中文版">
<meta property="og:url" content="http://noahsnail.com/2018/01/03/2018-01-03-Faster R-CNN论文翻译——中文版/index.html">
<meta property="og:site_name" content="SnailTyan">
<meta property="og:description" content="Faster R-CNN论文翻译——中文版">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Figure_1.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Figure_2.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Figure_3.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_8.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_9.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_1.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_2.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_5.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_3.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_4.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_6.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_7.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Figure_5.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Figure_4.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_10.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_11.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Figure_6.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_6.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_12.png">
<meta property="og:image" content="http://noahsnail.com/images/faster_rcnn/Table_7.png">
<meta property="og:updated_time" content="2018-01-03T03:48:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Faster R-CNN论文翻译——中文版">
<meta name="twitter:description" content="Faster R-CNN论文翻译——中文版">
<meta name="twitter:image" content="http://noahsnail.com/images/faster_rcnn/Figure_1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 'undefined',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://noahsnail.com/2018/01/03/2018-01-03-Faster R-CNN论文翻译——中文版/"/>





  <title>Faster R-CNN论文翻译——中文版 | SnailTyan</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-83591315-1', 'auto');
  ga('send', 'pageview');
</script>











</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SnailTyan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://noahsnail.com/2018/01/03/2018-01-03-Faster R-CNN论文翻译——中文版/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Tyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SnailTyan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Faster R-CNN论文翻译——中文版</h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/01/03/2018-01-03-Faster R-CNN论文翻译——中文版/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/01/03/2018-01-03-Faster R-CNN论文翻译——中文版/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          
              <div class="post-description">
                  Faster R-CNN论文翻译——中文版
              </div>
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p>
<p>翻译论文汇总：<a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="external">https://github.com/SnailTyan/deep-learning-papers-translation</a></p>
<h1 id="Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><a href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks" class="headerlink" title="Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"></a>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最先进的目标检测网络依靠区域提出算法来假设目标的位置。SPPnet[1]和Fast R-CNN[2]等研究已经减少了这些检测网络的运行时间，使得区域提出计算成为一个瓶颈。在这项工作中，我们引入了一个区域提出网络（RPN），该网络与检测网络共享全图像的卷积特征，从而使近乎零成本的区域提出成为可能。RPN是一个全卷积网络，可以同时在每个位置预测目标边界和目标分数。RPN经过端到端的训练，可以生成高质量的区域提出，由Fast R-CNN用于检测。我们将RPN和Fast R-CNN通过共享卷积特征进一步合并为一个单一的网络——使用最近流行的具有“注意力”机制的神经网络术语，RPN组件告诉统一网络在哪里寻找。对于非常深的VGG-16模型[3]，我们的检测系统在GPU上的帧率为5fps（包括所有步骤），同时在PASCAL VOC 2007，2012和MS COCO数据集上实现了最新的目标检测精度，每个图像只有300个提出。在ILSVRC和COCO 2015竞赛中，Faster R-CNN和RPN是多个比赛中获得第一名输入的基础。代码可公开获得。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>目标检测的最新进展是由区域提出方法（例如[4]）和基于区域的卷积神经网络（R-CNN）[5]的成功驱动的。尽管在[5]中最初开发的基于区域的CNN计算成本很高，但是由于在各种提议中共享卷积，所以其成本已经大大降低了[1][2]。<em>忽略花费在区域提议上的时间</em>，最新版本Fast R-CNN[2]利用非常深的网络[3]实现了接近实时的速率。现在，提议是最新的检测系统中测试时间的计算瓶颈。</p>
<p>区域提议方法通常依赖廉价的特征和简练的推断方案。选择性搜索[4]是最流行的方法之一，它贪婪地合并基于设计的低级特征的超级像素。然而，与有效的检测网络[2]相比，选择性搜索速度慢了一个数量级，在CPU实现中每张图像的时间为2秒。EdgeBoxes[6]目前提供了在提议质量和速度之间的最佳权衡，每张图像0.2秒。尽管如此，区域提议步骤仍然像检测网络那样消耗同样多的运行时间。</p>
<p>有人可能会注意到，基于区域的快速CNN利用GPU，而在研究中使用的区域提议方法在CPU上实现，使得运行时间比较不公平。加速提议计算的一个显而易见的方法是将其在GPU上重新实现。这可能是一个有效的工程解决方案，但重新实现忽略了下游检测网络，因此错过了共享计算的重要机会。</p>
<p>在本文中，我们展示了算法的变化——用深度卷积神经网络计算区域提议——导致了一个优雅和有效的解决方案，其中在给定检测网络计算的情况下区域提议计算接近领成本。为此，我们引入了新的<em>区域提议网络</em>（RPN），它们共享最先进目标检测网络的卷积层[1]，[2]。通过在测试时共享卷积，计算区域提议的边际成本很小（例如，每张图像10ms）。</p>
<p>我们的观察是，基于区域的检测器所使用的卷积特征映射，如Fast R-CNN，也可以用于生成区域提议。在这些卷积特征之上，我们通过添加一些额外的卷积层来构建RPN，这些卷积层同时在规则网格上的每个位置上回归区域边界和目标分数。因此RPN是一种全卷积网络（FCN）[7]，可以针对生成检测区域建议的任务进行端到端的训练。</p>
<p>RPN旨在有效预测具有广泛尺度和长宽比的区域提议。与使用图像金字塔（图1，a）或滤波器金字塔（图1，b）的流行方法[8]，[9]，[1]相比，我们引入新的“锚”盒作为多种尺度和长宽比的参考。我们的方案可以被认为是回归参考金字塔（图1，c），它避免了枚举多种比例或长宽比的图像或滤波器。这个模型在使用单尺度图像进行训练和测试时运行良好，从而有利于运行速度。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Figure_1.png" alt="Figure 1"></p>
<p>图1：解决多尺度和尺寸的不同方案。（a）构建图像和特征映射金字塔，分类器以各种尺度运行。（b）在特征映射上运行具有多个比例/大小的滤波器的金字塔。（c）我们在回归函数中使用参考边界框金字塔。</p>
<p>为了将RPN与Fast R-CNN 2]目标检测网络相结合，我们提出了一种训练方案，在微调区域提议任务和微调目标检测之间进行交替，同时保持区域提议的固定。该方案快速收敛，并产生两个任务之间共享的具有卷积特征的统一网络。</p>
<p>我们在PASCAL VOC检测基准数据集上[11]综合评估了我们的方法，其中具有Fast R-CNN的RPN产生的检测精度优于使用选择性搜索的Fast R-CNN的强基准。同时，我们的方法在测试时几乎免除了选择性搜索的所有计算负担——区域提议的有效运行时间仅为10毫秒。使用[3]的昂贵的非常深的模型，我们的检测方法在GPU上仍然具有5fps的帧率（包括所有步骤），因此在速度和准确性方面是实用的目标检测系统。我们还报告了在MS COCO数据集上[12]的结果，并使用COCO数据研究了在PASCAL VOC上的改进。代码可公开获得<a href="https://github.com/shaoqingren/faster_rcnn" target="_blank" rel="external">https://github.com/shaoqingren/faster_rcnn</a>（在MATLAB中）和<a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">https://github.com/rbgirshick/py-faster-rcnn</a>（在Python中）。</p>
<p>这个手稿的初步版本是以前发表的[10]。从那时起，RPN和Faster R-CNN的框架已经被采用并推广到其他方法，如3D目标检测[13]，基于部件的检测[14]，实例分割[15]和图像标题[16]。我们快速和有效的目标检测系统也已经在Pinterest[17]的商业系统中建立了，并报告了用户参与度的提高。</p>
<p>在ILSVRC和COCO 2015竞赛中，Faster R-CNN和RPN是ImageNet检测，ImageNet定位，COCO检测和COCO分割中几个第一名参赛者[18]的基础。RPN完全从数据中学习提议区域，因此可以从更深入和更具表达性的特征（例如[18]中采用的101层残差网络）中轻松获益。Faster R-CNN和RPN也被这些比赛中的其他几个主要参赛者所使用。这些结果表明，我们的方法不仅是一个实用合算的解决方案，而且是一个提高目标检测精度的有效方法。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><p><strong>目标提议</strong>。目标提议方法方面有大量的文献。目标提议方法的综合调查和比较可以在[19]，[20]，[21]中找到。广泛使用的目标提议方法包括基于超像素分组（例如，选择性搜索[4]，CPMC[22]，MCG[23]）和那些基于滑动窗口的方法（例如窗口中的目标[24]，EdgeBoxes[6]）。目标提议方法被采用为独立于检测器（例如，选择性搜索[4]目标检测器，R-CNN[5]和Fast R-CNN[2]）的外部模块。</p>
<p>用于目标检测的深度网络。R-CNN方法[5]端到端地对CNN进行训练，将提议区域分类为目标类别或背景。R-CNN主要作为分类器，并不能预测目标边界（除了通过边界框回归进行细化）。其准确度取决于区域提议模块的性能（参见[20]中的比较）。一些论文提出了使用深度网络来预测目标边界框的方法[25]，[9]，[26]，[27]。在OverFeat方法[9]中，训练一个全连接层来预测假定单个目标定位任务的边界框坐标。然后将全连接层变成卷积层，用于检测多个类别的目标。MultiBox方法[26]，[27]从网络中生成区域提议，网络最后的全连接层同时预测多个类别不相关的边界框，并推广到OverFeat的“单边界框”方式。这些类别不可知的边界框框被用作R-CNN的提议区域[5]。与我们的全卷积方案相比，MultiBox提议网络适用于单张裁剪图像或多张大型裁剪图像（例如224×224）。MultiBox在提议区域和检测网络之间不共享特征。稍后在我们的方法上下文中会讨论OverFeat和MultiBox。与我们的工作同时进行的，DeepMask方法[28]是为学习分割提议区域而开发的。</p>
<p>卷积[9]，[1]，[29]，[7]，[2]的共享计算已经越来越受到人们的关注，因为它可以有效而准确地进行视觉识别。OverFeat论文[9]计算图像金字塔的卷积特征用于分类，定位和检测。共享卷积特征映射的自适应大小池化（SPP）[1]被开发用于有效的基于区域的目标检测[1]，[30]和语义分割[29]。Fast R-CNN[2]能够对共享卷积特征进行端到端的检测器训练，并显示出令人信服的准确性和速度。</p>
<h2 id="3-FASTER-R-CNN"><a href="#3-FASTER-R-CNN" class="headerlink" title="3. FASTER R-CNN"></a>3. FASTER R-CNN</h2><p>我们的目标检测系统，称为Faster R-CNN，由两个模块组成。第一个模块是提议区域的深度全卷积网络，第二个模块是使用提议区域的Fast R-CNN检测器[2]。整个系统是一个单个的，统一的目标检测网络（图2）。使用最近流行的“注意力”[31]机制的神经网络术语，RPN模块告诉Fast R-CNN模块在哪里寻找。在第3.1节中，我们介绍了区域提议网络的设计和属性。在第3.2节中，我们开发了用于训练具有共享特征模块的算法。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Figure_2.png" alt="Figure 2"></p>
<p>图2：Faster R-CNN是一个单一，统一的目标检测网络。RPN模块作为这个统一网络的“注意力”。</p>
<h3 id="3-1-区域提议网络"><a href="#3-1-区域提议网络" class="headerlink" title="3.1 区域提议网络"></a>3.1 区域提议网络</h3><p>区域提议网络（RPN）以任意大小的图像作为输入，输出一组矩形的目标提议，每个提议都有一个目标得分。我们用全卷积网络[7]对这个过程进行建模，我们将在本节进行描述。因为我们的最终目标是与Fast R-CNN目标检测网络[2]共享计算，所以我们假设两个网络共享一组共同的卷积层。在我们的实验中，我们研究了具有5个共享卷积层的Zeiler和Fergus模型[32]（ZF）和具有13个共享卷积层的Simonyan和Zisserman模型[3]（VGG-16）。</p>
<p>为了生成区域提议，我们在最后的共享卷积层输出的卷积特征映射上滑动一个小网络。这个小网络将输入卷积特征映射的$n×n$空间窗口作为输入。每个滑动窗口映射到一个低维特征（ZF为256维，VGG为512维，后面是ReLU[33]）。这个特征被输入到两个子全连接层——一个边界框回归层（reg）和一个边界框分类层（cls）。在本文中，我们使用$n=3$，注意输入图像上的有效感受野是大的（ZF和VGG分别为171和228个像素）。图3（左）显示了这个小型网络的一个位置。请注意，因为小网络以滑动窗口方式运行，所有空间位置共享全连接层。这种架构通过一个n×n卷积层，后面是两个子1×1卷积层（分别用于reg和cls）自然地实现。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Figure_3.png" alt="Figure 3"></p>
<p>图3：左：区域提议网络（RPN）。右：在PASCAL VOC 2007测试集上使用RPN提议的示例检测。我们的方法可以检测各种尺度和长宽比的目标。</p>
<h4 id="3-1-1-锚点"><a href="#3-1-1-锚点" class="headerlink" title="3.1.1 锚点"></a>3.1.1 锚点</h4><p>在每个滑动窗口位置，我们同时预测多个区域提议，其中每个位置可能提议的最大数目表示为$k$。因此，<em>reg</em>层具有$4k$个输出，编码$k$个边界框的坐标，<em>cls</em>层输出$2k$个分数，估计每个提议是目标或不是目标的概率。相对于我们称之为锚点的$k$个参考边界框，$k$个提议是参数化的。锚点位于所讨论的滑动窗口的中心，并与一个尺度和长宽比相关（图3左）。默认情况下，我们使用3个尺度和3个长宽比，在每个滑动位置产生$k=9$个锚点。对于大小为W×H（通常约为2400）的卷积特征映射，总共有$WHk$个锚点。</p>
<p><strong>平移不变的锚点</strong></p>
<p>我们的方法的一个重要特性是它是<em>平移不变</em>的，无论是在锚点还是计算相对于锚点的区域提议的函数。如果在图像中平移目标，提议应该平移，并且同样的函数应该能够在任一位置预测提议。这个平移不变的属性是由我们的方法保证的。作为比较，MultiBox方法[27]使用k-means生成800个锚点，这不是平移不变的。所以如果平移目标，MultiBox不保证会生成相同的提议。</p>
<p>平移不变特性也减小了模型的大小。MultiBox有$(4+1)\times 800$维的全连接输出层，而我们的方法在$k=9$个锚点的情况下有$(4+2)\times 9$维的卷积输出层。因此，对于VGG-16，我们的输出层具有$2.8\times10^4$个参数（对于VGG-16为$512\times(4+2)\times9$），比MultiBox输出层的$6.1\times10^6$个参数少了两个数量级（对于MultiBox [27]中的GoogleNet[34]为$1536\times(4+1)\times800$）。如果考虑到特征投影层，我们的提议层仍然比MultiBox少一个数量级。我们期望我们的方法在PASCAL VOC等小数据集上有更小的过拟合风险。</p>
<p><strong>多尺度锚点作为回归参考</strong></p>
<p>我们的锚点设计提出了一个新的方案来解决多尺度（和长宽比）。如图1所示，多尺度预测有两种流行的方法。第一种方法是基于图像/特征金字塔，例如DPM[8]和基于CNN的方法[9]，[1]，[2]中。图像在多个尺度上进行缩放，并且针对每个尺度（图1（a））计算特征映射（HOG[8]或深卷积特征[9]，[1]，[2]）。这种方法通常是有用的，但是非常耗时。第二种方法是在特征映射上使用多尺度（和/或长宽比）的滑动窗口。例如，在DPM[8]中，使用不同的滤波器大小（例如5×7和7×5）分别对不同长宽比的模型进行训练。如果用这种方法来解决多尺度问题，可以把它看作是一个“滤波器金字塔”（图1（b））。第二种方法通常与第一种方法联合采用[8]。</p>
<p>作为比较，我们的基于锚点方法建立在锚点金字塔上，这是更具成本效益的。我们的方法参照多尺度和长宽比的锚盒来分类和回归边界框。它只依赖单一尺度的图像和特征映射，并使用单一尺寸的滤波器（特征映射上的滑动窗口）。我们通过实验来展示这个方案解决多尺度和尺寸的效果（表8）。</p>
<p>表8：Faster R-CNN在PAS-CAL VOC 2007测试数据集上使用不同锚点设置的检测结果。网络是VGG-16。训练数据是VOC 2007训练集。使用3个尺度和3个长宽比（$69.9\%$）的默认设置，与表3中的相同。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_8.png" alt="Table 8"></p>
<p>由于这种基于锚点的多尺度设计，我们可以简单地使用在单尺度图像上计算的卷积特征，Fast R-CNN检测器也是这样做的[2]。多尺度锚点设计是共享特征的关键组件，不需要额外的成本来处理尺度。</p>
<h4 id="3-1-2-损失函数"><a href="#3-1-2-损失函数" class="headerlink" title="3.1.2 损失函数"></a>3.1.2 损失函数</h4><p>为了训练RPN，我们为每个锚点分配一个二值类别标签（是目标或不是目标）。我们给两种锚点分配一个正标签：（i）具有与实际边界框的重叠最高交并比（IoU）的锚点，或者（ii）具有与实际边界框的重叠超过0.7 IoU的锚点。注意，单个真实边界框可以为多个锚点分配正标签。通常第二个条件足以确定正样本；但我们仍然采用第一个条件，因为在一些极少数情况下，第二个条件可能找不到正样本。对于所有的真实边界框，如果一个锚点的IoU比率低于0.3，我们给非正面的锚点分配一个负标签。既不正面也不负面的锚点不会有助于训练目标函数。</p>
<p>根据这些定义，我们对目标函数Fast R-CNN[2]中的多任务损失进行最小化。我们对图像的损失函数定义为：$$<br>L(\lbrace p_i \rbrace, \lbrace t_i \rbrace) = \frac{1}{N_{cls}}\sum_i L_{cls}(p_i, p^{*}_i) \\ + \lambda\frac{1}{N_{reg}}\sum_i p^{*}_i L_{reg}(t_i, t^{*}_i).<br>$$其中，$i$是一个小批量数据中锚点的索引，$p_i$是锚点$i$作为目标的预测概率。如果锚点为正，真实标签$p^{*}_i$为1，如果锚点为负，则为0。$t_i$是表示预测边界框4个参数化坐标的向量，而$t^{*}_i$是与正锚点相关的真实边界框的向量。分类损失$L_{cls}$是两个类别上（目标或不是目标）的对数损失。对于回归损失，我们使用$L_{reg}(t_i, t^{*}_i)=R(t_i - t^{*}_i)$，其中$R$是在[2]中定义的鲁棒损失函数（平滑$L_1$）。项$p^{*}_i L_{reg}$表示回归损失仅对于正锚点激活，否则被禁用（$p^{*}_i=0$）。<em>cls</em>和<em>reg</em>层的输出分别由${p_i}$和${t_i}$组成。</p>
<p>这两个项用$N_{cls}$和$N_{reg}$进行标准化，并由一个平衡参数$\lambda$加权。在我们目前的实现中（如在发布的代码中），方程（1）中的$cls$项通过小批量数据的大小（即$N_{cls}=256$）进行归一化，$reg$项根据锚点位置的数量（即，$N_{reg}\sim 24000$）进行归一化。默认情况下，我们设置$\lambda=10$，因此<em>cls</em>和<em>reg</em>项的权重大致相等。我们通过实验显示，结果对宽范围的$\lambda$值不敏感(表9)。我们还注意到，上面的归一化不是必需的，可以简化。</p>
<p>表9：Faster R-CNN使用方程(1)中不同的$\lambda$值在PASCAL VOC 2007测试集上的检测结果。网络是VGG-16。训练数据是VOC 2007训练集。使用$\lambda = 10$（$69.9\%$）的默认设置与表3中的相同。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_9.png" alt="Table 9"></p>
<p>对于边界框回归，我们采用[5]中的4个坐标参数化：$$<br>t_{\textrm{x}} =  (x - x_{\textrm{a}})/w_{\textrm{a}},\quad<br>t_{\textrm{y}} = (y - y_{\textrm{a}})/h_{\textrm{a}},\\<br>t_{\textrm{w}} = \log(w / w_{\textrm{a}}), \quad<br>t_{\textrm{h}} = \log(h / h_{\textrm{a}}),\\<br>t^{*}_{\textrm{x}} =  (x^{*} - x_{\textrm{a}})/w_{\textrm{a}},\quad<br>t^{*}_{\textrm{y}} = (y^{*} - y_{\textrm{a}})/h_{\textrm{a}},\\<br>t^{*}_{\textrm{w}} = \log(w^{*} / w_{\textrm{a}}),\quad<br>t^{*}_{\textrm{h}} = \log(h^{*} / h_{\textrm{a}}),<br>$$ 其中，$x$，$y$，$w$和$h$表示边界框的中心坐标及其宽和高。变量$x$，$x_{\textrm{a}}$和$x^{*}$分别表示预测边界框，锚盒和实际边界框（类似于$y, w, h$）。这可以被认为是从锚盒到邻近的实际边界框的回归。</p>
<p>然而，我们的方法通过与之前的基于RoI（感兴趣区域）方法[1]，[2]不同的方式来实现边界框回归。在[1]，[2]中，对任意大小的RoI池化的特征执行边界框回归，并且回归权重由所有区域大小共享。在我们的公式中，用于回归的特征在特征映射上具有相同的空间大小（3×3）。为了说明不同的大小，学习一组$k$个边界框回归器。每个回归器负责一个尺度和一个长宽比，而$k$个回归器不共享权重。因此，由于锚点的设计，即使特征具有固定的尺度/比例，仍然可以预测各种尺寸的边界框。</p>
<h4 id="3-1-3-训练RPN"><a href="#3-1-3-训练RPN" class="headerlink" title="3.1.3 训练RPN"></a>3.1.3 训练RPN</h4><p>RPN可以通过反向传播和随机梯度下降（SGD）进行端对端训练[35]。我们遵循[2]的“以图像为中心”的采样策略来训练这个网络。每个小批量数据都从包含许多正面和负面示例锚点的单张图像中产生。对所有锚点的损失函数进行优化是可能的，但是这样会偏向于负样本，因为它们是占主导地位的。取而代之的是，我们在图像中随机采样256个锚点，计算一个小批量数据的损失函数，其中采样的正锚点和负锚点的比率可达1:1。如果图像中的正样本少于128个，我们使用负样本填充小批量数据。</p>
<p>我们通过从标准方差为0.01的零均值高斯分布中提取权重来随机初始化所有新层。所有其他层（即共享卷积层）通过预训练的ImageNet分类模型[36]来初始化，如同标准实践[5]。我们调整ZF网络的所有层，以及VGG网络的conv3_1及其之上的层以节省内存[2]。对于60k的小批量数据，我们使用0.001的学习率，对于PASCAL VOC数据集中的下一个20k小批量数据，使用0.0001。我们使用0.9的动量和0.0005的重量衰减[37]。我们的实现使用Caffe[38]。</p>
<h3 id="3-2-RPN和Fast-R-CNN共享特征"><a href="#3-2-RPN和Fast-R-CNN共享特征" class="headerlink" title="3.2 RPN和Fast R-CNN共享特征"></a>3.2 RPN和Fast R-CNN共享特征</h3><p>到目前为止，我们已经描述了如何训练用于区域提议生成的网络，没有考虑将利用这些提议的基于区域的目标检测CNN。对于检测网络，我们采用Fast R-CNN[2]。接下来我们介绍一些算法，学习由RPN和Fast R-CNN组成的具有共享卷积层的统一网络（图2）。</p>
<p>独立训练的RPN和Fast R-CNN将以不同的方式修改卷积层。因此，我们需要开发一种允许在两个网络之间共享卷积层的技术，而不是学习两个独立的网络。我们讨论三个方法来训练具有共享特征的网络：</p>
<p>（一）交替训练。在这个解决方案中，我们首先训练RPN，并使用这些提议来训练Fast R-CNN。由Fast R-CNN微调的网络然后被用于初始化RPN，并且重复这个过程。这是本文所有实验中使用的解决方案。</p>
<p>（二）近似联合训练。在这个解决方案中，RPN和Fast R-CNN网络在训练期间合并成一个网络，如图2所示。在每次SGD迭代中，前向传递生成区域提议，在训练Fast R-CNN检测器将这看作是固定的、预计算的提议。反向传播像往常一样进行，其中对于共享层，组合来自RPN损失和Fast R-CNN损失的反向传播信号。这个解决方案很容易实现。但是这个解决方案忽略了关于提议边界框的坐标（也是网络响应）的导数，因此是近似的。在我们的实验中，我们实验发现这个求解器产生了相当的结果，与交替训练相比，训练时间减少了大约$25-50\%$。这个求解器包含在我们发布的Python代码中。</p>
<p>（三）非近似的联合训练。如上所述，由RPN预测的边界框也是输入的函数。Fast R-CNN中的RoI池化层[2]接受卷积特征以及预测的边界框作为输入，所以理论上有效的反向传播求解器也应该包括关于边界框坐标的梯度。在上述近似联合训练中，这些梯度被忽略。在一个非近似的联合训练解决方案中，我们需要一个关于边界框坐标可微分的RoI池化层。这是一个重要的问题，可以通过[15]中提出的“RoI扭曲”层给出解决方案，这超出了本文的范围。</p>
<p>四步交替训练。在本文中，我们采用实用的四步训练算法，通过交替优化学习共享特征。在第一步中，我们按照3.1.3节的描述训练RPN。该网络使用ImageNet的预训练模型进行初始化，并针对区域提议任务进行了端到端的微调。在第二步中，我们使用由第一步RPN生成的提议，由Fast R-CNN训练单独的检测网络。该检测网络也由ImageNet的预训练模型进行初始化。此时两个网络不共享卷积层。在第三步中，我们使用检测器网络来初始化RPN训练，但是我们修正共享的卷积层，并且只对RPN特有的层进行微调。现在这两个网络共享卷积层。最后，保持共享卷积层的固定，我们对Fast R-CNN的独有层进行微调。因此，两个网络共享相同的卷积层并形成统一的网络。类似的交替训练可以运行更多的迭代，但是我们只观察到可以忽略的改进。</p>
<h3 id="3-3-实现细节"><a href="#3-3-实现细节" class="headerlink" title="3.3 实现细节"></a>3.3 实现细节</h3><p>我们在单尺度图像上训练和测试区域提议和目标检测网络[1]，[2]。我们重新缩放图像，使得它们的短边是$s=600$像素[2]。多尺度特征提取（使用图像金字塔）可能会提高精度，但不会表现出速度与精度的良好折衷[2]。在重新缩放的图像上，最后卷积层上的ZF和VGG网络的总步长为16个像素，因此在调整大小（〜500×375）之前，典型的PASCAL图像上的总步长为〜10个像素。即使如此大的步长也能提供良好的效果，尽管步幅更小，精度可能会进一步提高。</p>
<p>对于锚点，我们使用了3个尺度，边界框面积分别为$128^2$，$256^2$和$512^2$个像素，以及1:1，1:2和2:1的长宽比。这些超参数不是针对特定数据集仔细选择的，我们将在下一节中提供有关其作用的消融实验。如上所述，我们的解决方案不需要图像金字塔或滤波器金字塔来预测多个尺度的区域，节省了大量的运行时间。图3（右）显示了我们的方法在广泛的尺度和长宽比方面的能力。表1显示了使用ZF网络的每个锚点学习到的平均提议大小。我们注意到，我们的算法允许预测比基础感受野更大。这样的预测不是不可能的——如果只有目标的中间部分是可见的，那么仍然可以粗略地推断出目标的范围。</p>
<p>表1：使用ZF网络的每个锚点学习到的平均提议大小（$s=600$的数字）。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_1.png" alt="Table 1"></p>
<p>跨越图像边界的锚盒需要小心处理。在训练过程中，我们忽略了所有的跨界锚点，所以不会造成损失。对于一个典型的$1000 \times 600$的图片，总共将会有大约20000（$\approx 60 \times 40 \times 9$）个锚点。跨界锚点被忽略，每张图像约有6000个锚点用于训练。如果跨界异常值在训练中不被忽略，则会在目标函数中引入大的，难以纠正的误差项，且训练不会收敛。但在测试过程中，我们仍然将全卷积RPN应用于整张图像。这可能会产生跨边界的提议边界框，我们剪切到图像边界。</p>
<p>一些RPN提议互相之间高度重叠。为了减少冗余，我们在提议区域根据他们的<em>cls</em>分数采取非极大值抑制（NMS）。我们将NMS的IoU阈值固定为0.7，这就给每张图像留下了大约2000个提议区域。正如我们将要展示的那样，NMS不会损害最终的检测准确性，但会大大减少提议的数量。在NMS之后，我们使用前N个提议区域来进行检测。接下来，我们使用2000个RPN提议对Fast R-CNN进行训练，但在测试时评估不同数量的提议。</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><h3 id="4-1-PASCAL-VOC上的实验"><a href="#4-1-PASCAL-VOC上的实验" class="headerlink" title="4.1 PASCAL VOC上的实验"></a>4.1 PASCAL VOC上的实验</h3><p>我们在PASCAL VOC 2007检测基准数据集[11]上全面评估了我们的方法。这个数据集包含大约5000张训练评估图像和在20个目标类别上的5000张测试图像。我们还提供了一些模型在PASCAL VOC 2012基准数据集上的测试结果。对于ImageNet预训练网络，我们使用具有5个卷积层和3个全连接层的ZF网络[32]的“快速”版本以及具有13个卷积层和3个全连接层的公开的VGG-16模型[3]。我们主要评估检测的平均精度均值（mAP），因为这是检测目标的实际指标（而不是关注目标提议代理度量）。</p>
<p>表2（顶部）显示了使用各种区域提议方法进行训练和测试的Fast R-CNN结果。这些结果使用ZF网络。对于选择性搜索（SS）[4]，我们通过“快速”模式生成约2000个提议。对于EdgeBoxes（EB）[6]，我们通过调整0.7 IoU的默认EB设置生成提议。SS在Fast R-CNN框架下的mAP为$58.7\%$，EB的mAP为$58.6\%$。RPN与Fast R-CNN取得了有竞争力的结果，使用多达300个提议，mAP为$59.9\%$。由于共享卷积计算，使用RPN比使用SS或EB产生了更快的检测系统；较少的建议也减少了区域方面的全连接层成本（表5）。</p>
<p>表2：PASCAL VOC 2007测试集上的检测结果（在VOC 2007训练评估集上进行了训练）。检测器是带有ZF的Fast R-CNN，但使用各种提议方法进行训练和测试。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_2.png" alt="Table 2"></p>
<p>表5：K40 GPU上的时间（ms），除了SS提议是在CPU上评估。“区域方面”包括NMS，池化，全连接和softmax层。查看我们发布的代码来分析运行时间。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_5.png" alt="Table 5"></p>
<p><strong>RPN上的消融实验</strong>。为了研究RPN作为提议方法的性能，我们进行了几项消融研究。首先，我们显示了RPN和Fast R-CNN检测网络共享卷积层的效果。为此，我们在四步训练过程的第二步之后停止训练。使用单独的网络将结果略微减少到$58.7\%$（RPN+ZF，非共享，表2）。我们观察到，这是因为在第三步中，当使用检测器调整的特征来微调RPN时，提议质量得到了改善。</p>
<p>接下来，我们分析RPN对训练Fast R-CNN检测网络的影响。为此，我们通过使用2000个SS提议和ZF网络来训练Fast R-CNN模型。我们固定这个检测器，并通过改变测试时使用的提议区域来评估检测的mAP。在这些消融实验中，RPN不与检测器共享特征。</p>
<p>在测试阶段用300个RPN提议替换SS提议得到了$56.8\%$的MAP。mAP的损失是因为训练/测试提议不一致。这个结果作为以下比较的基准。</p>
<p>有些令人惊讶的是，RPN在测试时使用排名最高的100个提议仍然会导致有竞争力的结果（$55.1\%$），表明排名靠前的RPN提议是准确的。相反的，使用排名靠前的6000个RPN提议（无NMS）具有相当的mAP（$55.2\%$），这表明NMS不会损害检测mAP并可能减少误报。</p>
<p>接下来，我们通过在测试时分别关闭RPN的<em>cls</em>和<em>reg</em>输出来调查RPN的作用。当<em>cls</em>层在测试时被移除（因此不使用NMS/排名），我们从未得分的区域中随机采样$N$个提议。当$N=1000$($55.8\<br>%$)时，mAP几乎没有变化，但是当$N=100$时，会大大降低到$44.6\%$。这表明<em>cls</em>分数考虑了排名最高的提议的准确性。</p>
<p>另一方面，当在测试阶段移除<em>reg</em>层（所以提议变成锚盒）时，mAP将下降到$52.1\%$。这表明高质量的提议主要是由于回归的边界框。锚盒虽然具有多个尺度和长宽比，但不足以进行准确的检测。</p>
<p>我们还单独评估了更强大的网络对RPN提议质量的影响。我们使用VGG-16来训练RPN，仍然使用上述的SS+ZF检测器。mAP从$56.8\%$（使用RPN+ZF）提高到$59.2\%$（使用RPN+VGG）。这是一个很有希望的结果，因为这表明RPN+VGG的提议质量要好于RPN+ZF。由于RPN+ZF的提议与SS具有竞争性（当一致用于训练和测试时，都是$58.7\%$），所以我们可以预期RPN+VGG比SS更好。以下实验验证了这个假设。</p>
<p><strong>VGG-16</strong>的性能。表3显示了VGG-16的提议和检测结果。使用RPN+VGG，非共享特征的结果是$68.5\%$，略高于SS的基准。如上所示，这是因为RPN+VGG生成的提议比SS更准确。与预先定义的SS不同，RPN是主动训练的并从更好的网络中受益。对于特性共享的变种，结果是$69.9\%$——比强壮的SS基准更好，但几乎是零成本的提议。我们在PASCAL VOC 2007和2012的训练评估数据集上进一步训练RPN和检测网络。该mAP是$73.2\%$。图5显示了PASCAL VOC 2007测试集的一些结果。在PASCAL VOC 2012测试集（表4）中，我们的方法在VOC 2007的<code>trainval+test</code>和VOC 2012的<code>trainval</code>的联合数据集上训练的模型取得了$70.4\%$的mAP。表6和表7显示了详细的数字。</p>
<p>表3：PASCAL VOC 2007测试集的检测结果。检测器是Fast R-CNN和VGG-16。训练数据：“07”：VOC 2007 trainval，“07 + 12”：VOC 2007 trainval和VOC 2012 trainval的联合训练集。对于RPN，训练时Fast R-CNN的提议数量为2000。†：[2]中报道的数字；使用本文提供的仓库，这个结果更高（68.1）。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_3.png" alt="Table 3"></p>
<p>表4：PASCAL VOC 2012测试集的检测结果。检测器是Fast R-CNN和VGG-16。训练数据：“07”：VOC 2007 trainval，“07 + 12”：VOC 2007 trainval和VOC 2012 trainval的联合训练集。对于RPN，训练时Fast R-CNN的提议数量为2000。†：<a href="http://host.robots.ox.ac.uk:8080/anonymous/HZJTQA.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/HZJTQA.html</a>。‡：<a href="http://host.robots.ox.ac.uk:8080/anonymous/YNPLXB.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/YNPLXB.html</a>。§：<a href="http://host.robots.ox.ac.uk:8080/anonymous/XEDH10.html" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/anonymous/XEDH10.html</a>。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_4.png" alt="Table 4"></p>
<p>表6：使用Fast R-CNN检测器和VGG-16在PASCAL VOC 2007测试集上的结果。对于RPN，训练时Fast R-CNN的提议数量为2000。${RPN}^*$表示没有共享特征的版本。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_6.png" alt="Table 6"></p>
<p>表7：使用Fast R-CNN检测器和VGG-16在PASCAL VOC 2012测试集上的结果。对于RPN，训练时Fast R-CNN的提议数量为2000。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_7.png" alt="Table 7"></p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Figure_5.png" alt="Figure 5"></p>
<p>图5：使用Faster R-CNN系统在PASCAL VOC 2007测试集上选择的目标检测结果示例。该模型是VGG-16，训练数据是07+12 trainval（2007年测试集中$73.2\%$的mAP）。我们的方法检测广泛的尺度和长宽比目标。每个输出框都与类别标签和[0，1]之间的softmax分数相关联。使用0.6的分数阈值来显示这些图像。获得这些结果的运行时间为每张图像198ms，包括所有步骤。</p>
<p>在表5中我们总结了整个目标检测系统的运行时间。根据内容（平均大约1.5s），SS需要1-2秒，而使用VGG-16的Fast R-CNN在2000个SS提议上需要320ms（如果在全连接层上使用SVD[2]，则需要223ms）。我们的VGG-16系统在提议和检测上总共需要198ms。在共享卷积特征的情况下，单独RPN只需要10ms计算附加层。我们的区域计算也较低，这要归功于较少的提议（每张图片300个）。我们的采用ZF网络的系统，帧速率为17fps。</p>
<p><strong>对超参数的敏感度</strong>。在表8中，我们调查锚点的设置。默认情况下，我们使用3个尺度和3个长宽比（表8中$69.9\%$的mAP）。如果在每个位置只使用一个锚点，那么mAP的下降幅度将是$3-4\%$。如果使用3个尺度（1个长宽比）或3个长宽比（1个尺度），则mAP更高，表明使用多种尺寸的锚点作为回归参考是有效的解决方案。在这个数据集上，仅使用具有1个长宽比（$69.8\%$）的3个尺度与使用具有3个长宽比的3个尺度一样好，这表明尺度和长宽比不是检测准确度的解决维度。但我们仍然在设计中采用这两个维度来保持我们的系统灵活性。</p>
<p>在表9中，我们比较了公式（1）中$\lambda$的不同值。默认情况下，我们使用$\lambda=10$，这使方程（1）中的两个项在归一化之后大致相等地加权。表9显示，当$\lambda$在大约两个数量级（1到100）的范围内时，我们的结果只是稍微受到影响（$\sim 1\%$）。这表明结果对宽范围内的$\lambda$不敏感。</p>
<p><strong>分析IoU召回率</strong>。接下来，我们使用实际边界框来计算不同IoU比率的提议召回率。值得注意的是，Recall-to-IoU度量与最终的检测精度的相关性是松散的[19，20，21]。使用这个指标来诊断提议方法比评估提议方法更合适。</p>
<p>在图4中，我们显示了使用300，1000和2000个提议的结果。我们与SS和EB进行比较，根据这些方法产生的置信度，N个提议是排名前N的提议。从图中可以看出，当提议数量从2000个减少到300个时，RPN方法表现优雅。这就解释了为什么RPN在使用300个提议时具有良好的最终检测mAP。正如我们之前分析过的，这个属性主要归因于RPN的<em>cls</em>项。当提议较少时，SS和EB的召回率下降的比RPN更快。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Figure_4.png" alt="Figure 4"></p>
<p>图4：PASCAL VOC 2007测试集上的召回率和IoU重叠率。</p>
<p><strong>一阶段检测与两阶段提议+检测</strong>。OverFeat论文[9]提出了一种在卷积特征映射的滑动窗口上使用回归器和分类器的检测方法。OverFeat是一个一阶段，类别特定的检测流程，而我们的是两阶段级联，包括类不可知的提议和类别特定的检测。在OverFeat中，区域特征来自一个尺度金字塔上一个长宽比的滑动窗口。这些特征用于同时确定目标的位置和类别。在RPN中，这些特征来自正方形（$3\times 3$）滑动窗口，并且预测相对于锚点具有不同尺度和长宽比的提议。虽然这两种方法都使用滑动窗口，但区域提议任务只是Faster R-CNN的第一阶段——下游的Fast R-CNN检测器会致力于对提议进行细化。在我们级联的第二阶段，在更忠实覆盖区域特征的提议框中，区域特征自适应地聚集[1]，[2]。我们相信这些功能会带来更准确的检测结果。</p>
<p>为了比较一阶段和两阶段系统，我们通过一阶段Fast R-CNN来模拟OverFeat系统（从而也规避了实现细节的其他差异）。在这个系统中，“提议”是3个尺度（128，256，512）和3个长宽比（1:1，1:2，2:1）的密集滑动窗口。训练Fast R-CNN来预测类别特定的分数，并从这些滑动窗口中回归边界框位置。由于OverFeat系统采用图像金字塔，我们也使用从5个尺度中提取的卷积特征进行评估。我们使用[1]，[2]中5个尺度。</p>
<p>表10比较了两阶段系统和一阶段系统的两个变种。使用ZF模型，一阶段系统具有$53.9\%$的mAP。这比两阶段系统（$58.7\%$）低$4.8\%$。这个实验验证了级联区域提议和目标检测的有效性。在文献[2]，[39]中报道了类似的观察结果，在这两篇论文中，用滑动窗取代SS区域提议会导致$\sim 6\%$的退化。我们也注意到，一阶段系统更慢，因为它产生了更多的提议。</p>
<p>表10：一阶段检测与两阶段提议+检测。使用ZF模型和Fast R-CNN在PASCAL VOC 2007测试集上的检测结果。RPN使用未共享的功能。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_10.png" alt="Table 10"></p>
<h3 id="4-2-在MS-COCO上的实验"><a href="#4-2-在MS-COCO上的实验" class="headerlink" title="4.2 在MS COCO上的实验"></a>4.2 在MS COCO上的实验</h3><p>我们在Microsoft COCO目标检测数据集[12]上提供了更多的结果。这个数据集包含80个目标类别。我们用训练集上的8万张图像，验证集上的4万张图像以及测试开发集上的2万张图像进行实验。我们评估了$IoU \in [0.5:0.05:0.95]$的平均mAP（COCO标准度量，简称为mAP@[.5,.95]）和mAP@0.5（PASCAL VOC度量）。</p>
<p>我们的系统对这个数据集做了一些小的改动。我们在8 GPU实现上训练我们的模型，RPN（每个GPU 1个）和Fast R-CNN（每个GPU 2个）的有效最小批大小为8个。RPN步骤和Fast R-CNN步骤都以24万次迭代进行训练，学习率为0.003，然后以0.0003的学习率进行8万次迭代。我们修改了学习率（从0.003而不是0.001开始），因为小批量数据的大小发生了变化。对于锚点，我们使用3个长宽比和4个尺度（加上$64^2$），这主要是通过处理这个数据集上的小目标来激发的。此外，在我们的Fast R-CNN步骤中，负样本定义为与实际边界框的最大IOU在[0，0.5)区间内的样本，而不是[1]，[2]中使用的[0.1,0.5)之间。我们注意到，在SPPnet系统[1]中，在[0.1，0.5)中的负样本用于网络微调，但[0,0.5)中的负样本仍然在具有难例挖掘SVM步骤中被访问。但是Fast R-CNN系统[2]放弃了SVM步骤，所以[0,0.1]中的负样本都不会被访问。包括这些[0,0.1)的样本，在Fast R-CNN和Faster R-CNN系统在COCO数据集上改进了mAP@0.5（但对PASCAL VOC的影响可以忽略不计）。</p>
<p>其余的实现细节与PASCAL VOC相同。特别的是，我们继续使用300个提议和单一尺度（$s=600$）测试。COCO数据集上的测试时间仍然是大约200ms处理一张图像。</p>
<p>在表11中，我们首先报告了使用本文实现的Fast R-CNN系统[2]的结果。我们的Fast R-CNN基准在<code>test-dev</code>数据集上有$39.3\%$的mAP@0.5，比[2]中报告的更高。我们推测造成这种差距的原因主要是由于负样本的定义以及小批量大小的变化。我们也注意到mAP@[.5，.95]恰好相当。</p>
<p>表11：在MS COCO数据集上的目标检测结果(%)。模型是VGG-16。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_11.png" alt="Table 11"></p>
<p>接下来我们评估我们的Faster R-CNN系统。使用COCO训练集训练，在COCO测试开发集上Faster R-CNNN有$42.1\%$的mAP@0.5和$21.5\%$的mAP@[0.5，0.95]。与相同协议下的Fast R-CNN相比，mAP@0.5要高$2.8\%$，mAP@[.5, .95]要高$2.2\%$（表11）。这表明，在更高的IoU阈值上，RPN对提高定位精度表现出色。使用COCO训练集训练，在COCO测试开发集上Faster R-CNN有$42.7\%$的mAP@0.5和$21.9\%$的mAP@[.5, .95]。图6显示了MS COCO测试开发数据集中的一些结果。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Figure_6.png" alt="Figure 6"></p>
<p>图6：使用Faster R-CNN系统在MS COCO test-dev数据集上选择的目标检测结果示例。该模型是VGG-16，训练数据是COCO训练数据（在测试开发数据集上为$42.7\%$的mAP@0.5）。每个输出框都与一个类别标签和[0, 1]之间的softmax分数相关联。使用0.6的分数阈值来显示这些图像。对于每张图像，一种颜色表示该图像中的一个目标类别。</p>
<p><strong>在ILSVRC和COCO 2015比赛中的Faster R-CNN</strong>。我们已经证明，由于RPN通过神经网络完全学习了提议区域，Faster R-CNN从更好的特征中受益更多。即使将深度增加到100层以上，这种观察仍然是有效的[18]。仅用101层残差网络（ResNet-101）代替VGG-16，Faster R-CNN系统就将mAP从$41.5<br>%/21.2\%$（VGG-16）增加到$48.4\%/27.2\%$（ResNet-101）。与其他改进正交于Faster R-CNN，何等人[18]在COCO测试开发数据集上获得了单模型$55.7\%/34.9\%$的结果和$59.0\%/37.4\%$的组合结果，在COCO 2015目标检测竞赛中获得了第一名。同样的系统[18]也在ILSVRC 2015目标检测竞赛中获得了第一名，超过第二名绝对的$8.5\%$。RPN也是ILSVRC2015定位和COCO2015分割竞赛第一名获奖输入的基石，详情请分别参见[18]和[15]。</p>
<h3 id="4-3-从MS-COCO到PASCAL-VOC"><a href="#4-3-从MS-COCO到PASCAL-VOC" class="headerlink" title="4.3 从MS COCO到PASCAL VOC"></a>4.3 从MS COCO到PASCAL VOC</h3><p>大规模数据对改善深度神经网络至关重要。接下来，我们调查MS COCO数据集如何帮助改进在PASCAL VOC上的检测性能。</p>
<p>作为一个简单的基准数据，我们直接在PASCAL VOC数据集上评估COCO检测模型，<em>而无需在任何PASCAL VOC数据上进行微调</em>。这种评估是可能的，因为COCO类别是PASCAL VOC上类别的超集。在这个实验中忽略COCO专有的类别，softmax层仅在20个类别和背景上执行。这种设置下PASCAL VOC 2007测试集上的mAP为$76.1\%$（表12）。即使没有利用PASCAL VOC的数据，这个结果也好于在VOC07+12($73.2\%$)上训练的模型的结果。</p>
<p>然后我们在VOC数据集上对COCO检测模型进行微调。在这个实验中，COCO模型代替了ImageNet的预训练模型（用于初始化网络权重），Faster R-CNN系统按3.2节所述进行微调。这样做在PASCAL VOC 2007测试集上可以达到$78.8\%$的mAP。来自COCO集合的额外数据增加了$5.6\%$的mAP。表6显示，在PASCAL VOC 2007上，使用COCO+VOC训练的模型在每个类别上具有最好的AP值。在PASCAL VOC 2012测试集（表12和表7）中也观察到类似的改进。我们注意到获得这些强大结果的测试时间速度仍然是每张图像200ms左右。</p>
<p>表6：Fast R-CNN检测器和VGG-16在PASCAL VOC 2007测试集上的结果。对于RPN，Fast R-CNN的训练时的提议数量是2000。$RPN^*$表示取消共享特征的版本。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_6.png" alt="Table 6"></p>
<p>表12：使用不同的训练数据在PASCAL VOC 2007测试集和2012测试集上检测Faster R-CNN的检测mAP（％）。模型是VGG-16。“COCO”表示COCO<code>trainval</code>数据集用于训练。另见表6和表7。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_12.png" alt="Table 12"></p>
<p>表7：Fast R-CNN检测器和VGG-16在PASCAL VOC 2012测试集上的结果。对于RPN，Fast R-CNN的训练时的提议数量是2000。</p>
<p><img src="http://noahsnail.com/images/faster_rcnn/Table_7.png" alt="Table 7"></p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们已经提出了RPN来生成高效，准确的区域提议。通过与下游检测网络共享卷积特征，区域提议步骤几乎是零成本的。我们的方法使统一的，基于深度学习的目标检测系统能够以接近实时的帧率运行。学习到的RPN也提高了区域提议的质量，从而提高了整体的目标检测精度。</p>
<h2 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title="REFERENCES"></a>REFERENCES</h2><p>[1] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” in European Conference on Computer Vision (ECCV), 2014.</p>
<p>[2] R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.</p>
<p>[3] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in International Conference on Learning Representations (ICLR), 2015.</p>
<p>[4] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders, “Selective search for object recognition,” International<br>Journal of Computer Vision (IJCV), 2013.</p>
<p>[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.</p>
<p>[6] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from edges,” in European Conference on Computer Vision(ECCV),2014.</p>
<p>[7] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</p>
<p>[8] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object detection with discriminatively trained part-based models,” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2010.</p>
<p>[9] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, “Overfeat: Integrated recognition, localization and detection using convolutional networks,” in International Conference on Learning Representations (ICLR), 2014.</p>
<p>[10] S. Ren, K. He, R. Girshick, and J. Sun, “FasterR-CNN: Towards real-time object detection with region proposal networks,” in<br>Neural Information Processing Systems (NIPS), 2015.</p>
<p>[11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,” 2007.</p>
<p>[12] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft COCO: Common Objects in Context,” in European Conference on Computer Vision (ECCV), 2014.</p>
<p>[13] S. Song and J. Xiao, “Deep sliding shapes for amodal 3d object detection in rgb-d images,” arXiv:1511.02300, 2015.</p>
<p>[14] J. Zhu, X. Chen, and A. L. Yuille, “DeePM: A deep part-based model for object detection and semantic part localization,” arXiv:1511.07131, 2015.</p>
<p>[15] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via multi-task network cascades,” arXiv:1512.04412, 2015.</p>
<p>[16] J. Johnson, A. Karpathy, and L. Fei-Fei, “Densecap: Fully convolutional localization networks for dense captioning,” arXiv:1511.07571, 2015.</p>
<p>[17] D. Kislyuk, Y. Liu, D. Liu, E. Tzeng, and Y. Jing, “Human curation and convnets: Powering item-to-item recommendations on pinterest,” arXiv:1511.04003, 2015.</p>
<p>[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” arXiv:1512.03385, 2015.</p>
<p>[19] J. Hosang, R. Benenson, and B. Schiele, “How good are detection proposals, really?” in British Machine Vision Conference (BMVC), 2014.</p>
<p>[20] J. Hosang, R. Benenson, P. Dollar, and B. Schiele, “What makes for effective detection proposals?” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2015.</p>
<p>[21] N. Chavali, H. Agrawal, A. Mahendru, and D. Batra, “Object-Proposal Evaluation Protocol is ’Gameable’,” arXiv: 1505.05836, 2015.</p>
<p>[22] J. Carreira and C. Sminchisescu, “CPMC: Automatic object segmentation using constrained parametric min-cuts,” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2012.</p>
<p>[23] P. Arbelaez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, “Multiscale combinatorial grouping,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.</p>
<p>[24] B. Alexe, T. Deselaers, and V. Ferrari, “Measuring the objectness of image windows,” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2012.</p>
<p>[25] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object detection,” in Neural Information Processing Systems (NIPS), 2013.</p>
<p>[26] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable object detection using deep neural networks,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.</p>
<p>[27] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov, “Scalable, high-quality object detection,” arXiv:1412.1441 (v1), 2015.</p>
<p>[28] P. O. Pinheiro, R. Collobert, and P. Dollar, “Learning to segment object candidates,” in Neural Information Processing Systems (NIPS), 2015.</p>
<p>[29] J. Dai, K. He, and J. Sun, “Convolutional feature masking for joint object and stuff segmentation,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</p>
<p>[30] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection networks on convolutional feature maps,” arXiv:1504.06066, 2015.</p>
<p>[31] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Neural Information Processing Systems (NIPS), 2015.</p>
<p>[32] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional neural networks,” in European Conference on Computer Vision (ECCV), 2014.</p>
<p>[33] V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann machines,” in International Conference on Machine Learning (ICML), 2010.</p>
<p>[34] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, and A. Rabinovich, “Going deeper with convolutions,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.</p>
<p>[35] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,” Neural computation, 1989.</p>
<p>[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” in International Journal of Computer Vision (IJCV), 2015.</p>
<p>[37] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classification with deep convolutional neural networks,” in Neural Information Processing Systems (NIPS), 2012.</p>
<p>[38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for fast feature embedding,” arXiv:1408.5093, 2014.</p>
<p>[39] K. Lenc and A. Vedaldi, “R-CNN minus R,” in British Machine Vision Conference (BMVC), 2015.</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>如果有收获，可以请我喝杯咖啡！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="https://user-images.githubusercontent.com/21311442/54660728-7c650300-4b12-11e9-9b0a-1a5c09323afe.png" alt="Tyan WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="https://user-images.githubusercontent.com/21311442/54660740-87b82e80-4b12-11e9-96e4-911014779bdc.png" alt="Tyan Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/03/2018-01-03-Faster R-CNN论文翻译——中英文对照/" rel="next" title="Faster R-CNN论文翻译——中英文对照">
                <i class="fa fa-chevron-left"></i> Faster R-CNN论文翻译——中英文对照
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/01/05/2017-01-05-Mac下图像标注工具labelImg的安装/" rel="prev" title="Mac下图像标注工具labelImg的安装">
                Mac下图像标注工具labelImg的安装 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Tyan" />
          <p class="site-author-name" itemprop="name">Tyan</p>
           
              <p class="site-description motion-element" itemprop="description">工作中的技术总结</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">828</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks"><span class="nav-number">1.</span> <span class="nav-text">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要"><span class="nav-number">1.1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-引言"><span class="nav-number">1.2.</span> <span class="nav-text">1. 引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-相关工作"><span class="nav-number">1.3.</span> <span class="nav-text">2. 相关工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-FASTER-R-CNN"><span class="nav-number">1.4.</span> <span class="nav-text">3. FASTER R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-区域提议网络"><span class="nav-number">1.4.1.</span> <span class="nav-text">3.1 区域提议网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-锚点"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">3.1.1 锚点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-损失函数"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">3.1.2 损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-训练RPN"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">3.1.3 训练RPN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-RPN和Fast-R-CNN共享特征"><span class="nav-number">1.4.2.</span> <span class="nav-text">3.2 RPN和Fast R-CNN共享特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-实现细节"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.3 实现细节</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-实验"><span class="nav-number">1.5.</span> <span class="nav-text">4. 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-PASCAL-VOC上的实验"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.1 PASCAL VOC上的实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-在MS-COCO上的实验"><span class="nav-number">1.5.2.</span> <span class="nav-text">4.2 在MS COCO上的实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-从MS-COCO到PASCAL-VOC"><span class="nav-number">1.5.3.</span> <span class="nav-text">4.3 从MS COCO到PASCAL VOC</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-结论"><span class="nav-number">1.6.</span> <span class="nav-text">5. 结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#REFERENCES"><span class="nav-number">1.7.</span> <span class="nav-text">REFERENCES</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tyan</span>
</div>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  
    <script id="dsq-count-scr" src="https://snailtyan.disqus.com/count.js" async></script>
  

  
    <script type="text/javascript">
      var disqus_config = function () {
        this.page.url = 'http://noahsnail.com/2018/01/03/2018-01-03-Faster R-CNN论文翻译——中文版/';
        this.page.identifier = '2018/01/03/2018-01-03-Faster R-CNN论文翻译——中文版/';
        this.page.title = 'Faster R-CNN论文翻译——中文版';
      };
      var d = document, s = d.createElement('script');
      s.src = 'https://snailtyan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    </script>
  




	





  








  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
