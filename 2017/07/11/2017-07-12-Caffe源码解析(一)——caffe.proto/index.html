<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Caffe,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="Caffe源码解析(一)——caffe.proto">
<meta name="keywords" content="Caffe">
<meta property="og:type" content="article">
<meta property="og:title" content="Caffe源码解析(一)——caffe.proto">
<meta property="og:url" content="http://noahsnail.com/2017/07/11/2017-07-12-Caffe源码解析(一)——caffe.proto/index.html">
<meta property="og:site_name" content="SnailTyan">
<meta property="og:description" content="Caffe源码解析(一)——caffe.proto">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2017-07-13T07:14:29.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Caffe源码解析(一)——caffe.proto">
<meta name="twitter:description" content="Caffe源码解析(一)——caffe.proto">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 'undefined',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="http://noahsnail.com/2017/07/11/2017-07-12-Caffe源码解析(一)——caffe.proto/">





  <title>Caffe源码解析(一)——caffe.proto | SnailTyan</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SnailTyan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://noahsnail.com/2017/07/11/2017-07-12-Caffe源码解析(一)——caffe.proto/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Tyan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SnailTyan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Caffe源码解析(一)——caffe.proto</h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/07/11/2017-07-12-Caffe源码解析(一)——caffe.proto/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/07/11/2017-07-12-Caffe源码解析(一)——caffe.proto/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          
              <div class="post-description">
                  Caffe源码解析(一)——caffe.proto
              </div>
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="noopener">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="noopener">简书</a></p>
<p>caffe.proto是caffe数据结构定义的主要文件，本文主要是在caffe.proto代码的基础上加上了部分中文注释，其中的内容与caffe的prototxt文件中的结构相对应。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br><span class="line">1222</span><br><span class="line">1223</span><br><span class="line">1224</span><br><span class="line">1225</span><br><span class="line">1226</span><br><span class="line">1227</span><br><span class="line">1228</span><br><span class="line">1229</span><br><span class="line">1230</span><br><span class="line">1231</span><br><span class="line">1232</span><br><span class="line">1233</span><br><span class="line">1234</span><br><span class="line">1235</span><br><span class="line">1236</span><br><span class="line">1237</span><br><span class="line">1238</span><br><span class="line">1239</span><br><span class="line">1240</span><br><span class="line">1241</span><br><span class="line">1242</span><br><span class="line">1243</span><br><span class="line">1244</span><br><span class="line">1245</span><br><span class="line">1246</span><br><span class="line">1247</span><br><span class="line">1248</span><br><span class="line">1249</span><br><span class="line">1250</span><br><span class="line">1251</span><br><span class="line">1252</span><br><span class="line">1253</span><br><span class="line">1254</span><br><span class="line">1255</span><br><span class="line">1256</span><br><span class="line">1257</span><br><span class="line">1258</span><br><span class="line">1259</span><br><span class="line">1260</span><br><span class="line">1261</span><br><span class="line">1262</span><br><span class="line">1263</span><br><span class="line">1264</span><br><span class="line">1265</span><br><span class="line">1266</span><br><span class="line">1267</span><br><span class="line">1268</span><br><span class="line">1269</span><br><span class="line">1270</span><br><span class="line">1271</span><br><span class="line">1272</span><br><span class="line">1273</span><br><span class="line">1274</span><br><span class="line">1275</span><br><span class="line">1276</span><br><span class="line">1277</span><br><span class="line">1278</span><br><span class="line">1279</span><br><span class="line">1280</span><br><span class="line">1281</span><br><span class="line">1282</span><br><span class="line">1283</span><br><span class="line">1284</span><br><span class="line">1285</span><br><span class="line">1286</span><br><span class="line">1287</span><br><span class="line">1288</span><br><span class="line">1289</span><br><span class="line">1290</span><br><span class="line">1291</span><br><span class="line">1292</span><br><span class="line">1293</span><br><span class="line">1294</span><br><span class="line">1295</span><br><span class="line">1296</span><br><span class="line">1297</span><br><span class="line">1298</span><br><span class="line">1299</span><br><span class="line">1300</span><br><span class="line">1301</span><br><span class="line">1302</span><br><span class="line">1303</span><br><span class="line">1304</span><br><span class="line">1305</span><br><span class="line">1306</span><br><span class="line">1307</span><br><span class="line">1308</span><br><span class="line">1309</span><br><span class="line">1310</span><br><span class="line">1311</span><br><span class="line">1312</span><br><span class="line">1313</span><br><span class="line">1314</span><br><span class="line">1315</span><br><span class="line">1316</span><br><span class="line">1317</span><br><span class="line">1318</span><br><span class="line">1319</span><br><span class="line">1320</span><br><span class="line">1321</span><br><span class="line">1322</span><br><span class="line">1323</span><br><span class="line">1324</span><br><span class="line">1325</span><br><span class="line">1326</span><br><span class="line">1327</span><br><span class="line">1328</span><br><span class="line">1329</span><br><span class="line">1330</span><br><span class="line">1331</span><br><span class="line">1332</span><br><span class="line">1333</span><br><span class="line">1334</span><br><span class="line">1335</span><br><span class="line">1336</span><br><span class="line">1337</span><br><span class="line">1338</span><br><span class="line">1339</span><br><span class="line">1340</span><br><span class="line">1341</span><br><span class="line">1342</span><br><span class="line">1343</span><br><span class="line">1344</span><br><span class="line">1345</span><br><span class="line">1346</span><br><span class="line">1347</span><br><span class="line">1348</span><br><span class="line">1349</span><br><span class="line">1350</span><br><span class="line">1351</span><br><span class="line">1352</span><br><span class="line">1353</span><br><span class="line">1354</span><br><span class="line">1355</span><br><span class="line">1356</span><br><span class="line">1357</span><br><span class="line">1358</span><br><span class="line">1359</span><br><span class="line">1360</span><br><span class="line">1361</span><br><span class="line">1362</span><br><span class="line">1363</span><br><span class="line">1364</span><br><span class="line">1365</span><br><span class="line">1366</span><br><span class="line">1367</span><br><span class="line">1368</span><br><span class="line">1369</span><br><span class="line">1370</span><br><span class="line">1371</span><br><span class="line">1372</span><br><span class="line">1373</span><br><span class="line">1374</span><br><span class="line">1375</span><br><span class="line">1376</span><br><span class="line">1377</span><br><span class="line">1378</span><br><span class="line">1379</span><br><span class="line">1380</span><br><span class="line">1381</span><br><span class="line">1382</span><br><span class="line">1383</span><br><span class="line">1384</span><br><span class="line">1385</span><br><span class="line">1386</span><br><span class="line">1387</span><br><span class="line">1388</span><br><span class="line">1389</span><br><span class="line">1390</span><br><span class="line">1391</span><br><span class="line">1392</span><br><span class="line">1393</span><br><span class="line">1394</span><br><span class="line">1395</span><br><span class="line">1396</span><br><span class="line">1397</span><br><span class="line">1398</span><br><span class="line">1399</span><br><span class="line">1400</span><br><span class="line">1401</span><br><span class="line">1402</span><br><span class="line">1403</span><br><span class="line">1404</span><br><span class="line">1405</span><br><span class="line">1406</span><br><span class="line">1407</span><br><span class="line">1408</span><br><span class="line">1409</span><br><span class="line">1410</span><br><span class="line">1411</span><br><span class="line">1412</span><br><span class="line">1413</span><br><span class="line">1414</span><br><span class="line">1415</span><br><span class="line">1416</span><br><span class="line">1417</span><br><span class="line">1418</span><br><span class="line">1419</span><br><span class="line">1420</span><br><span class="line">1421</span><br><span class="line">1422</span><br><span class="line">1423</span><br><span class="line">1424</span><br><span class="line">1425</span><br><span class="line">1426</span><br><span class="line">1427</span><br><span class="line">1428</span><br><span class="line">1429</span><br><span class="line">1430</span><br><span class="line">1431</span><br><span class="line">1432</span><br><span class="line">1433</span><br><span class="line">1434</span><br><span class="line">1435</span><br><span class="line">1436</span><br><span class="line">1437</span><br><span class="line">1438</span><br><span class="line">1439</span><br><span class="line">1440</span><br><span class="line">1441</span><br><span class="line">1442</span><br><span class="line">1443</span><br><span class="line">1444</span><br><span class="line">1445</span><br><span class="line">1446</span><br><span class="line">1447</span><br><span class="line">1448</span><br><span class="line">1449</span><br><span class="line">1450</span><br><span class="line">1451</span><br><span class="line">1452</span><br><span class="line">1453</span><br><span class="line">1454</span><br><span class="line">1455</span><br><span class="line">1456</span><br><span class="line">1457</span><br><span class="line">1458</span><br><span class="line">1459</span><br><span class="line">1460</span><br><span class="line">1461</span><br><span class="line">1462</span><br><span class="line">1463</span><br><span class="line">1464</span><br><span class="line">1465</span><br><span class="line">1466</span><br><span class="line">1467</span><br><span class="line">1468</span><br><span class="line">1469</span><br><span class="line">1470</span><br><span class="line">1471</span><br><span class="line">1472</span><br><span class="line">1473</span><br><span class="line">1474</span><br><span class="line">1475</span><br><span class="line">1476</span><br><span class="line">1477</span><br><span class="line">1478</span><br><span class="line">1479</span><br><span class="line">1480</span><br><span class="line">1481</span><br><span class="line">1482</span><br><span class="line">1483</span><br><span class="line">1484</span><br><span class="line">1485</span><br><span class="line">1486</span><br><span class="line">1487</span><br><span class="line">1488</span><br><span class="line">1489</span><br><span class="line">1490</span><br><span class="line">1491</span><br><span class="line">1492</span><br><span class="line">1493</span><br><span class="line">1494</span><br><span class="line">1495</span><br><span class="line">1496</span><br><span class="line">1497</span><br><span class="line">1498</span><br><span class="line">1499</span><br><span class="line">1500</span><br><span class="line">1501</span><br><span class="line">1502</span><br><span class="line">1503</span><br><span class="line">1504</span><br><span class="line">1505</span><br><span class="line">1506</span><br><span class="line">1507</span><br><span class="line">1508</span><br><span class="line">1509</span><br><span class="line">1510</span><br><span class="line">1511</span><br><span class="line">1512</span><br><span class="line">1513</span><br><span class="line">1514</span><br><span class="line">1515</span><br><span class="line">1516</span><br><span class="line">1517</span><br><span class="line">1518</span><br><span class="line">1519</span><br><span class="line">1520</span><br><span class="line">1521</span><br><span class="line">1522</span><br><span class="line">1523</span><br><span class="line">1524</span><br><span class="line">1525</span><br><span class="line">1526</span><br><span class="line">1527</span><br><span class="line">1528</span><br><span class="line">1529</span><br><span class="line">1530</span><br><span class="line">1531</span><br><span class="line">1532</span><br><span class="line">1533</span><br><span class="line">1534</span><br><span class="line">1535</span><br><span class="line">1536</span><br><span class="line">1537</span><br><span class="line">1538</span><br><span class="line">1539</span><br><span class="line">1540</span><br><span class="line">1541</span><br><span class="line">1542</span><br><span class="line">1543</span><br><span class="line">1544</span><br><span class="line">1545</span><br><span class="line">1546</span><br><span class="line">1547</span><br><span class="line">1548</span><br><span class="line">1549</span><br><span class="line">1550</span><br><span class="line">1551</span><br><span class="line">1552</span><br><span class="line">1553</span><br><span class="line">1554</span><br><span class="line">1555</span><br><span class="line">1556</span><br><span class="line">1557</span><br><span class="line">1558</span><br><span class="line">1559</span><br><span class="line">1560</span><br><span class="line">1561</span><br><span class="line">1562</span><br><span class="line">1563</span><br><span class="line">1564</span><br><span class="line">1565</span><br><span class="line">1566</span><br><span class="line">1567</span><br><span class="line">1568</span><br><span class="line">1569</span><br><span class="line">1570</span><br><span class="line">1571</span><br><span class="line">1572</span><br><span class="line">1573</span><br><span class="line">1574</span><br><span class="line">1575</span><br><span class="line">1576</span><br><span class="line">1577</span><br><span class="line">1578</span><br><span class="line">1579</span><br><span class="line">1580</span><br><span class="line">1581</span><br><span class="line">1582</span><br><span class="line">1583</span><br><span class="line">1584</span><br><span class="line">1585</span><br><span class="line">1586</span><br><span class="line">1587</span><br><span class="line">1588</span><br><span class="line">1589</span><br><span class="line">1590</span><br><span class="line">1591</span><br><span class="line">1592</span><br><span class="line">1593</span><br><span class="line">1594</span><br><span class="line">1595</span><br><span class="line">1596</span><br><span class="line">1597</span><br><span class="line">1598</span><br><span class="line">1599</span><br><span class="line">1600</span><br><span class="line">1601</span><br><span class="line">1602</span><br><span class="line">1603</span><br><span class="line">1604</span><br><span class="line">1605</span><br><span class="line">1606</span><br><span class="line">1607</span><br><span class="line">1608</span><br><span class="line">1609</span><br><span class="line">1610</span><br><span class="line">1611</span><br><span class="line">1612</span><br><span class="line">1613</span><br><span class="line">1614</span><br><span class="line">1615</span><br><span class="line">1616</span><br><span class="line">1617</span><br><span class="line">1618</span><br><span class="line">1619</span><br><span class="line">1620</span><br><span class="line">1621</span><br><span class="line">1622</span><br><span class="line">1623</span><br><span class="line">1624</span><br><span class="line">1625</span><br><span class="line">1626</span><br><span class="line">1627</span><br><span class="line">1628</span><br><span class="line">1629</span><br><span class="line">1630</span><br><span class="line">1631</span><br><span class="line">1632</span><br><span class="line">1633</span><br><span class="line">1634</span><br><span class="line">1635</span><br><span class="line">1636</span><br><span class="line">1637</span><br><span class="line">1638</span><br><span class="line">1639</span><br><span class="line">1640</span><br><span class="line">1641</span><br><span class="line">1642</span><br><span class="line">1643</span><br><span class="line">1644</span><br><span class="line">1645</span><br><span class="line">1646</span><br><span class="line">1647</span><br><span class="line">1648</span><br><span class="line">1649</span><br><span class="line">1650</span><br><span class="line">1651</span><br><span class="line">1652</span><br><span class="line">1653</span><br><span class="line">1654</span><br><span class="line">1655</span><br><span class="line">1656</span><br><span class="line">1657</span><br><span class="line">1658</span><br><span class="line">1659</span><br><span class="line">1660</span><br><span class="line">1661</span><br><span class="line">1662</span><br><span class="line">1663</span><br><span class="line">1664</span><br><span class="line">1665</span><br><span class="line">1666</span><br><span class="line">1667</span><br><span class="line">1668</span><br><span class="line">1669</span><br><span class="line">1670</span><br><span class="line">1671</span><br><span class="line">1672</span><br><span class="line">1673</span><br><span class="line">1674</span><br><span class="line">1675</span><br><span class="line">1676</span><br><span class="line">1677</span><br><span class="line">1678</span><br><span class="line">1679</span><br><span class="line">1680</span><br><span class="line">1681</span><br><span class="line">1682</span><br><span class="line">1683</span><br><span class="line">1684</span><br><span class="line">1685</span><br><span class="line">1686</span><br><span class="line">1687</span><br><span class="line">1688</span><br><span class="line">1689</span><br><span class="line">1690</span><br><span class="line">1691</span><br><span class="line">1692</span><br><span class="line">1693</span><br><span class="line">1694</span><br><span class="line">1695</span><br><span class="line">1696</span><br><span class="line">1697</span><br><span class="line">1698</span><br><span class="line">1699</span><br><span class="line">1700</span><br><span class="line">1701</span><br><span class="line">1702</span><br><span class="line">1703</span><br><span class="line">1704</span><br><span class="line">1705</span><br><span class="line">1706</span><br><span class="line">1707</span><br><span class="line">1708</span><br><span class="line">1709</span><br><span class="line">1710</span><br><span class="line">1711</span><br><span class="line">1712</span><br><span class="line">1713</span><br><span class="line">1714</span><br><span class="line">1715</span><br><span class="line">1716</span><br><span class="line">1717</span><br><span class="line">1718</span><br><span class="line">1719</span><br><span class="line">1720</span><br><span class="line">1721</span><br><span class="line">1722</span><br><span class="line">1723</span><br><span class="line">1724</span><br><span class="line">1725</span><br><span class="line">1726</span><br><span class="line">1727</span><br><span class="line">1728</span><br><span class="line">1729</span><br><span class="line">1730</span><br><span class="line">1731</span><br><span class="line">1732</span><br><span class="line">1733</span><br><span class="line">1734</span><br><span class="line">1735</span><br><span class="line">1736</span><br><span class="line">1737</span><br><span class="line">1738</span><br><span class="line">1739</span><br><span class="line">1740</span><br><span class="line">1741</span><br><span class="line">1742</span><br><span class="line">1743</span><br><span class="line">1744</span><br><span class="line">1745</span><br><span class="line">1746</span><br><span class="line">1747</span><br><span class="line">1748</span><br><span class="line">1749</span><br><span class="line">1750</span><br><span class="line">1751</span><br><span class="line">1752</span><br></pre></td><td class="code"><pre><span class="line">// syntax用来指定protobuf的版本</span><br><span class="line">syntax = &quot;proto2&quot;;</span><br><span class="line"></span><br><span class="line">// package可以看作C++中的namespace，与Caffe C++代码中的namespace caffe对应</span><br><span class="line">// package用来避免名称冲突</span><br><span class="line">package caffe;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。</span><br><span class="line">// 注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。</span><br><span class="line">// required：一个格式良好的消息一定要含有一个这种字段，表示该值是必须要设置的。</span><br><span class="line">// optional：消息格式中该字段可以有0个或1个值（不超过1个）。</span><br><span class="line">// repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留，表示该值可以重复，相当于Java中的List。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// Specifies the shape (dimensions) of a Blob.</span><br><span class="line">// 指定Blob的shape，4-D shape</span><br><span class="line">message BlobShape &#123;</span><br><span class="line">  //数据块形状定义为Num * Channel * Height * Wight, 原因在于caffe基于容器的多维嵌套来实现高维数据的封装, 即vector。 </span><br><span class="line">  repeated int64 dim = 1 [packed = true];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// Blob数据块，包括Blob shape，数据和微分</span><br><span class="line">message BlobProto &#123;</span><br><span class="line">  // Blob的shape, 即numpy中的shape</span><br><span class="line">  optional BlobShape shape = 7;</span><br><span class="line">  // Blob的数据部分</span><br><span class="line">  repeated float data = 5 [packed = true];</span><br><span class="line">  // Blob的微分部分</span><br><span class="line">  repeated float diff = 6 [packed = true];</span><br><span class="line">  // Blob中的数据部分(double类型)</span><br><span class="line">  repeated double double_data = 8 [packed = true];</span><br><span class="line">  // Blob的微分部分(double类型)</span><br><span class="line">  repeated double double_diff = 9 [packed = true];</span><br><span class="line"></span><br><span class="line">  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.</span><br><span class="line">  // Blob的4个维度，已被Blob shape代替</span><br><span class="line">  // Blob中数据的个数(例如卷积核的个数)</span><br><span class="line">  optional int32 num = 1 [default = 0];</span><br><span class="line">  // Blob中数据的通道数</span><br><span class="line">  optional int32 channels = 2 [default = 0];</span><br><span class="line">  // Blob中数据的高度</span><br><span class="line">  optional int32 height = 3 [default = 0];</span><br><span class="line">  // Blob中数据的宽度</span><br><span class="line">  optional int32 width = 4 [default = 0];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// The BlobProtoVector is simply a way to pass multiple blobproto instances</span><br><span class="line">// around.</span><br><span class="line">// BlobProtoVector, 用来保存多个BlobProb对象的Vector</span><br><span class="line">message BlobProtoVector &#123;</span><br><span class="line">  repeated BlobProto blobs = 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//图像数据, channel-图像通道数, height-高度, width-宽度, data-图像像素数据, label-图像标签, float_data-图像浮点型数据(0-1之间), encoded-图像编码方式</span><br><span class="line">message Datum &#123;</span><br><span class="line">  // 图像的通道数</span><br><span class="line">  optional int32 channels = 1;</span><br><span class="line">  // 图像的高度</span><br><span class="line">  optional int32 height = 2;</span><br><span class="line">  // 图像的宽度</span><br><span class="line">  optional int32 width = 3;</span><br><span class="line">  // the actual image data, in bytes</span><br><span class="line">  // 实际的图像数据，以字节形式(uint8)表示</span><br><span class="line">  optional bytes data = 4;</span><br><span class="line">  // 图像对应的标签，必须为整形</span><br><span class="line">  optional int32 label = 5;</span><br><span class="line">  // Optionally, the datum could also hold float data.</span><br><span class="line">  // 可选表示，图像数据表示为float数据，即0-255归一化到0-1之间</span><br><span class="line">  repeated float float_data = 6;</span><br><span class="line">  // If true data contains an encoded image that need to be decoded</span><br><span class="line">  // encoded为true表示图像采用压缩表示，需要解码</span><br><span class="line">  optional bool encoded = 7 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// Filler参数, filler主要对网络权重进行初始化</span><br><span class="line">// Filler类型分为常量初始化（constant）、高斯分布初始化（gaussian）、positive_unitball初始化、均匀分布初始化（uniform）、xavier初始化、msra初始化、双线性初始化（bilinear）</span><br><span class="line">message FillerParameter &#123;</span><br><span class="line">  // The filler type.</span><br><span class="line">  // Filler的类型</span><br><span class="line">  optional string type = 1 [default = &apos;constant&apos;];</span><br><span class="line">  // 常量初始化的值</span><br><span class="line">  optional float value = 2 [default = 0]; // the value in constant filler</span><br><span class="line">  // 均匀分布初始化中的最小值</span><br><span class="line">  optional float min = 3 [default = 0]; // the min value in uniform filler</span><br><span class="line">  // 均匀分布初始化中的最大值</span><br><span class="line">  optional float max = 4 [default = 1]; // the max value in uniform filler</span><br><span class="line">  // 高斯分布初始化中的均值</span><br><span class="line">  optional float mean = 5 [default = 0]; // the mean value in Gaussian filler</span><br><span class="line">  // 高斯分布初始化中的标准差</span><br><span class="line">  optional float std = 6 [default = 1]; // the std value in Gaussian filler</span><br><span class="line">  // The expected number of non-zero output weights for a given input in</span><br><span class="line">  // Gaussian filler -- the default -1 means don&apos;t perform sparsification.</span><br><span class="line">  // 在高斯分布初始化中给定输入及权重，期望输出非0值，默认值-1表示不进行稀疏化</span><br><span class="line">  optional int32 sparse = 7 [default = -1];</span><br><span class="line">  // Normalize the filler variance by fan_in, fan_out, or their average.</span><br><span class="line">  // Applies to &apos;xavier&apos; and &apos;msra&apos; fillers.</span><br><span class="line">  // 通过fan_in, fan_out或average来归一化filler方差，主要应用到&apos;xavier&apos;和&apos;msra&apos; filler中</span><br><span class="line">  enum VarianceNorm &#123;</span><br><span class="line">    FAN_IN = 0;</span><br><span class="line">    FAN_OUT = 1;</span><br><span class="line">    AVERAGE = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 定义filler方差归一化，默认为FAN_IN</span><br><span class="line">  optional VarianceNorm variance_norm = 8 [default = FAN_IN];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//神经网络参数</span><br><span class="line">message NetParameter &#123;</span><br><span class="line">  // 神经网络名字</span><br><span class="line">  optional string name = 1; // consider giving the network a name</span><br><span class="line"></span><br><span class="line">  // DEPRECATED. See InputParameter. The input blobs to the network.</span><br><span class="line">  // 已废弃。网络的输入部分，具体请看InputParameter。</span><br><span class="line">  repeated string input = 3;</span><br><span class="line"></span><br><span class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</span><br><span class="line">  // 已废弃。输入blob的shape，具体请看InputParameter。</span><br><span class="line">  repeated BlobShape input_shape = 8;</span><br><span class="line"></span><br><span class="line">  // 4D input dimensions -- deprecated.  Use &quot;input_shape&quot; instead.</span><br><span class="line">  // If specified, for each input blob there should be four</span><br><span class="line">  // values specifying the num, channels, height and width of the input blob.</span><br><span class="line">  // Thus, there should be a total of (4 * #input) numbers.</span><br><span class="line">  // 已废弃。用input_shape代替。</span><br><span class="line">  repeated int32 input_dim = 4;</span><br><span class="line"></span><br><span class="line">  // Whether the network will force every layer to carry out backward operation.</span><br><span class="line">  // If set False, then whether to carry out backward is determined</span><br><span class="line">  // automatically according to the net structure and learning rates.</span><br><span class="line">  // 网络中是否每一层都执行反向传播的标志，如果设为false，反向传播会根据网络结构和学习率自动进行。</span><br><span class="line">  optional bool force_backward = 5 [default = false];</span><br><span class="line"></span><br><span class="line">  // The current &quot;state&quot; of the network, including the phase, level, and stage.</span><br><span class="line">  // Some layers may be included/excluded depending on this state and the states</span><br><span class="line">  // specified in the layers&apos; include and exclude fields.</span><br><span class="line">  // 网络的当前状态，包括phase, level和stage，(phase应该是对应prototxt文件中的TRAIN,TEST)</span><br><span class="line">  // 某些层是否included/excluded依赖于层中include，exclue字段指定的state。</span><br><span class="line">  optional NetState state = 6;</span><br><span class="line"></span><br><span class="line">  // Print debugging information about results while running Net::Forward,</span><br><span class="line">  // Net::Backward, and Net::Update.</span><br><span class="line">  // 在执行Net::Forward,Net::Backward, Net::Update时是否打印调试信息。</span><br><span class="line">  optional bool debug_info = 7 [default = false];</span><br><span class="line"></span><br><span class="line">  // The layers that make up the net.  Each of their configurations, including</span><br><span class="line">  // connectivity and behavior, is specified as a LayerParameter.</span><br><span class="line">  // 构成网络的layer，每一个layer的配置，包括连接性和行为都在LayerParameter中指定。</span><br><span class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</span><br><span class="line"></span><br><span class="line">  // DEPRECATED: use &apos;layer&apos; instead.</span><br><span class="line">  // 已废弃，用layer代替。</span><br><span class="line">  repeated V1LayerParameter layers = 2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// NOTE</span><br><span class="line">// Update the next available ID when you add a new SolverParameter field.</span><br><span class="line">// 注意：当你添加一个新的SolverParameter字段时，要更新下一个可获取的ID</span><br><span class="line">// SolverParameter next available ID: 41 (last added: type)</span><br><span class="line">// Solver参数</span><br><span class="line">message SolverParameter &#123;</span><br><span class="line">  //////////////////////////////////////////////////////////////////////////////</span><br><span class="line">  // Specifying the train and test networks</span><br><span class="line">  //</span><br><span class="line">  // Exactly one train net must be specified using one of the following fields:</span><br><span class="line">  //     train_net_param, train_net, net_param, net</span><br><span class="line">  // One or more test nets may be specified using any of the following fields:</span><br><span class="line">  //     test_net_param, test_net, net_param, net</span><br><span class="line">  // If more than one test net field is specified (e.g., both net and</span><br><span class="line">  // test_net are specified), they will be evaluated in the field order given</span><br><span class="line">  // above: (1) test_net_param, (2) test_net, (3) net_param/net.</span><br><span class="line">  // A test_iter must be specified for each test_net.</span><br><span class="line">  // A test_level and/or a test_stage may also be specified for each test_net.</span><br><span class="line">  //////////////////////////////////////////////////////////////////////////////</span><br><span class="line"></span><br><span class="line">  // Proto filename for the train net, possibly combined with one or more test nets.</span><br><span class="line">  // 训练网络的prototxt文件名，可能结合一个或多个测试网络</span><br><span class="line">  optional string net = 24;</span><br><span class="line">  // Inline train net param, possibly combined with one or more test nets.</span><br><span class="line">  // 内联训练网络参数，可能结合一个或多个测试网络</span><br><span class="line">  optional NetParameter net_param = 25;</span><br><span class="line"></span><br><span class="line">  // 训练网络的proto文件名</span><br><span class="line">  optional string train_net = 1; // Proto filename for the train net.</span><br><span class="line">  // 测试网络的proto文件名</span><br><span class="line">  repeated string test_net = 2; // Proto filenames for the test nets.</span><br><span class="line">  // 内联训练网络参数</span><br><span class="line">  optional NetParameter train_net_param = 21; // Inline train net params.</span><br><span class="line">  // 内联测试网络参数</span><br><span class="line">  repeated NetParameter test_net_param = 22; // Inline test net params.</span><br><span class="line"></span><br><span class="line">  // The states for the train/test nets. Must be unspecified or specified once per net.</span><br><span class="line">  // By default, all states will have solver = true;</span><br><span class="line">  // train_state will have phase = TRAIN,</span><br><span class="line">  // and all test_state&apos;s will have phase = TEST.</span><br><span class="line">  // Other defaults are set according to the NetState defaults.</span><br><span class="line">  // train/test网络的状态，必须不指定或每个网络指定一次</span><br><span class="line">  // 默认情况下，所有的状态都有solver = true，train_state的phase = TRAIN，其它默认情况根据NetState默认值设定。</span><br><span class="line">  </span><br><span class="line">  // train网络的状态，必须不指定或每个网络指定一次</span><br><span class="line">  optional NetState train_state = 26;</span><br><span class="line">  // test网络的状态，必须不指定或每个网络指定一次</span><br><span class="line">  repeated NetState test_state = 27;</span><br><span class="line"></span><br><span class="line">  // The number of iterations for each test net.</span><br><span class="line">  // 每个测试网络的迭代次数，即测试数据的迭代次数，测试数据总数=测试迭代次数*测试数据的batch_size。</span><br><span class="line">  repeated int32 test_iter = 3;</span><br><span class="line"></span><br><span class="line">  // The number of iterations between two testing phases.</span><br><span class="line">  // 两次测试间隔的迭代次数，即训练数据迭代多少次进行一次测试。</span><br><span class="line">  optional int32 test_interval = 4 [default = 0];</span><br><span class="line">  // 测试数据的loss，默认情况下不计算</span><br><span class="line">  optional bool test_compute_loss = 19 [default = false];</span><br><span class="line">  // If true, run an initial test pass before the first iteration,</span><br><span class="line">  // ensuring memory availability and printing the starting value of the loss.</span><br><span class="line">  // 如果为true，在第一次迭代之前进行一次初始测试，从而确保内存可用性并输出初始损失值。</span><br><span class="line">  optional bool test_initialization = 32 [default = true];</span><br><span class="line">  // 基本学习率</span><br><span class="line">  optional float base_lr = 5; // The base learning rate</span><br><span class="line">  // the number of iterations between displaying info. If display = 0, no info will be displayed.</span><br><span class="line">  // 执行多少次迭代显示一次信息，如果display = 0，不输出信息。</span><br><span class="line">  optional int32 display = 6;</span><br><span class="line">  // Display the loss averaged over the last average_loss iterations</span><br><span class="line">  // 输出的平均损失是之前多少次迭代的平均损失。</span><br><span class="line">  optional int32 average_loss = 33 [default = 1];</span><br><span class="line">  // 训练的最大迭代次数</span><br><span class="line">  optional int32 max_iter = 7; // the maximum number of iterations</span><br><span class="line">  // accumulate gradients over `iter_size` x `batch_size` instances</span><br><span class="line">  // 累积`iter_size` x `batch_size`个实例的梯度</span><br><span class="line">  optional int32 iter_size = 36 [default = 1];</span><br><span class="line"></span><br><span class="line">  // The learning rate decay policy. The currently implemented learning rate</span><br><span class="line">  // policies are as follows:</span><br><span class="line">  //    - fixed: always return base_lr.</span><br><span class="line">  //    - step: return base_lr * gamma ^ (floor(iter / step))</span><br><span class="line">  //    - exp: return base_lr * gamma ^ iter</span><br><span class="line">  //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)</span><br><span class="line">  //    - multistep: similar to step but it allows non uniform steps defined by</span><br><span class="line">  //      stepvalue</span><br><span class="line">  //    - poly: the effective learning rate follows a polynomial decay, to be</span><br><span class="line">  //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)</span><br><span class="line">  //    - sigmoid: the effective learning rate follows a sigmod decay</span><br><span class="line">  //      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</span><br><span class="line">  //</span><br><span class="line">  // where base_lr, max_iter, gamma, step, stepvalue and power are defined</span><br><span class="line">  // in the solver parameter protocol buffer, and iter is the current iteration.</span><br><span class="line"></span><br><span class="line">  // 学习率的变化策略</span><br><span class="line">  optional string lr_policy = 8;</span><br><span class="line">  // 学习率的计算参数</span><br><span class="line">  optional float gamma = 9; // The parameter to compute the learning rate.</span><br><span class="line">  // 学习率的计算参数</span><br><span class="line">  optional float power = 10; // The parameter to compute the learning rate.</span><br><span class="line">  // 动量参数</span><br><span class="line">  optional float momentum = 11; // The momentum value.</span><br><span class="line">  // 权重衰减，权重衰减主要影响神经网络的正则项，具体可参考Caffe文档</span><br><span class="line">  optional float weight_decay = 12; // The weight decay.</span><br><span class="line">  // regularization types supported: L1 and L2, controlled by weight_decay</span><br><span class="line">  // 正则化类型支持L1和L2，受weight_decay控制。</span><br><span class="line">  optional string regularization_type = 29 [default = &quot;L2&quot;];</span><br><span class="line">  // the stepsize for learning rate policy &quot;step&quot;</span><br><span class="line">  // 学习率方案为step时的参数</span><br><span class="line">  optional int32 stepsize = 13;</span><br><span class="line">  // the stepsize for learning rate policy &quot;multistep&quot;</span><br><span class="line">  // 学习率方案为multistep时的参数</span><br><span class="line">  repeated int32 stepvalue = 34;</span><br><span class="line"></span><br><span class="line">  // Set clip_gradients to &gt;= 0 to clip parameter gradients to that L2 norm,</span><br><span class="line">  // whenever their actual L2 norm is larger.</span><br><span class="line">  // 设置clip_gradients &gt;= 0可以削减L2范数的梯度，当真实L2范数的梯度大于clip_gradients，将L2范数的梯度设为clip_gradients</span><br><span class="line">  optional float clip_gradients = 35 [default = -1];</span><br><span class="line">  // snapshot的间隔，即迭代多少次保存一次snapshot</span><br><span class="line">  optional int32 snapshot = 14 [default = 0]; // The snapshot interval</span><br><span class="line">  // snapshot的前缀</span><br><span class="line">  optional string snapshot_prefix = 15; // The prefix for the snapshot.</span><br><span class="line">  // whether to snapshot diff in the results or not. Snapshotting diff will help</span><br><span class="line">  // debugging but the final protocol buffer size will be much larger.</span><br><span class="line">  // 是否在结果中保存snapshot的差分，snapshot diff有助于调试，但snapshot的文件会更大。</span><br><span class="line">  optional bool snapshot_diff = 16 [default = false];</span><br><span class="line">  // snapshot的保存格式（hdf5,binaryproto）。</span><br><span class="line">  enum SnapshotFormat &#123;</span><br><span class="line">    HDF5 = 0;</span><br><span class="line">    BINARYPROTO = 1;</span><br><span class="line">  &#125;</span><br><span class="line">  // snapshot默认保存为BINARYPROTO。</span><br><span class="line">  optional SnapshotFormat snapshot_format = 37 [default = BINARYPROTO];</span><br><span class="line">  // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default.</span><br><span class="line">  // 求解神经网络的方式，0 CPU, 1 GPU。默认使用GPU</span><br><span class="line">  enum SolverMode &#123;</span><br><span class="line">    CPU = 0;</span><br><span class="line">    GPU = 1;</span><br><span class="line">  &#125;</span><br><span class="line">  // 求解神经网络的模式，0 CPU, 1 GPU。默认使用GPU</span><br><span class="line">  optional SolverMode solver_mode = 17 [default = GPU];</span><br><span class="line">  // the device_id will that be used in GPU mode. Use device_id = 0 in default.</span><br><span class="line">  // device_id是GPU模式下GPU的ID。</span><br><span class="line">  optional int32 device_id = 18 [default = 0];</span><br><span class="line">  // If non-negative, the seed with which the Solver will initialize the Caffe</span><br><span class="line">  // random number generator -- useful for reproducible results. Otherwise,</span><br><span class="line">  // (and by default) initialize using a seed derived from the system clock.</span><br><span class="line">  // 如果是非负值，seed用来初始化Caffe的随机数生成器，对于再见结果是很有用的，默认情况下，seed的是从系统时钟获取。</span><br><span class="line">  optional int64 random_seed = 20 [default = -1];</span><br><span class="line"></span><br><span class="line">  // type of the solver</span><br><span class="line">  // 神经网络求解的类型, 默认为SGD</span><br><span class="line">  optional string type = 40 [default = &quot;SGD&quot;];</span><br><span class="line"></span><br><span class="line">  // numerical stability for RMSProp, AdaGrad and AdaDelta and Adam</span><br><span class="line">  // RMSProp, AdaGrad, AdaDelta, Adam求解类型的参数</span><br><span class="line">  optional float delta = 31 [default = 1e-8];</span><br><span class="line">  // parameters for the Adam solver</span><br><span class="line">  // Adam求解类型的参数</span><br><span class="line">  optional float momentum2 = 39 [default = 0.999];</span><br><span class="line"></span><br><span class="line">  // RMSProp decay value</span><br><span class="line">  // MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)</span><br><span class="line">  // RMSProp类型的衰减值</span><br><span class="line">  optional float rms_decay = 38 [default = 0.99];</span><br><span class="line"></span><br><span class="line">  // If true, print information about the state of the net that may help with</span><br><span class="line">  // debugging learning problems.</span><br><span class="line">  // 如果设为true，会输出网络的状态信息，有助于调试</span><br><span class="line">  optional bool debug_info = 23 [default = false];</span><br><span class="line"></span><br><span class="line">  // If false, don&apos;t save a snapshot after training finishes.</span><br><span class="line">  // 如果设为false，不保存训练结束的snapshot。</span><br><span class="line">  optional bool snapshot_after_train = 28 [default = true];</span><br><span class="line"></span><br><span class="line">  // DEPRECATED: old solver enum types, use string instead</span><br><span class="line">  // 已废弃，使用string代替</span><br><span class="line">  enum SolverType &#123;</span><br><span class="line">    SGD = 0;</span><br><span class="line">    NESTEROV = 1;</span><br><span class="line">    ADAGRAD = 2;</span><br><span class="line">    RMSPROP = 3;</span><br><span class="line">    ADADELTA = 4;</span><br><span class="line">    ADAM = 5;</span><br><span class="line">  &#125;</span><br><span class="line">  // DEPRECATED: use type instead of solver_type</span><br><span class="line">  // 已废弃：使用type代替</span><br><span class="line">  optional SolverType solver_type = 30 [default = SGD];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// A message that stores the solver snapshots</span><br><span class="line">// 保存solver snapshots</span><br><span class="line">message SolverState &#123;</span><br><span class="line">  // 当前的迭代次数</span><br><span class="line">  optional int32 iter = 1; // The current iteration</span><br><span class="line">  // 保存学习到的网络</span><br><span class="line">  optional string learned_net = 2; // The file that stores the learned net.</span><br><span class="line">  // sgd的求解历史</span><br><span class="line">  repeated BlobProto history = 3; // The history for sgd solvers</span><br><span class="line">  // 学习的当前step</span><br><span class="line">  optional int32 current_step = 4 [default = 0]; // The current step for learning rate</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 定义phase</span><br><span class="line">enum Phase &#123;</span><br><span class="line">   TRAIN = 0;</span><br><span class="line">   TEST = 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 网络状态</span><br><span class="line">message NetState &#123;</span><br><span class="line">  // 属于哪个phase</span><br><span class="line">  optional Phase phase = 1 [default = TEST];</span><br><span class="line">  optional int32 level = 2 [default = 0];</span><br><span class="line">  repeated string stage = 3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 网络状态分类</span><br><span class="line">message NetStateRule &#123;</span><br><span class="line">  // Set phase to require the NetState have a particular phase (TRAIN or TEST)</span><br><span class="line">  // to meet this rule.</span><br><span class="line">  // 设置phase</span><br><span class="line">  optional Phase phase = 1;</span><br><span class="line"></span><br><span class="line">  // Set the minimum and/or maximum levels in which the layer should be used.</span><br><span class="line">  // Leave undefined to meet the rule regardless of level.</span><br><span class="line">  // 设置layer的level</span><br><span class="line">  optional int32 min_level = 2;</span><br><span class="line">  optional int32 max_level = 3;</span><br><span class="line"></span><br><span class="line">  // Customizable sets of stages to include or exclude.</span><br><span class="line">  // The net must have ALL of the specified stages and NONE of the specified</span><br><span class="line">  // &quot;not_stage&quot;s to meet the rule.</span><br><span class="line">  // (Use multiple NetStateRules to specify conjunctions of stages.)</span><br><span class="line">  // 可定制的stage集合</span><br><span class="line">  repeated string stage = 4;</span><br><span class="line">  repeated string not_stage = 5;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Specifies training parameters (multipliers on global learning constants,</span><br><span class="line">// and the name and other settings used for weight sharing).</span><br><span class="line">// 指定训练参数及名称以及权重共享的其它设置</span><br><span class="line">message ParamSpec &#123;</span><br><span class="line">  // The names of the parameter blobs -- useful for sharing parameters among</span><br><span class="line">  // layers, but never required otherwise.  To share a parameter between two</span><br><span class="line">  // layers, give it a (non-empty) name.</span><br><span class="line">  // 两个layer之间进行参数共享的blob名字</span><br><span class="line">  optional string name = 1;</span><br><span class="line"></span><br><span class="line">  // Whether to require shared weights to have the same shape, or just the same</span><br><span class="line">  // count -- defaults to STRICT if unspecified.</span><br><span class="line">  // 参数共享时是否需要具有相同的shape，默认情况下需要有相同的shape</span><br><span class="line">  optional DimCheckMode share_mode = 2;</span><br><span class="line">  // 参数共享时的维度检查</span><br><span class="line">  enum DimCheckMode &#123;</span><br><span class="line">    // STRICT (default) requires that num, channels, height, width each match.</span><br><span class="line">    STRICT = 0;</span><br><span class="line">    // PERMISSIVE requires only the count (num*channels*height*width) to match.</span><br><span class="line">    PERMISSIVE = 1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // The multiplier on the global learning rate for this parameter.</span><br><span class="line">  // 学习率参数, learning rate = base_lr * lr_mult</span><br><span class="line">  optional float lr_mult = 3 [default = 1.0];</span><br><span class="line"></span><br><span class="line">  // The multiplier on the global weight decay for this parameter.</span><br><span class="line">  // 权重衰减参数, weight = weight_decay * decay_mult</span><br><span class="line">  optional float decay_mult = 4 [default = 1.0];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// NOTE</span><br><span class="line">// Update the next available ID when you add a new LayerParameter field.</span><br><span class="line">// LayerParameter next available layer-specific ID: 147 (last added: recurrent_param)</span><br><span class="line">// 注意：当你添加一个新的LayerParameter字段时，要更新下一个可获取的ID</span><br><span class="line">message LayerParameter &#123;</span><br><span class="line">  // layer名称</span><br><span class="line">  optional string name = 1; // the layer name</span><br><span class="line">  // layer类型</span><br><span class="line">  optional string type = 2; // the layer type</span><br><span class="line">  // layer的输入</span><br><span class="line">  repeated string bottom = 3; // the name of each bottom blob</span><br><span class="line">  // layer的输出</span><br><span class="line">  repeated string top = 4; // the name of each top blob</span><br><span class="line"></span><br><span class="line">  // The train / test phase for computation.</span><br><span class="line">  // layer用在train/test phase</span><br><span class="line">  optional Phase phase = 10;</span><br><span class="line"></span><br><span class="line">  // The amount of weight to assign each top blob in the objective.</span><br><span class="line">  // Each layer assigns a default value, usually of either 0 or 1,</span><br><span class="line">  // to each top blob.</span><br><span class="line">  // layer对最终的loss损失值的贡献率</span><br><span class="line">  repeated float loss_weight = 5;</span><br><span class="line"></span><br><span class="line">  // Specifies training parameters (multipliers on global learning constants,</span><br><span class="line">  // and the name and other settings used for weight sharing).</span><br><span class="line">  // 指定训练参数</span><br><span class="line">  repeated ParamSpec param = 6;</span><br><span class="line"></span><br><span class="line">  // The blobs containing the numeric parameters of the layer.</span><br><span class="line">  // layer的blobs</span><br><span class="line">  repeated BlobProto blobs = 7;</span><br><span class="line"></span><br><span class="line">  // Specifies whether to backpropagate to each bottom. If unspecified,</span><br><span class="line">  // Caffe will automatically infer whether each input needs backpropagation</span><br><span class="line">  // to compute parameter gradients. If set to true for some inputs,</span><br><span class="line">  // backpropagation to those inputs is forced; if set false for some inputs,</span><br><span class="line">  // backpropagation to those inputs is skipped.</span><br><span class="line">  //</span><br><span class="line">  // The size must be either 0 or equal to the number of bottoms.</span><br><span class="line">  // 指定反向传播是否传播到每一个bottom，如果不指定，caffe会自动检查推断是否每一个输入都需要反向传播来计算梯度。如果一些输入设为true,</span><br><span class="line">  // 则这些layer强制进行反向传播，如果设为false，这些layer将跳过反向传播。</span><br><span class="line">  repeated bool propagate_down = 11;</span><br><span class="line"></span><br><span class="line">  // Rules controlling whether and when a layer is included in the network,</span><br><span class="line">  // based on the current NetState.  You may specify a non-zero number of rules</span><br><span class="line">  // to include OR exclude, but not both.  If no include or exclude rules are</span><br><span class="line">  // specified, the layer is always included.  If the current NetState meets</span><br><span class="line">  // ANY (i.e., one or more) of the specified rules, the layer is</span><br><span class="line">  // included/excluded.</span><br><span class="line">  // 控制layer included/excluded</span><br><span class="line">  repeated NetStateRule include = 8;</span><br><span class="line">  repeated NetStateRule exclude = 9;</span><br><span class="line"></span><br><span class="line">  // Parameters for data pre-processing.</span><br><span class="line">  // 数据预处理参数</span><br><span class="line">  optional TransformationParameter transform_param = 100;</span><br><span class="line"></span><br><span class="line">  // Parameters shared by loss layers.</span><br><span class="line">  // loss layer的参数共享</span><br><span class="line">  optional LossParameter loss_param = 101;</span><br><span class="line"></span><br><span class="line">  // Layer type-specific parameters.</span><br><span class="line">  //</span><br><span class="line">  // Note: certain layers may have more than one computational engine</span><br><span class="line">  // for their implementation. These layers include an Engine type and</span><br><span class="line">  // engine parameter for selecting the implementation.</span><br><span class="line">  // The default for the engine is set by the ENGINE switch at compile-time.</span><br><span class="line"></span><br><span class="line">  // 特定layer的参数</span><br><span class="line">  optional AccuracyParameter accuracy_param = 102;</span><br><span class="line">  optional ArgMaxParameter argmax_param = 103;</span><br><span class="line">  optional BatchNormParameter batch_norm_param = 139;</span><br><span class="line">  optional BiasParameter bias_param = 141;</span><br><span class="line">  optional ConcatParameter concat_param = 104;</span><br><span class="line">  optional ContrastiveLossParameter contrastive_loss_param = 105;</span><br><span class="line">  optional ConvolutionParameter convolution_param = 106;</span><br><span class="line">  optional CropParameter crop_param = 144;</span><br><span class="line">  optional DataParameter data_param = 107;</span><br><span class="line">  optional DropoutParameter dropout_param = 108;</span><br><span class="line">  optional DummyDataParameter dummy_data_param = 109;</span><br><span class="line">  optional EltwiseParameter eltwise_param = 110;</span><br><span class="line">  optional ELUParameter elu_param = 140;</span><br><span class="line">  optional EmbedParameter embed_param = 137;</span><br><span class="line">  optional ExpParameter exp_param = 111;</span><br><span class="line">  optional FlattenParameter flatten_param = 135;</span><br><span class="line">  optional HDF5DataParameter hdf5_data_param = 112;</span><br><span class="line">  optional HDF5OutputParameter hdf5_output_param = 113;</span><br><span class="line">  optional HingeLossParameter hinge_loss_param = 114;</span><br><span class="line">  optional ImageDataParameter image_data_param = 115;</span><br><span class="line">  optional InfogainLossParameter infogain_loss_param = 116;</span><br><span class="line">  optional InnerProductParameter inner_product_param = 117;</span><br><span class="line">  optional InputParameter input_param = 143;</span><br><span class="line">  optional LogParameter log_param = 134;</span><br><span class="line">  optional LRNParameter lrn_param = 118;</span><br><span class="line">  optional MemoryDataParameter memory_data_param = 119;</span><br><span class="line">  optional MVNParameter mvn_param = 120;</span><br><span class="line">  optional ParameterParameter parameter_param = 145;</span><br><span class="line">  optional PoolingParameter pooling_param = 121;</span><br><span class="line">  optional PowerParameter power_param = 122;</span><br><span class="line">  optional PReLUParameter prelu_param = 131;</span><br><span class="line">  optional PythonParameter python_param = 130;</span><br><span class="line">  optional RecurrentParameter recurrent_param = 146;</span><br><span class="line">  optional ReductionParameter reduction_param = 136;</span><br><span class="line">  optional ReLUParameter relu_param = 123;</span><br><span class="line">  optional ReshapeParameter reshape_param = 133;</span><br><span class="line">  optional ScaleParameter scale_param = 142;</span><br><span class="line">  optional SigmoidParameter sigmoid_param = 124;</span><br><span class="line">  optional SoftmaxParameter softmax_param = 125;</span><br><span class="line">  optional SPPParameter spp_param = 132;</span><br><span class="line">  optional SliceParameter slice_param = 126;</span><br><span class="line">  optional TanHParameter tanh_param = 127;</span><br><span class="line">  optional ThresholdParameter threshold_param = 128;</span><br><span class="line">  optional TileParameter tile_param = 138;</span><br><span class="line">  optional WindowDataParameter window_data_param = 129;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used to apply transformation to the data layer&apos;s data</span><br><span class="line">// 用来进行数据层（图像）变换的参数</span><br><span class="line">message TransformationParameter &#123;</span><br><span class="line">  // For data pre-processing, we can do simple scaling and subtracting the</span><br><span class="line">  // data mean, if provided. Note that the mean subtraction is always carried</span><br><span class="line">  // out before scaling.</span><br><span class="line">  // 像素归一化，归一化之前会减去均值</span><br><span class="line">  optional float scale = 1 [default = 1];</span><br><span class="line">  // Specify if we want to randomly mirror data.</span><br><span class="line">  // 图像进行随机mirror操作</span><br><span class="line">  optional bool mirror = 2 [default = false];</span><br><span class="line">  // Specify if we would like to randomly crop an image.</span><br><span class="line">  // 图像随机crop操作</span><br><span class="line">  optional uint32 crop_size = 3 [default = 0];</span><br><span class="line">  // mean_file and mean_value cannot be specified at the same time</span><br><span class="line">  // 图像的均值文件</span><br><span class="line">  optional string mean_file = 4;</span><br><span class="line">  // if specified can be repeated once (would subtract it from all the channels)</span><br><span class="line">  // or can be repeated the same number of times as channels</span><br><span class="line">  // (would subtract them from the corresponding channel)</span><br><span class="line">  // 图像的均值，手动指定，通常是三个</span><br><span class="line">  repeated float mean_value = 5;</span><br><span class="line">  // Force the decoded image to have 3 color channels.</span><br><span class="line">  // 强制图像必须有三个颜色通道</span><br><span class="line">  optional bool force_color = 6 [default = false];</span><br><span class="line">  // Force the decoded image to have 1 color channels.</span><br><span class="line">  // 强制图像为灰度图像</span><br><span class="line">  optional bool force_gray = 7 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters shared by loss layers</span><br><span class="line">// loss层参数</span><br><span class="line">message LossParameter &#123;</span><br><span class="line">  // If specified, ignore instances with the given label.</span><br><span class="line">  // 如果指定，则label等于ignore_label的样本将不参与Loss计算，并且反向传播时梯度直接置0。</span><br><span class="line">  optional int32 ignore_label = 1;</span><br><span class="line">  // How to normalize the loss for loss layers that aggregate across batches,</span><br><span class="line">  // spatial dimensions, or other dimensions.  Currently only implemented in</span><br><span class="line">  // SoftmaxWithLoss and SigmoidCrossEntropyLoss layers.</span><br><span class="line">  // 指定loss归一化的方式</span><br><span class="line">  enum NormalizationMode &#123;</span><br><span class="line">    // Divide by the number of examples in the batch times spatial dimensions.</span><br><span class="line">    // Outputs that receive the ignore label will NOT be ignored in computing</span><br><span class="line">    // the normalization factor.</span><br><span class="line">    // 所有样本都参与计算，包括ignore label</span><br><span class="line">    FULL = 0;</span><br><span class="line">    // Divide by the total number of output locations that do not take the</span><br><span class="line">    // ignore_label.  If ignore_label is not set, this behaves like FULL.</span><br><span class="line">    // 所有样本都参与计算，不包括ignore label</span><br><span class="line">    VALID = 1;</span><br><span class="line">    // Divide by the batch size.</span><br><span class="line">    // 除以给定的batch size。</span><br><span class="line">    BATCH_SIZE = 2;</span><br><span class="line">    // Do not normalize the loss.</span><br><span class="line">    // 不归一化loss</span><br><span class="line">    NONE = 3;</span><br><span class="line">  &#125;</span><br><span class="line">  // For historical reasons, the default normalization for</span><br><span class="line">  // SigmoidCrossEntropyLoss is BATCH_SIZE and *not* VALID.</span><br><span class="line">  // loss归一化方式</span><br><span class="line">  optional NormalizationMode normalization = 3 [default = VALID];</span><br><span class="line">  // Deprecated.  Ignored if normalization is specified.  If normalization</span><br><span class="line">  // is not specified, then setting this to false will be equivalent to</span><br><span class="line">  // normalization = BATCH_SIZE to be consistent with previous behavior.</span><br><span class="line">  // 已废弃。Loss会除以参与计算的样本总数；否则Loss等于直接求和</span><br><span class="line">  optional bool normalize = 2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Messages that store parameters used by individual layer types follow, in</span><br><span class="line">// alphabetical order.</span><br><span class="line">// accuracy层参数</span><br><span class="line">message AccuracyParameter &#123;</span><br><span class="line">  // When computing accuracy, count as correct by comparing the true label to</span><br><span class="line">  // the top k scoring classes.  By default, only compare to the top scoring</span><br><span class="line">  // class (i.e. argmax).</span><br><span class="line">  // 计算前top-k的准确率，默认计算top-1准确率</span><br><span class="line">  optional uint32 top_k = 1 [default = 1];</span><br><span class="line"></span><br><span class="line">  // The &quot;label&quot; axis of the prediction blob, whose argmax corresponds to the</span><br><span class="line">  // predicted label -- may be negative to index from the end (e.g., -1 for the</span><br><span class="line">  // last axis).  For example, if axis == 1 and the predictions are</span><br><span class="line">  // (N x C x H x W), the label blob is expected to contain N*H*W ground truth</span><br><span class="line">  // labels with integer values in &#123;0, 1, ..., C-1&#125;.</span><br><span class="line">  // 指定在哪个维度上计算label</span><br><span class="line">  optional int32 axis = 2 [default = 1];</span><br><span class="line"></span><br><span class="line">  // If specified, ignore instances with the given label.</span><br><span class="line">  // 如果指定，则忽略给定标签的实例</span><br><span class="line">  optional int32 ignore_label = 3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 标签最大化参数，标签最大化即确定概率最大的label</span><br><span class="line">message ArgMaxParameter &#123;</span><br><span class="line">  // If true produce pairs (argmax, maxval)</span><br><span class="line">  // 如果为真，则生成(argmax, maxval)</span><br><span class="line">  optional bool out_max_val = 1 [default = false];</span><br><span class="line">  // 类别的top-k</span><br><span class="line">  optional uint32 top_k = 2 [default = 1];</span><br><span class="line">  // The axis along which to maximise -- may be negative to index from the</span><br><span class="line">  // end (e.g., -1 for the last axis).</span><br><span class="line">  // By default ArgMaxLayer maximizes over the flattened trailing dimensions</span><br><span class="line">  // for each index of the first / num dimension.</span><br><span class="line">  // 根据axis进行标签最大化</span><br><span class="line">  optional int32 axis = 3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 参数拼接，在deconv的prototxt文件中见过</span><br><span class="line">message ConcatParameter &#123;</span><br><span class="line">  // The axis along which to concatenate -- may be negative to index from the</span><br><span class="line">  // end (e.g., -1 for the last axis).  Other axes must have the</span><br><span class="line">  // same dimension for all the bottom blobs.</span><br><span class="line">  // By default, ConcatLayer concatenates blobs along the &quot;channels&quot; axis (1).</span><br><span class="line">  // 参数拼接时的维度，按axis进行拼接</span><br><span class="line">  optional int32 axis = 2 [default = 1];</span><br><span class="line"></span><br><span class="line">  // DEPRECATED: alias for &quot;axis&quot; -- does not support negative indexing.</span><br><span class="line">  // 已废弃。与axis一样。</span><br><span class="line">  optional uint32 concat_dim = 1 [default = 1];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// batch norm层的相关参数, batch norm layer通常配与scale layer一起使用，具体用法可参考Resnet结构</span><br><span class="line">message BatchNormParameter &#123;</span><br><span class="line">  // If false, accumulate global mean/variance values via a moving average. If</span><br><span class="line">  // true, use those accumulated values instead of computing mean/variance</span><br><span class="line">  // across the batch.</span><br><span class="line">  // 如果设为false，累计全部的mean/variance，如果为true，使用累计值代替batch上mean/variance的计算</span><br><span class="line">  // true是使用了caffe内部的均值和方差，false是使用了每个Batch里的数据的均值和方差</span><br><span class="line">  optional bool use_global_stats = 1;</span><br><span class="line">  // How much does the moving average decay each iteration?</span><br><span class="line">  // 每次迭代平均值衰减比例</span><br><span class="line">  optional float moving_average_fraction = 2 [default = .999];</span><br><span class="line">  // Small value to add to the variance estimate so that we don&apos;t divide by</span><br><span class="line">  // zero.</span><br><span class="line">  // variance估计时为了使除数不为0，需要加上eps</span><br><span class="line">  optional float eps = 3 [default = 1e-5];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// bias层参数，没找到实际的应用例子</span><br><span class="line">message BiasParameter &#123;</span><br><span class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</span><br><span class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</span><br><span class="line">  // (e.g., -1 for the last axis).</span><br><span class="line">  //</span><br><span class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</span><br><span class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</span><br><span class="line">  // following shapes (for the given value of axis):</span><br><span class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</span><br><span class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</span><br><span class="line">  //    (axis == 2 == -2)                   40;       40x60</span><br><span class="line">  //    (axis == 3 == -1)                                60</span><br><span class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</span><br><span class="line">  // &quot;axis&quot;) -- a scalar bias.</span><br><span class="line">  optional int32 axis = 1 [default = 1];</span><br><span class="line"></span><br><span class="line">  // (num_axes is ignored unless just one bottom is given and the bias is</span><br><span class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</span><br><span class="line">  // number of axes by the second bottom.)</span><br><span class="line">  // The number of axes of the input (bottom[0]) covered by the bias</span><br><span class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</span><br><span class="line">  // Set num_axes := 0, to add a zero-axis Blob: a scalar.</span><br><span class="line">  optional int32 num_axes = 2 [default = 1];</span><br><span class="line"></span><br><span class="line">  // (filler is ignored unless just one bottom is given and the bias is</span><br><span class="line">  // a learned parameter of the layer.)</span><br><span class="line">  // The initialization for the learned bias parameter.</span><br><span class="line">  // Default is the zero (0) initialization, resulting in the BiasLayer</span><br><span class="line">  // initially performing the identity operation.</span><br><span class="line">  optional FillerParameter filler = 3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 对比损失层，siamese network中使用了对比损失</span><br><span class="line">message ContrastiveLossParameter &#123;</span><br><span class="line">  // margin for dissimilar pair</span><br><span class="line">  // 不相似的样本对的距离保持在margin以上</span><br><span class="line">  optional float margin = 1 [default = 1.0];</span><br><span class="line">  // The first implementation of this cost did not exactly match the cost of</span><br><span class="line">  // Hadsell et al 2006 -- using (margin - d^2) instead of (margin - d)^2.</span><br><span class="line">  // legacy_version = false (the default) uses (margin - d)^2 as proposed in the</span><br><span class="line">  // Hadsell paper. New models should probably use this version.</span><br><span class="line">  // legacy_version = true uses (margin - d^2). This is kept to support /</span><br><span class="line">  // reproduce existing models and results</span><br><span class="line">  // 第一版对比损失没有完全按论文写，如果为false，则按照论文原来的公式计算</span><br><span class="line">  optional bool legacy_version = 2 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 卷积层参数</span><br><span class="line">message ConvolutionParameter &#123;</span><br><span class="line">  // 输出数据的个数</span><br><span class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</span><br><span class="line">  // 是否有偏置项</span><br><span class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</span><br><span class="line"></span><br><span class="line">  // Pad, kernel size, and stride are all given as a single value for equal</span><br><span class="line">  // dimensions in all spatial dimensions, or once per spatial dimension.</span><br><span class="line">  // 卷积padding的大小</span><br><span class="line">  repeated uint32 pad = 3; // The padding size; defaults to 0</span><br><span class="line">  // 卷积核的大小</span><br><span class="line">  repeated uint32 kernel_size = 4; // The kernel size</span><br><span class="line">  // 卷积的步长</span><br><span class="line">  repeated uint32 stride = 6; // The stride; defaults to 1</span><br><span class="line">  // Factor used to dilate the kernel, (implicitly) zero-filling the resulting</span><br><span class="line">  // holes. (Kernel dilation is sometimes referred to by its use in the</span><br><span class="line">  // algorithme à trous from Holschneider et al. 1987.)</span><br><span class="line">  // 卷积膨胀，在卷积的时候可以skip一定长度的像素</span><br><span class="line">  repeated uint32 dilation = 18; // The dilation; defaults to 1</span><br><span class="line"></span><br><span class="line">  // For 2D convolution only, the *_h and *_w versions may also be used to</span><br><span class="line">  // specify both spatial dimensions.</span><br><span class="line">  // padding, kernel, stride的宽度和高度</span><br><span class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height (2D only)</span><br><span class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width (2D only)</span><br><span class="line">  optional uint32 kernel_h = 11; // The kernel height (2D only)</span><br><span class="line">  optional uint32 kernel_w = 12; // The kernel width (2D only)</span><br><span class="line">  optional uint32 stride_h = 13; // The stride height (2D only)</span><br><span class="line">  optional uint32 stride_w = 14; // The stride width (2D only)</span><br><span class="line"></span><br><span class="line">  // 来自于AlexNet论文</span><br><span class="line">  optional uint32 group = 5 [default = 1]; // The group size for group conv</span><br><span class="line"></span><br><span class="line">  // 权重初始化</span><br><span class="line">  optional FillerParameter weight_filler = 7; // The filler for the weight</span><br><span class="line">  // 偏置初始化</span><br><span class="line">  optional FillerParameter bias_filler = 8; // The filler for the bias</span><br><span class="line">  enum Engine &#123;</span><br><span class="line">    DEFAULT = 0;</span><br><span class="line">    CAFFE = 1;</span><br><span class="line">    CUDNN = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 卷积的方式的选择，default是正常的卷积，caffe是矩阵乘法的卷积，cudnn是cuda库流并行式的卷积</span><br><span class="line">  optional Engine engine = 15 [default = DEFAULT];</span><br><span class="line"></span><br><span class="line">  // The axis to interpret as &quot;channels&quot; when performing convolution.</span><br><span class="line">  // Preceding dimensions are treated as independent inputs;</span><br><span class="line">  // succeeding dimensions are treated as &quot;spatial&quot;.</span><br><span class="line">  // With (N, C, H, W) inputs, and axis == 1 (the default), we perform</span><br><span class="line">  // N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</span><br><span class="line">  // groups g&gt;1) filters across the spatial axes (H, W) of the input.</span><br><span class="line">  // With (N, C, D, H, W) inputs, and axis == 1, we perform</span><br><span class="line">  // N independent 3D convolutions, sliding (C/g)-channels</span><br><span class="line">  // filters across the spatial axes (D, H, W) of the input.</span><br><span class="line">  // 通道channel所在的维度</span><br><span class="line">  optional int32 axis = 16 [default = 1];</span><br><span class="line"></span><br><span class="line">  // Whether to force use of the general ND convolution, even if a specific</span><br><span class="line">  // implementation for blobs of the appropriate number of spatial dimensions</span><br><span class="line">  // is available. (Currently, there is only a 2D-specific convolution</span><br><span class="line">  // implementation; for input blobs with num_axes != 2, this option is</span><br><span class="line">  // ignored and the ND implementation will be used.)</span><br><span class="line">  // 如果输入数据维度等于2，则执行通用的ND卷积，否则正常执行卷积</span><br><span class="line">  optional bool force_nd_im2col = 17 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 图像裁剪参数</span><br><span class="line">message CropParameter &#123;</span><br><span class="line">  // To crop, elements of the first bottom are selected to fit the dimensions</span><br><span class="line">  // of the second, reference bottom. The crop is configured by</span><br><span class="line">  // - the crop `axis` to pick the dimensions for cropping</span><br><span class="line">  // - the crop `offset` to set the shift for all/each dimension</span><br><span class="line">  // to align the cropped bottom with the reference bottom.</span><br><span class="line">  // All dimensions up to but excluding `axis` are preserved, while</span><br><span class="line">  // the dimensions including and trailing `axis` are cropped.</span><br><span class="line">  // If only one `offset` is set, then all dimensions are offset by this amount.</span><br><span class="line">  // Otherwise, the number of offsets must equal the number of cropped axes to</span><br><span class="line">  // shift the crop in each dimension accordingly.</span><br><span class="line">  // Note: standard dimensions are N,C,H,W so the default is a spatial crop,</span><br><span class="line">  // and `axis` may be negative to index from the end (e.g., -1 for the last</span><br><span class="line">  // axis).</span><br><span class="line">  // axis是在哪个维度上进行裁剪，会裁剪轴2及之后的所有轴</span><br><span class="line">  optional int32 axis = 1 [default = 2];</span><br><span class="line">  // offset设置是每个维度进行裁剪时的偏移量</span><br><span class="line">  repeated uint32 offset = 2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 数据层参数</span><br><span class="line">message DataParameter &#123;</span><br><span class="line">  enum DB &#123;</span><br><span class="line">    LEVELDB = 0;</span><br><span class="line">    LMDB = 1;</span><br><span class="line">  &#125;</span><br><span class="line">  // Specify the data source.</span><br><span class="line">  // 设定数据源路径</span><br><span class="line">  optional string source = 1;</span><br><span class="line">  // Specify the batch size.</span><br><span class="line">  // 指定一次处理的图片数量</span><br><span class="line">  optional uint32 batch_size = 4;</span><br><span class="line">  // The rand_skip variable is for the data layer to skip a few data points</span><br><span class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</span><br><span class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</span><br><span class="line">  // be larger than the number of keys in the database.</span><br><span class="line">  // DEPRECATED. Each solver accesses a different subset of the database.</span><br><span class="line">  // rand_skip跳过指定的数据点，避免异步的sgd从同一个数据点开始</span><br><span class="line">  optional uint32 rand_skip = 7 [default = 0];</span><br><span class="line">  // 使用的数据库类型，LMDB or LEVELDB</span><br><span class="line">  optional DB backend = 8 [default = LEVELDB];</span><br><span class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</span><br><span class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</span><br><span class="line">  // mean subtraction is always carried out before scaling.</span><br><span class="line">  // 已废弃。图像归一化，在TransformationParameter中。</span><br><span class="line">  optional float scale = 2 [default = 1];</span><br><span class="line">  // 已废弃。均值文件，在TransformationParameter中。</span><br><span class="line">  optional string mean_file = 3;</span><br><span class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</span><br><span class="line">  // crop an image.</span><br><span class="line">  // 已废弃。图像裁剪，在TransformationParameter中。</span><br><span class="line">  optional uint32 crop_size = 5 [default = 0];</span><br><span class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</span><br><span class="line">  // data.</span><br><span class="line">  // 已废弃。图像翻转，在TransformationParameter中。</span><br><span class="line">  optional bool mirror = 6 [default = false];</span><br><span class="line">  // Force the encoded image to have 3 color channels</span><br><span class="line">  // 强制图像数据有三个颜色通道</span><br><span class="line">  optional bool force_encoded_color = 9 [default = false];</span><br><span class="line">  // Prefetch queue (Number of batches to prefetch to host memory, increase if</span><br><span class="line">  // data access bandwidth varies).</span><br><span class="line">  // 预先拉取batch的数目</span><br><span class="line">  optional uint32 prefetch = 10 [default = 4];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// dropout层参数</span><br><span class="line">message DropoutParameter &#123;</span><br><span class="line">  // 为了避免过拟合，参数随机失活的比例</span><br><span class="line">  optional float dropout_ratio = 1 [default = 0.5]; // dropout ratio</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// DummyDataLayer fills any number of arbitrarily shaped blobs with random</span><br><span class="line">// (or constant) data generated by &quot;Fillers&quot; (see &quot;message FillerParameter&quot;).</span><br><span class="line">// DummyData层的参数</span><br><span class="line">message DummyDataParameter &#123;</span><br><span class="line">  // This layer produces N &gt;= 1 top blobs.  DummyDataParameter must specify 1 or N</span><br><span class="line">  // shape fields, and 0, 1 or N data_fillers.</span><br><span class="line">  // If 0 data_fillers are specified, ConstantFiller with a value of 0 is used.</span><br><span class="line">  // If 1 data_filler is specified, it is applied to all top blobs.  If N are</span><br><span class="line">  // specified, the ith is applied to the ith top blob.</span><br><span class="line">  // blob数据的生成方式</span><br><span class="line">  repeated FillerParameter data_filler = 1;</span><br><span class="line">  // 数据的维度</span><br><span class="line">  repeated BlobShape shape = 6;</span><br><span class="line"></span><br><span class="line">  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.</span><br><span class="line">  // 已废弃。使用shape代替。</span><br><span class="line">  repeated uint32 num = 2;</span><br><span class="line">  repeated uint32 channels = 3;</span><br><span class="line">  repeated uint32 height = 4;</span><br><span class="line">  repeated uint32 width = 5;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//Eltwise层的参数</span><br><span class="line">message EltwiseParameter &#123;</span><br><span class="line">  // 操作的类型</span><br><span class="line">  enum EltwiseOp &#123;</span><br><span class="line">    PROD = 0;</span><br><span class="line">    SUM = 1;</span><br><span class="line">    MAX = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 数据操作分三种：点乘，相加，取最大值</span><br><span class="line">  optional EltwiseOp operation = 1 [default = SUM]; // element-wise operation</span><br><span class="line">  // SUM操作时各个blob对应的系数</span><br><span class="line">  repeated float coeff = 2; // blob-wise coefficient for SUM operation</span><br><span class="line"></span><br><span class="line">  // Whether to use an asymptotically slower (for &gt;2 inputs) but stabler method</span><br><span class="line">  // of computing the gradient for the PROD operation. (No effect for SUM op.)</span><br><span class="line">  // 在进行PROD操作，即乘法时是否使用异步操作来计算梯度，更慢但更稳定。</span><br><span class="line">  optional bool stable_prod_grad = 3 [default = true];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by ELULayer</span><br><span class="line">// ELU层的参数，具体看论文</span><br><span class="line">message ELUParameter &#123;</span><br><span class="line">  // Described in:</span><br><span class="line">  // Clevert, D.-A., Unterthiner, T., &amp; Hochreiter, S. (2015). Fast and Accurate</span><br><span class="line">  // Deep Network Learning by Exponential Linear Units (ELUs). arXiv</span><br><span class="line">  optional float alpha = 1 [default = 1];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by EmbedLayer</span><br><span class="line">// Embed层的参数，主要用于LSTM等翻译网络</span><br><span class="line">message EmbedParameter &#123;</span><br><span class="line">  // Embed层的输出</span><br><span class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</span><br><span class="line">  // The input is given as integers to be interpreted as one-hot</span><br><span class="line">  // vector indices with dimension num_input.  Hence num_input should be</span><br><span class="line">  // 1 greater than the maximum possible input value.</span><br><span class="line">  // Embed层的输入</span><br><span class="line">  optional uint32 input_dim = 2;</span><br><span class="line">  // 是否使用偏置项</span><br><span class="line">  optional bool bias_term = 3 [default = true]; // Whether to use a bias term</span><br><span class="line">  // 权重生成</span><br><span class="line">  optional FillerParameter weight_filler = 4; // The filler for the weight</span><br><span class="line">  // 偏置生成</span><br><span class="line">  optional FillerParameter bias_filler = 5; // The filler for the bias</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by ExpLayer</span><br><span class="line">// Exp层的参数，即指数层参数</span><br><span class="line">message ExpParameter &#123;</span><br><span class="line">  // ExpLayer computes outputs y = base ^ (shift + scale * x), for base &gt; 0.</span><br><span class="line">  // Or if base is set to the default (-1), base is set to e,</span><br><span class="line">  // so y = exp(shift + scale * x).</span><br><span class="line">  // 指数层的计算是y = base ^ (shift + scale * x)，下面分别是公式中的三个参数</span><br><span class="line">  optional float base = 1 [default = -1.0];</span><br><span class="line">  optional float scale = 2 [default = 1.0];</span><br><span class="line">  optional float shift = 3 [default = 0.0];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by FlattenLayer</span><br><span class="line">// Flatten层的参数，主要是按某个轴展开（平铺），mnist demo的mnist_autoencode就使用了Flatten层</span><br><span class="line">message FlattenParameter &#123;</span><br><span class="line">  // The first axis to flatten: all preceding axes are retained in the output.</span><br><span class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</span><br><span class="line">  // 从哪一层开始展开</span><br><span class="line">  optional int32 axis = 1 [default = 1];</span><br><span class="line"></span><br><span class="line">  // The last axis to flatten: all following axes are retained in the output.</span><br><span class="line">  // May be negative to index from the end (e.g., the default -1 for the last</span><br><span class="line">  // axis).</span><br><span class="line">  // 展开到哪一层结束</span><br><span class="line">  optional int32 end_axis = 2 [default = -1];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by HDF5DataLayer</span><br><span class="line">// HDF5数据层的参数</span><br><span class="line">message HDF5DataParameter &#123;</span><br><span class="line">  // Specify the data source.</span><br><span class="line">  // HDF5层输入数据的数据源</span><br><span class="line">  optional string source = 1;</span><br><span class="line">  // Specify the batch size.</span><br><span class="line">  // 训练的batch_size</span><br><span class="line">  optional uint32 batch_size = 2;</span><br><span class="line"></span><br><span class="line">  // Specify whether to shuffle the data.</span><br><span class="line">  // If shuffle == true, the ordering of the HDF5 files is shuffled,</span><br><span class="line">  // and the ordering of data within any given HDF5 file is shuffled,</span><br><span class="line">  // but data between different files are not interleaved; all of a file&apos;s</span><br><span class="line">  // data are output (in a random order) before moving onto another file.</span><br><span class="line">  // 是否对HDF5的输入数据进行shuffle</span><br><span class="line">  optional bool shuffle = 3 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// HDF5输出层参数</span><br><span class="line">message HDF5OutputParameter &#123;</span><br><span class="line">  // 输出的HDF5文件的文件名</span><br><span class="line">  optional string file_name = 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// HingeLoss层参数</span><br><span class="line">message HingeLossParameter &#123;</span><br><span class="line">  enum Norm &#123;</span><br><span class="line">    L1 = 1;</span><br><span class="line">    L2 = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // Specify the Norm to use L1 or L2</span><br><span class="line">  // 指定HingeLoss的类型</span><br><span class="line">  optional Norm norm = 1 [default = L1];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// ImageData层参数，网络中直接输入原图</span><br><span class="line">message ImageDataParameter &#123;</span><br><span class="line">  // Specify the data source.</span><br><span class="line">  // 描述图像路径及标签的文件</span><br><span class="line">  optional string source = 1;</span><br><span class="line">  // Specify the batch size.</span><br><span class="line">  // 训练的batch size</span><br><span class="line">  optional uint32 batch_size = 4 [default = 1];</span><br><span class="line">  // The rand_skip variable is for the data layer to skip a few data points</span><br><span class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</span><br><span class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</span><br><span class="line">  // be larger than the number of keys in the database.</span><br><span class="line">  // rand_skip跳过指定的数据点，避免异步的sgd从同一个数据点开始，与Data层中是一样的</span><br><span class="line">  optional uint32 rand_skip = 7 [default = 0];</span><br><span class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</span><br><span class="line">  // 是否对图像顺序进行shuffle</span><br><span class="line">  optional bool shuffle = 8 [default = false];</span><br><span class="line">  // It will also resize images if new_height or new_width are not zero.</span><br><span class="line">  // 图像resize的高度</span><br><span class="line">  optional uint32 new_height = 9 [default = 0];</span><br><span class="line">  // 图像resize的宽度</span><br><span class="line">  optional uint32 new_width = 10 [default = 0];</span><br><span class="line">  // Specify if the images are color or gray</span><br><span class="line">  // 指定图像彩色图像还是灰度图像，默认彩色</span><br><span class="line">  optional bool is_color = 11 [default = true];</span><br><span class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</span><br><span class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</span><br><span class="line">  // mean subtraction is always carried out before scaling.</span><br><span class="line">  // 已废弃。参考TransformationParameter中的scale</span><br><span class="line">  optional float scale = 2 [default = 1];</span><br><span class="line">  // 指定均值文件</span><br><span class="line">  optional string mean_file = 3;</span><br><span class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</span><br><span class="line">  // crop an image.</span><br><span class="line">  // 已废弃。参考TransformationParameter中的crop_size</span><br><span class="line">  optional uint32 crop_size = 5 [default = 0];</span><br><span class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</span><br><span class="line">  // data.</span><br><span class="line">  // 已废弃，参考TransformationParameter的mirror。</span><br><span class="line">  optional bool mirror = 6 [default = false];</span><br><span class="line">  // 不太清楚root_folder具体是什么</span><br><span class="line">  optional string root_folder = 12 [default = &quot;&quot;];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 信息增益损失层参数</span><br><span class="line">message InfogainLossParameter &#123;</span><br><span class="line">  // Specify the infogain matrix source.</span><br><span class="line">  // 指定存储信息增益矩阵的源文件</span><br><span class="line">  optional string source = 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// InnerProduct层的参数</span><br><span class="line">message InnerProductParameter &#123;</span><br><span class="line">  // InnerProduct层的输出</span><br><span class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</span><br><span class="line">  // 是否有偏置项</span><br><span class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</span><br><span class="line">  // 权重初始化，随机生成</span><br><span class="line">  optional FillerParameter weight_filler = 3; // The filler for the weight</span><br><span class="line">  // 偏置初始化，随机生成</span><br><span class="line">  optional FillerParameter bias_filler = 4; // The filler for the bias</span><br><span class="line"></span><br><span class="line">  // The first axis to be lumped into a single inner product computation;</span><br><span class="line">  // all preceding axes are retained in the output.</span><br><span class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</span><br><span class="line">  // 从某一维度开始进行内积计算，前面的维度保留</span><br><span class="line">  optional int32 axis = 5 [default = 1];</span><br><span class="line">  // Specify whether to transpose the weight matrix or not.</span><br><span class="line">  // If transpose == true, any operations will be performed on the transpose</span><br><span class="line">  // of the weight matrix. The weight matrix itself is not going to be transposed</span><br><span class="line">  // but rather the transfer flag of operations will be toggled accordingly.</span><br><span class="line">  // 是否对权重矩阵进行转置</span><br><span class="line">  optional bool transpose = 6 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Input参数，caffe网络部署时会用到</span><br><span class="line">message InputParameter &#123;</span><br><span class="line">  // This layer produces N &gt;= 1 top blob(s) to be assigned manually.</span><br><span class="line">  // Define N shapes to set a shape for each top.</span><br><span class="line">  // Define 1 shape to set the same shape for every top.</span><br><span class="line">  // Define no shape to defer to reshaping manually.</span><br><span class="line">  // 输入数据的shape</span><br><span class="line">  repeated BlobShape shape = 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by LogLayer</span><br><span class="line">// Log层参数，对数据进行Log运算</span><br><span class="line">message LogParameter &#123;</span><br><span class="line">  // LogLayer computes outputs y = log_base(shift + scale * x), for base &gt; 0.</span><br><span class="line">  // Or if base is set to the default (-1), base is set to e,</span><br><span class="line">  // so y = ln(shift + scale * x) = log_e(shift + scale * x)</span><br><span class="line">  // Log层计算公式为y = log_base(shift + scale * x)，下面分别是公式中的三个参数</span><br><span class="line">  optional float base = 1 [default = -1.0];</span><br><span class="line">  optional float scale = 2 [default = 1.0];</span><br><span class="line">  optional float shift = 3 [default = 0.0];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by LRNLayer</span><br><span class="line">// LRN层的参数，局部归一化，AlexNet中的LRN</span><br><span class="line">message LRNParameter &#123;</span><br><span class="line">  // 如果是跨通道LRN，则表示求和的通道数；如果是在通道内LRN，则表示求和的正方形区域长度。</span><br><span class="line">  optional uint32 local_size = 1 [default = 5];</span><br><span class="line">  // 归一化公式中的参数</span><br><span class="line">  optional float alpha = 2 [default = 1.];</span><br><span class="line">  optional float beta = 3 [default = 0.75];</span><br><span class="line">  enum NormRegion &#123;</span><br><span class="line">    ACROSS_CHANNELS = 0;</span><br><span class="line">    WITHIN_CHANNEL = 1;</span><br><span class="line">  &#125;</span><br><span class="line">  // 归一化的区域，分为通道内和跨通道两种</span><br><span class="line">  optional NormRegion norm_region = 4 [default = ACROSS_CHANNELS];</span><br><span class="line">  optional float k = 5 [default = 1.];</span><br><span class="line">  enum Engine &#123;</span><br><span class="line">    DEFAULT = 0;</span><br><span class="line">    CAFFE = 1;</span><br><span class="line">    CUDNN = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 与前面的engine是一样的</span><br><span class="line">  optional Engine engine = 6 [default = DEFAULT];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 内存数据层参数</span><br><span class="line">message MemoryDataParameter &#123;</span><br><span class="line">  // 训练的batch_size</span><br><span class="line">  optional uint32 batch_size = 1;</span><br><span class="line">  // 图像通道数</span><br><span class="line">  optional uint32 channels = 2;</span><br><span class="line">  // 图像高度</span><br><span class="line">  optional uint32 height = 3;</span><br><span class="line">  // 图像宽度</span><br><span class="line">  optional uint32 width = 4;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// mean-variance normalization层参数</span><br><span class="line">message MVNParameter &#123;</span><br><span class="line">  // This parameter can be set to false to normalize mean only</span><br><span class="line">  // 是否对方差进行归一化</span><br><span class="line">  optional bool normalize_variance = 1 [default = true];</span><br><span class="line"></span><br><span class="line">  // This parameter can be set to true to perform DNN-like MVN</span><br><span class="line">  // 是否进行跨通道的MVN</span><br><span class="line">  optional bool across_channels = 2 [default = false];</span><br><span class="line"></span><br><span class="line">  // Epsilon for not dividing by zero while normalizing variance</span><br><span class="line">  // 避免除数为0，与前面的一样</span><br><span class="line">  optional float eps = 3 [default = 1e-9];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 参数层参数</span><br><span class="line">message ParameterParameter &#123;</span><br><span class="line">  // 用户自己定义的shape</span><br><span class="line">  optional BlobShape shape = 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 池化层参数</span><br><span class="line">message PoolingParameter &#123;</span><br><span class="line">  enum PoolMethod &#123;</span><br><span class="line">    MAX = 0;</span><br><span class="line">    AVE = 1;</span><br><span class="line">    STOCHASTIC = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 池化的方式</span><br><span class="line">  optional PoolMethod pool = 1 [default = MAX]; // The pooling method</span><br><span class="line">  // Pad, kernel size, and stride are all given as a single value for equal</span><br><span class="line">  // dimensions in height and width or as Y, X pairs.</span><br><span class="line">  // padding的大小</span><br><span class="line">  optional uint32 pad = 4 [default = 0]; // The padding size (equal in Y, X)</span><br><span class="line">  // padding的高度</span><br><span class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height</span><br><span class="line">  // padding的宽度</span><br><span class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width</span><br><span class="line">  // 池化的核大小</span><br><span class="line">  optional uint32 kernel_size = 2; // The kernel size (square)</span><br><span class="line">  // 核高度</span><br><span class="line">  optional uint32 kernel_h = 5; // The kernel height</span><br><span class="line">  // 核宽度</span><br><span class="line">  optional uint32 kernel_w = 6; // The kernel width</span><br><span class="line">  // 池化的步长</span><br><span class="line">  optional uint32 stride = 3 [default = 1]; // The stride (equal in Y, X)</span><br><span class="line">  // 步长的高度</span><br><span class="line">  optional uint32 stride_h = 7; // The stride height</span><br><span class="line">  // 步长的宽度</span><br><span class="line">  optional uint32 stride_w = 8; // The stride width</span><br><span class="line">  enum Engine &#123;</span><br><span class="line">    DEFAULT = 0;</span><br><span class="line">    CAFFE = 1;</span><br><span class="line">    CUDNN = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 执行池化操作的类型，与前面的一样</span><br><span class="line">  optional Engine engine = 11 [default = DEFAULT];</span><br><span class="line">  // If global_pooling then it will pool over the size of the bottom by doing</span><br><span class="line">  // kernel_h = bottom-&gt;height and kernel_w = bottom-&gt;width</span><br><span class="line">  // global_pooling是对多个通道进行pooling，例如从三通道pooling为单通道</span><br><span class="line">  optional bool global_pooling = 12 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Power层参数</span><br><span class="line">message PowerParameter &#123;</span><br><span class="line">  // PowerLayer computes outputs y = (shift + scale * x) ^ power.</span><br><span class="line">  // Power的计算公式为y = (shift + scale * x) ^ power，下面是公式中的参数</span><br><span class="line">  optional float power = 1 [default = 1.0];</span><br><span class="line">  optional float scale = 2 [default = 1.0];</span><br><span class="line">  optional float shift = 3 [default = 0.0];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// python layer参数，在faster rcnn中有应用</span><br><span class="line">message PythonParameter &#123;</span><br><span class="line">  // python模块名称</span><br><span class="line">  optional string module = 1;</span><br><span class="line">  // python模块中层的名字，即类名</span><br><span class="line">  optional string layer = 2;</span><br><span class="line">  // This value is set to the attribute `param_str` of the `PythonLayer` object</span><br><span class="line">  // in Python before calling the `setup()` method. This could be a number,</span><br><span class="line">  // string, dictionary in Python dict format, JSON, etc. You may parse this</span><br><span class="line">  // string in `setup` method and use it in `forward` and `backward`.</span><br><span class="line">  // 可以用来设置参数，key-value形式，可以参考faster rcnn中模型的train.prototxt</span><br><span class="line">  optional string param_str = 3 [default = &apos;&apos;];</span><br><span class="line">  // Whether this PythonLayer is shared among worker solvers during data parallelism.</span><br><span class="line">  // If true, each worker solver sequentially run forward from this layer.</span><br><span class="line">  // This value should be set true if you are using it as a data layer.</span><br><span class="line">  // 是否需要在并行时共享layer</span><br><span class="line">  optional bool share_in_parallel = 4 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by RecurrentLayer</span><br><span class="line">// Recurrent层参数</span><br><span class="line">message RecurrentParameter &#123;</span><br><span class="line">  // The dimension of the output (and usually hidden state) representation --</span><br><span class="line">  // must be explicitly set to non-zero.</span><br><span class="line">  // Recurrent层的输出——必须非零</span><br><span class="line">  optional uint32 num_output = 1 [default = 0];</span><br><span class="line">  // 权重初始化，随机生成初始化</span><br><span class="line">  optional FillerParameter weight_filler = 2; // The filler for the weight</span><br><span class="line">  // 偏置初始化，随机生成</span><br><span class="line">  optional FillerParameter bias_filler = 3; // The filler for the bias</span><br><span class="line"></span><br><span class="line">  // Whether to enable displaying debug_info in the unrolled recurrent net.</span><br><span class="line">  // 是否输出调试信息</span><br><span class="line">  optional bool debug_info = 4 [default = false];</span><br><span class="line"></span><br><span class="line">  // Whether to add as additional inputs (bottoms) the initial hidden state</span><br><span class="line">  // blobs, and add as additional outputs (tops) the final timestep hidden state</span><br><span class="line">  // blobs.  The number of additional bottom/top blobs required depends on the</span><br><span class="line">  // recurrent architecture -- e.g., 1 for RNNs, 2 for LSTMs.</span><br><span class="line">  // 是否添加额外的输入</span><br><span class="line">  optional bool expose_hidden = 5 [default = false];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by ReductionLayer</span><br><span class="line">// Reduction层参数</span><br><span class="line">message ReductionParameter &#123;</span><br><span class="line">  enum ReductionOp &#123;</span><br><span class="line">    SUM = 1;</span><br><span class="line">    ASUM = 2;</span><br><span class="line">    SUMSQ = 3;</span><br><span class="line">    MEAN = 4;</span><br><span class="line">  &#125;</span><br><span class="line">  // 通过reduction操作来将数据减少到一维，可以通过上面的四种方式</span><br><span class="line">  optional ReductionOp operation = 1 [default = SUM]; // reduction operation</span><br><span class="line"></span><br><span class="line">  // The first axis to reduce to a scalar -- may be negative to index from the</span><br><span class="line">  // end (e.g., -1 for the last axis).</span><br><span class="line">  // (Currently, only reduction along ALL &quot;tail&quot; axes is supported; reduction</span><br><span class="line">  // of axis M through N, where N &lt; num_axes - 1, is unsupported.)</span><br><span class="line">  // Suppose we have an n-axis bottom Blob with shape:</span><br><span class="line">  //     (d0, d1, d2, ..., d(m-1), dm, d(m+1), ..., d(n-1)).</span><br><span class="line">  // If axis == m, the output Blob will have shape</span><br><span class="line">  //     (d0, d1, d2, ..., d(m-1)),</span><br><span class="line">  // and the ReductionOp operation is performed (d0 * d1 * d2 * ... * d(m-1))</span><br><span class="line">  // times, each including (dm * d(m+1) * ... * d(n-1)) individual data.</span><br><span class="line">  // If axis == 0 (the default), the output Blob always has the empty shape</span><br><span class="line">  // (count 1), performing reduction across the entire input --</span><br><span class="line">  // often useful for creating new loss functions.</span><br><span class="line">  // 在哪个轴上执行reduction操作</span><br><span class="line">  optional int32 axis = 2 [default = 0];</span><br><span class="line">  // 输出系数</span><br><span class="line">  optional float coeff = 3 [default = 1.0]; // coefficient for output</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by ReLULayer</span><br><span class="line">// ReLU层参数</span><br><span class="line">message ReLUParameter &#123;</span><br><span class="line">  // Allow non-zero slope for negative inputs to speed up optimization</span><br><span class="line">  // Described in:</span><br><span class="line">  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities</span><br><span class="line">  // improve neural network acoustic models. In ICML Workshop on Deep Learning</span><br><span class="line">  // for Audio, Speech, and Language Processing.</span><br><span class="line">  // ReLUU操作的阈值</span><br><span class="line">  optional float negative_slope = 1 [default = 0];</span><br><span class="line">  enum Engine &#123;</span><br><span class="line">    DEFAULT = 0;</span><br><span class="line">    CAFFE = 1;</span><br><span class="line">    CUDNN = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 执行ReLU操作的类型，与前面的一样</span><br><span class="line">  optional Engine engine = 2 [default = DEFAULT];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Reshape层参数，与numpy中的Reshape作用是一样的</span><br><span class="line">message ReshapeParameter &#123;</span><br><span class="line">  // Specify the output dimensions. If some of the dimensions are set to 0,</span><br><span class="line">  // the corresponding dimension from the bottom layer is used (unchanged).</span><br><span class="line">  // Exactly one dimension may be set to -1, in which case its value is</span><br><span class="line">  // inferred from the count of the bottom blob and the remaining dimensions.</span><br><span class="line">  // For example, suppose we want to reshape a 2D blob &quot;input&quot; with shape 2 x 8:</span><br><span class="line">  //</span><br><span class="line">  //   layer &#123;</span><br><span class="line">  //     type: &quot;Reshape&quot; bottom: &quot;input&quot; top: &quot;output&quot;</span><br><span class="line">  //     reshape_param &#123; ... &#125;</span><br><span class="line">  //   &#125;</span><br><span class="line">  //</span><br><span class="line">  // If &quot;input&quot; is 2D with shape 2 x 8, then the following reshape_param</span><br><span class="line">  // specifications are all equivalent, producing a 3D blob &quot;output&quot; with shape</span><br><span class="line">  // 2 x 2 x 4:</span><br><span class="line">  //</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim:  2  dim: 2  dim:  4 &#125; &#125;</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim:  4 &#125; &#125;</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim: -1 &#125; &#125;</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim:-1  dim:  4 &#125; &#125;</span><br><span class="line">  // reshape之后输出的维度</span><br><span class="line">  optional BlobShape shape = 1;</span><br><span class="line"></span><br><span class="line">  // axis and num_axes control the portion of the bottom blob&apos;s shape that are</span><br><span class="line">  // replaced by (included in) the reshape. By default (axis == 0 and</span><br><span class="line">  // num_axes == -1), the entire bottom blob shape is included in the reshape,</span><br><span class="line">  // and hence the shape field must specify the entire output shape.</span><br><span class="line">  //</span><br><span class="line">  // axis may be non-zero to retain some portion of the beginning of the input</span><br><span class="line">  // shape (and may be negative to index from the end; e.g., -1 to begin the</span><br><span class="line">  // reshape after the last axis, including nothing in the reshape,</span><br><span class="line">  // -2 to include only the last axis, etc.).</span><br><span class="line">  //</span><br><span class="line">  // For example, suppose &quot;input&quot; is a 2D blob with shape 2 x 8.</span><br><span class="line">  // Then the following ReshapeLayer specifications are all equivalent,</span><br><span class="line">  // producing a blob &quot;output&quot; with shape 2 x 2 x 4:</span><br><span class="line">  //</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 2  dim: 4 &#125; &#125;</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis:  1 &#125;</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis: -3 &#125;</span><br><span class="line">  //</span><br><span class="line">  // num_axes specifies the extent of the reshape.</span><br><span class="line">  // If num_axes &gt;= 0 (and axis &gt;= 0), the reshape will be performed only on</span><br><span class="line">  // input axes in the range [axis, axis+num_axes].</span><br><span class="line">  // num_axes may also be -1, the default, to include all remaining axes</span><br><span class="line">  // (starting from axis).</span><br><span class="line">  //</span><br><span class="line">  // For example, suppose &quot;input&quot; is a 2D blob with shape 2 x 8.</span><br><span class="line">  // Then the following ReshapeLayer specifications are equivalent,</span><br><span class="line">  // producing a blob &quot;output&quot; with shape 1 x 2 x 8.</span><br><span class="line">  //</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  dim:  8 &#125; &#125;</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  &#125;  num_axes: 1 &#125;</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim:  1  &#125;  num_axes: 0 &#125;</span><br><span class="line">  //</span><br><span class="line">  // On the other hand, these would produce output blob shape 2 x 1 x 8:</span><br><span class="line">  //</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 1  dim: 8  &#125;  &#125;</span><br><span class="line">  //   reshape_param &#123; shape &#123; dim: 1 &#125;  axis: 1  num_axes: 0 &#125;</span><br><span class="line"></span><br><span class="line">  optional int32 axis = 2 [default = 0];</span><br><span class="line">  optional int32 num_axes = 3 [default = -1];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Scale层参数，与batch norm layer配合使用，可参考Resnet结构</span><br><span class="line">message ScaleParameter &#123;</span><br><span class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</span><br><span class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</span><br><span class="line">  // (e.g., -1 for the last axis).</span><br><span class="line">  //</span><br><span class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</span><br><span class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</span><br><span class="line">  // following shapes (for the given value of axis):</span><br><span class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</span><br><span class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</span><br><span class="line">  //    (axis == 2 == -2)                   40;       40x60</span><br><span class="line">  //    (axis == 3 == -1)                                60</span><br><span class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</span><br><span class="line">  // &quot;axis&quot;) -- a scalar multiplier.</span><br><span class="line">  optional int32 axis = 1 [default = 1];</span><br><span class="line"></span><br><span class="line">  // (num_axes is ignored unless just one bottom is given and the scale is</span><br><span class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</span><br><span class="line">  // number of axes by the second bottom.)</span><br><span class="line">  // The number of axes of the input (bottom[0]) covered by the scale</span><br><span class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</span><br><span class="line">  // Set num_axes := 0, to multiply with a zero-axis Blob: a scalar.</span><br><span class="line">  optional int32 num_axes = 2 [default = 1];</span><br><span class="line"></span><br><span class="line">  // (filler is ignored unless just one bottom is given and the scale is</span><br><span class="line">  // a learned parameter of the layer.)</span><br><span class="line">  // The initialization for the learned scale parameter.</span><br><span class="line">  // Default is the unit (1) initialization, resulting in the ScaleLayer</span><br><span class="line">  // initially performing the identity operation.</span><br><span class="line">  optional FillerParameter filler = 3;</span><br><span class="line"></span><br><span class="line">  // Whether to also learn a bias (equivalent to a ScaleLayer+BiasLayer, but</span><br><span class="line">  // may be more efficient).  Initialized with bias_filler (defaults to 0).</span><br><span class="line">  // 是否使用偏置项</span><br><span class="line">  optional bool bias_term = 4 [default = false];</span><br><span class="line">  // 偏置项初始化</span><br><span class="line">  optional FillerParameter bias_filler = 5;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Sigmoid层参数</span><br><span class="line">message SigmoidParameter &#123;</span><br><span class="line">  enum Engine &#123;</span><br><span class="line">    DEFAULT = 0;</span><br><span class="line">    CAFFE = 1;</span><br><span class="line">    CUDNN = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 使用哪种sigmoid实现</span><br><span class="line">  optional Engine engine = 1 [default = DEFAULT];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Slice层参数</span><br><span class="line">message SliceParameter &#123;</span><br><span class="line">  // The axis along which to slice -- may be negative to index from the end</span><br><span class="line">  // (e.g., -1 for the last axis).</span><br><span class="line">  // By default, SliceLayer concatenates blobs along the &quot;channels&quot; axis (1).</span><br><span class="line">  // 在哪个维度上进行拆分</span><br><span class="line">  optional int32 axis = 3 [default = 1];</span><br><span class="line">  // 指定拆分点</span><br><span class="line">  repeated uint32 slice_point = 2;</span><br><span class="line"></span><br><span class="line">  // DEPRECATED: alias for &quot;axis&quot; -- does not support negative indexing.</span><br><span class="line">  // 已废弃。</span><br><span class="line">  optional uint32 slice_dim = 1 [default = 1];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer</span><br><span class="line">// Softmax层参数</span><br><span class="line">message SoftmaxParameter &#123;</span><br><span class="line">  enum Engine &#123;</span><br><span class="line">    DEFAULT = 0;</span><br><span class="line">    CAFFE = 1;</span><br><span class="line">    CUDNN = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 使用哪种softmax实现</span><br><span class="line">  optional Engine engine = 1 [default = DEFAULT];</span><br><span class="line"></span><br><span class="line">  // The axis along which to perform the softmax -- may be negative to index</span><br><span class="line">  // from the end (e.g., -1 for the last axis).</span><br><span class="line">  // Any other axes will be evaluated as independent softmaxes.</span><br><span class="line">  // 在哪个维度上进行softmax</span><br><span class="line">  optional int32 axis = 2 [default = 1];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// TanH层参数</span><br><span class="line">message TanHParameter &#123;</span><br><span class="line">  enum Engine &#123;</span><br><span class="line">    DEFAULT = 0;</span><br><span class="line">    CAFFE = 1;</span><br><span class="line">    CUDNN = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 执行tanh激活函数的类型</span><br><span class="line">  optional Engine engine = 1 [default = DEFAULT];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by TileLayer</span><br><span class="line">// Tile层参数，扩大某一维度</span><br><span class="line">message TileParameter &#123;</span><br><span class="line">  // The index of the axis to tile.</span><br><span class="line">  // 扩大哪个维度</span><br><span class="line">  optional int32 axis = 1 [default = 1];</span><br><span class="line"></span><br><span class="line">  // The number of copies (tiles) of the blob to output.</span><br><span class="line">  // 创建多少个副本</span><br><span class="line">  optional int32 tiles = 2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Message that stores parameters used by ThresholdLayer</span><br><span class="line">// Threshold层参数，主要用来测试输入是否超过阈值</span><br><span class="line">message ThresholdParameter &#123;</span><br><span class="line">  // 设置阈值</span><br><span class="line">  optional float threshold = 1 [default = 0]; // Strictly positive values</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// WindowData层参数</span><br><span class="line">message WindowDataParameter &#123;</span><br><span class="line">  // Specify the data source.</span><br><span class="line">  // 指定数据源</span><br><span class="line">  optional string source = 1;</span><br><span class="line">  // For data pre-processing, we can do simple scaling and subtracting the</span><br><span class="line">  // data mean, if provided. Note that the mean subtraction is always carried</span><br><span class="line">  // out before scaling.</span><br><span class="line">  // 是否归一化</span><br><span class="line">  optional float scale = 2 [default = 1];</span><br><span class="line">  // 图像均值文件</span><br><span class="line">  optional string mean_file = 3;</span><br><span class="line">  // Specify the batch size.</span><br><span class="line">  // 训练的batch_size</span><br><span class="line">  optional uint32 batch_size = 4;</span><br><span class="line">  // Specify if we would like to randomly crop an image.</span><br><span class="line">  // 是否随机crop</span><br><span class="line">  optional uint32 crop_size = 5 [default = 0];</span><br><span class="line">  // Specify if we want to randomly mirror data.</span><br><span class="line">  // 是否随机mirror</span><br><span class="line">  optional bool mirror = 6 [default = false];</span><br><span class="line">  // Foreground (object) overlap threshold</span><br><span class="line">  // 前景重叠阈值</span><br><span class="line">  optional float fg_threshold = 7 [default = 0.5];</span><br><span class="line">  // Background (non-object) overlap threshold</span><br><span class="line">  // 背景重叠阈值</span><br><span class="line">  optional float bg_threshold = 8 [default = 0.5];</span><br><span class="line">  // Fraction of batch that should be foreground objects</span><br><span class="line">  // 前景比例</span><br><span class="line">  optional float fg_fraction = 9 [default = 0.25];</span><br><span class="line">  // Amount of contextual padding to add around a window</span><br><span class="line">  // (used only by the window_data_layer)</span><br><span class="line">  // 是否padding</span><br><span class="line">  optional uint32 context_pad = 10 [default = 0];</span><br><span class="line">  // Mode for cropping out a detection window</span><br><span class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</span><br><span class="line">  // square: the tightest square around the window is cropped</span><br><span class="line">  // crop的方式</span><br><span class="line">  optional string crop_mode = 11 [default = &quot;warp&quot;];</span><br><span class="line">  // cache_images: will load all images in memory for faster access</span><br><span class="line">  // 是否缓存图像，即将图像都转入内存</span><br><span class="line">  optional bool cache_images = 12 [default = false];</span><br><span class="line">  // append root_folder to locate images</span><br><span class="line">  // 图像文件的根目录</span><br><span class="line">  optional string root_folder = 13 [default = &quot;&quot;];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// SPP层参数，SPP是spatial pyramid pooling，空间金字塔池化，具体可参考何凯明论文Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</span><br><span class="line">message SPPParameter &#123;</span><br><span class="line">  enum PoolMethod &#123;</span><br><span class="line">    MAX = 0;</span><br><span class="line">    AVE = 1;</span><br><span class="line">    STOCHASTIC = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 空间金字塔高度</span><br><span class="line">  optional uint32 pyramid_height = 1;</span><br><span class="line">  // 池化方法</span><br><span class="line">  optional PoolMethod pool = 2 [default = MAX]; // The pooling method</span><br><span class="line">  enum Engine &#123;</span><br><span class="line">    DEFAULT = 0;</span><br><span class="line">    CAFFE = 1;</span><br><span class="line">    CUDNN = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  // 执行SPP的方式</span><br><span class="line">  optional Engine engine = 6 [default = DEFAULT];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// DEPRECATED: use LayerParameter.</span><br><span class="line">// 已废弃，使用LayerParameter。</span><br><span class="line">message V1LayerParameter &#123;</span><br><span class="line">  repeated string bottom = 2;</span><br><span class="line">  repeated string top = 3;</span><br><span class="line">  optional string name = 4;</span><br><span class="line">  repeated NetStateRule include = 32;</span><br><span class="line">  repeated NetStateRule exclude = 33;</span><br><span class="line">  enum LayerType &#123;</span><br><span class="line">    NONE = 0;</span><br><span class="line">    ABSVAL = 35;</span><br><span class="line">    ACCURACY = 1;</span><br><span class="line">    ARGMAX = 30;</span><br><span class="line">    BNLL = 2;</span><br><span class="line">    CONCAT = 3;</span><br><span class="line">    CONTRASTIVE_LOSS = 37;</span><br><span class="line">    CONVOLUTION = 4;</span><br><span class="line">    DATA = 5;</span><br><span class="line">    DECONVOLUTION = 39;</span><br><span class="line">    DROPOUT = 6;</span><br><span class="line">    DUMMY_DATA = 32;</span><br><span class="line">    EUCLIDEAN_LOSS = 7;</span><br><span class="line">    ELTWISE = 25;</span><br><span class="line">    EXP = 38;</span><br><span class="line">    FLATTEN = 8;</span><br><span class="line">    HDF5_DATA = 9;</span><br><span class="line">    HDF5_OUTPUT = 10;</span><br><span class="line">    HINGE_LOSS = 28;</span><br><span class="line">    IM2COL = 11;</span><br><span class="line">    IMAGE_DATA = 12;</span><br><span class="line">    INFOGAIN_LOSS = 13;</span><br><span class="line">    INNER_PRODUCT = 14;</span><br><span class="line">    LRN = 15;</span><br><span class="line">    MEMORY_DATA = 29;</span><br><span class="line">    MULTINOMIAL_LOGISTIC_LOSS = 16;</span><br><span class="line">    MVN = 34;</span><br><span class="line">    POOLING = 17;</span><br><span class="line">    POWER = 26;</span><br><span class="line">    RELU = 18;</span><br><span class="line">    SIGMOID = 19;</span><br><span class="line">    SIGMOID_CROSS_ENTROPY_LOSS = 27;</span><br><span class="line">    SILENCE = 36;</span><br><span class="line">    SOFTMAX = 20;</span><br><span class="line">    SOFTMAX_LOSS = 21;</span><br><span class="line">    SPLIT = 22;</span><br><span class="line">    SLICE = 33;</span><br><span class="line">    TANH = 23;</span><br><span class="line">    WINDOW_DATA = 24;</span><br><span class="line">    THRESHOLD = 31;</span><br><span class="line">  &#125;</span><br><span class="line">  optional LayerType type = 5;</span><br><span class="line">  repeated BlobProto blobs = 6;</span><br><span class="line">  repeated string param = 1001;</span><br><span class="line">  repeated DimCheckMode blob_share_mode = 1002;</span><br><span class="line">  enum DimCheckMode &#123;</span><br><span class="line">    STRICT = 0;</span><br><span class="line">    PERMISSIVE = 1;</span><br><span class="line">  &#125;</span><br><span class="line">  repeated float blobs_lr = 7;</span><br><span class="line">  repeated float weight_decay = 8;</span><br><span class="line">  repeated float loss_weight = 35;</span><br><span class="line">  optional AccuracyParameter accuracy_param = 27;</span><br><span class="line">  optional ArgMaxParameter argmax_param = 23;</span><br><span class="line">  optional ConcatParameter concat_param = 9;</span><br><span class="line">  optional ContrastiveLossParameter contrastive_loss_param = 40;</span><br><span class="line">  optional ConvolutionParameter convolution_param = 10;</span><br><span class="line">  optional DataParameter data_param = 11;</span><br><span class="line">  optional DropoutParameter dropout_param = 12;</span><br><span class="line">  optional DummyDataParameter dummy_data_param = 26;</span><br><span class="line">  optional EltwiseParameter eltwise_param = 24;</span><br><span class="line">  optional ExpParameter exp_param = 41;</span><br><span class="line">  optional HDF5DataParameter hdf5_data_param = 13;</span><br><span class="line">  optional HDF5OutputParameter hdf5_output_param = 14;</span><br><span class="line">  optional HingeLossParameter hinge_loss_param = 29;</span><br><span class="line">  optional ImageDataParameter image_data_param = 15;</span><br><span class="line">  optional InfogainLossParameter infogain_loss_param = 16;</span><br><span class="line">  optional InnerProductParameter inner_product_param = 17;</span><br><span class="line">  optional LRNParameter lrn_param = 18;</span><br><span class="line">  optional MemoryDataParameter memory_data_param = 22;</span><br><span class="line">  optional MVNParameter mvn_param = 34;</span><br><span class="line">  optional PoolingParameter pooling_param = 19;</span><br><span class="line">  optional PowerParameter power_param = 21;</span><br><span class="line">  optional ReLUParameter relu_param = 30;</span><br><span class="line">  optional SigmoidParameter sigmoid_param = 38;</span><br><span class="line">  optional SoftmaxParameter softmax_param = 39;</span><br><span class="line">  optional SliceParameter slice_param = 31;</span><br><span class="line">  optional TanHParameter tanh_param = 37;</span><br><span class="line">  optional ThresholdParameter threshold_param = 25;</span><br><span class="line">  optional WindowDataParameter window_data_param = 20;</span><br><span class="line">  optional TransformationParameter transform_param = 36;</span><br><span class="line">  optional LossParameter loss_param = 42;</span><br><span class="line">  optional V0LayerParameter layer = 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// DEPRECATED: V0LayerParameter is the old way of specifying layer parameters</span><br><span class="line">// in Caffe.  We keep this message type around for legacy support.</span><br><span class="line">// 已废弃。</span><br><span class="line">message V0LayerParameter &#123;</span><br><span class="line">  optional string name = 1; // the layer name</span><br><span class="line">  optional string type = 2; // the string to specify the layer type</span><br><span class="line"></span><br><span class="line">  // Parameters to specify layers with inner products.</span><br><span class="line">  optional uint32 num_output = 3; // The number of outputs for the layer</span><br><span class="line">  optional bool biasterm = 4 [default = true]; // whether to have bias terms</span><br><span class="line">  optional FillerParameter weight_filler = 5; // The filler for the weight</span><br><span class="line">  optional FillerParameter bias_filler = 6; // The filler for the bias</span><br><span class="line"></span><br><span class="line">  optional uint32 pad = 7 [default = 0]; // The padding size</span><br><span class="line">  optional uint32 kernelsize = 8; // The kernel size</span><br><span class="line">  optional uint32 group = 9 [default = 1]; // The group size for group conv</span><br><span class="line">  optional uint32 stride = 10 [default = 1]; // The stride</span><br><span class="line">  enum PoolMethod &#123;</span><br><span class="line">    MAX = 0;</span><br><span class="line">    AVE = 1;</span><br><span class="line">    STOCHASTIC = 2;</span><br><span class="line">  &#125;</span><br><span class="line">  optional PoolMethod pool = 11 [default = MAX]; // The pooling method</span><br><span class="line">  optional float dropout_ratio = 12 [default = 0.5]; // dropout ratio</span><br><span class="line"></span><br><span class="line">  optional uint32 local_size = 13 [default = 5]; // for local response norm</span><br><span class="line">  optional float alpha = 14 [default = 1.]; // for local response norm</span><br><span class="line">  optional float beta = 15 [default = 0.75]; // for local response norm</span><br><span class="line">  optional float k = 22 [default = 1.];</span><br><span class="line"></span><br><span class="line">  // For data layers, specify the data source</span><br><span class="line">  optional string source = 16;</span><br><span class="line">  // For data pre-processing, we can do simple scaling and subtracting the</span><br><span class="line">  // data mean, if provided. Note that the mean subtraction is always carried</span><br><span class="line">  // out before scaling.</span><br><span class="line">  optional float scale = 17 [default = 1];</span><br><span class="line">  optional string meanfile = 18;</span><br><span class="line">  // For data layers, specify the batch size.</span><br><span class="line">  optional uint32 batchsize = 19;</span><br><span class="line">  // For data layers, specify if we would like to randomly crop an image.</span><br><span class="line">  optional uint32 cropsize = 20 [default = 0];</span><br><span class="line">  // For data layers, specify if we want to randomly mirror data.</span><br><span class="line">  optional bool mirror = 21 [default = false];</span><br><span class="line"></span><br><span class="line">  // The blobs containing the numeric parameters of the layer</span><br><span class="line">  repeated BlobProto blobs = 50;</span><br><span class="line">  // The ratio that is multiplied on the global learning rate. If you want to</span><br><span class="line">  // set the learning ratio for one blob, you need to set it for all blobs.</span><br><span class="line">  repeated float blobs_lr = 51;</span><br><span class="line">  // The weight decay that is multiplied on the global weight decay.</span><br><span class="line">  repeated float weight_decay = 52;</span><br><span class="line"></span><br><span class="line">  // The rand_skip variable is for the data layer to skip a few data points</span><br><span class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</span><br><span class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</span><br><span class="line">  // be larger than the number of keys in the database.</span><br><span class="line">  optional uint32 rand_skip = 53 [default = 0];</span><br><span class="line"></span><br><span class="line">  // Fields related to detection (det_*)</span><br><span class="line">  // foreground (object) overlap threshold</span><br><span class="line">  optional float det_fg_threshold = 54 [default = 0.5];</span><br><span class="line">  // background (non-object) overlap threshold</span><br><span class="line">  optional float det_bg_threshold = 55 [default = 0.5];</span><br><span class="line">  // Fraction of batch that should be foreground objects</span><br><span class="line">  optional float det_fg_fraction = 56 [default = 0.25];</span><br><span class="line"></span><br><span class="line">  // optional bool OBSOLETE_can_clobber = 57 [default = true];</span><br><span class="line"></span><br><span class="line">  // Amount of contextual padding to add around a window</span><br><span class="line">  // (used only by the window_data_layer)</span><br><span class="line">  optional uint32 det_context_pad = 58 [default = 0];</span><br><span class="line"></span><br><span class="line">  // Mode for cropping out a detection window</span><br><span class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</span><br><span class="line">  // square: the tightest square around the window is cropped</span><br><span class="line">  optional string det_crop_mode = 59 [default = &quot;warp&quot;];</span><br><span class="line"></span><br><span class="line">  // For ReshapeLayer, one needs to specify the new dimensions.</span><br><span class="line">  optional int32 new_num = 60 [default = 0];</span><br><span class="line">  optional int32 new_channels = 61 [default = 0];</span><br><span class="line">  optional int32 new_height = 62 [default = 0];</span><br><span class="line">  optional int32 new_width = 63 [default = 0];</span><br><span class="line"></span><br><span class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</span><br><span class="line">  // It will also resize images if new_height or new_width are not zero.</span><br><span class="line">  optional bool shuffle_images = 64 [default = false];</span><br><span class="line"></span><br><span class="line">  // For ConcatLayer, one needs to specify the dimension for concatenation, and</span><br><span class="line">  // the other dimensions must be the same for all the bottom blobs.</span><br><span class="line">  // By default it will concatenate blobs along the channels dimension.</span><br><span class="line">  optional uint32 concat_dim = 65 [default = 1];</span><br><span class="line"></span><br><span class="line">  optional HDF5OutputParameter hdf5_output_param = 1001;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// PReLU层参数，ReLU的进化版本</span><br><span class="line">message PReLUParameter &#123;</span><br><span class="line">  // Parametric ReLU described in K. He et al, Delving Deep into Rectifiers:</span><br><span class="line">  // Surpassing Human-Level Performance on ImageNet Classification, 2015.</span><br><span class="line"></span><br><span class="line">  // Initial value of a_i. Default is a_i=0.25 for all i.</span><br><span class="line">  // 参数初始化</span><br><span class="line">  optional FillerParameter filler = 1;</span><br><span class="line">  // Whether or not slope parameters are shared across channels.</span><br><span class="line">  // 是否在各通道共享参数</span><br><span class="line">  optional bool channel_shared = 2 [default = false];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>如果有收获，可以请我喝杯咖啡！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="https://user-images.githubusercontent.com/21311442/54660728-7c650300-4b12-11e9-9b0a-1a5c09323afe.png" alt="Tyan WeChat Pay">
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="https://user-images.githubusercontent.com/21311442/54660740-87b82e80-4b12-11e9-96e4-911014779bdc.png" alt="Tyan Alipay">
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Caffe/" rel="tag"># Caffe</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/07/04/2017-07-04-AlexNet论文翻译/" rel="next" title="AlexNet论文翻译——中英文对照">
                <i class="fa fa-chevron-left"></i> AlexNet论文翻译——中英文对照
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/13/2017-07-13-vim使用总结/" rel="prev" title="Vim使用总结">
                Vim使用总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="Tyan">
          <p class="site-author-name" itemprop="name">Tyan</p>
           
              <p class="site-description motion-element" itemprop="description">工作中的技术总结</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">851</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tyan</span>
</div>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  
    <script id="dsq-count-scr" src="https://snailtyan.disqus.com/count.js" async></script>
  

  
    <script type="text/javascript">
      var disqus_config = function () {
        this.page.url = 'http://noahsnail.com/2017/07/11/2017-07-12-Caffe源码解析(一)——caffe.proto/';
        this.page.identifier = '2017/07/11/2017-07-12-Caffe源码解析(一)——caffe.proto/';
        this.page.title = 'Caffe源码解析(一)——caffe.proto';
      };
      var d = document, s = d.createElement('script');
      s.src = 'https://snailtyan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    </script>
  




	





  








  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
