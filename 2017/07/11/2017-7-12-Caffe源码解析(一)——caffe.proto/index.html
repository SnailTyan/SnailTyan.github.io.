<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Caffe," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Caffe源码解析(一)——caffe.proto">
<meta property="og:type" content="article">
<meta property="og:title" content="Caffe源码解析(一)——caffe.proto">
<meta property="og:url" content="noahsnail.com/2017/07/11/2017-7-12-Caffe源码解析(一)——caffe.proto/index.html">
<meta property="og:site_name" content="SnailTyan">
<meta property="og:description" content="Caffe源码解析(一)——caffe.proto">
<meta property="og:updated_time" content="2017-07-13T07:14:29.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Caffe源码解析(一)——caffe.proto">
<meta name="twitter:description" content="Caffe源码解析(一)——caffe.proto">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 'undefined',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: "",
      labels: ""
    }
  };
</script>



  <link rel="canonical" href="noahsnail.com/2017/07/11/2017-7-12-Caffe源码解析(一)——caffe.proto/"/>





  <title>Caffe源码解析(一)——caffe.proto | SnailTyan</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-83591315-1', 'auto');
  ga('send', 'pageview');
</script>











</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SnailTyan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="noahsnail.com/2017/07/11/2017-7-12-Caffe源码解析(一)——caffe.proto/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Tyan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SnailTyan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Caffe源码解析(一)——caffe.proto</h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          
              <div class="post-description">
                  Caffe源码解析(一)——caffe.proto
              </div>
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>文章作者：Tyan<br>博客：<a href="http://noahsnail.com">noahsnail.com</a> &nbsp;|&nbsp; <a href="http://blog.csdn.net/quincuntial" target="_blank" rel="external">CSDN</a> &nbsp;|&nbsp; <a href="http://www.jianshu.com/users/7731e83f3a4e/latest_articles" target="_blank" rel="external">简书</a></p>
<p>caffe.proto是caffe数据结构定义的主要文件，本文主要是在caffe.proto代码的基础上加上了部分中文注释，其中的内容与caffe的prototxt文件中的结构相对应。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div><div class="line">488</div><div class="line">489</div><div class="line">490</div><div class="line">491</div><div class="line">492</div><div class="line">493</div><div class="line">494</div><div class="line">495</div><div class="line">496</div><div class="line">497</div><div class="line">498</div><div class="line">499</div><div class="line">500</div><div class="line">501</div><div class="line">502</div><div class="line">503</div><div class="line">504</div><div class="line">505</div><div class="line">506</div><div class="line">507</div><div class="line">508</div><div class="line">509</div><div class="line">510</div><div class="line">511</div><div class="line">512</div><div class="line">513</div><div class="line">514</div><div class="line">515</div><div class="line">516</div><div class="line">517</div><div class="line">518</div><div class="line">519</div><div class="line">520</div><div class="line">521</div><div class="line">522</div><div class="line">523</div><div class="line">524</div><div class="line">525</div><div class="line">526</div><div class="line">527</div><div class="line">528</div><div class="line">529</div><div class="line">530</div><div class="line">531</div><div class="line">532</div><div class="line">533</div><div class="line">534</div><div class="line">535</div><div class="line">536</div><div class="line">537</div><div class="line">538</div><div class="line">539</div><div class="line">540</div><div class="line">541</div><div class="line">542</div><div class="line">543</div><div class="line">544</div><div class="line">545</div><div class="line">546</div><div class="line">547</div><div class="line">548</div><div class="line">549</div><div class="line">550</div><div class="line">551</div><div class="line">552</div><div class="line">553</div><div class="line">554</div><div class="line">555</div><div class="line">556</div><div class="line">557</div><div class="line">558</div><div class="line">559</div><div class="line">560</div><div class="line">561</div><div class="line">562</div><div class="line">563</div><div class="line">564</div><div class="line">565</div><div class="line">566</div><div class="line">567</div><div class="line">568</div><div class="line">569</div><div class="line">570</div><div class="line">571</div><div class="line">572</div><div class="line">573</div><div class="line">574</div><div class="line">575</div><div class="line">576</div><div class="line">577</div><div class="line">578</div><div class="line">579</div><div class="line">580</div><div class="line">581</div><div class="line">582</div><div class="line">583</div><div class="line">584</div><div class="line">585</div><div class="line">586</div><div class="line">587</div><div class="line">588</div><div class="line">589</div><div class="line">590</div><div class="line">591</div><div class="line">592</div><div class="line">593</div><div class="line">594</div><div class="line">595</div><div class="line">596</div><div class="line">597</div><div class="line">598</div><div class="line">599</div><div class="line">600</div><div class="line">601</div><div class="line">602</div><div class="line">603</div><div class="line">604</div><div class="line">605</div><div class="line">606</div><div class="line">607</div><div class="line">608</div><div class="line">609</div><div class="line">610</div><div class="line">611</div><div class="line">612</div><div class="line">613</div><div class="line">614</div><div class="line">615</div><div class="line">616</div><div class="line">617</div><div class="line">618</div><div class="line">619</div><div class="line">620</div><div class="line">621</div><div class="line">622</div><div class="line">623</div><div class="line">624</div><div class="line">625</div><div class="line">626</div><div class="line">627</div><div class="line">628</div><div class="line">629</div><div class="line">630</div><div class="line">631</div><div class="line">632</div><div class="line">633</div><div class="line">634</div><div class="line">635</div><div class="line">636</div><div class="line">637</div><div class="line">638</div><div class="line">639</div><div class="line">640</div><div class="line">641</div><div class="line">642</div><div class="line">643</div><div class="line">644</div><div class="line">645</div><div class="line">646</div><div class="line">647</div><div class="line">648</div><div class="line">649</div><div class="line">650</div><div class="line">651</div><div class="line">652</div><div class="line">653</div><div class="line">654</div><div class="line">655</div><div class="line">656</div><div class="line">657</div><div class="line">658</div><div class="line">659</div><div class="line">660</div><div class="line">661</div><div class="line">662</div><div class="line">663</div><div class="line">664</div><div class="line">665</div><div class="line">666</div><div class="line">667</div><div class="line">668</div><div class="line">669</div><div class="line">670</div><div class="line">671</div><div class="line">672</div><div class="line">673</div><div class="line">674</div><div class="line">675</div><div class="line">676</div><div class="line">677</div><div class="line">678</div><div class="line">679</div><div class="line">680</div><div class="line">681</div><div class="line">682</div><div class="line">683</div><div class="line">684</div><div class="line">685</div><div class="line">686</div><div class="line">687</div><div class="line">688</div><div class="line">689</div><div class="line">690</div><div class="line">691</div><div class="line">692</div><div class="line">693</div><div class="line">694</div><div class="line">695</div><div class="line">696</div><div class="line">697</div><div class="line">698</div><div class="line">699</div><div class="line">700</div><div class="line">701</div><div class="line">702</div><div class="line">703</div><div class="line">704</div><div class="line">705</div><div class="line">706</div><div class="line">707</div><div class="line">708</div><div class="line">709</div><div class="line">710</div><div class="line">711</div><div class="line">712</div><div class="line">713</div><div class="line">714</div><div class="line">715</div><div class="line">716</div><div class="line">717</div><div class="line">718</div><div class="line">719</div><div class="line">720</div><div class="line">721</div><div class="line">722</div><div class="line">723</div><div class="line">724</div><div class="line">725</div><div class="line">726</div><div class="line">727</div><div class="line">728</div><div class="line">729</div><div class="line">730</div><div class="line">731</div><div class="line">732</div><div class="line">733</div><div class="line">734</div><div class="line">735</div><div class="line">736</div><div class="line">737</div><div class="line">738</div><div class="line">739</div><div class="line">740</div><div class="line">741</div><div class="line">742</div><div class="line">743</div><div class="line">744</div><div class="line">745</div><div class="line">746</div><div class="line">747</div><div class="line">748</div><div class="line">749</div><div class="line">750</div><div class="line">751</div><div class="line">752</div><div class="line">753</div><div class="line">754</div><div class="line">755</div><div class="line">756</div><div class="line">757</div><div class="line">758</div><div class="line">759</div><div class="line">760</div><div class="line">761</div><div class="line">762</div><div class="line">763</div><div class="line">764</div><div class="line">765</div><div class="line">766</div><div class="line">767</div><div class="line">768</div><div class="line">769</div><div class="line">770</div><div class="line">771</div><div class="line">772</div><div class="line">773</div><div class="line">774</div><div class="line">775</div><div class="line">776</div><div class="line">777</div><div class="line">778</div><div class="line">779</div><div class="line">780</div><div class="line">781</div><div class="line">782</div><div class="line">783</div><div class="line">784</div><div class="line">785</div><div class="line">786</div><div class="line">787</div><div class="line">788</div><div class="line">789</div><div class="line">790</div><div class="line">791</div><div class="line">792</div><div class="line">793</div><div class="line">794</div><div class="line">795</div><div class="line">796</div><div class="line">797</div><div class="line">798</div><div class="line">799</div><div class="line">800</div><div class="line">801</div><div class="line">802</div><div class="line">803</div><div class="line">804</div><div class="line">805</div><div class="line">806</div><div class="line">807</div><div class="line">808</div><div class="line">809</div><div class="line">810</div><div class="line">811</div><div class="line">812</div><div class="line">813</div><div class="line">814</div><div class="line">815</div><div class="line">816</div><div class="line">817</div><div class="line">818</div><div class="line">819</div><div class="line">820</div><div class="line">821</div><div class="line">822</div><div class="line">823</div><div class="line">824</div><div class="line">825</div><div class="line">826</div><div class="line">827</div><div class="line">828</div><div class="line">829</div><div class="line">830</div><div class="line">831</div><div class="line">832</div><div class="line">833</div><div class="line">834</div><div class="line">835</div><div class="line">836</div><div class="line">837</div><div class="line">838</div><div class="line">839</div><div class="line">840</div><div class="line">841</div><div class="line">842</div><div class="line">843</div><div class="line">844</div><div class="line">845</div><div class="line">846</div><div class="line">847</div><div class="line">848</div><div class="line">849</div><div class="line">850</div><div class="line">851</div><div class="line">852</div><div class="line">853</div><div class="line">854</div><div class="line">855</div><div class="line">856</div><div class="line">857</div><div class="line">858</div><div class="line">859</div><div class="line">860</div><div class="line">861</div><div class="line">862</div><div class="line">863</div><div class="line">864</div><div class="line">865</div><div class="line">866</div><div class="line">867</div><div class="line">868</div><div class="line">869</div><div class="line">870</div><div class="line">871</div><div class="line">872</div><div class="line">873</div><div class="line">874</div><div class="line">875</div><div class="line">876</div><div class="line">877</div><div class="line">878</div><div class="line">879</div><div class="line">880</div><div class="line">881</div><div class="line">882</div><div class="line">883</div><div class="line">884</div><div class="line">885</div><div class="line">886</div><div class="line">887</div><div class="line">888</div><div class="line">889</div><div class="line">890</div><div class="line">891</div><div class="line">892</div><div class="line">893</div><div class="line">894</div><div class="line">895</div><div class="line">896</div><div class="line">897</div><div class="line">898</div><div class="line">899</div><div class="line">900</div><div class="line">901</div><div class="line">902</div><div class="line">903</div><div class="line">904</div><div class="line">905</div><div class="line">906</div><div class="line">907</div><div class="line">908</div><div class="line">909</div><div class="line">910</div><div class="line">911</div><div class="line">912</div><div class="line">913</div><div class="line">914</div><div class="line">915</div><div class="line">916</div><div class="line">917</div><div class="line">918</div><div class="line">919</div><div class="line">920</div><div class="line">921</div><div class="line">922</div><div class="line">923</div><div class="line">924</div><div class="line">925</div><div class="line">926</div><div class="line">927</div><div class="line">928</div><div class="line">929</div><div class="line">930</div><div class="line">931</div><div class="line">932</div><div class="line">933</div><div class="line">934</div><div class="line">935</div><div class="line">936</div><div class="line">937</div><div class="line">938</div><div class="line">939</div><div class="line">940</div><div class="line">941</div><div class="line">942</div><div class="line">943</div><div class="line">944</div><div class="line">945</div><div class="line">946</div><div class="line">947</div><div class="line">948</div><div class="line">949</div><div class="line">950</div><div class="line">951</div><div class="line">952</div><div class="line">953</div><div class="line">954</div><div class="line">955</div><div class="line">956</div><div class="line">957</div><div class="line">958</div><div class="line">959</div><div class="line">960</div><div class="line">961</div><div class="line">962</div><div class="line">963</div><div class="line">964</div><div class="line">965</div><div class="line">966</div><div class="line">967</div><div class="line">968</div><div class="line">969</div><div class="line">970</div><div class="line">971</div><div class="line">972</div><div class="line">973</div><div class="line">974</div><div class="line">975</div><div class="line">976</div><div class="line">977</div><div class="line">978</div><div class="line">979</div><div class="line">980</div><div class="line">981</div><div class="line">982</div><div class="line">983</div><div class="line">984</div><div class="line">985</div><div class="line">986</div><div class="line">987</div><div class="line">988</div><div class="line">989</div><div class="line">990</div><div class="line">991</div><div class="line">992</div><div class="line">993</div><div class="line">994</div><div class="line">995</div><div class="line">996</div><div class="line">997</div><div class="line">998</div><div class="line">999</div><div class="line">1000</div><div class="line">1001</div><div class="line">1002</div><div class="line">1003</div><div class="line">1004</div><div class="line">1005</div><div class="line">1006</div><div class="line">1007</div><div class="line">1008</div><div class="line">1009</div><div class="line">1010</div><div class="line">1011</div><div class="line">1012</div><div class="line">1013</div><div class="line">1014</div><div class="line">1015</div><div class="line">1016</div><div class="line">1017</div><div class="line">1018</div><div class="line">1019</div><div class="line">1020</div><div class="line">1021</div><div class="line">1022</div><div class="line">1023</div><div class="line">1024</div><div class="line">1025</div><div class="line">1026</div><div class="line">1027</div><div class="line">1028</div><div class="line">1029</div><div class="line">1030</div><div class="line">1031</div><div class="line">1032</div><div class="line">1033</div><div class="line">1034</div><div class="line">1035</div><div class="line">1036</div><div class="line">1037</div><div class="line">1038</div><div class="line">1039</div><div class="line">1040</div><div class="line">1041</div><div class="line">1042</div><div class="line">1043</div><div class="line">1044</div><div class="line">1045</div><div class="line">1046</div><div class="line">1047</div><div class="line">1048</div><div class="line">1049</div><div class="line">1050</div><div class="line">1051</div><div class="line">1052</div><div class="line">1053</div><div class="line">1054</div><div class="line">1055</div><div class="line">1056</div><div class="line">1057</div><div class="line">1058</div><div class="line">1059</div><div class="line">1060</div><div class="line">1061</div><div class="line">1062</div><div class="line">1063</div><div class="line">1064</div><div class="line">1065</div><div class="line">1066</div><div class="line">1067</div><div class="line">1068</div><div class="line">1069</div><div class="line">1070</div><div class="line">1071</div><div class="line">1072</div><div class="line">1073</div><div class="line">1074</div><div class="line">1075</div><div class="line">1076</div><div class="line">1077</div><div class="line">1078</div><div class="line">1079</div><div class="line">1080</div><div class="line">1081</div><div class="line">1082</div><div class="line">1083</div><div class="line">1084</div><div class="line">1085</div><div class="line">1086</div><div class="line">1087</div><div class="line">1088</div><div class="line">1089</div><div class="line">1090</div><div class="line">1091</div><div class="line">1092</div><div class="line">1093</div><div class="line">1094</div><div class="line">1095</div><div class="line">1096</div><div class="line">1097</div><div class="line">1098</div><div class="line">1099</div><div class="line">1100</div><div class="line">1101</div><div class="line">1102</div><div class="line">1103</div><div class="line">1104</div><div class="line">1105</div><div class="line">1106</div><div class="line">1107</div><div class="line">1108</div><div class="line">1109</div><div class="line">1110</div><div class="line">1111</div><div class="line">1112</div><div class="line">1113</div><div class="line">1114</div><div class="line">1115</div><div class="line">1116</div><div class="line">1117</div><div class="line">1118</div><div class="line">1119</div><div class="line">1120</div><div class="line">1121</div><div class="line">1122</div><div class="line">1123</div><div class="line">1124</div><div class="line">1125</div><div class="line">1126</div><div class="line">1127</div><div class="line">1128</div><div class="line">1129</div><div class="line">1130</div><div class="line">1131</div><div class="line">1132</div><div class="line">1133</div><div class="line">1134</div><div class="line">1135</div><div class="line">1136</div><div class="line">1137</div><div class="line">1138</div><div class="line">1139</div><div class="line">1140</div><div class="line">1141</div><div class="line">1142</div><div class="line">1143</div><div class="line">1144</div><div class="line">1145</div><div class="line">1146</div><div class="line">1147</div><div class="line">1148</div><div class="line">1149</div><div class="line">1150</div><div class="line">1151</div><div class="line">1152</div><div class="line">1153</div><div class="line">1154</div><div class="line">1155</div><div class="line">1156</div><div class="line">1157</div><div class="line">1158</div><div class="line">1159</div><div class="line">1160</div><div class="line">1161</div><div class="line">1162</div><div class="line">1163</div><div class="line">1164</div><div class="line">1165</div><div class="line">1166</div><div class="line">1167</div><div class="line">1168</div><div class="line">1169</div><div class="line">1170</div><div class="line">1171</div><div class="line">1172</div><div class="line">1173</div><div class="line">1174</div><div class="line">1175</div><div class="line">1176</div><div class="line">1177</div><div class="line">1178</div><div class="line">1179</div><div class="line">1180</div><div class="line">1181</div><div class="line">1182</div><div class="line">1183</div><div class="line">1184</div><div class="line">1185</div><div class="line">1186</div><div class="line">1187</div><div class="line">1188</div><div class="line">1189</div><div class="line">1190</div><div class="line">1191</div><div class="line">1192</div><div class="line">1193</div><div class="line">1194</div><div class="line">1195</div><div class="line">1196</div><div class="line">1197</div><div class="line">1198</div><div class="line">1199</div><div class="line">1200</div><div class="line">1201</div><div class="line">1202</div><div class="line">1203</div><div class="line">1204</div><div class="line">1205</div><div class="line">1206</div><div class="line">1207</div><div class="line">1208</div><div class="line">1209</div><div class="line">1210</div><div class="line">1211</div><div class="line">1212</div><div class="line">1213</div><div class="line">1214</div><div class="line">1215</div><div class="line">1216</div><div class="line">1217</div><div class="line">1218</div><div class="line">1219</div><div class="line">1220</div><div class="line">1221</div><div class="line">1222</div><div class="line">1223</div><div class="line">1224</div><div class="line">1225</div><div class="line">1226</div><div class="line">1227</div><div class="line">1228</div><div class="line">1229</div><div class="line">1230</div><div class="line">1231</div><div class="line">1232</div><div class="line">1233</div><div class="line">1234</div><div class="line">1235</div><div class="line">1236</div><div class="line">1237</div><div class="line">1238</div><div class="line">1239</div><div class="line">1240</div><div class="line">1241</div><div class="line">1242</div><div class="line">1243</div><div class="line">1244</div><div class="line">1245</div><div class="line">1246</div><div class="line">1247</div><div class="line">1248</div><div class="line">1249</div><div class="line">1250</div><div class="line">1251</div><div class="line">1252</div><div class="line">1253</div><div class="line">1254</div><div class="line">1255</div><div class="line">1256</div><div class="line">1257</div><div class="line">1258</div><div class="line">1259</div><div class="line">1260</div><div class="line">1261</div><div class="line">1262</div><div class="line">1263</div><div class="line">1264</div><div class="line">1265</div><div class="line">1266</div><div class="line">1267</div><div class="line">1268</div><div class="line">1269</div><div class="line">1270</div><div class="line">1271</div><div class="line">1272</div><div class="line">1273</div><div class="line">1274</div><div class="line">1275</div><div class="line">1276</div><div class="line">1277</div><div class="line">1278</div><div class="line">1279</div><div class="line">1280</div><div class="line">1281</div><div class="line">1282</div><div class="line">1283</div><div class="line">1284</div><div class="line">1285</div><div class="line">1286</div><div class="line">1287</div><div class="line">1288</div><div class="line">1289</div><div class="line">1290</div><div class="line">1291</div><div class="line">1292</div><div class="line">1293</div><div class="line">1294</div><div class="line">1295</div><div class="line">1296</div><div class="line">1297</div><div class="line">1298</div><div class="line">1299</div><div class="line">1300</div><div class="line">1301</div><div class="line">1302</div><div class="line">1303</div><div class="line">1304</div><div class="line">1305</div><div class="line">1306</div><div class="line">1307</div><div class="line">1308</div><div class="line">1309</div><div class="line">1310</div><div class="line">1311</div><div class="line">1312</div><div class="line">1313</div><div class="line">1314</div><div class="line">1315</div><div class="line">1316</div><div class="line">1317</div><div class="line">1318</div><div class="line">1319</div><div class="line">1320</div><div class="line">1321</div><div class="line">1322</div><div class="line">1323</div><div class="line">1324</div><div class="line">1325</div><div class="line">1326</div><div class="line">1327</div><div class="line">1328</div><div class="line">1329</div><div class="line">1330</div><div class="line">1331</div><div class="line">1332</div><div class="line">1333</div><div class="line">1334</div><div class="line">1335</div><div class="line">1336</div><div class="line">1337</div><div class="line">1338</div><div class="line">1339</div><div class="line">1340</div><div class="line">1341</div><div class="line">1342</div><div class="line">1343</div><div class="line">1344</div><div class="line">1345</div><div class="line">1346</div><div class="line">1347</div><div class="line">1348</div><div class="line">1349</div><div class="line">1350</div><div class="line">1351</div><div class="line">1352</div><div class="line">1353</div><div class="line">1354</div><div class="line">1355</div><div class="line">1356</div><div class="line">1357</div><div class="line">1358</div><div class="line">1359</div><div class="line">1360</div><div class="line">1361</div><div class="line">1362</div><div class="line">1363</div><div class="line">1364</div><div class="line">1365</div><div class="line">1366</div><div class="line">1367</div><div class="line">1368</div><div class="line">1369</div><div class="line">1370</div><div class="line">1371</div><div class="line">1372</div><div class="line">1373</div><div class="line">1374</div><div class="line">1375</div><div class="line">1376</div><div class="line">1377</div><div class="line">1378</div><div class="line">1379</div><div class="line">1380</div><div class="line">1381</div><div class="line">1382</div><div class="line">1383</div><div class="line">1384</div><div class="line">1385</div><div class="line">1386</div><div class="line">1387</div><div class="line">1388</div><div class="line">1389</div><div class="line">1390</div><div class="line">1391</div><div class="line">1392</div><div class="line">1393</div><div class="line">1394</div><div class="line">1395</div><div class="line">1396</div><div class="line">1397</div><div class="line">1398</div><div class="line">1399</div><div class="line">1400</div><div class="line">1401</div><div class="line">1402</div><div class="line">1403</div><div class="line">1404</div><div class="line">1405</div><div class="line">1406</div><div class="line">1407</div><div class="line">1408</div><div class="line">1409</div><div class="line">1410</div><div class="line">1411</div><div class="line">1412</div><div class="line">1413</div><div class="line">1414</div><div class="line">1415</div><div class="line">1416</div><div class="line">1417</div><div class="line">1418</div><div class="line">1419</div><div class="line">1420</div><div class="line">1421</div><div class="line">1422</div><div class="line">1423</div><div class="line">1424</div><div class="line">1425</div><div class="line">1426</div><div class="line">1427</div><div class="line">1428</div><div class="line">1429</div><div class="line">1430</div><div class="line">1431</div><div class="line">1432</div><div class="line">1433</div><div class="line">1434</div><div class="line">1435</div><div class="line">1436</div><div class="line">1437</div><div class="line">1438</div><div class="line">1439</div><div class="line">1440</div><div class="line">1441</div><div class="line">1442</div><div class="line">1443</div><div class="line">1444</div><div class="line">1445</div><div class="line">1446</div><div class="line">1447</div><div class="line">1448</div><div class="line">1449</div><div class="line">1450</div><div class="line">1451</div><div class="line">1452</div><div class="line">1453</div><div class="line">1454</div><div class="line">1455</div><div class="line">1456</div><div class="line">1457</div><div class="line">1458</div><div class="line">1459</div><div class="line">1460</div><div class="line">1461</div><div class="line">1462</div><div class="line">1463</div><div class="line">1464</div><div class="line">1465</div><div class="line">1466</div><div class="line">1467</div><div class="line">1468</div><div class="line">1469</div><div class="line">1470</div><div class="line">1471</div><div class="line">1472</div><div class="line">1473</div><div class="line">1474</div><div class="line">1475</div><div class="line">1476</div><div class="line">1477</div><div class="line">1478</div><div class="line">1479</div><div class="line">1480</div><div class="line">1481</div><div class="line">1482</div><div class="line">1483</div><div class="line">1484</div><div class="line">1485</div><div class="line">1486</div><div class="line">1487</div><div class="line">1488</div><div class="line">1489</div><div class="line">1490</div><div class="line">1491</div><div class="line">1492</div><div class="line">1493</div><div class="line">1494</div><div class="line">1495</div><div class="line">1496</div><div class="line">1497</div><div class="line">1498</div><div class="line">1499</div><div class="line">1500</div><div class="line">1501</div><div class="line">1502</div><div class="line">1503</div><div class="line">1504</div><div class="line">1505</div><div class="line">1506</div><div class="line">1507</div><div class="line">1508</div><div class="line">1509</div><div class="line">1510</div><div class="line">1511</div><div class="line">1512</div><div class="line">1513</div><div class="line">1514</div><div class="line">1515</div><div class="line">1516</div><div class="line">1517</div><div class="line">1518</div><div class="line">1519</div><div class="line">1520</div><div class="line">1521</div><div class="line">1522</div><div class="line">1523</div><div class="line">1524</div><div class="line">1525</div><div class="line">1526</div><div class="line">1527</div><div class="line">1528</div><div class="line">1529</div><div class="line">1530</div><div class="line">1531</div><div class="line">1532</div><div class="line">1533</div><div class="line">1534</div><div class="line">1535</div><div class="line">1536</div><div class="line">1537</div><div class="line">1538</div><div class="line">1539</div><div class="line">1540</div><div class="line">1541</div><div class="line">1542</div><div class="line">1543</div><div class="line">1544</div><div class="line">1545</div><div class="line">1546</div><div class="line">1547</div><div class="line">1548</div><div class="line">1549</div><div class="line">1550</div><div class="line">1551</div><div class="line">1552</div><div class="line">1553</div><div class="line">1554</div><div class="line">1555</div><div class="line">1556</div><div class="line">1557</div><div class="line">1558</div><div class="line">1559</div><div class="line">1560</div><div class="line">1561</div><div class="line">1562</div><div class="line">1563</div><div class="line">1564</div><div class="line">1565</div><div class="line">1566</div><div class="line">1567</div><div class="line">1568</div><div class="line">1569</div><div class="line">1570</div><div class="line">1571</div><div class="line">1572</div><div class="line">1573</div><div class="line">1574</div><div class="line">1575</div><div class="line">1576</div><div class="line">1577</div><div class="line">1578</div><div class="line">1579</div><div class="line">1580</div><div class="line">1581</div><div class="line">1582</div><div class="line">1583</div><div class="line">1584</div><div class="line">1585</div><div class="line">1586</div><div class="line">1587</div><div class="line">1588</div><div class="line">1589</div><div class="line">1590</div><div class="line">1591</div><div class="line">1592</div><div class="line">1593</div><div class="line">1594</div><div class="line">1595</div><div class="line">1596</div><div class="line">1597</div><div class="line">1598</div><div class="line">1599</div><div class="line">1600</div><div class="line">1601</div><div class="line">1602</div><div class="line">1603</div><div class="line">1604</div><div class="line">1605</div><div class="line">1606</div><div class="line">1607</div><div class="line">1608</div><div class="line">1609</div><div class="line">1610</div><div class="line">1611</div><div class="line">1612</div><div class="line">1613</div><div class="line">1614</div><div class="line">1615</div><div class="line">1616</div><div class="line">1617</div><div class="line">1618</div><div class="line">1619</div><div class="line">1620</div><div class="line">1621</div><div class="line">1622</div><div class="line">1623</div><div class="line">1624</div><div class="line">1625</div><div class="line">1626</div><div class="line">1627</div><div class="line">1628</div><div class="line">1629</div><div class="line">1630</div><div class="line">1631</div><div class="line">1632</div><div class="line">1633</div><div class="line">1634</div><div class="line">1635</div><div class="line">1636</div><div class="line">1637</div><div class="line">1638</div><div class="line">1639</div><div class="line">1640</div><div class="line">1641</div><div class="line">1642</div><div class="line">1643</div><div class="line">1644</div><div class="line">1645</div><div class="line">1646</div><div class="line">1647</div><div class="line">1648</div><div class="line">1649</div><div class="line">1650</div><div class="line">1651</div><div class="line">1652</div><div class="line">1653</div><div class="line">1654</div><div class="line">1655</div><div class="line">1656</div><div class="line">1657</div><div class="line">1658</div><div class="line">1659</div><div class="line">1660</div><div class="line">1661</div><div class="line">1662</div><div class="line">1663</div><div class="line">1664</div><div class="line">1665</div><div class="line">1666</div><div class="line">1667</div><div class="line">1668</div><div class="line">1669</div><div class="line">1670</div><div class="line">1671</div><div class="line">1672</div><div class="line">1673</div><div class="line">1674</div><div class="line">1675</div><div class="line">1676</div><div class="line">1677</div><div class="line">1678</div><div class="line">1679</div><div class="line">1680</div><div class="line">1681</div><div class="line">1682</div><div class="line">1683</div><div class="line">1684</div><div class="line">1685</div><div class="line">1686</div><div class="line">1687</div><div class="line">1688</div><div class="line">1689</div><div class="line">1690</div><div class="line">1691</div><div class="line">1692</div><div class="line">1693</div><div class="line">1694</div><div class="line">1695</div><div class="line">1696</div><div class="line">1697</div><div class="line">1698</div><div class="line">1699</div><div class="line">1700</div><div class="line">1701</div><div class="line">1702</div><div class="line">1703</div><div class="line">1704</div><div class="line">1705</div><div class="line">1706</div><div class="line">1707</div><div class="line">1708</div><div class="line">1709</div><div class="line">1710</div><div class="line">1711</div><div class="line">1712</div><div class="line">1713</div><div class="line">1714</div><div class="line">1715</div><div class="line">1716</div><div class="line">1717</div><div class="line">1718</div><div class="line">1719</div><div class="line">1720</div><div class="line">1721</div><div class="line">1722</div><div class="line">1723</div><div class="line">1724</div><div class="line">1725</div><div class="line">1726</div><div class="line">1727</div><div class="line">1728</div><div class="line">1729</div><div class="line">1730</div><div class="line">1731</div><div class="line">1732</div><div class="line">1733</div><div class="line">1734</div><div class="line">1735</div><div class="line">1736</div><div class="line">1737</div><div class="line">1738</div><div class="line">1739</div><div class="line">1740</div><div class="line">1741</div><div class="line">1742</div><div class="line">1743</div><div class="line">1744</div><div class="line">1745</div><div class="line">1746</div><div class="line">1747</div><div class="line">1748</div><div class="line">1749</div><div class="line">1750</div><div class="line">1751</div><div class="line">1752</div></pre></td><td class="code"><pre><div class="line">// syntax用来指定protobuf的版本</div><div class="line">syntax = &quot;proto2&quot;;</div><div class="line"></div><div class="line">// package可以看作C++中的namespace，与Caffe C++代码中的namespace caffe对应</div><div class="line">// package用来避免名称冲突</div><div class="line">package caffe;</div><div class="line"></div><div class="line"></div><div class="line">// 在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。</div><div class="line">// 注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。</div><div class="line">// required：一个格式良好的消息一定要含有一个这种字段，表示该值是必须要设置的。</div><div class="line">// optional：消息格式中该字段可以有0个或1个值（不超过1个）。</div><div class="line">// repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留，表示该值可以重复，相当于Java中的List。</div><div class="line"></div><div class="line"></div><div class="line">// Specifies the shape (dimensions) of a Blob.</div><div class="line">// 指定Blob的shape，4-D shape</div><div class="line">message BlobShape &#123;</div><div class="line">  //数据块形状定义为Num * Channel * Height * Wight, 原因在于caffe基于容器的多维嵌套来实现高维数据的封装, 即vector。 </div><div class="line">  repeated int64 dim = 1 [packed = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Blob数据块，包括Blob shape，数据和微分</div><div class="line">message BlobProto &#123;</div><div class="line">  // Blob的shape, 即numpy中的shape</div><div class="line">  optional BlobShape shape = 7;</div><div class="line">  // Blob的数据部分</div><div class="line">  repeated float data = 5 [packed = true];</div><div class="line">  // Blob的微分部分</div><div class="line">  repeated float diff = 6 [packed = true];</div><div class="line">  // Blob中的数据部分(double类型)</div><div class="line">  repeated double double_data = 8 [packed = true];</div><div class="line">  // Blob的微分部分(double类型)</div><div class="line">  repeated double double_diff = 9 [packed = true];</div><div class="line"></div><div class="line">  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.</div><div class="line">  // Blob的4个维度，已被Blob shape代替</div><div class="line">  // Blob中数据的个数(例如卷积核的个数)</div><div class="line">  optional int32 num = 1 [default = 0];</div><div class="line">  // Blob中数据的通道数</div><div class="line">  optional int32 channels = 2 [default = 0];</div><div class="line">  // Blob中数据的高度</div><div class="line">  optional int32 height = 3 [default = 0];</div><div class="line">  // Blob中数据的宽度</div><div class="line">  optional int32 width = 4 [default = 0];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// The BlobProtoVector is simply a way to pass multiple blobproto instances</div><div class="line">// around.</div><div class="line">// BlobProtoVector, 用来保存多个BlobProb对象的Vector</div><div class="line">message BlobProtoVector &#123;</div><div class="line">  repeated BlobProto blobs = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//图像数据, channel-图像通道数, height-高度, width-宽度, data-图像像素数据, label-图像标签, float_data-图像浮点型数据(0-1之间), encoded-图像编码方式</div><div class="line">message Datum &#123;</div><div class="line">  // 图像的通道数</div><div class="line">  optional int32 channels = 1;</div><div class="line">  // 图像的高度</div><div class="line">  optional int32 height = 2;</div><div class="line">  // 图像的宽度</div><div class="line">  optional int32 width = 3;</div><div class="line">  // the actual image data, in bytes</div><div class="line">  // 实际的图像数据，以字节形式(uint8)表示</div><div class="line">  optional bytes data = 4;</div><div class="line">  // 图像对应的标签，必须为整形</div><div class="line">  optional int32 label = 5;</div><div class="line">  // Optionally, the datum could also hold float data.</div><div class="line">  // 可选表示，图像数据表示为float数据，即0-255归一化到0-1之间</div><div class="line">  repeated float float_data = 6;</div><div class="line">  // If true data contains an encoded image that need to be decoded</div><div class="line">  // encoded为true表示图像采用压缩表示，需要解码</div><div class="line">  optional bool encoded = 7 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Filler参数, filler主要对网络权重进行初始化</div><div class="line">// Filler类型分为常量初始化（constant）、高斯分布初始化（gaussian）、positive_unitball初始化、均匀分布初始化（uniform）、xavier初始化、msra初始化、双线性初始化（bilinear）</div><div class="line">message FillerParameter &#123;</div><div class="line">  // The filler type.</div><div class="line">  // Filler的类型</div><div class="line">  optional string type = 1 [default = &apos;constant&apos;];</div><div class="line">  // 常量初始化的值</div><div class="line">  optional float value = 2 [default = 0]; // the value in constant filler</div><div class="line">  // 均匀分布初始化中的最小值</div><div class="line">  optional float min = 3 [default = 0]; // the min value in uniform filler</div><div class="line">  // 均匀分布初始化中的最大值</div><div class="line">  optional float max = 4 [default = 1]; // the max value in uniform filler</div><div class="line">  // 高斯分布初始化中的均值</div><div class="line">  optional float mean = 5 [default = 0]; // the mean value in Gaussian filler</div><div class="line">  // 高斯分布初始化中的标准差</div><div class="line">  optional float std = 6 [default = 1]; // the std value in Gaussian filler</div><div class="line">  // The expected number of non-zero output weights for a given input in</div><div class="line">  // Gaussian filler -- the default -1 means don&apos;t perform sparsification.</div><div class="line">  // 在高斯分布初始化中给定输入及权重，期望输出非0值，默认值-1表示不进行稀疏化</div><div class="line">  optional int32 sparse = 7 [default = -1];</div><div class="line">  // Normalize the filler variance by fan_in, fan_out, or their average.</div><div class="line">  // Applies to &apos;xavier&apos; and &apos;msra&apos; fillers.</div><div class="line">  // 通过fan_in, fan_out或average来归一化filler方差，主要应用到&apos;xavier&apos;和&apos;msra&apos; filler中</div><div class="line">  enum VarianceNorm &#123;</div><div class="line">    FAN_IN = 0;</div><div class="line">    FAN_OUT = 1;</div><div class="line">    AVERAGE = 2;</div><div class="line">  &#125;</div><div class="line">  // 定义filler方差归一化，默认为FAN_IN</div><div class="line">  optional VarianceNorm variance_norm = 8 [default = FAN_IN];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//神经网络参数</div><div class="line">message NetParameter &#123;</div><div class="line">  // 神经网络名字</div><div class="line">  optional string name = 1; // consider giving the network a name</div><div class="line"></div><div class="line">  // DEPRECATED. See InputParameter. The input blobs to the network.</div><div class="line">  // 已废弃。网络的输入部分，具体请看InputParameter。</div><div class="line">  repeated string input = 3;</div><div class="line"></div><div class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</div><div class="line">  // 已废弃。输入blob的shape，具体请看InputParameter。</div><div class="line">  repeated BlobShape input_shape = 8;</div><div class="line"></div><div class="line">  // 4D input dimensions -- deprecated.  Use &quot;input_shape&quot; instead.</div><div class="line">  // If specified, for each input blob there should be four</div><div class="line">  // values specifying the num, channels, height and width of the input blob.</div><div class="line">  // Thus, there should be a total of (4 * #input) numbers.</div><div class="line">  // 已废弃。用input_shape代替。</div><div class="line">  repeated int32 input_dim = 4;</div><div class="line"></div><div class="line">  // Whether the network will force every layer to carry out backward operation.</div><div class="line">  // If set False, then whether to carry out backward is determined</div><div class="line">  // automatically according to the net structure and learning rates.</div><div class="line">  // 网络中是否每一层都执行反向传播的标志，如果设为false，反向传播会根据网络结构和学习率自动进行。</div><div class="line">  optional bool force_backward = 5 [default = false];</div><div class="line"></div><div class="line">  // The current &quot;state&quot; of the network, including the phase, level, and stage.</div><div class="line">  // Some layers may be included/excluded depending on this state and the states</div><div class="line">  // specified in the layers&apos; include and exclude fields.</div><div class="line">  // 网络的当前状态，包括phase, level和stage，(phase应该是对应prototxt文件中的TRAIN,TEST)</div><div class="line">  // 某些层是否included/excluded依赖于层中include，exclue字段指定的state。</div><div class="line">  optional NetState state = 6;</div><div class="line"></div><div class="line">  // Print debugging information about results while running Net::Forward,</div><div class="line">  // Net::Backward, and Net::Update.</div><div class="line">  // 在执行Net::Forward,Net::Backward, Net::Update时是否打印调试信息。</div><div class="line">  optional bool debug_info = 7 [default = false];</div><div class="line"></div><div class="line">  // The layers that make up the net.  Each of their configurations, including</div><div class="line">  // connectivity and behavior, is specified as a LayerParameter.</div><div class="line">  // 构成网络的layer，每一个layer的配置，包括连接性和行为都在LayerParameter中指定。</div><div class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</div><div class="line"></div><div class="line">  // DEPRECATED: use &apos;layer&apos; instead.</div><div class="line">  // 已废弃，用layer代替。</div><div class="line">  repeated V1LayerParameter layers = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// NOTE</div><div class="line">// Update the next available ID when you add a new SolverParameter field.</div><div class="line">// 注意：当你添加一个新的SolverParameter字段时，要更新下一个可获取的ID</div><div class="line">// SolverParameter next available ID: 41 (last added: type)</div><div class="line">// Solver参数</div><div class="line">message SolverParameter &#123;</div><div class="line">  //////////////////////////////////////////////////////////////////////////////</div><div class="line">  // Specifying the train and test networks</div><div class="line">  //</div><div class="line">  // Exactly one train net must be specified using one of the following fields:</div><div class="line">  //     train_net_param, train_net, net_param, net</div><div class="line">  // One or more test nets may be specified using any of the following fields:</div><div class="line">  //     test_net_param, test_net, net_param, net</div><div class="line">  // If more than one test net field is specified (e.g., both net and</div><div class="line">  // test_net are specified), they will be evaluated in the field order given</div><div class="line">  // above: (1) test_net_param, (2) test_net, (3) net_param/net.</div><div class="line">  // A test_iter must be specified for each test_net.</div><div class="line">  // A test_level and/or a test_stage may also be specified for each test_net.</div><div class="line">  //////////////////////////////////////////////////////////////////////////////</div><div class="line"></div><div class="line">  // Proto filename for the train net, possibly combined with one or more test nets.</div><div class="line">  // 训练网络的prototxt文件名，可能结合一个或多个测试网络</div><div class="line">  optional string net = 24;</div><div class="line">  // Inline train net param, possibly combined with one or more test nets.</div><div class="line">  // 内联训练网络参数，可能结合一个或多个测试网络</div><div class="line">  optional NetParameter net_param = 25;</div><div class="line"></div><div class="line">  // 训练网络的proto文件名</div><div class="line">  optional string train_net = 1; // Proto filename for the train net.</div><div class="line">  // 测试网络的proto文件名</div><div class="line">  repeated string test_net = 2; // Proto filenames for the test nets.</div><div class="line">  // 内联训练网络参数</div><div class="line">  optional NetParameter train_net_param = 21; // Inline train net params.</div><div class="line">  // 内联测试网络参数</div><div class="line">  repeated NetParameter test_net_param = 22; // Inline test net params.</div><div class="line"></div><div class="line">  // The states for the train/test nets. Must be unspecified or specified once per net.</div><div class="line">  // By default, all states will have solver = true;</div><div class="line">  // train_state will have phase = TRAIN,</div><div class="line">  // and all test_state&apos;s will have phase = TEST.</div><div class="line">  // Other defaults are set according to the NetState defaults.</div><div class="line">  // train/test网络的状态，必须不指定或每个网络指定一次</div><div class="line">  // 默认情况下，所有的状态都有solver = true，train_state的phase = TRAIN，其它默认情况根据NetState默认值设定。</div><div class="line">  </div><div class="line">  // train网络的状态，必须不指定或每个网络指定一次</div><div class="line">  optional NetState train_state = 26;</div><div class="line">  // test网络的状态，必须不指定或每个网络指定一次</div><div class="line">  repeated NetState test_state = 27;</div><div class="line"></div><div class="line">  // The number of iterations for each test net.</div><div class="line">  // 每个测试网络的迭代次数，即测试数据的迭代次数，测试数据总数=测试迭代次数*测试数据的batch_size。</div><div class="line">  repeated int32 test_iter = 3;</div><div class="line"></div><div class="line">  // The number of iterations between two testing phases.</div><div class="line">  // 两次测试间隔的迭代次数，即训练数据迭代多少次进行一次测试。</div><div class="line">  optional int32 test_interval = 4 [default = 0];</div><div class="line">  // 测试数据的loss，默认情况下不计算</div><div class="line">  optional bool test_compute_loss = 19 [default = false];</div><div class="line">  // If true, run an initial test pass before the first iteration,</div><div class="line">  // ensuring memory availability and printing the starting value of the loss.</div><div class="line">  // 如果为true，在第一次迭代之前进行一次初始测试，从而确保内存可用性并输出初始损失值。</div><div class="line">  optional bool test_initialization = 32 [default = true];</div><div class="line">  // 基本学习率</div><div class="line">  optional float base_lr = 5; // The base learning rate</div><div class="line">  // the number of iterations between displaying info. If display = 0, no info will be displayed.</div><div class="line">  // 执行多少次迭代显示一次信息，如果display = 0，不输出信息。</div><div class="line">  optional int32 display = 6;</div><div class="line">  // Display the loss averaged over the last average_loss iterations</div><div class="line">  // 输出的平均损失是之前多少次迭代的平均损失。</div><div class="line">  optional int32 average_loss = 33 [default = 1];</div><div class="line">  // 训练的最大迭代次数</div><div class="line">  optional int32 max_iter = 7; // the maximum number of iterations</div><div class="line">  // accumulate gradients over `iter_size` x `batch_size` instances</div><div class="line">  // 累积`iter_size` x `batch_size`个实例的梯度</div><div class="line">  optional int32 iter_size = 36 [default = 1];</div><div class="line"></div><div class="line">  // The learning rate decay policy. The currently implemented learning rate</div><div class="line">  // policies are as follows:</div><div class="line">  //    - fixed: always return base_lr.</div><div class="line">  //    - step: return base_lr * gamma ^ (floor(iter / step))</div><div class="line">  //    - exp: return base_lr * gamma ^ iter</div><div class="line">  //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)</div><div class="line">  //    - multistep: similar to step but it allows non uniform steps defined by</div><div class="line">  //      stepvalue</div><div class="line">  //    - poly: the effective learning rate follows a polynomial decay, to be</div><div class="line">  //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)</div><div class="line">  //    - sigmoid: the effective learning rate follows a sigmod decay</div><div class="line">  //      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</div><div class="line">  //</div><div class="line">  // where base_lr, max_iter, gamma, step, stepvalue and power are defined</div><div class="line">  // in the solver parameter protocol buffer, and iter is the current iteration.</div><div class="line"></div><div class="line">  // 学习率的变化策略</div><div class="line">  optional string lr_policy = 8;</div><div class="line">  // 学习率的计算参数</div><div class="line">  optional float gamma = 9; // The parameter to compute the learning rate.</div><div class="line">  // 学习率的计算参数</div><div class="line">  optional float power = 10; // The parameter to compute the learning rate.</div><div class="line">  // 动量参数</div><div class="line">  optional float momentum = 11; // The momentum value.</div><div class="line">  // 权重衰减，权重衰减主要影响神经网络的正则项，具体可参考Caffe文档</div><div class="line">  optional float weight_decay = 12; // The weight decay.</div><div class="line">  // regularization types supported: L1 and L2, controlled by weight_decay</div><div class="line">  // 正则化类型支持L1和L2，受weight_decay控制。</div><div class="line">  optional string regularization_type = 29 [default = &quot;L2&quot;];</div><div class="line">  // the stepsize for learning rate policy &quot;step&quot;</div><div class="line">  // 学习率方案为step时的参数</div><div class="line">  optional int32 stepsize = 13;</div><div class="line">  // the stepsize for learning rate policy &quot;multistep&quot;</div><div class="line">  // 学习率方案为multistep时的参数</div><div class="line">  repeated int32 stepvalue = 34;</div><div class="line"></div><div class="line">  // Set clip_gradients to &gt;= 0 to clip parameter gradients to that L2 norm,</div><div class="line">  // whenever their actual L2 norm is larger.</div><div class="line">  // 设置clip_gradients &gt;= 0可以削减L2范数的梯度，当真实L2范数的梯度大于clip_gradients，将L2范数的梯度设为clip_gradients</div><div class="line">  optional float clip_gradients = 35 [default = -1];</div><div class="line">  // snapshot的间隔，即迭代多少次保存一次snapshot</div><div class="line">  optional int32 snapshot = 14 [default = 0]; // The snapshot interval</div><div class="line">  // snapshot的前缀</div><div class="line">  optional string snapshot_prefix = 15; // The prefix for the snapshot.</div><div class="line">  // whether to snapshot diff in the results or not. Snapshotting diff will help</div><div class="line">  // debugging but the final protocol buffer size will be much larger.</div><div class="line">  // 是否在结果中保存snapshot的差分，snapshot diff有助于调试，但snapshot的文件会更大。</div><div class="line">  optional bool snapshot_diff = 16 [default = false];</div><div class="line">  // snapshot的保存格式（hdf5,binaryproto）。</div><div class="line">  enum SnapshotFormat &#123;</div><div class="line">    HDF5 = 0;</div><div class="line">    BINARYPROTO = 1;</div><div class="line">  &#125;</div><div class="line">  // snapshot默认保存为BINARYPROTO。</div><div class="line">  optional SnapshotFormat snapshot_format = 37 [default = BINARYPROTO];</div><div class="line">  // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default.</div><div class="line">  // 求解神经网络的方式，0 CPU, 1 GPU。默认使用GPU</div><div class="line">  enum SolverMode &#123;</div><div class="line">    CPU = 0;</div><div class="line">    GPU = 1;</div><div class="line">  &#125;</div><div class="line">  // 求解神经网络的模式，0 CPU, 1 GPU。默认使用GPU</div><div class="line">  optional SolverMode solver_mode = 17 [default = GPU];</div><div class="line">  // the device_id will that be used in GPU mode. Use device_id = 0 in default.</div><div class="line">  // device_id是GPU模式下GPU的ID。</div><div class="line">  optional int32 device_id = 18 [default = 0];</div><div class="line">  // If non-negative, the seed with which the Solver will initialize the Caffe</div><div class="line">  // random number generator -- useful for reproducible results. Otherwise,</div><div class="line">  // (and by default) initialize using a seed derived from the system clock.</div><div class="line">  // 如果是非负值，seed用来初始化Caffe的随机数生成器，对于再见结果是很有用的，默认情况下，seed的是从系统时钟获取。</div><div class="line">  optional int64 random_seed = 20 [default = -1];</div><div class="line"></div><div class="line">  // type of the solver</div><div class="line">  // 神经网络求解的类型, 默认为SGD</div><div class="line">  optional string type = 40 [default = &quot;SGD&quot;];</div><div class="line"></div><div class="line">  // numerical stability for RMSProp, AdaGrad and AdaDelta and Adam</div><div class="line">  // RMSProp, AdaGrad, AdaDelta, Adam求解类型的参数</div><div class="line">  optional float delta = 31 [default = 1e-8];</div><div class="line">  // parameters for the Adam solver</div><div class="line">  // Adam求解类型的参数</div><div class="line">  optional float momentum2 = 39 [default = 0.999];</div><div class="line"></div><div class="line">  // RMSProp decay value</div><div class="line">  // MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)</div><div class="line">  // RMSProp类型的衰减值</div><div class="line">  optional float rms_decay = 38 [default = 0.99];</div><div class="line"></div><div class="line">  // If true, print information about the state of the net that may help with</div><div class="line">  // debugging learning problems.</div><div class="line">  // 如果设为true，会输出网络的状态信息，有助于调试</div><div class="line">  optional bool debug_info = 23 [default = false];</div><div class="line"></div><div class="line">  // If false, don&apos;t save a snapshot after training finishes.</div><div class="line">  // 如果设为false，不保存训练结束的snapshot。</div><div class="line">  optional bool snapshot_after_train = 28 [default = true];</div><div class="line"></div><div class="line">  // DEPRECATED: old solver enum types, use string instead</div><div class="line">  // 已废弃，使用string代替</div><div class="line">  enum SolverType &#123;</div><div class="line">    SGD = 0;</div><div class="line">    NESTEROV = 1;</div><div class="line">    ADAGRAD = 2;</div><div class="line">    RMSPROP = 3;</div><div class="line">    ADADELTA = 4;</div><div class="line">    ADAM = 5;</div><div class="line">  &#125;</div><div class="line">  // DEPRECATED: use type instead of solver_type</div><div class="line">  // 已废弃：使用type代替</div><div class="line">  optional SolverType solver_type = 30 [default = SGD];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// A message that stores the solver snapshots</div><div class="line">// 保存solver snapshots</div><div class="line">message SolverState &#123;</div><div class="line">  // 当前的迭代次数</div><div class="line">  optional int32 iter = 1; // The current iteration</div><div class="line">  // 保存学习到的网络</div><div class="line">  optional string learned_net = 2; // The file that stores the learned net.</div><div class="line">  // sgd的求解历史</div><div class="line">  repeated BlobProto history = 3; // The history for sgd solvers</div><div class="line">  // 学习的当前step</div><div class="line">  optional int32 current_step = 4 [default = 0]; // The current step for learning rate</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 定义phase</div><div class="line">enum Phase &#123;</div><div class="line">   TRAIN = 0;</div><div class="line">   TEST = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 网络状态</div><div class="line">message NetState &#123;</div><div class="line">  // 属于哪个phase</div><div class="line">  optional Phase phase = 1 [default = TEST];</div><div class="line">  optional int32 level = 2 [default = 0];</div><div class="line">  repeated string stage = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 网络状态分类</div><div class="line">message NetStateRule &#123;</div><div class="line">  // Set phase to require the NetState have a particular phase (TRAIN or TEST)</div><div class="line">  // to meet this rule.</div><div class="line">  // 设置phase</div><div class="line">  optional Phase phase = 1;</div><div class="line"></div><div class="line">  // Set the minimum and/or maximum levels in which the layer should be used.</div><div class="line">  // Leave undefined to meet the rule regardless of level.</div><div class="line">  // 设置layer的level</div><div class="line">  optional int32 min_level = 2;</div><div class="line">  optional int32 max_level = 3;</div><div class="line"></div><div class="line">  // Customizable sets of stages to include or exclude.</div><div class="line">  // The net must have ALL of the specified stages and NONE of the specified</div><div class="line">  // &quot;not_stage&quot;s to meet the rule.</div><div class="line">  // (Use multiple NetStateRules to specify conjunctions of stages.)</div><div class="line">  // 可定制的stage集合</div><div class="line">  repeated string stage = 4;</div><div class="line">  repeated string not_stage = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Specifies training parameters (multipliers on global learning constants,</div><div class="line">// and the name and other settings used for weight sharing).</div><div class="line">// 指定训练参数及名称以及权重共享的其它设置</div><div class="line">message ParamSpec &#123;</div><div class="line">  // The names of the parameter blobs -- useful for sharing parameters among</div><div class="line">  // layers, but never required otherwise.  To share a parameter between two</div><div class="line">  // layers, give it a (non-empty) name.</div><div class="line">  // 两个layer之间进行参数共享的blob名字</div><div class="line">  optional string name = 1;</div><div class="line"></div><div class="line">  // Whether to require shared weights to have the same shape, or just the same</div><div class="line">  // count -- defaults to STRICT if unspecified.</div><div class="line">  // 参数共享时是否需要具有相同的shape，默认情况下需要有相同的shape</div><div class="line">  optional DimCheckMode share_mode = 2;</div><div class="line">  // 参数共享时的维度检查</div><div class="line">  enum DimCheckMode &#123;</div><div class="line">    // STRICT (default) requires that num, channels, height, width each match.</div><div class="line">    STRICT = 0;</div><div class="line">    // PERMISSIVE requires only the count (num*channels*height*width) to match.</div><div class="line">    PERMISSIVE = 1;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // The multiplier on the global learning rate for this parameter.</div><div class="line">  // 学习率参数, learning rate = base_lr * lr_mult</div><div class="line">  optional float lr_mult = 3 [default = 1.0];</div><div class="line"></div><div class="line">  // The multiplier on the global weight decay for this parameter.</div><div class="line">  // 权重衰减参数, weight = weight_decay * decay_mult</div><div class="line">  optional float decay_mult = 4 [default = 1.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// NOTE</div><div class="line">// Update the next available ID when you add a new LayerParameter field.</div><div class="line">// LayerParameter next available layer-specific ID: 147 (last added: recurrent_param)</div><div class="line">// 注意：当你添加一个新的LayerParameter字段时，要更新下一个可获取的ID</div><div class="line">message LayerParameter &#123;</div><div class="line">  // layer名称</div><div class="line">  optional string name = 1; // the layer name</div><div class="line">  // layer类型</div><div class="line">  optional string type = 2; // the layer type</div><div class="line">  // layer的输入</div><div class="line">  repeated string bottom = 3; // the name of each bottom blob</div><div class="line">  // layer的输出</div><div class="line">  repeated string top = 4; // the name of each top blob</div><div class="line"></div><div class="line">  // The train / test phase for computation.</div><div class="line">  // layer用在train/test phase</div><div class="line">  optional Phase phase = 10;</div><div class="line"></div><div class="line">  // The amount of weight to assign each top blob in the objective.</div><div class="line">  // Each layer assigns a default value, usually of either 0 or 1,</div><div class="line">  // to each top blob.</div><div class="line">  // layer对最终的loss损失值的贡献率</div><div class="line">  repeated float loss_weight = 5;</div><div class="line"></div><div class="line">  // Specifies training parameters (multipliers on global learning constants,</div><div class="line">  // and the name and other settings used for weight sharing).</div><div class="line">  // 指定训练参数</div><div class="line">  repeated ParamSpec param = 6;</div><div class="line"></div><div class="line">  // The blobs containing the numeric parameters of the layer.</div><div class="line">  // layer的blobs</div><div class="line">  repeated BlobProto blobs = 7;</div><div class="line"></div><div class="line">  // Specifies whether to backpropagate to each bottom. If unspecified,</div><div class="line">  // Caffe will automatically infer whether each input needs backpropagation</div><div class="line">  // to compute parameter gradients. If set to true for some inputs,</div><div class="line">  // backpropagation to those inputs is forced; if set false for some inputs,</div><div class="line">  // backpropagation to those inputs is skipped.</div><div class="line">  //</div><div class="line">  // The size must be either 0 or equal to the number of bottoms.</div><div class="line">  // 指定反向传播是否传播到每一个bottom，如果不指定，caffe会自动检查推断是否每一个输入都需要反向传播来计算梯度。如果一些输入设为true,</div><div class="line">  // 则这些layer强制进行反向传播，如果设为false，这些layer将跳过反向传播。</div><div class="line">  repeated bool propagate_down = 11;</div><div class="line"></div><div class="line">  // Rules controlling whether and when a layer is included in the network,</div><div class="line">  // based on the current NetState.  You may specify a non-zero number of rules</div><div class="line">  // to include OR exclude, but not both.  If no include or exclude rules are</div><div class="line">  // specified, the layer is always included.  If the current NetState meets</div><div class="line">  // ANY (i.e., one or more) of the specified rules, the layer is</div><div class="line">  // included/excluded.</div><div class="line">  // 控制layer included/excluded</div><div class="line">  repeated NetStateRule include = 8;</div><div class="line">  repeated NetStateRule exclude = 9;</div><div class="line"></div><div class="line">  // Parameters for data pre-processing.</div><div class="line">  // 数据预处理参数</div><div class="line">  optional TransformationParameter transform_param = 100;</div><div class="line"></div><div class="line">  // Parameters shared by loss layers.</div><div class="line">  // loss layer的参数共享</div><div class="line">  optional LossParameter loss_param = 101;</div><div class="line"></div><div class="line">  // Layer type-specific parameters.</div><div class="line">  //</div><div class="line">  // Note: certain layers may have more than one computational engine</div><div class="line">  // for their implementation. These layers include an Engine type and</div><div class="line">  // engine parameter for selecting the implementation.</div><div class="line">  // The default for the engine is set by the ENGINE switch at compile-time.</div><div class="line"></div><div class="line">  // 特定layer的参数</div><div class="line">  optional AccuracyParameter accuracy_param = 102;</div><div class="line">  optional ArgMaxParameter argmax_param = 103;</div><div class="line">  optional BatchNormParameter batch_norm_param = 139;</div><div class="line">  optional BiasParameter bias_param = 141;</div><div class="line">  optional ConcatParameter concat_param = 104;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = 105;</div><div class="line">  optional ConvolutionParameter convolution_param = 106;</div><div class="line">  optional CropParameter crop_param = 144;</div><div class="line">  optional DataParameter data_param = 107;</div><div class="line">  optional DropoutParameter dropout_param = 108;</div><div class="line">  optional DummyDataParameter dummy_data_param = 109;</div><div class="line">  optional EltwiseParameter eltwise_param = 110;</div><div class="line">  optional ELUParameter elu_param = 140;</div><div class="line">  optional EmbedParameter embed_param = 137;</div><div class="line">  optional ExpParameter exp_param = 111;</div><div class="line">  optional FlattenParameter flatten_param = 135;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = 112;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 113;</div><div class="line">  optional HingeLossParameter hinge_loss_param = 114;</div><div class="line">  optional ImageDataParameter image_data_param = 115;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = 116;</div><div class="line">  optional InnerProductParameter inner_product_param = 117;</div><div class="line">  optional InputParameter input_param = 143;</div><div class="line">  optional LogParameter log_param = 134;</div><div class="line">  optional LRNParameter lrn_param = 118;</div><div class="line">  optional MemoryDataParameter memory_data_param = 119;</div><div class="line">  optional MVNParameter mvn_param = 120;</div><div class="line">  optional ParameterParameter parameter_param = 145;</div><div class="line">  optional PoolingParameter pooling_param = 121;</div><div class="line">  optional PowerParameter power_param = 122;</div><div class="line">  optional PReLUParameter prelu_param = 131;</div><div class="line">  optional PythonParameter python_param = 130;</div><div class="line">  optional RecurrentParameter recurrent_param = 146;</div><div class="line">  optional ReductionParameter reduction_param = 136;</div><div class="line">  optional ReLUParameter relu_param = 123;</div><div class="line">  optional ReshapeParameter reshape_param = 133;</div><div class="line">  optional ScaleParameter scale_param = 142;</div><div class="line">  optional SigmoidParameter sigmoid_param = 124;</div><div class="line">  optional SoftmaxParameter softmax_param = 125;</div><div class="line">  optional SPPParameter spp_param = 132;</div><div class="line">  optional SliceParameter slice_param = 126;</div><div class="line">  optional TanHParameter tanh_param = 127;</div><div class="line">  optional ThresholdParameter threshold_param = 128;</div><div class="line">  optional TileParameter tile_param = 138;</div><div class="line">  optional WindowDataParameter window_data_param = 129;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used to apply transformation to the data layer&apos;s data</div><div class="line">// 用来进行数据层（图像）变换的参数</div><div class="line">message TransformationParameter &#123;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  // 像素归一化，归一化之前会减去均值</div><div class="line">  optional float scale = 1 [default = 1];</div><div class="line">  // Specify if we want to randomly mirror data.</div><div class="line">  // 图像进行随机mirror操作</div><div class="line">  optional bool mirror = 2 [default = false];</div><div class="line">  // Specify if we would like to randomly crop an image.</div><div class="line">  // 图像随机crop操作</div><div class="line">  optional uint32 crop_size = 3 [default = 0];</div><div class="line">  // mean_file and mean_value cannot be specified at the same time</div><div class="line">  // 图像的均值文件</div><div class="line">  optional string mean_file = 4;</div><div class="line">  // if specified can be repeated once (would subtract it from all the channels)</div><div class="line">  // or can be repeated the same number of times as channels</div><div class="line">  // (would subtract them from the corresponding channel)</div><div class="line">  // 图像的均值，手动指定，通常是三个</div><div class="line">  repeated float mean_value = 5;</div><div class="line">  // Force the decoded image to have 3 color channels.</div><div class="line">  // 强制图像必须有三个颜色通道</div><div class="line">  optional bool force_color = 6 [default = false];</div><div class="line">  // Force the decoded image to have 1 color channels.</div><div class="line">  // 强制图像为灰度图像</div><div class="line">  optional bool force_gray = 7 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters shared by loss layers</div><div class="line">// loss层参数</div><div class="line">message LossParameter &#123;</div><div class="line">  // If specified, ignore instances with the given label.</div><div class="line">  // 如果指定，则label等于ignore_label的样本将不参与Loss计算，并且反向传播时梯度直接置0。</div><div class="line">  optional int32 ignore_label = 1;</div><div class="line">  // How to normalize the loss for loss layers that aggregate across batches,</div><div class="line">  // spatial dimensions, or other dimensions.  Currently only implemented in</div><div class="line">  // SoftmaxWithLoss and SigmoidCrossEntropyLoss layers.</div><div class="line">  // 指定loss归一化的方式</div><div class="line">  enum NormalizationMode &#123;</div><div class="line">    // Divide by the number of examples in the batch times spatial dimensions.</div><div class="line">    // Outputs that receive the ignore label will NOT be ignored in computing</div><div class="line">    // the normalization factor.</div><div class="line">    // 所有样本都参与计算，包括ignore label</div><div class="line">    FULL = 0;</div><div class="line">    // Divide by the total number of output locations that do not take the</div><div class="line">    // ignore_label.  If ignore_label is not set, this behaves like FULL.</div><div class="line">    // 所有样本都参与计算，不包括ignore label</div><div class="line">    VALID = 1;</div><div class="line">    // Divide by the batch size.</div><div class="line">    // 除以给定的batch size。</div><div class="line">    BATCH_SIZE = 2;</div><div class="line">    // Do not normalize the loss.</div><div class="line">    // 不归一化loss</div><div class="line">    NONE = 3;</div><div class="line">  &#125;</div><div class="line">  // For historical reasons, the default normalization for</div><div class="line">  // SigmoidCrossEntropyLoss is BATCH_SIZE and *not* VALID.</div><div class="line">  // loss归一化方式</div><div class="line">  optional NormalizationMode normalization = 3 [default = VALID];</div><div class="line">  // Deprecated.  Ignored if normalization is specified.  If normalization</div><div class="line">  // is not specified, then setting this to false will be equivalent to</div><div class="line">  // normalization = BATCH_SIZE to be consistent with previous behavior.</div><div class="line">  // 已废弃。Loss会除以参与计算的样本总数；否则Loss等于直接求和</div><div class="line">  optional bool normalize = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Messages that store parameters used by individual layer types follow, in</div><div class="line">// alphabetical order.</div><div class="line">// accuracy层参数</div><div class="line">message AccuracyParameter &#123;</div><div class="line">  // When computing accuracy, count as correct by comparing the true label to</div><div class="line">  // the top k scoring classes.  By default, only compare to the top scoring</div><div class="line">  // class (i.e. argmax).</div><div class="line">  // 计算前top-k的准确率，默认计算top-1准确率</div><div class="line">  optional uint32 top_k = 1 [default = 1];</div><div class="line"></div><div class="line">  // The &quot;label&quot; axis of the prediction blob, whose argmax corresponds to the</div><div class="line">  // predicted label -- may be negative to index from the end (e.g., -1 for the</div><div class="line">  // last axis).  For example, if axis == 1 and the predictions are</div><div class="line">  // (N x C x H x W), the label blob is expected to contain N*H*W ground truth</div><div class="line">  // labels with integer values in &#123;0, 1, ..., C-1&#125;.</div><div class="line">  // 指定在哪个维度上计算label</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line"></div><div class="line">  // If specified, ignore instances with the given label.</div><div class="line">  // 如果指定，则忽略给定标签的实例</div><div class="line">  optional int32 ignore_label = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// 标签最大化参数，标签最大化即确定概率最大的label</div><div class="line">message ArgMaxParameter &#123;</div><div class="line">  // If true produce pairs (argmax, maxval)</div><div class="line">  // 如果为真，则生成(argmax, maxval)</div><div class="line">  optional bool out_max_val = 1 [default = false];</div><div class="line">  // 类别的top-k</div><div class="line">  optional uint32 top_k = 2 [default = 1];</div><div class="line">  // The axis along which to maximise -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).</div><div class="line">  // By default ArgMaxLayer maximizes over the flattened trailing dimensions</div><div class="line">  // for each index of the first / num dimension.</div><div class="line">  // 根据axis进行标签最大化</div><div class="line">  optional int32 axis = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 参数拼接，在deconv的prototxt文件中见过</div><div class="line">message ConcatParameter &#123;</div><div class="line">  // The axis along which to concatenate -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).  Other axes must have the</div><div class="line">  // same dimension for all the bottom blobs.</div><div class="line">  // By default, ConcatLayer concatenates blobs along the &quot;channels&quot; axis (1).</div><div class="line">  // 参数拼接时的维度，按axis进行拼接</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line"></div><div class="line">  // DEPRECATED: alias for &quot;axis&quot; -- does not support negative indexing.</div><div class="line">  // 已废弃。与axis一样。</div><div class="line">  optional uint32 concat_dim = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// batch norm层的相关参数, batch norm layer通常配与scale layer一起使用，具体用法可参考Resnet结构</div><div class="line">message BatchNormParameter &#123;</div><div class="line">  // If false, accumulate global mean/variance values via a moving average. If</div><div class="line">  // true, use those accumulated values instead of computing mean/variance</div><div class="line">  // across the batch.</div><div class="line">  // 如果设为false，累计全部的mean/variance，如果为true，使用累计值代替batch上mean/variance的计算</div><div class="line">  // true是使用了caffe内部的均值和方差，false是使用了每个Batch里的数据的均值和方差</div><div class="line">  optional bool use_global_stats = 1;</div><div class="line">  // How much does the moving average decay each iteration?</div><div class="line">  // 每次迭代平均值衰减比例</div><div class="line">  optional float moving_average_fraction = 2 [default = .999];</div><div class="line">  // Small value to add to the variance estimate so that we don&apos;t divide by</div><div class="line">  // zero.</div><div class="line">  // variance估计时为了使除数不为0，需要加上eps</div><div class="line">  optional float eps = 3 [default = 1e-5];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// bias层参数，没找到实际的应用例子</div><div class="line">message BiasParameter &#123;</div><div class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</div><div class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  //</div><div class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</div><div class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</div><div class="line">  // following shapes (for the given value of axis):</div><div class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</div><div class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</div><div class="line">  //    (axis == 2 == -2)                   40;       40x60</div><div class="line">  //    (axis == 3 == -1)                                60</div><div class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</div><div class="line">  // &quot;axis&quot;) -- a scalar bias.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // (num_axes is ignored unless just one bottom is given and the bias is</div><div class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</div><div class="line">  // number of axes by the second bottom.)</div><div class="line">  // The number of axes of the input (bottom[0]) covered by the bias</div><div class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</div><div class="line">  // Set num_axes := 0, to add a zero-axis Blob: a scalar.</div><div class="line">  optional int32 num_axes = 2 [default = 1];</div><div class="line"></div><div class="line">  // (filler is ignored unless just one bottom is given and the bias is</div><div class="line">  // a learned parameter of the layer.)</div><div class="line">  // The initialization for the learned bias parameter.</div><div class="line">  // Default is the zero (0) initialization, resulting in the BiasLayer</div><div class="line">  // initially performing the identity operation.</div><div class="line">  optional FillerParameter filler = 3;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 对比损失层，siamese network中使用了对比损失</div><div class="line">message ContrastiveLossParameter &#123;</div><div class="line">  // margin for dissimilar pair</div><div class="line">  // 不相似的样本对的距离保持在margin以上</div><div class="line">  optional float margin = 1 [default = 1.0];</div><div class="line">  // The first implementation of this cost did not exactly match the cost of</div><div class="line">  // Hadsell et al 2006 -- using (margin - d^2) instead of (margin - d)^2.</div><div class="line">  // legacy_version = false (the default) uses (margin - d)^2 as proposed in the</div><div class="line">  // Hadsell paper. New models should probably use this version.</div><div class="line">  // legacy_version = true uses (margin - d^2). This is kept to support /</div><div class="line">  // reproduce existing models and results</div><div class="line">  // 第一版对比损失没有完全按论文写，如果为false，则按照论文原来的公式计算</div><div class="line">  optional bool legacy_version = 2 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 卷积层参数</div><div class="line">message ConvolutionParameter &#123;</div><div class="line">  // 输出数据的个数</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // 是否有偏置项</div><div class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</div><div class="line"></div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in all spatial dimensions, or once per spatial dimension.</div><div class="line">  // 卷积padding的大小</div><div class="line">  repeated uint32 pad = 3; // The padding size; defaults to 0</div><div class="line">  // 卷积核的大小</div><div class="line">  repeated uint32 kernel_size = 4; // The kernel size</div><div class="line">  // 卷积的步长</div><div class="line">  repeated uint32 stride = 6; // The stride; defaults to 1</div><div class="line">  // Factor used to dilate the kernel, (implicitly) zero-filling the resulting</div><div class="line">  // holes. (Kernel dilation is sometimes referred to by its use in the</div><div class="line">  // algorithme à trous from Holschneider et al. 1987.)</div><div class="line">  // 卷积膨胀，在卷积的时候可以skip一定长度的像素</div><div class="line">  repeated uint32 dilation = 18; // The dilation; defaults to 1</div><div class="line"></div><div class="line">  // For 2D convolution only, the *_h and *_w versions may also be used to</div><div class="line">  // specify both spatial dimensions.</div><div class="line">  // padding, kernel, stride的宽度和高度</div><div class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height (2D only)</div><div class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width (2D only)</div><div class="line">  optional uint32 kernel_h = 11; // The kernel height (2D only)</div><div class="line">  optional uint32 kernel_w = 12; // The kernel width (2D only)</div><div class="line">  optional uint32 stride_h = 13; // The stride height (2D only)</div><div class="line">  optional uint32 stride_w = 14; // The stride width (2D only)</div><div class="line"></div><div class="line">  // 来自于AlexNet论文</div><div class="line">  optional uint32 group = 5 [default = 1]; // The group size for group conv</div><div class="line"></div><div class="line">  // 权重初始化</div><div class="line">  optional FillerParameter weight_filler = 7; // The filler for the weight</div><div class="line">  // 偏置初始化</div><div class="line">  optional FillerParameter bias_filler = 8; // The filler for the bias</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 卷积的方式的选择，default是正常的卷积，caffe是矩阵乘法的卷积，cudnn是cuda库流并行式的卷积</div><div class="line">  optional Engine engine = 15 [default = DEFAULT];</div><div class="line"></div><div class="line">  // The axis to interpret as &quot;channels&quot; when performing convolution.</div><div class="line">  // Preceding dimensions are treated as independent inputs;</div><div class="line">  // succeeding dimensions are treated as &quot;spatial&quot;.</div><div class="line">  // With (N, C, H, W) inputs, and axis == 1 (the default), we perform</div><div class="line">  // N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for</div><div class="line">  // groups g&gt;1) filters across the spatial axes (H, W) of the input.</div><div class="line">  // With (N, C, D, H, W) inputs, and axis == 1, we perform</div><div class="line">  // N independent 3D convolutions, sliding (C/g)-channels</div><div class="line">  // filters across the spatial axes (D, H, W) of the input.</div><div class="line">  // 通道channel所在的维度</div><div class="line">  optional int32 axis = 16 [default = 1];</div><div class="line"></div><div class="line">  // Whether to force use of the general ND convolution, even if a specific</div><div class="line">  // implementation for blobs of the appropriate number of spatial dimensions</div><div class="line">  // is available. (Currently, there is only a 2D-specific convolution</div><div class="line">  // implementation; for input blobs with num_axes != 2, this option is</div><div class="line">  // ignored and the ND implementation will be used.)</div><div class="line">  // 如果输入数据维度等于2，则执行通用的ND卷积，否则正常执行卷积</div><div class="line">  optional bool force_nd_im2col = 17 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 图像裁剪参数</div><div class="line">message CropParameter &#123;</div><div class="line">  // To crop, elements of the first bottom are selected to fit the dimensions</div><div class="line">  // of the second, reference bottom. The crop is configured by</div><div class="line">  // - the crop `axis` to pick the dimensions for cropping</div><div class="line">  // - the crop `offset` to set the shift for all/each dimension</div><div class="line">  // to align the cropped bottom with the reference bottom.</div><div class="line">  // All dimensions up to but excluding `axis` are preserved, while</div><div class="line">  // the dimensions including and trailing `axis` are cropped.</div><div class="line">  // If only one `offset` is set, then all dimensions are offset by this amount.</div><div class="line">  // Otherwise, the number of offsets must equal the number of cropped axes to</div><div class="line">  // shift the crop in each dimension accordingly.</div><div class="line">  // Note: standard dimensions are N,C,H,W so the default is a spatial crop,</div><div class="line">  // and `axis` may be negative to index from the end (e.g., -1 for the last</div><div class="line">  // axis).</div><div class="line">  // axis是在哪个维度上进行裁剪，会裁剪轴2及之后的所有轴</div><div class="line">  optional int32 axis = 1 [default = 2];</div><div class="line">  // offset设置是每个维度进行裁剪时的偏移量</div><div class="line">  repeated uint32 offset = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 数据层参数</div><div class="line">message DataParameter &#123;</div><div class="line">  enum DB &#123;</div><div class="line">    LEVELDB = 0;</div><div class="line">    LMDB = 1;</div><div class="line">  &#125;</div><div class="line">  // Specify the data source.</div><div class="line">  // 设定数据源路径</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 指定一次处理的图片数量</div><div class="line">  optional uint32 batch_size = 4;</div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  // DEPRECATED. Each solver accesses a different subset of the database.</div><div class="line">  // rand_skip跳过指定的数据点，避免异步的sgd从同一个数据点开始</div><div class="line">  optional uint32 rand_skip = 7 [default = 0];</div><div class="line">  // 使用的数据库类型，LMDB or LEVELDB</div><div class="line">  optional DB backend = 8 [default = LEVELDB];</div><div class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</div><div class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</div><div class="line">  // mean subtraction is always carried out before scaling.</div><div class="line">  // 已废弃。图像归一化，在TransformationParameter中。</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 已废弃。均值文件，在TransformationParameter中。</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</div><div class="line">  // crop an image.</div><div class="line">  // 已废弃。图像裁剪，在TransformationParameter中。</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</div><div class="line">  // data.</div><div class="line">  // 已废弃。图像翻转，在TransformationParameter中。</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // Force the encoded image to have 3 color channels</div><div class="line">  // 强制图像数据有三个颜色通道</div><div class="line">  optional bool force_encoded_color = 9 [default = false];</div><div class="line">  // Prefetch queue (Number of batches to prefetch to host memory, increase if</div><div class="line">  // data access bandwidth varies).</div><div class="line">  // 预先拉取batch的数目</div><div class="line">  optional uint32 prefetch = 10 [default = 4];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// dropout层参数</div><div class="line">message DropoutParameter &#123;</div><div class="line">  // 为了避免过拟合，参数随机失活的比例</div><div class="line">  optional float dropout_ratio = 1 [default = 0.5]; // dropout ratio</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// DummyDataLayer fills any number of arbitrarily shaped blobs with random</div><div class="line">// (or constant) data generated by &quot;Fillers&quot; (see &quot;message FillerParameter&quot;).</div><div class="line">// DummyData层的参数</div><div class="line">message DummyDataParameter &#123;</div><div class="line">  // This layer produces N &gt;= 1 top blobs.  DummyDataParameter must specify 1 or N</div><div class="line">  // shape fields, and 0, 1 or N data_fillers.</div><div class="line">  // If 0 data_fillers are specified, ConstantFiller with a value of 0 is used.</div><div class="line">  // If 1 data_filler is specified, it is applied to all top blobs.  If N are</div><div class="line">  // specified, the ith is applied to the ith top blob.</div><div class="line">  // blob数据的生成方式</div><div class="line">  repeated FillerParameter data_filler = 1;</div><div class="line">  // 数据的维度</div><div class="line">  repeated BlobShape shape = 6;</div><div class="line"></div><div class="line">  // 4D dimensions -- deprecated.  Use &quot;shape&quot; instead.</div><div class="line">  // 已废弃。使用shape代替。</div><div class="line">  repeated uint32 num = 2;</div><div class="line">  repeated uint32 channels = 3;</div><div class="line">  repeated uint32 height = 4;</div><div class="line">  repeated uint32 width = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">//Eltwise层的参数</div><div class="line">message EltwiseParameter &#123;</div><div class="line">  // 操作的类型</div><div class="line">  enum EltwiseOp &#123;</div><div class="line">    PROD = 0;</div><div class="line">    SUM = 1;</div><div class="line">    MAX = 2;</div><div class="line">  &#125;</div><div class="line">  // 数据操作分三种：点乘，相加，取最大值</div><div class="line">  optional EltwiseOp operation = 1 [default = SUM]; // element-wise operation</div><div class="line">  // SUM操作时各个blob对应的系数</div><div class="line">  repeated float coeff = 2; // blob-wise coefficient for SUM operation</div><div class="line"></div><div class="line">  // Whether to use an asymptotically slower (for &gt;2 inputs) but stabler method</div><div class="line">  // of computing the gradient for the PROD operation. (No effect for SUM op.)</div><div class="line">  // 在进行PROD操作，即乘法时是否使用异步操作来计算梯度，更慢但更稳定。</div><div class="line">  optional bool stable_prod_grad = 3 [default = true];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ELULayer</div><div class="line">// ELU层的参数，具体看论文</div><div class="line">message ELUParameter &#123;</div><div class="line">  // Described in:</div><div class="line">  // Clevert, D.-A., Unterthiner, T., &amp; Hochreiter, S. (2015). Fast and Accurate</div><div class="line">  // Deep Network Learning by Exponential Linear Units (ELUs). arXiv</div><div class="line">  optional float alpha = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by EmbedLayer</div><div class="line">// Embed层的参数，主要用于LSTM等翻译网络</div><div class="line">message EmbedParameter &#123;</div><div class="line">  // Embed层的输出</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // The input is given as integers to be interpreted as one-hot</div><div class="line">  // vector indices with dimension num_input.  Hence num_input should be</div><div class="line">  // 1 greater than the maximum possible input value.</div><div class="line">  // Embed层的输入</div><div class="line">  optional uint32 input_dim = 2;</div><div class="line">  // 是否使用偏置项</div><div class="line">  optional bool bias_term = 3 [default = true]; // Whether to use a bias term</div><div class="line">  // 权重生成</div><div class="line">  optional FillerParameter weight_filler = 4; // The filler for the weight</div><div class="line">  // 偏置生成</div><div class="line">  optional FillerParameter bias_filler = 5; // The filler for the bias</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ExpLayer</div><div class="line">// Exp层的参数，即指数层参数</div><div class="line">message ExpParameter &#123;</div><div class="line">  // ExpLayer computes outputs y = base ^ (shift + scale * x), for base &gt; 0.</div><div class="line">  // Or if base is set to the default (-1), base is set to e,</div><div class="line">  // so y = exp(shift + scale * x).</div><div class="line">  // 指数层的计算是y = base ^ (shift + scale * x)，下面分别是公式中的三个参数</div><div class="line">  optional float base = 1 [default = -1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Message that stores parameters used by FlattenLayer</div><div class="line">// Flatten层的参数，主要是按某个轴展开（平铺），mnist demo的mnist_autoencode就使用了Flatten层</div><div class="line">message FlattenParameter &#123;</div><div class="line">  // The first axis to flatten: all preceding axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</div><div class="line">  // 从哪一层开始展开</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // The last axis to flatten: all following axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., the default -1 for the last</div><div class="line">  // axis).</div><div class="line">  // 展开到哪一层结束</div><div class="line">  optional int32 end_axis = 2 [default = -1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by HDF5DataLayer</div><div class="line">// HDF5数据层的参数</div><div class="line">message HDF5DataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // HDF5层输入数据的数据源</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 2;</div><div class="line"></div><div class="line">  // Specify whether to shuffle the data.</div><div class="line">  // If shuffle == true, the ordering of the HDF5 files is shuffled,</div><div class="line">  // and the ordering of data within any given HDF5 file is shuffled,</div><div class="line">  // but data between different files are not interleaved; all of a file&apos;s</div><div class="line">  // data are output (in a random order) before moving onto another file.</div><div class="line">  // 是否对HDF5的输入数据进行shuffle</div><div class="line">  optional bool shuffle = 3 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// HDF5输出层参数</div><div class="line">message HDF5OutputParameter &#123;</div><div class="line">  // 输出的HDF5文件的文件名</div><div class="line">  optional string file_name = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// HingeLoss层参数</div><div class="line">message HingeLossParameter &#123;</div><div class="line">  enum Norm &#123;</div><div class="line">    L1 = 1;</div><div class="line">    L2 = 2;</div><div class="line">  &#125;</div><div class="line">  // Specify the Norm to use L1 or L2</div><div class="line">  // 指定HingeLoss的类型</div><div class="line">  optional Norm norm = 1 [default = L1];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// ImageData层参数，网络中直接输入原图</div><div class="line">message ImageDataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // 描述图像路径及标签的文件</div><div class="line">  optional string source = 1;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch size</div><div class="line">  optional uint32 batch_size = 4 [default = 1];</div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  // rand_skip跳过指定的数据点，避免异步的sgd从同一个数据点开始，与Data层中是一样的</div><div class="line">  optional uint32 rand_skip = 7 [default = 0];</div><div class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</div><div class="line">  // 是否对图像顺序进行shuffle</div><div class="line">  optional bool shuffle = 8 [default = false];</div><div class="line">  // It will also resize images if new_height or new_width are not zero.</div><div class="line">  // 图像resize的高度</div><div class="line">  optional uint32 new_height = 9 [default = 0];</div><div class="line">  // 图像resize的宽度</div><div class="line">  optional uint32 new_width = 10 [default = 0];</div><div class="line">  // Specify if the images are color or gray</div><div class="line">  // 指定图像彩色图像还是灰度图像，默认彩色</div><div class="line">  optional bool is_color = 11 [default = true];</div><div class="line">  // DEPRECATED. See TransformationParameter. For data pre-processing, we can do</div><div class="line">  // simple scaling and subtracting the data mean, if provided. Note that the</div><div class="line">  // mean subtraction is always carried out before scaling.</div><div class="line">  // 已废弃。参考TransformationParameter中的scale</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 指定均值文件</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we would like to randomly</div><div class="line">  // crop an image.</div><div class="line">  // 已废弃。参考TransformationParameter中的crop_size</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // DEPRECATED. See TransformationParameter. Specify if we want to randomly mirror</div><div class="line">  // data.</div><div class="line">  // 已废弃，参考TransformationParameter的mirror。</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // 不太清楚root_folder具体是什么</div><div class="line">  optional string root_folder = 12 [default = &quot;&quot;];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 信息增益损失层参数</div><div class="line">message InfogainLossParameter &#123;</div><div class="line">  // Specify the infogain matrix source.</div><div class="line">  // 指定存储信息增益矩阵的源文件</div><div class="line">  optional string source = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// InnerProduct层的参数</div><div class="line">message InnerProductParameter &#123;</div><div class="line">  // InnerProduct层的输出</div><div class="line">  optional uint32 num_output = 1; // The number of outputs for the layer</div><div class="line">  // 是否有偏置项</div><div class="line">  optional bool bias_term = 2 [default = true]; // whether to have bias terms</div><div class="line">  // 权重初始化，随机生成</div><div class="line">  optional FillerParameter weight_filler = 3; // The filler for the weight</div><div class="line">  // 偏置初始化，随机生成</div><div class="line">  optional FillerParameter bias_filler = 4; // The filler for the bias</div><div class="line"></div><div class="line">  // The first axis to be lumped into a single inner product computation;</div><div class="line">  // all preceding axes are retained in the output.</div><div class="line">  // May be negative to index from the end (e.g., -1 for the last axis).</div><div class="line">  // 从某一维度开始进行内积计算，前面的维度保留</div><div class="line">  optional int32 axis = 5 [default = 1];</div><div class="line">  // Specify whether to transpose the weight matrix or not.</div><div class="line">  // If transpose == true, any operations will be performed on the transpose</div><div class="line">  // of the weight matrix. The weight matrix itself is not going to be transposed</div><div class="line">  // but rather the transfer flag of operations will be toggled accordingly.</div><div class="line">  // 是否对权重矩阵进行转置</div><div class="line">  optional bool transpose = 6 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Input参数，caffe网络部署时会用到</div><div class="line">message InputParameter &#123;</div><div class="line">  // This layer produces N &gt;= 1 top blob(s) to be assigned manually.</div><div class="line">  // Define N shapes to set a shape for each top.</div><div class="line">  // Define 1 shape to set the same shape for every top.</div><div class="line">  // Define no shape to defer to reshaping manually.</div><div class="line">  // 输入数据的shape</div><div class="line">  repeated BlobShape shape = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by LogLayer</div><div class="line">// Log层参数，对数据进行Log运算</div><div class="line">message LogParameter &#123;</div><div class="line">  // LogLayer computes outputs y = log_base(shift + scale * x), for base &gt; 0.</div><div class="line">  // Or if base is set to the default (-1), base is set to e,</div><div class="line">  // so y = ln(shift + scale * x) = log_e(shift + scale * x)</div><div class="line">  // Log层计算公式为y = log_base(shift + scale * x)，下面分别是公式中的三个参数</div><div class="line">  optional float base = 1 [default = -1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by LRNLayer</div><div class="line">// LRN层的参数，局部归一化，AlexNet中的LRN</div><div class="line">message LRNParameter &#123;</div><div class="line">  // 如果是跨通道LRN，则表示求和的通道数；如果是在通道内LRN，则表示求和的正方形区域长度。</div><div class="line">  optional uint32 local_size = 1 [default = 5];</div><div class="line">  // 归一化公式中的参数</div><div class="line">  optional float alpha = 2 [default = 1.];</div><div class="line">  optional float beta = 3 [default = 0.75];</div><div class="line">  enum NormRegion &#123;</div><div class="line">    ACROSS_CHANNELS = 0;</div><div class="line">    WITHIN_CHANNEL = 1;</div><div class="line">  &#125;</div><div class="line">  // 归一化的区域，分为通道内和跨通道两种</div><div class="line">  optional NormRegion norm_region = 4 [default = ACROSS_CHANNELS];</div><div class="line">  optional float k = 5 [default = 1.];</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 与前面的engine是一样的</div><div class="line">  optional Engine engine = 6 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 内存数据层参数</div><div class="line">message MemoryDataParameter &#123;</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 1;</div><div class="line">  // 图像通道数</div><div class="line">  optional uint32 channels = 2;</div><div class="line">  // 图像高度</div><div class="line">  optional uint32 height = 3;</div><div class="line">  // 图像宽度</div><div class="line">  optional uint32 width = 4;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// mean-variance normalization层参数</div><div class="line">message MVNParameter &#123;</div><div class="line">  // This parameter can be set to false to normalize mean only</div><div class="line">  // 是否对方差进行归一化</div><div class="line">  optional bool normalize_variance = 1 [default = true];</div><div class="line"></div><div class="line">  // This parameter can be set to true to perform DNN-like MVN</div><div class="line">  // 是否进行跨通道的MVN</div><div class="line">  optional bool across_channels = 2 [default = false];</div><div class="line"></div><div class="line">  // Epsilon for not dividing by zero while normalizing variance</div><div class="line">  // 避免除数为0，与前面的一样</div><div class="line">  optional float eps = 3 [default = 1e-9];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 参数层参数</div><div class="line">message ParameterParameter &#123;</div><div class="line">  // 用户自己定义的shape</div><div class="line">  optional BlobShape shape = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 池化层参数</div><div class="line">message PoolingParameter &#123;</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  // 池化的方式</div><div class="line">  optional PoolMethod pool = 1 [default = MAX]; // The pooling method</div><div class="line">  // Pad, kernel size, and stride are all given as a single value for equal</div><div class="line">  // dimensions in height and width or as Y, X pairs.</div><div class="line">  // padding的大小</div><div class="line">  optional uint32 pad = 4 [default = 0]; // The padding size (equal in Y, X)</div><div class="line">  // padding的高度</div><div class="line">  optional uint32 pad_h = 9 [default = 0]; // The padding height</div><div class="line">  // padding的宽度</div><div class="line">  optional uint32 pad_w = 10 [default = 0]; // The padding width</div><div class="line">  // 池化的核大小</div><div class="line">  optional uint32 kernel_size = 2; // The kernel size (square)</div><div class="line">  // 核高度</div><div class="line">  optional uint32 kernel_h = 5; // The kernel height</div><div class="line">  // 核宽度</div><div class="line">  optional uint32 kernel_w = 6; // The kernel width</div><div class="line">  // 池化的步长</div><div class="line">  optional uint32 stride = 3 [default = 1]; // The stride (equal in Y, X)</div><div class="line">  // 步长的高度</div><div class="line">  optional uint32 stride_h = 7; // The stride height</div><div class="line">  // 步长的宽度</div><div class="line">  optional uint32 stride_w = 8; // The stride width</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行池化操作的类型，与前面的一样</div><div class="line">  optional Engine engine = 11 [default = DEFAULT];</div><div class="line">  // If global_pooling then it will pool over the size of the bottom by doing</div><div class="line">  // kernel_h = bottom-&gt;height and kernel_w = bottom-&gt;width</div><div class="line">  // global_pooling是对多个通道进行pooling，例如从三通道pooling为单通道</div><div class="line">  optional bool global_pooling = 12 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Power层参数</div><div class="line">message PowerParameter &#123;</div><div class="line">  // PowerLayer computes outputs y = (shift + scale * x) ^ power.</div><div class="line">  // Power的计算公式为y = (shift + scale * x) ^ power，下面是公式中的参数</div><div class="line">  optional float power = 1 [default = 1.0];</div><div class="line">  optional float scale = 2 [default = 1.0];</div><div class="line">  optional float shift = 3 [default = 0.0];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// python layer参数，在faster rcnn中有应用</div><div class="line">message PythonParameter &#123;</div><div class="line">  // python模块名称</div><div class="line">  optional string module = 1;</div><div class="line">  // python模块中层的名字，即类名</div><div class="line">  optional string layer = 2;</div><div class="line">  // This value is set to the attribute `param_str` of the `PythonLayer` object</div><div class="line">  // in Python before calling the `setup()` method. This could be a number,</div><div class="line">  // string, dictionary in Python dict format, JSON, etc. You may parse this</div><div class="line">  // string in `setup` method and use it in `forward` and `backward`.</div><div class="line">  // 可以用来设置参数，key-value形式，可以参考faster rcnn中模型的train.prototxt</div><div class="line">  optional string param_str = 3 [default = &apos;&apos;];</div><div class="line">  // Whether this PythonLayer is shared among worker solvers during data parallelism.</div><div class="line">  // If true, each worker solver sequentially run forward from this layer.</div><div class="line">  // This value should be set true if you are using it as a data layer.</div><div class="line">  // 是否需要在并行时共享layer</div><div class="line">  optional bool share_in_parallel = 4 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">// Message that stores parameters used by RecurrentLayer</div><div class="line">// Recurrent层参数</div><div class="line">message RecurrentParameter &#123;</div><div class="line">  // The dimension of the output (and usually hidden state) representation --</div><div class="line">  // must be explicitly set to non-zero.</div><div class="line">  // Recurrent层的输出——必须非零</div><div class="line">  optional uint32 num_output = 1 [default = 0];</div><div class="line">  // 权重初始化，随机生成初始化</div><div class="line">  optional FillerParameter weight_filler = 2; // The filler for the weight</div><div class="line">  // 偏置初始化，随机生成</div><div class="line">  optional FillerParameter bias_filler = 3; // The filler for the bias</div><div class="line"></div><div class="line">  // Whether to enable displaying debug_info in the unrolled recurrent net.</div><div class="line">  // 是否输出调试信息</div><div class="line">  optional bool debug_info = 4 [default = false];</div><div class="line"></div><div class="line">  // Whether to add as additional inputs (bottoms) the initial hidden state</div><div class="line">  // blobs, and add as additional outputs (tops) the final timestep hidden state</div><div class="line">  // blobs.  The number of additional bottom/top blobs required depends on the</div><div class="line">  // recurrent architecture -- e.g., 1 for RNNs, 2 for LSTMs.</div><div class="line">  // 是否添加额外的输入</div><div class="line">  optional bool expose_hidden = 5 [default = false];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ReductionLayer</div><div class="line">// Reduction层参数</div><div class="line">message ReductionParameter &#123;</div><div class="line">  enum ReductionOp &#123;</div><div class="line">    SUM = 1;</div><div class="line">    ASUM = 2;</div><div class="line">    SUMSQ = 3;</div><div class="line">    MEAN = 4;</div><div class="line">  &#125;</div><div class="line">  // 通过reduction操作来将数据减少到一维，可以通过上面的四种方式</div><div class="line">  optional ReductionOp operation = 1 [default = SUM]; // reduction operation</div><div class="line"></div><div class="line">  // The first axis to reduce to a scalar -- may be negative to index from the</div><div class="line">  // end (e.g., -1 for the last axis).</div><div class="line">  // (Currently, only reduction along ALL &quot;tail&quot; axes is supported; reduction</div><div class="line">  // of axis M through N, where N &lt; num_axes - 1, is unsupported.)</div><div class="line">  // Suppose we have an n-axis bottom Blob with shape:</div><div class="line">  //     (d0, d1, d2, ..., d(m-1), dm, d(m+1), ..., d(n-1)).</div><div class="line">  // If axis == m, the output Blob will have shape</div><div class="line">  //     (d0, d1, d2, ..., d(m-1)),</div><div class="line">  // and the ReductionOp operation is performed (d0 * d1 * d2 * ... * d(m-1))</div><div class="line">  // times, each including (dm * d(m+1) * ... * d(n-1)) individual data.</div><div class="line">  // If axis == 0 (the default), the output Blob always has the empty shape</div><div class="line">  // (count 1), performing reduction across the entire input --</div><div class="line">  // often useful for creating new loss functions.</div><div class="line">  // 在哪个轴上执行reduction操作</div><div class="line">  optional int32 axis = 2 [default = 0];</div><div class="line">  // 输出系数</div><div class="line">  optional float coeff = 3 [default = 1.0]; // coefficient for output</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ReLULayer</div><div class="line">// ReLU层参数</div><div class="line">message ReLUParameter &#123;</div><div class="line">  // Allow non-zero slope for negative inputs to speed up optimization</div><div class="line">  // Described in:</div><div class="line">  // Maas, A. L., Hannun, A. Y., &amp; Ng, A. Y. (2013). Rectifier nonlinearities</div><div class="line">  // improve neural network acoustic models. In ICML Workshop on Deep Learning</div><div class="line">  // for Audio, Speech, and Language Processing.</div><div class="line">  // ReLUU操作的阈值</div><div class="line">  optional float negative_slope = 1 [default = 0];</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行ReLU操作的类型，与前面的一样</div><div class="line">  optional Engine engine = 2 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Reshape层参数，与numpy中的Reshape作用是一样的</div><div class="line">message ReshapeParameter &#123;</div><div class="line">  // Specify the output dimensions. If some of the dimensions are set to 0,</div><div class="line">  // the corresponding dimension from the bottom layer is used (unchanged).</div><div class="line">  // Exactly one dimension may be set to -1, in which case its value is</div><div class="line">  // inferred from the count of the bottom blob and the remaining dimensions.</div><div class="line">  // For example, suppose we want to reshape a 2D blob &quot;input&quot; with shape 2 x 8:</div><div class="line">  //</div><div class="line">  //   layer &#123;</div><div class="line">  //     type: &quot;Reshape&quot; bottom: &quot;input&quot; top: &quot;output&quot;</div><div class="line">  //     reshape_param &#123; ... &#125;</div><div class="line">  //   &#125;</div><div class="line">  //</div><div class="line">  // If &quot;input&quot; is 2D with shape 2 x 8, then the following reshape_param</div><div class="line">  // specifications are all equivalent, producing a 3D blob &quot;output&quot; with shape</div><div class="line">  // 2 x 2 x 4:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  2  dim: 2  dim:  4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim:  4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim: 2  dim: -1 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  0  dim:-1  dim:  4 &#125; &#125;</div><div class="line">  // reshape之后输出的维度</div><div class="line">  optional BlobShape shape = 1;</div><div class="line"></div><div class="line">  // axis and num_axes control the portion of the bottom blob&apos;s shape that are</div><div class="line">  // replaced by (included in) the reshape. By default (axis == 0 and</div><div class="line">  // num_axes == -1), the entire bottom blob shape is included in the reshape,</div><div class="line">  // and hence the shape field must specify the entire output shape.</div><div class="line">  //</div><div class="line">  // axis may be non-zero to retain some portion of the beginning of the input</div><div class="line">  // shape (and may be negative to index from the end; e.g., -1 to begin the</div><div class="line">  // reshape after the last axis, including nothing in the reshape,</div><div class="line">  // -2 to include only the last axis, etc.).</div><div class="line">  //</div><div class="line">  // For example, suppose &quot;input&quot; is a 2D blob with shape 2 x 8.</div><div class="line">  // Then the following ReshapeLayer specifications are all equivalent,</div><div class="line">  // producing a blob &quot;output&quot; with shape 2 x 2 x 4:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 2  dim: 4 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis:  1 &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 4 &#125; axis: -3 &#125;</div><div class="line">  //</div><div class="line">  // num_axes specifies the extent of the reshape.</div><div class="line">  // If num_axes &gt;= 0 (and axis &gt;= 0), the reshape will be performed only on</div><div class="line">  // input axes in the range [axis, axis+num_axes].</div><div class="line">  // num_axes may also be -1, the default, to include all remaining axes</div><div class="line">  // (starting from axis).</div><div class="line">  //</div><div class="line">  // For example, suppose &quot;input&quot; is a 2D blob with shape 2 x 8.</div><div class="line">  // Then the following ReshapeLayer specifications are equivalent,</div><div class="line">  // producing a blob &quot;output&quot; with shape 1 x 2 x 8.</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  dim:  8 &#125; &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  dim: 2  &#125;  num_axes: 1 &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim:  1  &#125;  num_axes: 0 &#125;</div><div class="line">  //</div><div class="line">  // On the other hand, these would produce output blob shape 2 x 1 x 8:</div><div class="line">  //</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 2  dim: 1  dim: 8  &#125;  &#125;</div><div class="line">  //   reshape_param &#123; shape &#123; dim: 1 &#125;  axis: 1  num_axes: 0 &#125;</div><div class="line"></div><div class="line">  optional int32 axis = 2 [default = 0];</div><div class="line">  optional int32 num_axes = 3 [default = -1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Scale层参数，与batch norm layer配合使用，可参考Resnet结构</div><div class="line">message ScaleParameter &#123;</div><div class="line">  // The first axis of bottom[0] (the first input Blob) along which to apply</div><div class="line">  // bottom[1] (the second input Blob).  May be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  //</div><div class="line">  // For example, if bottom[0] is 4D with shape 100x3x40x60, the output</div><div class="line">  // top[0] will have the same shape, and bottom[1] may have any of the</div><div class="line">  // following shapes (for the given value of axis):</div><div class="line">  //    (axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60</div><div class="line">  //    (axis == 1 == -3)          3;     3x40;     3x40x60</div><div class="line">  //    (axis == 2 == -2)                   40;       40x60</div><div class="line">  //    (axis == 3 == -1)                                60</div><div class="line">  // Furthermore, bottom[1] may have the empty shape (regardless of the value of</div><div class="line">  // &quot;axis&quot;) -- a scalar multiplier.</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // (num_axes is ignored unless just one bottom is given and the scale is</div><div class="line">  // a learned parameter of the layer.  Otherwise, num_axes is determined by the</div><div class="line">  // number of axes by the second bottom.)</div><div class="line">  // The number of axes of the input (bottom[0]) covered by the scale</div><div class="line">  // parameter, or -1 to cover all axes of bottom[0] starting from `axis`.</div><div class="line">  // Set num_axes := 0, to multiply with a zero-axis Blob: a scalar.</div><div class="line">  optional int32 num_axes = 2 [default = 1];</div><div class="line"></div><div class="line">  // (filler is ignored unless just one bottom is given and the scale is</div><div class="line">  // a learned parameter of the layer.)</div><div class="line">  // The initialization for the learned scale parameter.</div><div class="line">  // Default is the unit (1) initialization, resulting in the ScaleLayer</div><div class="line">  // initially performing the identity operation.</div><div class="line">  optional FillerParameter filler = 3;</div><div class="line"></div><div class="line">  // Whether to also learn a bias (equivalent to a ScaleLayer+BiasLayer, but</div><div class="line">  // may be more efficient).  Initialized with bias_filler (defaults to 0).</div><div class="line">  // 是否使用偏置项</div><div class="line">  optional bool bias_term = 4 [default = false];</div><div class="line">  // 偏置项初始化</div><div class="line">  optional FillerParameter bias_filler = 5;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Sigmoid层参数</div><div class="line">message SigmoidParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 使用哪种sigmoid实现</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Slice层参数</div><div class="line">message SliceParameter &#123;</div><div class="line">  // The axis along which to slice -- may be negative to index from the end</div><div class="line">  // (e.g., -1 for the last axis).</div><div class="line">  // By default, SliceLayer concatenates blobs along the &quot;channels&quot; axis (1).</div><div class="line">  // 在哪个维度上进行拆分</div><div class="line">  optional int32 axis = 3 [default = 1];</div><div class="line">  // 指定拆分点</div><div class="line">  repeated uint32 slice_point = 2;</div><div class="line"></div><div class="line">  // DEPRECATED: alias for &quot;axis&quot; -- does not support negative indexing.</div><div class="line">  // 已废弃。</div><div class="line">  optional uint32 slice_dim = 1 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by SoftmaxLayer, SoftmaxWithLossLayer</div><div class="line">// Softmax层参数</div><div class="line">message SoftmaxParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 使用哪种softmax实现</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line"></div><div class="line">  // The axis along which to perform the softmax -- may be negative to index</div><div class="line">  // from the end (e.g., -1 for the last axis).</div><div class="line">  // Any other axes will be evaluated as independent softmaxes.</div><div class="line">  // 在哪个维度上进行softmax</div><div class="line">  optional int32 axis = 2 [default = 1];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// TanH层参数</div><div class="line">message TanHParameter &#123;</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行tanh激活函数的类型</div><div class="line">  optional Engine engine = 1 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by TileLayer</div><div class="line">// Tile层参数，扩大某一维度</div><div class="line">message TileParameter &#123;</div><div class="line">  // The index of the axis to tile.</div><div class="line">  // 扩大哪个维度</div><div class="line">  optional int32 axis = 1 [default = 1];</div><div class="line"></div><div class="line">  // The number of copies (tiles) of the blob to output.</div><div class="line">  // 创建多少个副本</div><div class="line">  optional int32 tiles = 2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Message that stores parameters used by ThresholdLayer</div><div class="line">// Threshold层参数，主要用来测试输入是否超过阈值</div><div class="line">message ThresholdParameter &#123;</div><div class="line">  // 设置阈值</div><div class="line">  optional float threshold = 1 [default = 0]; // Strictly positive values</div><div class="line">&#125;</div><div class="line"></div><div class="line">// WindowData层参数</div><div class="line">message WindowDataParameter &#123;</div><div class="line">  // Specify the data source.</div><div class="line">  // 指定数据源</div><div class="line">  optional string source = 1;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  // 是否归一化</div><div class="line">  optional float scale = 2 [default = 1];</div><div class="line">  // 图像均值文件</div><div class="line">  optional string mean_file = 3;</div><div class="line">  // Specify the batch size.</div><div class="line">  // 训练的batch_size</div><div class="line">  optional uint32 batch_size = 4;</div><div class="line">  // Specify if we would like to randomly crop an image.</div><div class="line">  // 是否随机crop</div><div class="line">  optional uint32 crop_size = 5 [default = 0];</div><div class="line">  // Specify if we want to randomly mirror data.</div><div class="line">  // 是否随机mirror</div><div class="line">  optional bool mirror = 6 [default = false];</div><div class="line">  // Foreground (object) overlap threshold</div><div class="line">  // 前景重叠阈值</div><div class="line">  optional float fg_threshold = 7 [default = 0.5];</div><div class="line">  // Background (non-object) overlap threshold</div><div class="line">  // 背景重叠阈值</div><div class="line">  optional float bg_threshold = 8 [default = 0.5];</div><div class="line">  // Fraction of batch that should be foreground objects</div><div class="line">  // 前景比例</div><div class="line">  optional float fg_fraction = 9 [default = 0.25];</div><div class="line">  // Amount of contextual padding to add around a window</div><div class="line">  // (used only by the window_data_layer)</div><div class="line">  // 是否padding</div><div class="line">  optional uint32 context_pad = 10 [default = 0];</div><div class="line">  // Mode for cropping out a detection window</div><div class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</div><div class="line">  // square: the tightest square around the window is cropped</div><div class="line">  // crop的方式</div><div class="line">  optional string crop_mode = 11 [default = &quot;warp&quot;];</div><div class="line">  // cache_images: will load all images in memory for faster access</div><div class="line">  // 是否缓存图像，即将图像都转入内存</div><div class="line">  optional bool cache_images = 12 [default = false];</div><div class="line">  // append root_folder to locate images</div><div class="line">  // 图像文件的根目录</div><div class="line">  optional string root_folder = 13 [default = &quot;&quot;];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// SPP层参数，SPP是spatial pyramid pooling，空间金字塔池化，具体可参考何凯明论文Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</div><div class="line">message SPPParameter &#123;</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  // 空间金字塔高度</div><div class="line">  optional uint32 pyramid_height = 1;</div><div class="line">  // 池化方法</div><div class="line">  optional PoolMethod pool = 2 [default = MAX]; // The pooling method</div><div class="line">  enum Engine &#123;</div><div class="line">    DEFAULT = 0;</div><div class="line">    CAFFE = 1;</div><div class="line">    CUDNN = 2;</div><div class="line">  &#125;</div><div class="line">  // 执行SPP的方式</div><div class="line">  optional Engine engine = 6 [default = DEFAULT];</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DEPRECATED: use LayerParameter.</div><div class="line">// 已废弃，使用LayerParameter。</div><div class="line">message V1LayerParameter &#123;</div><div class="line">  repeated string bottom = 2;</div><div class="line">  repeated string top = 3;</div><div class="line">  optional string name = 4;</div><div class="line">  repeated NetStateRule include = 32;</div><div class="line">  repeated NetStateRule exclude = 33;</div><div class="line">  enum LayerType &#123;</div><div class="line">    NONE = 0;</div><div class="line">    ABSVAL = 35;</div><div class="line">    ACCURACY = 1;</div><div class="line">    ARGMAX = 30;</div><div class="line">    BNLL = 2;</div><div class="line">    CONCAT = 3;</div><div class="line">    CONTRASTIVE_LOSS = 37;</div><div class="line">    CONVOLUTION = 4;</div><div class="line">    DATA = 5;</div><div class="line">    DECONVOLUTION = 39;</div><div class="line">    DROPOUT = 6;</div><div class="line">    DUMMY_DATA = 32;</div><div class="line">    EUCLIDEAN_LOSS = 7;</div><div class="line">    ELTWISE = 25;</div><div class="line">    EXP = 38;</div><div class="line">    FLATTEN = 8;</div><div class="line">    HDF5_DATA = 9;</div><div class="line">    HDF5_OUTPUT = 10;</div><div class="line">    HINGE_LOSS = 28;</div><div class="line">    IM2COL = 11;</div><div class="line">    IMAGE_DATA = 12;</div><div class="line">    INFOGAIN_LOSS = 13;</div><div class="line">    INNER_PRODUCT = 14;</div><div class="line">    LRN = 15;</div><div class="line">    MEMORY_DATA = 29;</div><div class="line">    MULTINOMIAL_LOGISTIC_LOSS = 16;</div><div class="line">    MVN = 34;</div><div class="line">    POOLING = 17;</div><div class="line">    POWER = 26;</div><div class="line">    RELU = 18;</div><div class="line">    SIGMOID = 19;</div><div class="line">    SIGMOID_CROSS_ENTROPY_LOSS = 27;</div><div class="line">    SILENCE = 36;</div><div class="line">    SOFTMAX = 20;</div><div class="line">    SOFTMAX_LOSS = 21;</div><div class="line">    SPLIT = 22;</div><div class="line">    SLICE = 33;</div><div class="line">    TANH = 23;</div><div class="line">    WINDOW_DATA = 24;</div><div class="line">    THRESHOLD = 31;</div><div class="line">  &#125;</div><div class="line">  optional LayerType type = 5;</div><div class="line">  repeated BlobProto blobs = 6;</div><div class="line">  repeated string param = 1001;</div><div class="line">  repeated DimCheckMode blob_share_mode = 1002;</div><div class="line">  enum DimCheckMode &#123;</div><div class="line">    STRICT = 0;</div><div class="line">    PERMISSIVE = 1;</div><div class="line">  &#125;</div><div class="line">  repeated float blobs_lr = 7;</div><div class="line">  repeated float weight_decay = 8;</div><div class="line">  repeated float loss_weight = 35;</div><div class="line">  optional AccuracyParameter accuracy_param = 27;</div><div class="line">  optional ArgMaxParameter argmax_param = 23;</div><div class="line">  optional ConcatParameter concat_param = 9;</div><div class="line">  optional ContrastiveLossParameter contrastive_loss_param = 40;</div><div class="line">  optional ConvolutionParameter convolution_param = 10;</div><div class="line">  optional DataParameter data_param = 11;</div><div class="line">  optional DropoutParameter dropout_param = 12;</div><div class="line">  optional DummyDataParameter dummy_data_param = 26;</div><div class="line">  optional EltwiseParameter eltwise_param = 24;</div><div class="line">  optional ExpParameter exp_param = 41;</div><div class="line">  optional HDF5DataParameter hdf5_data_param = 13;</div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 14;</div><div class="line">  optional HingeLossParameter hinge_loss_param = 29;</div><div class="line">  optional ImageDataParameter image_data_param = 15;</div><div class="line">  optional InfogainLossParameter infogain_loss_param = 16;</div><div class="line">  optional InnerProductParameter inner_product_param = 17;</div><div class="line">  optional LRNParameter lrn_param = 18;</div><div class="line">  optional MemoryDataParameter memory_data_param = 22;</div><div class="line">  optional MVNParameter mvn_param = 34;</div><div class="line">  optional PoolingParameter pooling_param = 19;</div><div class="line">  optional PowerParameter power_param = 21;</div><div class="line">  optional ReLUParameter relu_param = 30;</div><div class="line">  optional SigmoidParameter sigmoid_param = 38;</div><div class="line">  optional SoftmaxParameter softmax_param = 39;</div><div class="line">  optional SliceParameter slice_param = 31;</div><div class="line">  optional TanHParameter tanh_param = 37;</div><div class="line">  optional ThresholdParameter threshold_param = 25;</div><div class="line">  optional WindowDataParameter window_data_param = 20;</div><div class="line">  optional TransformationParameter transform_param = 36;</div><div class="line">  optional LossParameter loss_param = 42;</div><div class="line">  optional V0LayerParameter layer = 1;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// DEPRECATED: V0LayerParameter is the old way of specifying layer parameters</div><div class="line">// in Caffe.  We keep this message type around for legacy support.</div><div class="line">// 已废弃。</div><div class="line">message V0LayerParameter &#123;</div><div class="line">  optional string name = 1; // the layer name</div><div class="line">  optional string type = 2; // the string to specify the layer type</div><div class="line"></div><div class="line">  // Parameters to specify layers with inner products.</div><div class="line">  optional uint32 num_output = 3; // The number of outputs for the layer</div><div class="line">  optional bool biasterm = 4 [default = true]; // whether to have bias terms</div><div class="line">  optional FillerParameter weight_filler = 5; // The filler for the weight</div><div class="line">  optional FillerParameter bias_filler = 6; // The filler for the bias</div><div class="line"></div><div class="line">  optional uint32 pad = 7 [default = 0]; // The padding size</div><div class="line">  optional uint32 kernelsize = 8; // The kernel size</div><div class="line">  optional uint32 group = 9 [default = 1]; // The group size for group conv</div><div class="line">  optional uint32 stride = 10 [default = 1]; // The stride</div><div class="line">  enum PoolMethod &#123;</div><div class="line">    MAX = 0;</div><div class="line">    AVE = 1;</div><div class="line">    STOCHASTIC = 2;</div><div class="line">  &#125;</div><div class="line">  optional PoolMethod pool = 11 [default = MAX]; // The pooling method</div><div class="line">  optional float dropout_ratio = 12 [default = 0.5]; // dropout ratio</div><div class="line"></div><div class="line">  optional uint32 local_size = 13 [default = 5]; // for local response norm</div><div class="line">  optional float alpha = 14 [default = 1.]; // for local response norm</div><div class="line">  optional float beta = 15 [default = 0.75]; // for local response norm</div><div class="line">  optional float k = 22 [default = 1.];</div><div class="line"></div><div class="line">  // For data layers, specify the data source</div><div class="line">  optional string source = 16;</div><div class="line">  // For data pre-processing, we can do simple scaling and subtracting the</div><div class="line">  // data mean, if provided. Note that the mean subtraction is always carried</div><div class="line">  // out before scaling.</div><div class="line">  optional float scale = 17 [default = 1];</div><div class="line">  optional string meanfile = 18;</div><div class="line">  // For data layers, specify the batch size.</div><div class="line">  optional uint32 batchsize = 19;</div><div class="line">  // For data layers, specify if we would like to randomly crop an image.</div><div class="line">  optional uint32 cropsize = 20 [default = 0];</div><div class="line">  // For data layers, specify if we want to randomly mirror data.</div><div class="line">  optional bool mirror = 21 [default = false];</div><div class="line"></div><div class="line">  // The blobs containing the numeric parameters of the layer</div><div class="line">  repeated BlobProto blobs = 50;</div><div class="line">  // The ratio that is multiplied on the global learning rate. If you want to</div><div class="line">  // set the learning ratio for one blob, you need to set it for all blobs.</div><div class="line">  repeated float blobs_lr = 51;</div><div class="line">  // The weight decay that is multiplied on the global weight decay.</div><div class="line">  repeated float weight_decay = 52;</div><div class="line"></div><div class="line">  // The rand_skip variable is for the data layer to skip a few data points</div><div class="line">  // to avoid all asynchronous sgd clients to start at the same point. The skip</div><div class="line">  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not</div><div class="line">  // be larger than the number of keys in the database.</div><div class="line">  optional uint32 rand_skip = 53 [default = 0];</div><div class="line"></div><div class="line">  // Fields related to detection (det_*)</div><div class="line">  // foreground (object) overlap threshold</div><div class="line">  optional float det_fg_threshold = 54 [default = 0.5];</div><div class="line">  // background (non-object) overlap threshold</div><div class="line">  optional float det_bg_threshold = 55 [default = 0.5];</div><div class="line">  // Fraction of batch that should be foreground objects</div><div class="line">  optional float det_fg_fraction = 56 [default = 0.25];</div><div class="line"></div><div class="line">  // optional bool OBSOLETE_can_clobber = 57 [default = true];</div><div class="line"></div><div class="line">  // Amount of contextual padding to add around a window</div><div class="line">  // (used only by the window_data_layer)</div><div class="line">  optional uint32 det_context_pad = 58 [default = 0];</div><div class="line"></div><div class="line">  // Mode for cropping out a detection window</div><div class="line">  // warp: cropped window is warped to a fixed size and aspect ratio</div><div class="line">  // square: the tightest square around the window is cropped</div><div class="line">  optional string det_crop_mode = 59 [default = &quot;warp&quot;];</div><div class="line"></div><div class="line">  // For ReshapeLayer, one needs to specify the new dimensions.</div><div class="line">  optional int32 new_num = 60 [default = 0];</div><div class="line">  optional int32 new_channels = 61 [default = 0];</div><div class="line">  optional int32 new_height = 62 [default = 0];</div><div class="line">  optional int32 new_width = 63 [default = 0];</div><div class="line"></div><div class="line">  // Whether or not ImageLayer should shuffle the list of files at every epoch.</div><div class="line">  // It will also resize images if new_height or new_width are not zero.</div><div class="line">  optional bool shuffle_images = 64 [default = false];</div><div class="line"></div><div class="line">  // For ConcatLayer, one needs to specify the dimension for concatenation, and</div><div class="line">  // the other dimensions must be the same for all the bottom blobs.</div><div class="line">  // By default it will concatenate blobs along the channels dimension.</div><div class="line">  optional uint32 concat_dim = 65 [default = 1];</div><div class="line"></div><div class="line">  optional HDF5OutputParameter hdf5_output_param = 1001;</div><div class="line">&#125;</div><div class="line"></div><div class="line">// PReLU层参数，ReLU的进化版本</div><div class="line">message PReLUParameter &#123;</div><div class="line">  // Parametric ReLU described in K. He et al, Delving Deep into Rectifiers:</div><div class="line">  // Surpassing Human-Level Performance on ImageNet Classification, 2015.</div><div class="line"></div><div class="line">  // Initial value of a_i. Default is a_i=0.25 for all i.</div><div class="line">  // 参数初始化</div><div class="line">  optional FillerParameter filler = 1;</div><div class="line">  // Whether or not slope parameters are shared across channels.</div><div class="line">  // 是否在各通道共享参数</div><div class="line">  optional bool channel_shared = 2 [default = false];</div><div class="line">&#125;</div></pre></td></tr></table></figure>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>如果有收获，可以请我喝杯咖啡！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="http://ocs628urt.bkt.clouddn.com/weixin_pay_meitu_2.jpg" alt="Tyan WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="http://ocs628urt.bkt.clouddn.com/ali_pay_meitu_1.jpg" alt="Tyan Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Caffe/" rel="tag"># Caffe</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/07/04/2017-7-4-AlexNet论文翻译/" rel="next" title="AlexNet论文翻译——中英文对照">
                <i class="fa fa-chevron-left"></i> AlexNet论文翻译——中英文对照
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/13/2017-7-13-vim使用总结/" rel="prev" title="Vim使用总结">
                Vim使用总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Tyan" />
          <p class="site-author-name" itemprop="name">Tyan</p>
           
              <p class="site-description motion-element" itemprop="description">工作中的技术总结</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">344</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">40</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tyan</span>
</div>



        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
